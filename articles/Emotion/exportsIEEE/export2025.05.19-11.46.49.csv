"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"EEG-based Emotion Classification - A Theoretical Perusal of Deep Learning Methods","K. Sana Parveen; J. T. Panachakel; H. Ranjana; S. Sidharth; A. A. Samuel","Dept. of Electronics and Comm. Engg., College of Engineering, Trivandrum, India; Dept. of Electronics and Comm. Engineering, College of Engineering, Trivandrum, India; Dept. of Electronics and Comm. Engg., College of Engineering, Trivandrum, India; Dept. of Electronics and Comm. Engg., College of Engineering, Trivandrum, India; Dept. of Electronics and Comm. Engg., College of Engineering, Trivandrum, India",2023 2nd International Conference for Innovation in Technology (INOCON),"19 Apr 2023","2023","","","1","6","Human emotions are the basic element in the determination of a human’s cognitive abilities. These are the reaction of the human brain to internal or external events. Recognizing of different human emotions using a brain-machine interface is a crucial in understanding the emotional well-being of a patient who is completely paralyzed. One of the modalities for capturing neural activities in a brain-machine interface for recognizing human emotions is electroencephalogram (EEG). EEG-based systems for recognizing human emotions could offer a quick, simple, accessible, and user-friendly method for determining emotions. In the last few years emotion identification based on EEG has achieved significant attention. Many machine learning involved technologies have been developed for the classification of emotions of humans. Here we review the modern developments in the research area of emotion identification using deep learning methods, focusing on both feature extraction and classification methods. We also present the comparison of the accuracies obtained by different models.","","979-8-3503-2092-3","10.1109/INOCON57975.2023.10101002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10101002","Emotion Recognition;EEG;Emotion Classification;LSTM;CNN;Deep Learning","Deep learning;Learning systems;Emotion recognition;Technological innovation;Neural activity;Feature extraction;Brain modeling","","","","34","IEEE","19 Apr 2023","","","IEEE","IEEE Conferences"
"EEG feature extraction and classification using feed forward backpropogation algorithm for emotion detection","S. G. Mangalagowri; P. C. P. Raj","Dept. of ECE, M.S.Engg College, Bangalore; Dept. of ECE, M.S.Engg College, Bangalore","2016 International Conference on Electrical, Electronics, Communication, Computer and Optimization Techniques (ICEECCOT)","26 Jun 2017","2016","","","183","187","Electroencephalography is a clinical technique which reads the scalp electrical activity from brain structures. The electroencephalogram (EEG) records the scalp surface using metal electrodes and conductive media[6], and inhibits lots of information to different emotional states and the recorded data is very complex. In the present work EEG signal is used to detect emotions at different situations. The real emotions are detected which acts as a real indicator described by the human subject. In this paper, the EEG signals are analyzed for feature extraction using “db4” wavelet using multilevel decomposition. 10 real human subjects real EEG samples are collected using standard International 10-20 Electrode placement system which is placed over an entire scalp of human subject and it is decomposed into 5 different EEG bands using Discrete wavelet Transform (DWT). Emotion detection is the task of emotional state like recognizing a person's happiness, fear, anger, confusion or deceit across both voice and non voice channels. Emotion detection is based on a set of conventional features which are extracted like energy, Power spectral density, from the EEG signals for classifying emotions and to identify the intensity level of different bands of EEG signal. Feature extraction results are obtained by using “Bior 5.5” and “db4” wavelet for signal decomposition and to obtain the accurate frequency bands and feature Classification is performed by obtaining an accuracy of 75% for normal patient and 65% for abnormal patient.","","978-1-5090-4697-3","10.1109/ICEECCOT.2016.7955211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7955211","Electroencephalogram;Emotion detection;Discrete wavelet transform;Feature extraction;Bior 5.5","Electroencephalography;Feature extraction;Electrodes;Discrete wavelet transforms;Classification algorithms;Emotion recognition","","12","","20","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"A Novel Emotion Recognition System Based on Non-Invasive EEG Signals","H. Jin; Z. Jin; Y. -G. Kim; K. Li","School of Computer Science and Technology, Anhui University, Hefei, China; Foothill Preparatory School, Temple City, California, USA; Department of Computer Engineering, Sejong University, Seoul, South Korea; School of Electronic and Information Engineering, Anhui University, Hefei, China",2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL),"27 Mar 2025","2025","","","810","815","Emotion recognition is an important scenario for EEG applications, which can assist in the stable analysis of EEG information. In this study, the novel emotion recognition system based on non-invasive EEG signals is proposed. In the signal acquisition and the processing stage, this study adopted a standardized EEG signal acquisition process, combined with artifact space reconstruction technology to remove signal noise, laying the foundation for accurate analysis process. In the EEG data feature extraction stage, this study effectively captures the randomness and complexity characteristics of the EEG signals through an algorithm that combines differential entropy and power spectral density. Subsequently, at the emotion recognition level, this study designed and trained a deep neural network model, including four convolutional layers, four maximum pooling layers and a fully connected layer, to extract multi-level features and complete the emotion data classification. The experiment was verified on a self-collected EEG dataset, and 10 groups of comparative tests were conducted on the Accuracy and Precision indicators, with the traditional algorithm as the comparison object. The results show that the system has good performance in emotion recognition accuracy.","","979-8-3315-2392-3","10.1109/ICSADL65848.2025.10932987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10932987","Emotion Recognition;EEG Signals;Non-Invasive;Neural Networks;Brain-Computer Interface (BCI)","Deep learning;Emotion recognition;Accuracy;Space technology;Signal processing algorithms;Feature extraction;Brain modeling;Electroencephalography;Entropy;Classification algorithms","","","","26","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"Emotion Classification from Short-term EEG Signals in Deep Learning","Y. Mao; L. Song; X. Zeng","Academy for Eng. & Tech., Fudan University, Shanghai, China; Academy for Eng. & Tech., Fudan University, Shanghai, China; Academy for Eng. & Tech., Fudan University, Shanghai, China",2021 3rd International Symposium on Smart and Healthy Cities (ISHC),"29 Jun 2022","2021","","","169","173","Emotion is the result of brain processing after the combination of the surrounding environmental state and the individual physiological state. Its essence is the individual perceptual response to the things around him, and the result of the interaction between man and the environment. There is a strong correlation between human cognitive behavior and psychological activities and EEG signals. The physiological activities of the brain can directly reflect individual emotional activities. EEG signal is one of the important input signals in the affective calculation. Correct emotion representation is a key step in emotion recognition research. This paper aims to study emotion EEG recognition through the representation of emotion classification. The author collected 4000 EEG samples (including 64 channels) of four emotions with a time length of 3s, which were neutral, happy, sad, and fear. According to the characteristic that emotions can not be mutated, setting the sample length of 3s can more flexibly grasp the subjects' emotions in a short time. In this paper, two deep learning methods, LSTM and CNN, are used to identify the positive and negative emotions of the new dataset and DEAP (Database for Emotion Analysis using Physical Signal, including 40 channels). The accuracy of the new dataset is 89% and 74% respectively, which are higher than 72% and 70% of DEAP.","","978-1-6654-6743-8","10.1109/ISHC54333.2021.00038","Science and Technology Commission of Shanghai Municipality(grant numbers:21JC1405300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9804441","EEG;Data Set;Emotion Classification;Deep Learning","Deep learning;Emotion recognition;Correlation;Databases;Urban areas;Psychology;Brain modeling","","3","","18","IEEE","29 Jun 2022","","","IEEE","IEEE Conferences"
"Identifying Rules for Electroencephalograph (EEG) Emotion Recognition and Classification","E. S. Pane; M. A. Hendrawan; A. D. Wibawa; M. H. Purnomo","Department of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Computer Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Computer Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2017 5th International Conference on Instrumentation, Communications, Information Technology, and Biomedical Engineering (ICICI-BME)","18 Nov 2018","2017","","","167","172","In order to identify rules of emotion classification in EEG-based emotion recognition, this paper utilizes rule-based classifier and decision tree algorithm to construct EEG-based emotion classification models for four target emotions, namely happy, sad, angry, and relaxed. Considering different bands of frequency in EEG signals, the band pass IIR filter with Chebyshev type II window was applied to separate the EEG signal into gamma, beta, alpha, and theta bands. Time and frequency domain features extraction methods are presented to seek the relevant features within the EEG signals related to emotional states. For emotion classification, we compared three methods: rules algorithm (RIPPER), decision tree algorithm (J4.8), and SVM. To evaluate our proposed method, a real EEG dataset from DEAP database were used. For the recorded 5-channel EEG signals, the result shows that the RIPPER algorithm yields best performance accuracy of 92.01% in binary classification of sad versus relaxed emotional state. In regards with emotion classification accuracy, rule-based classification model performed better compared to decision tree algorithm. A promising result was obtained from features extracted from beta and gamma bands of EEG signals. From the experiment, the rules classifier model generated 10 rules of emotion classification, while the validation of the rules achieved an average accuracy of 81.64% for relaxed emotion class.","2158-0456","978-1-5386-3455-4","10.1109/ICICI-BME.2017.8537731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8537731","Emotion Recognition;Electroencephalograph (EEG);Emotion Classification;Rules-based Classifier;Rules identification","Electroencephalography;Feature extraction;Classification algorithms;Emotion recognition;Decision trees;Brain modeling;Prediction algorithms","","14","","19","IEEE","18 Nov 2018","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition with EERN Model","Z. Hou; Z. Lin; Q. Zheng; M. Jiang; Z. Lin; Y. Zhang; J. Bao","Beijing Institute of Fashion Technology, Chaoyang, Beijing, China; Beijing Institute of Fashion Technology, College of materials design & engineering, Chaoyang, Beijing, China; Department of Computer and Information Technology, Beijing Jiaotong University, Haidian, Beijing, China; Beijing Institute of Fashion Technology, College of Bushiness, Chaoyang, Beijing, China; Beijing Institute of Fashion Technology, College of Bushiness, Chaoyang, Beijing, China; Beijing Institute of Fashion Technology, College of Bushiness, Chaoyang, Beijing, China; Beijing Institute of Fashion Technology, College of materials design & engineering, Chaoyang, Beijing, China",2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP),"12 Nov 2024","2024","","","830","834","Emotion classification is a key technology in neuroscience and BCI, helping computers to describe and understand emotions. Compared to non-physiological signals such as physical behavior and voice, EEG is a more scientific technology for emotion classification task because of its immutability. With the advancement of Graphics Processing Units(GPU) and artificial intelligence,deep learning algorithms obtain excellent performance for emotion recognition with EEG. In this paper, we propose an Emo-EEGResNet (EERN) model, which consists of an EERN-Data tool for original data generation, an EERN-DE for feature extraction, and an EERN-net based on Res-net for emotion classification. Compared to SVM, Logistic Regression, Decision Tree and CNN, the EERN model has better performance with strong robustness in emotion recognition. The classification accuracy of these four algorithms are 74.75%, 76.01 %, 63.08%, 83% respectively in SEED datasets, while EERN is 93.4 %. The results prove that EERN is better suited for emotion recognition than several of the comparative methods.","","979-8-3503-7654-8","10.1109/ICSP62122.2024.10743301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10743301","EEG;SEED;Deep Learning;EERN;SVM","Support vector machines;Emotion recognition;Accuracy;Computational modeling;Graphics processing units;Data collection;Brain modeling;Feature extraction;Electroencephalography;Classification algorithms","","","","19","IEEE","12 Nov 2024","","","IEEE","IEEE Conferences"
"Multimodal EEG Emotion Recognition Using Bidirectional GRU with Attention Mechanism","L. Li","University of Leeds, Leeds, UK",2023 17th International Conference on Complex Medical Engineering (CME),"25 Jun 2024","2023","","","120","124","This paper introduces a multi-modal EEG emotion recognition approach utilizing bidirectional GRU models and attention mechanisms. It incorporates EEG and eye movement features to create multi-modal features rich in emotional information. The bidirectional GRU network with attention mechanisms captures deep context dependencies, improving emotion classification accuracy within subjects. Cross-subject experiments demonstrate the model's generalizability. Comparative experiments validate its effectiveness against single-modal features and existing models. This approach presents a promising avenue for the development of multi-modal emotion recognition in brain-computer interfaces, offering potential for further optimization and enhanced accuracy in future research.","","979-8-3503-1611-7","10.1109/CME60059.2023.10565605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10565605","machine learning;deep learning;multimodal emotion recognition;EEG","Deep learning;Emotion recognition;Accuracy;Fuses;Brain modeling;Electroencephalography;Brain-computer interfaces","","","","9","IEEE","25 Jun 2024","","","IEEE","IEEE Conferences"
"An EEG emotion recognition method based on AdaBoost classifier","T. Lv; J. Yan; H. Xu","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China",2017 Chinese Automation Congress (CAC),"1 Jan 2018","2017","","","6050","6054","At present, most of the EEG emotion recognition studies have taken all electric shocks or filtered electrodes as a feature and they are integrated (combined) with simple features that are extracted from other signals as a single classifier Emotional classification, but there are problems such as low efficiency and low accuracy. Aiming at this problem, this paper proposes an EEG emotion classification method based on AdaBoost classifier to optimize the algorithm, dividing the feature samples into different channels, and then using the genetic algorithm to identify the EEG emotion, and the algorithm achieves in Matlab. The experimental result shows that the average recognition rate of EEG is 90.8%, which is superior to the current single classifier and majority voting method, and it has better stability and generalization performance.","","978-1-5386-3524-7","10.1109/CAC.2017.8243867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8243867","EEG emotion recognition;Emotion classification;AdaBoost classifier;Genetic algorithm","Electroencephalography;Classification algorithms;Emotion recognition;Training;Feature extraction;Genetic algorithms;Prediction algorithms","","6","","18","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"An emotional EEG signal classification research based on deep learning","B. Cai; W. Quan; Q. Li; Y. Song","School of Computer Science and Technology, Changchun University of Science and Technology, Changchun, China; School of Computer Science and Technology, Changchun University of Science and Technology, Changchun, China; School of Computer Science and Technology, Changchun University of Science and Technology, Changchun, China; Zhongshan Institute of Changchun University of Science and Technology, Zhongshan, China","2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)","18 Jul 2022","2022","","","326","329","For the task of classifying emotional EEG signals with rich high-dimensional features in the time and frequency domains, this paper proposes a novel deep learning algorithm, SincNet-E, inspired by the speech recognition model SincNet. The convolutional structure of SincNet is adapted so that it can be considered as consisting of two pooling layers alternately superimposed with three convolutional groups. Subsequently, the LSTM structure is added after the convolutional structure to enhance its extraction of time-dependent features in the signal data. This novel model effectively improves the classification performance of the deep learning algorithm for emotional EEG data, ultimately achieving an accuracy of about 83.89% without the need for manual feature extraction.","","978-1-6654-5911-2","10.1109/CVIDLICCEA56201.2022.9825399","Jilin Scientific and Technological Development Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825399","EEG;deep learning;emotion classification;convolutional neural networks","Deep learning;Adaptation models;Convolution;Speech recognition;Feature extraction;Brain modeling;Electroencephalography","","","","12","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"EEG-based emotion classification using deep belief networks","W. -L. Zheng; J. -Y. Zhu; Y. Peng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2014 IEEE International Conference on Multimedia and Expo (ICME),"8 Sep 2014","2014","","","1","6","In recent years, there are many great successes in using deep architectures for unsupervised feature learning from data, especially for images and speech. In this paper, we introduce recent advanced deep learning models to classify two emotional categories (positive and negative) from EEG data. We train a deep belief network (DBN) with differential entropy features extracted from multichannel EEG as input. A hidden markov model (HMM) is integrated to accurately capture a more reliable emotional stage switching. We also compare the performance of the deep models to KNN, SVM and Graph regularized Extreme Learning Machine (GELM). The average accuracies of DBN-HMM, DBN, GELM, SVM, and KNN in our experiments are 87.62%, 86.91%, 85.67%, 84.08%, and 69.66%, respectively. Our experimental results show that the DBN and DBN-HMM models improve the accuracy of EEG-based emotion classification in comparison with the state-of-the-art methods.","1945-788X","978-1-4799-4761-4","10.1109/ICME.2014.6890166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890166","EEG;Emotion Classification;Affective Computing;Deep Belief Network","Electroencephalography;Hidden Markov models;Support vector machines;Feature extraction;Brain models;Accuracy","","226","","9","IEEE","8 Sep 2014","","","IEEE","IEEE Conferences"
"EEG signal processing and emotion recognition using Convolutional Neural Network","Q. Li; Y. Liu; C. Liu; F. Yan; Q. Zhang; Q. Liu; W. Gao","Changchun University of Science and Technology, Changchun, China; Changchun University of Science and Technology, Changchun, China; Changchun University of Science and Technology, Changchun, China; Changchun University of Science and Technology, Changchun, China; Changchun University of Science and Technology, Changchun, China; Changchun University of Science and Technology, Changchun, China; Changchun University of Science and Technology, Changchun, China",2021 International Conference on Electronic Information Engineering and Computer Science (EIECS),"9 Nov 2021","2021","","","81","84","As an important task in the advanced stage of artificial intelligence, the research of emotional EEG has received more and more attention in recent years. In order to improve the accuracy of EEG signal emotion recognition, in this paper, Fast Fourier Transform (FFT) and Continuous Wavelet Transform (CWT) are used to extract the features of EEG signals on the DEAP data set and build two CNN models for emotion recognition. The results show that the proposed algorithm is effective for EEG signal emotion recognition. The average recognition accuracy of emotion valence can reach 75.9%; the arousal can reach 79.3%; the like/dislike can reach 80.7%. This research can provide practical application reference for continuous dimension emotion automatic analysis and machine recognition.","","978-1-6654-1674-0","10.1109/EIECS53707.2021.9587900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9587900","component;EEG;FFT;CWT;CNN;emotion recognition","Emotion recognition;Continuous wavelet transforms;Fast Fourier transforms;Signal processing algorithms;Feature extraction;Electroencephalography;Classification algorithms","","8","","8","IEEE","9 Nov 2021","","","IEEE","IEEE Conferences"
"An EEG-Based Emotion Recognition Study Using Machine Learning and Deep Learning","H. Shrara; H. Ammar; M. Nasseredine; J. Charara; F. Sbeity","Department of physics and electronics, Faculty of Science, Lebanese University, Hadat, Lebanon; Department of physics and electronics, Faculty of Science, Lebanese University, Hadat, Lebanon; Department of physics and electronics, Faculty of Science, Lebanese University, Hadat, Lebanon; Department of physics and electronics, Faculty of Science, Lebanese University, Hadat, Lebanon; Department of physics and electronics, Faculty of Science, Lebanese University, Hadat, Lebanon",2023 Seventh International Conference on Advances in Biomedical Engineering (ICABME),"31 Oct 2023","2023","","","125","129","Emotion recognition, the automated determination of an individual's emotional state, holds significant potential in various fields, from mental health monitoring to human-computer interaction. Electroencephalography (EEG) has emerged as a powerful tool for emotion identification due to its direct measurement of brain activity. However, improving the accuracy of EEG-based emotion recognition remains a challenge. Our approach focuses on five distinct emotions: happiness, sadness, relaxation, stress, and love. To ensure standardized and controlled emotional experiences, we carefully select and utilize standardized movie clips as stimuli. EEG signals are collected from 10 participants using the Emotiv EPOC+ device. To capture the diverse aspects of emotional responses, we employ a mixed feature extraction method. A total of 29 features, including power spectral density, entropy, and statistical measures, are extracted from the EEG data, enabling a comprehensive representation of emotional patterns. For emotion recognition, we employ three classifiers: support vector machine (SVM), random forest (RF), and long short-term memory (LSTM). Results indicate that the LSTM model achieves the highest accuracy of 97%, outperforming RF (93%) and SVM (90%). The findings hold promise for accurate emotion recognition and potential real-world applications.","2377-5696","979-8-3503-2585-0","10.1109/ICABME59496.2023.10293013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10293013","emotion recognition;electroencephalography (EEG);Emotiv EPOC+;machine learning;deep learning","Support vector machines;Radio frequency;Deep learning;Human computer interaction;Training;Emotion recognition;Mental health","","1","","13","IEEE","31 Oct 2023","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Estimation with Different Deep Learning Models","T. B. Alakuş; İ. Türkoğlu","Department of Software Engineering, Kirklareli University, Kirklareli, Turkey; Department of Software Engineering, Firat University, Elazig, Turkey",2019 4th International Conference on Computer Science and Engineering (UBMK),"21 Nov 2019","2019","","","33","37","Emotion has a vital role in people's routine lives. It can be expressed via voice, facial expressions, body languages, mimics with intentionally or unintentionally to interact with the environment. In this regard, it is required to understand the emotion better to interpret the emotions. Emotion is generally used in many areas including rehabilitation applications, braincomputer interactions, genome-wide applications, healthcare services etc. There are many studies exist about emotion recognition with different approaches based on facial expression, voice and physiological signals. Yet, the first two of them can give incorrect information about emotions since these approaches can be manipulated by subjects easily. Thus, the more reliable and more durable approach proposed including EEG signals. Although it gives valuable information on emotion, EEG-based emotion estimation applications have not reached the desired level since its abstract and pattern recognition methods (falsified feature extraction methods, false classifier algorithms, big data, etc.) used for that applications. EEG-based emotion estimation is a complicated assignment which requires deep features, many EEG channels, clear signals and classifier algorithms. Determining the features and analyzing them requires time, thus in this study, we applied deep learning to discriminate the positive/negative emotional states. Our proposed method includes three parts; i) Collecting EEG data ii) Preprocessed the EEG data to denoise the signal iii) Deep learning with AlexNet and VGG-16 We collected EEG signals from 28 various subjects aged between 21-28 via portable and wearable EEG device called Emotiv Epoc+ 14 channel. In order to collect the signals, we applied four different video games as stimuli (2 negative and 2 positive labelled games) and collected signals totally 20 minutes long for each subject. At the end of the EEG collection process, we obtained 1568 number of EEG samples (14×28×4). To collect more reliable and healthy information from signals we preprocessed our signals. Finally, we performed two different deep learning algorithms to determine the positive-negative emotions and to compare their results. It is observed that the classification accuracies differ with different algorithms and the classification performance was found 92,09% with VGG16 which is superior to AlexNet algorithm 87,76%.","","978-1-7281-3964-7","10.1109/UBMK.2019.8907135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8907135","emotion recognition;deep learning;log loss;AlexNet;VGG-16","","","8","","25","IEEE","21 Nov 2019","","","IEEE","IEEE Conferences"
"Human Emotion Recognition using EEG Signal in Music Listening","M. Pisipati; A. Nandy","Department of Computer Science and Engineering, Machine Intelligence and Bio-Motion Lab, National Institute of Technology-Rourkela, Rourkela, India; Department of Computer Science and Engineering, Machine Intelligence and Bio-Motion Lab, National Institute of Technology-Rourkela, Rourkela, India",2021 IEEE 18th India Council International Conference (INDICON),"1 Feb 2022","2021","","","1","6","Electroencephalogram (EEG) signal provides information about the emotion state of an individual and music can be used as a stimulus to evoke specific kinds of emotions in the human brain. The proposed work focuses on discrete emotion recognition to classify the EEG signals into nine different emotion states. A novel hybrid feature extraction model based on time, frequency, and time-frequency domain is proposed to extract important features from EEG signal. Various machine learning models (k-nearest neighbor (k-NN), Random Forest, and XGBoost) and a deep learning algorithm (Convolution Neural Network (CNN)) are used to classify the emotions with promising results on DREAMER dataset. It is found that kNN, random forest, XGBoost, and CNN provide classification accuracy of 94.49%, 99.94%, 99.39%, and 99.90% respectively. The proposed work is compared with state-of-the-art techniques and the efficiency of the hybrid feature extraction model improves classification accuracy.","2325-9418","978-1-6654-4175-9","10.1109/INDICON52576.2021.9691724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691724","Electroencephalogram;Music;Mental Health;Emotion Recognition;Hybrid Feature Extraction;Machine Learning;Deep Learning","Emotion recognition;Time-frequency analysis;Machine learning algorithms;Music;Feature extraction;Brain modeling;Electroencephalography","","2","","17","IEEE","1 Feb 2022","","","IEEE","IEEE Conferences"
"Deep Learning Methods on Emotion Detection: Input Data Perspective","N. A. Nobari Aza; P. Esmaili","Biomedical Engineering Department, Near East University, Turkey; Electric- Electronic Engineering Department, Near East University, Turkey",2021 5th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT),"19 Nov 2021","2021","","","608","613","Emotions are known as a gestural way that shows a person's psychology and mental states of mind. There are various ways that a human can express his/her emotion. One of the new areas in emotion detection which grown faster depends on electroencephalography (EEG) signals. EEG is a technique that records signals generated by the brain because of emotional activities. The simplicity of analyzing, optimal time, and spatial resolution are features of EEG signals. Deep learning (DL) is a method that affected many scientific and engineering fields. There are several DL algorithms that can cause to gain more accuracy and robustness in models.","","978-1-6654-4930-4","10.1109/ISMSIT52890.2021.9604727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9604727","deep learning;emotion;EEG signals;DEAP dataset","Deep learning;Emotion recognition;Psychology;Brain modeling;Electroencephalography;Robustness;Spatial resolution","","","","24","IEEE","19 Nov 2021","","","IEEE","IEEE Conferences"
"Evaluating the Effectiveness of Various Feature Extraction Techniques for Emotion Classification using EEG Signals","A. A. Bamanikar; R. V. Patil; L. V. Patil; S. A. Mahajan","SKNCOE, Pune; PDEA's Colege of Engineering, Pune; SKNCOE, Pune; Department Of IT, PVG COE & Tech & GK Pate (Wani), IOM Pune",2025 International Conference on Emerging Systems and Intelligent Computing (ESIC),"16 Apr 2025","2025","","","380","382","Researchers have made great progress in deciphering our deepest emotions through brain signals, which is important in an era where comprehending human emotions is becoming more and more important. A novel study has been proposed that uses electroencephalography (EEG) signals to categorize fundamental emotions like happiness and rage. This ground-breaking study pushes the limits of what we previously believed to be achievable in emotion recognition by diving into the complex realm of feature extraction techniques. Through the use of EEG, scientists are able to get fresh insights into the intricate fabric of human emotions. The ramifications of this research are wide-ranging and fascinating, ranging from more intuitive user interfaces to individualized mental health care. One thing is certain as we approach this emotional revolution: intelligent, sympathetic human-computer contact is the way of the future. As scientists continue to investigate the possibility of EEG signals to offer unbiased and instantaneous insights into a person's emotional and cognitive state, the detection of emotion and stress has taken on greater significance in the field of affective computing [1]. In order to construct future personal affective systems, it is important to identify appropriate biomarkers for the detection of stress and emotion.","","979-8-3315-2210-0","10.1109/ESIC64052.2025.10962797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962797","Emotion;Stress detection;Feature Extraction;BFCC (Band Frequency Cepstral Coefficients) and ELC (Energy Logistic Coefficients);Alpha and Beta Bands","Emotion recognition;Resonant frequency;Mental health;Feature extraction;Electroencephalography;Fabrics;Distance measurement;Emotional responses;System validation;Logistics","","","","18","IEEE","16 Apr 2025","","","IEEE","IEEE Conferences"
"Emotion detection using EEG signals based on Multivariate Synchrosqueezing Transform and Deep Learning","T. Ergin; M. A. Ozdemir; O. Guren","Department of Biomedical Technologies, Izmir Katip Celebi University, Izmir, Turkey; Department of Biomedical Technologies, Izmir Katip Celebi University, Izmir, Turkey; Department of Biomedical Engineering, Izmir Katip Celebi University, Izmir, Turkey",2021 Medical Technologies Congress (TIPTEKNO),"9 Dec 2021","2021","","","1","5","Emotion recognition from EEG signals has gained a great research interest in brain-computer interface (BCI) studies. As the result of the outstanding success of deep neural networks in the image classification area, deep learning methods have become popular in the subject of emotion classification from EEG signals. In this study, we have used the Alexnet structure for the classification of emotions in Arousal and Valence domains separately. We generate TF images of 32-channel EEG data we collected by using Multivariate Synchrosqueezing Transform (MSST) and then these TF images are used to feed to the AlexNet model. A 3-fold cross-validation strategy was adopted to evaluate the robustness of the models. By training the AlexNet architecture an average accuracy of 71.60% is yielded on Arousal and an average accuracy of 67.93% is yielded on Valence. The results demonstrated that the proposed method achieved promising performance to classify emotions.","2687-7783","978-1-6654-3663-2","10.1109/TIPTEKNO53239.2021.9632970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9632970","Emotion recognition;SST;MSST;CNN;AlexNet;Multi-Channel EEG.","Deep learning;Training;Measurement;Emotion recognition;Visualization;Transforms;Brain modeling","","1","","18","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition: An In-depth Analysis using DEAP and SEED Datasets","S. K. Jha; S. Suvvari; M. Kumar","Department of Computer Science and Engineering, National Institute of Technology Patna, Patna, India; Department of Computer Science and Engineering, National Institute of Technology Patna, Patna, India; Department of Computer Science and Engineering, National Institute of Technology Patna, Patna, India",2024 11th International Conference on Computing for Sustainable Global Development (INDIACom),"18 Apr 2024","2024","","","1816","1821","Research on emotion recognition has made an increasing amount of emphasis on the understanding of Electroencephalogram (EEG) signals. Using two well-known datasets - the SEED (SEED Dataset for Emotion Analysis using EEG) and the DEAP (Dataset for Emotion Analysis using Physiological Signals), this work explores the complex analysis of EEG signals and their use in emotion recognition. The study highlights important characteristics suggestive of emotional states while delving into the basic ideas behind the acquisition, processing, and interpretation of EEG signals. We explore the viability and efficiency of using EEG signals for emotion recognition tasks by utilizing machine learning and signal processing techniques. We also explore the opportunities and problems associated with EEG-based emotion identification systems, such as feature selection, artefact removal, and signal noise. The goal of this study is to offer researchers and practitioners useful insights into using EEG signals for emotion identification applications through a thorough evaluation and analysis. Showcasing the effectiveness of our methodology in EEG-based emotion recognition. The study demonstrates promising arousal accuracy at 70.88% and notable valence accuracy at 76.00% using SVM Classifier.","","978-93-80544-51-9","10.23919/INDIACom61295.2024.10498398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498398","Electroencephalogram (EEG);signal noise;artefact removal;feature extraction;DEAP and SEED datasets","Support vector machines;Emotion recognition;Noise;Machine learning;Feature extraction;Electroencephalography;Physiology","","3","","22","","18 Apr 2024","","","IEEE","IEEE Conferences"
"Classification of EEG-based emotion for BCI applications","M. Mohammadpour; S. M. R. Hashemi; N. Houshmand","Department of Computer Engineering, Islamic Azad University, Qazvin, Iran; Young Researchers and Elite Club, Islamic Azad University, Qazvin, Iran; Department of Computer Engineering, Islamic Azad University, Qazvin, Iran",2017 Artificial Intelligence and Robotics (IRANOPEN),"26 Jun 2017","2017","","","127","131","Emotion plays an important role in human daily life and is a significant feature for interaction among people. Due to having adaptive role, it motivate human to respond stimuli in their environment quickly for improving their communication, learning and decision-making. With increasing role of brain computer interface (BCI) in interaction between users and computer, automatic emotion recognition has become an interesting area in the past decade. Emotion recognition could be carried out from the facial expression, gesture, speech and text, and could be record in several ways, like Electroencephalography (EEG), Positron Emission Tomography (PET), Magnetic Resonance Imaging (MRI), etc. In this work, feature extraction and classification of emotions have been evaluated on different methods to recognize and classify six emotional states such as fear, sad, frustrated, happy, pleasant and satisfied from inner emotion EEG signals. The results showed that using appropriate feature for extraction emotional state such as Discrete Wavelet Transform (DWT) and suitable learner such as Aftificial Neural Network (ANN), recognizer system can be accurately.","","978-1-5386-2862-1","10.1109/RIOS.2017.7956455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7956455","EEG;Emotion;BCI;Feature Extraction;Classification","Feature extraction;Emotion recognition;Electroencephalography;Brain modeling;Discrete wavelet transforms;Support vector machines;Computers","","34","","20","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"Emotion Recognition Using EEG Signals in Human Brain Waves and Deep Learning Approaches","S. Mekruksavanich; N. Hnoohom; W. Phaphan; A. Jitpattanakul","Department of Computer Engineering, School of Information and Communication Technology, University of Phayao, Thailand; Department of Computer Engineering Faculty of Engineering, Image Information and Intelligence Laboratory, Mahidol University, Nakhon Pathom, Thailand; Department of Applied Statistics Faculty of Applied Science, Research Group in Statistical Learning and Inference, King Mongkut’s University of Technology North Bangkok, Bangkok, Thailand; Department of Mathematics Faculty of Applied Science, Intelligent and Nonlinear Dynamic Innovations Research Center, King Mongkut’s University of Technology North Bangkok, Bangkok, Thailand","2025 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunications Engineering (ECTI DAMT & NCON)","15 Apr 2025","2025","","","363","367","Emotion recognition using EEG signals has gained growing attention in recent years due to its promising applications in human-computer interaction, affective computing, and mental health assessment. This study investigates the performance of various deep learning architectures in classifying emotional states from EEG recordings. We utilize the Emotion EEG dataset as a benchmark and evaluate five prominent deep learning models. The classification task targets three emotion categories: neu-tral, positive, and negative. Experimental results reveal notable differences in model performance. Among the tested models, BiGRU achieves the highest classification accuracy of 97.70% (±0.69%), followed closely by BiLSTM with 97.37% (±0.17%). These bidirectional architectures significantly outperform the CNN model, which attains an accuracy of only 61.59% (±1.22%). The LSTM and GRU models yield intermediate results, nearing 92% accuracy. The findings highlight the effectiveness of bidirectional recurrent neural networks in modeling the temporal characteristics of EEG signals for emotion detection. This research contributes to advancing EEG-based emotion recognition by identifying the most suitable deep learning strategies, thus supporting the development of more intelligent and emotion-aware computing systems.","2768-4644","979-8-3315-4327-3","10.1109/ECTIDAMTNCON64748.2025.10962120","Thailand Science Research and Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962120","human emotion recognition;EEG signal;human brain waves;machine learning;deep learning network","Deep learning;Emotion recognition;Accuracy;Computational modeling;Bidirectional long short term memory;Mental health;Computer architecture;Brain modeling;Electroencephalography;Recording","","","","40","IEEE","15 Apr 2025","","","IEEE","IEEE Conferences"
"EEG-Driven Emotional Mapping Using Machine Learning Algorithms","P. D. B; V. P; A. K; J. S; S. S","Department of Biomedical Engineering, KPR Institute of Engineering and Technology, Coimbatore, India; Department of Biomedical Engineering, KPR Institute of Engineering and Technology, Coimbatore, India; Department of Biomedical Engineering, KPR Institute of Engineering and Technology, Coimbatore, India; Department of Biomedical Engineering, KPR Institute of Engineering and Technology, Coimbatore, India; Department of Biomedical Engineering, KPR Institute of Engineering and Technology, Coimbatore, India",2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS),"23 Oct 2024","2024","1","","905","910","Emotions are the reflection of human thoughts which affects normal physiological activities and psychological behavior of an individual. Inferring the emotion state and their neural mechanisms is important in the fields of human-computer interaction, psychology, neuroscience, and affective computing. The brain is the control center and closely related with human emotions. Electroencephalography (EEG) are physiological signals that records the electrical activity of brain and can objectively determine the emotional state. Hence, EEG-guided emotion mapping provides an effective approach to study and recognize human emotions. This work involves analysis of brain signal by compatible EEG acquisition system, transforms for feature extraction and machine learning techniques for emotion classification. The features extracted from gamma, beta and alpha EEG sub-bands provide the essential information for characterizing the emotions. The classifiers employed are Gaussian Naïve Bayes (Gaussian NB), Support Vector Machine (SVM), Random Forest and Logistic Regression. The obtained results provide better analysis between the effects of negative, neutral and positive emotional states on human EEG and notable breakthroughs in identifying distinct brainwave patterns corresponding to specific emotions (positive, neutral, and negative).","2575-7288","979-8-3503-8436-9","10.1109/ICACCS60874.2024.10716934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10716934","human intelligence;machine learning;Electroencephalography (EEG);neuroscience;emotion classification;Support Vector Machine (SVM);Random Forest;Logistic Regression;Gaussian Naïve Bayes (Gaussian NB)","Support vector machines;Logistic regression;Machine learning algorithms;Accuracy;Feature extraction;Electroencephalography;Physiology;Classification algorithms;Bayes methods;Random forests","","","","15","IEEE","23 Oct 2024","","","IEEE","IEEE Conferences"
"Comparison of Human Emotion Classification on Single-Channel and Multi-Channel EEG using Gate Recurrent Unit Algorithm","Y. Pamungkas; U. W. Astuti","Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2023 International Conference on Computer Science, Information Technology and Engineering (ICCoSITE)","23 May 2023","2023","","","1","6","The use of EEG to recognize human emotions has become a notable trend and breakthrough today. EEG-based emotion recognition is a form of research that uses biomedical signals to distinguish a person's psychological condition (without directly paying attention to changes in facial gestures and attitudes). However, there are many studies related to emotion recognition whose classification accuracy is still low and needs to be improved. Therefore, we propose an EEG-based recognition of positive and negative emotions in this study using the Gate Recurrent Unit (GRU) algorithm. EEG data were taken from 38 participants with four recording channels (FP1, FP2, F7, and F8). In EEG recording, a video was played to stimulate the participants' emotions (positive and negative). Then, the EEG data is processed by filtering, artefact removal, frequency band decomposition, feature extraction, and emotion classification based on signal features. Several classification scenarios (such as by varying the activation function of the classifier and the number of EEG channels) are carried out to obtain an optimal level of accuracy. Based on the emotion classification results (using the Softmax activation function) on multi-channel EEG, the accuracy values reached 98.85% (for training) and 91.45% (for testing).","","979-8-3503-2095-4","10.1109/ICCoSITE57641.2023.10127686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10127686","EEG-based emotion recognition;feature extraction;band decomposition;emotion classification;Gate Recurrent Unit algorithm","Training;Emotion recognition;Psychology;Logic gates;Feature extraction;Market research;Electroencephalography","","4","","29","IEEE","23 May 2023","","","IEEE","IEEE Conferences"
"Comprehensive Analysis of Feature Extraction Methods for EEG-based Emotion Classification","S. S; P. V; R. S; R. S; S. S. S","Department of ECE, M. Kumarasamy College of Engineering, Karur, India; Department of ECE, M. Kumarasamy College of Engineering, Karur, India; Department of ECE, M. Kumarasamy College of Engineering, Karur, India; Department of ECE, M. Kumarasamy College of Engineering, Karur, India; Department of ECE, M. Kumarasamy College of Engineering, Karur, India",2025 International Conference on Electronics and Renewable Systems (ICEARS),"2 Apr 2025","2025","","","389","395","Emotion recognition from EEG signals is widely applicable and a rapidly emerging field in human-computer interaction, mental health analysis, and affective computing. The main problem of addressing a novel enhanced method for EEG-based emotion classification using the Discrete Wavelet Transform (DWT) to extract its features into delta, theta, alpha, beta, and gamma frequency components, and essential time-frequency features could be extracted that can improve the accuracy of the classification. The features are fed into the machine learning classifiers, namely CNN, RNN, SVM, LDA, and Decision Trees, in order to obtain a classification accuracy of 96%. The experimental results indicate a progressive increase in accuracy and consistent reduction in loss over 20 epochs, showing the effectiveness of DWT-based feature extraction. The study evaluates model robustness by using key performance metrics like accuracy, sensitivity, and specificity. It shall improve in future, including optimized wavelet parameters, refinement in architectures of the neural network, real-time processing improvement, and multimodal data fusion to be incorporated for enhancement of system performance.","","979-8-3315-0967-5","10.1109/ICEARS64219.2025.10940547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10940547","EEG;Emotion Recognition;Discrete Wavelet Transform;Machine Learning;Feature Extraction;Signal Processing;CNN;RNN;SVM","Support vector machines;Time-frequency analysis;Emotion recognition;Accuracy;System performance;Machine learning;Feature extraction;Wavelet analysis;Electroencephalography;Discrete wavelet transforms","","","","15","IEEE","2 Apr 2025","","","IEEE","IEEE Conferences"
"A Review of Research on Feature Extraction for Emotion Recognition Based on EEG Signals","L. Hongxing; D. Lifeng; L. Lihua; H. Shenhui","Department of Electronic Information Engineering, Chengdu Jincheng College, Chengdu, China; Department of Electronic Information Engineering, Chengdu Jincheng College, Chengdu, China; Department of Electronic Information Engineering, Chengdu Jincheng College, Chengdu, China; Department of Electronic Information Engineering, Chengdu Jincheng College, Chengdu, China",2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD),"12 Jul 2022","2022","","","416","421","Emotion is a general term for a series of subjective cognitive experiences. When the human brain reacts to the relationship between external things and the subject’s needs, it produces our experience and feelings of external things. Emotional information plays an important role in life today. The application of emotional information has spread to all aspects of life, such as medical, educational, psychological, business and military fields. This paper summarizes all the feature extraction methods currently used in the research field of EEG-based emotion recognition, and introduces the definition and calculation methods of various feature extraction from the time domain, frequency domain, time-frequency domain, and space domain, respectively. After summarizing and summarizing, a method suitable for EEG-based emotion recognition feature extraction is obtained. At the same time, it is pointed out that the combination of hybrid feature extraction and deep learning will become the mainstream trend. It is hoped that subsequent researchers can gain a preliminary understanding of feature extraction methods suitable for emotion recognition by studying this article when they engage in EEG-based emotion research, and provide new ideas for subsequent research.","","978-1-6654-9913-2","10.1109/ICAIBD55127.2022.9819976","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9819976","emotion recognition;EEG;feature extraction","Deep learning;Emotion recognition;Time-frequency analysis;Psychology;Feature extraction;Market research;Electroencephalography","","","","30","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"Stacked Auto-Encoder Driven Automatic Feature Extraction for Web-Enabled EEG Emotion Recognition","Y. Dai; E. Ji; Y. Yao","School of Electronics and Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Shenzhen University, Shenzhen, Shenzhen, China; School of Electronics and Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China",2021 7th Annual International Conference on Network and Information Systems for Computers (ICNISC),"23 Nov 2021","2021","","","991","997","EEG emotion recognition is able to provide a scientific solution for emotional health assessment. Feature extraction is the fundamental procedure. Traditionally, the future set is generated by the existing theories or rules, which is not convincing and objective enough. Therefore, this paper proposes a data-driven automatic feature extraction methodology for web-enabled EEG emotion recognition based on 2-hidden-layer stacked auto-encoder. Since the web-enabled framework provides large scale of EEG data, emotion-related EEG features can be extracted directly from the time-domain raw wave, which is different from the typical feature extraction methods based on rules and experiences. With the optimal experimental parameters setting, the proposed method extracts typical time-domain distinguishable features from the EEG raw data and obtains relatively low classification error rate. This paper takes a step further towards automatic feature extraction for web-enabled EEG emotion recognition and make the entire framework more impersonal and convincing.","","978-1-6654-0232-3","10.1109/ICNISC54316.2021.00187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9603966","EEG emotion recognition;automatic feature extraction;stacked auto-encoder","Computers;Emotion recognition;Error analysis;Feature extraction;Electroencephalography;Data mining;Time-domain analysis","","2","","7","IEEE","23 Nov 2021","","","IEEE","IEEE Conferences"
"On the development of EEG based emotion classification","K. Luangrat; Y. Punsawad; Y. Wongsawat","Department of Biomedical Engineering, Faculty of Engineering, Mahidol University, Thailand; Department of Biomedical Engineering, Faculty of Engineering, Mahidol University, Thailand; Department of Biomedical Engineering, Faculty of Engineering, Mahidol University, Thailand",The 5th 2012 Biomedical Engineering International Conference,"21 Feb 2013","2012","","","1","4","This paper proposes an investigation on classification of the positive and negative emotions via the use of electroencephalogram (EEG). EEG bandpowers are extracted as the feature of interest. Two simple decision rules to classify positive and negative emotions are proposed, i.e. 1) using both the left and right frontal information and 2) using only one side of the left or right frontal information. First decision reports low accuracy while the second decision rule can achieve higher accuracy between 80 to 90%. This can be concluded that the proposed method is possible for the real-time emotion classification in neuroeconomics.","","978-1-4673-4892-8","10.1109/BMEiCon.2012.6465506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465506","Electroencephalogram;EEG;Emotion","Electroencephalography;Accuracy;Feature extraction;Motion pictures;Humans;Emotion recognition;Conferences","","3","","6","IEEE","21 Feb 2013","","","IEEE","IEEE Conferences"
"Classification of Emotions (Positive-Negative) Based on EEG Statistical Features using RNN, LSTM, and Bi-LSTM Algorithms","Y. Pamungkas; A. D. Wibawa; Y. Rais","Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2022 2nd International Seminar on Machine Learning, Optimization, and Data Science (ISMODE)","19 Jul 2023","2022","","","275","280","Affective computing research related to EEG-based emotion recognition has become a current research trend. This research becomes very interesting because the EEG signal is complex and always changes depending on the condition of the individual at that time. So, if the information in the EEG signal can be extracted, a person’s emotional state (which tends to be hidden) will be revealed. Therefore, this study directly proposes an automatic emotion recognition system with recorded EEG data. In this study, EEG recording was performed on 32 participants. Raw EEG data is processed by stages such as pre-processing, subband decomposition, feature extraction, and classification of emotions based on feature values. The EEG signal features explored include mean value, MAV, standard deviation, variance, skewness, kurtosis, zerocrossing rate, and median. Based on the results of EEG feature extraction, it can be seen that positive-negative emotions have different feature values and the differences are also significant. The results of signal feature extraction are presented based on channels (FP1, FP2, F7, and F8) and EEG subbands (Alpha, Beta, and Gamma) for each emotional state (positive-negative). In addition, the best accuracy values for emotion classification are 93.75% (RNN), 93.75% (LSTM), and 92.97% (Bi-LSTM) in the classifier testing process.","","978-1-6654-5564-0","10.1109/ISMODE56940.2022.10180969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10180969","EEG Emotion Recognition;EEG Extraction Features;RNN;LSTM;Bi-LSTM","Seminars;Emotion recognition;Feature extraction;Market research;Electroencephalography;Recording;Data mining","","7","","18","IEEE","19 Jul 2023","","","IEEE","IEEE Conferences"
"Emotion Classification of EEG signals using Logistic Regression classification","A. R. Singh; G. Singh; N. Saluja","Chitkara University Institute of Engineering and Technology Chitkara University, Punjab, India; Chitkara University Institute of Engineering and Technology Chitkara University, Punjab, India; Chitkara University Institute of Engineering and Technology Chitkara University, Punjab, India",2024 3rd International Conference for Innovation in Technology (INOCON),"6 May 2024","2024","","","1","5","The exploration of emotion classification through the analysis of EEG signals presents a multifaceted research area that merges neuroscience, psychology, and machine learning. In this project, our focus shifts to using Logistic Regression (LR), another potent technique in machine learning, known for its proficiency in dealing with binary and multiclass classification problems and LR estimates probabilities using a logistic function, which is particularly effective for categorizing data into distinct groups. This project involves analyzing electroencephalogram (EEG) signals, the intricate reflections of the brain’s activities, influenced by various emotional states. To utilize LR, the initial steps mirror those in SVM-based approaches. We collect EEG data from subjects experiencing a range of emotions, induced through different stimuli such as images or audio clips. This data is inherently complex and multi-dimensional, necessitating thorough preprocessing to isolate pertinent features. Once the features are extracted, they are input into the LR model. This model is adept at handling binary classifications (e.g., happy vs. not happy) and can be extended to multiclass problems (e.g., distinguishing between happiness, sadness, and stress) using techniques like one-vs-rest (OvR) or multinomial logistic regression. The model learns to associate specific EEG patterns with particular emotional states, assigning probabilities to these classifications. The implications of this research are profound, especially in the realms of human-computer interaction and healthcare. In technology, it could lead to AI systems that better understand and respond to human emotions, enhancing user experience. In healthcare, it offers a novel approach to diagnosing and managing emotional disorders, providing insights into the brain's response to different emotional states. This project, therefore, is not only a step forward in machine learning applications but also a significant contribution to our understanding of the complex interplay between the brain and emotions.","","979-8-3503-8193-1","10.1109/INOCON60754.2024.10511417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10511417","EEG signals;Machine learning;Deep Analysis;Python;Logistics regression (LR);Brain","Human computer interaction;Logistic regression;Neuroscience;Medical services;Machine learning;Brain modeling;Feature extraction","","4","","19","IEEE","6 May 2024","","","IEEE","IEEE Conferences"
"EEG-Based Brain Computer Interface for Emotion Recognition","K. S. Bano; P. Bhuyan; A. Ray","School of Computer Engineering, KIIT Deemed to be University, Bhubaneswar, Odisha, India; School of Computer Engineering, KIIT Deemed to be University, Bhubaneswar, Odisha, India; School of Computer Engineering, KIIT Deemed to be University, Bhubaneswar, Odisha, India",2022 5th International Conference on Computational Intelligence and Networks (CINE),"9 Feb 2023","2022","","","1","6","Emotion recognition using electroencephalography (EEG) signal could be a current focus in brain-computer interface research, that is convenient and a reliable technique. EEG-based emotion detection studies are employed in a very spread of fields, including defence, aerospace, and medicine, among others. The purpose of this study is to discover the relationship between EEG signals and human emotions. EEG signals are commonly used to categorise emotions into three groups: positive, negative, and neutral. We first extracted features from the EEG signals in order to classify emotions and used a deep learning classifier: recurrent neural network (RNN) and gated recurrent unit (GRU). Second, a Muse EEG headband with four electrodes (TP9, AF7, AF8, TP10) is used to record brain activity. Positive and negative emotional states are elicited with lucid valence film clips, and neutral resting data with no stimuli is also recorded for one minute per session. EEG data was collected for 3 minutes per state from two people (one male and one female) (positive, neutral, and negative) [5]. This study helps to spot human emotions supported by EEG signals within the brain-computer interface and helps to know the emotion of the mind.","","978-1-6654-6465-9","10.1109/CINE56307.2022.10037255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10037255","Emotion recognition;EEG;deep learning;RNN;GRU;brainwave;EEG emotion Detection;brain-computer inter-face (BCI);MUSE EEG-Headband","Electrodes;Deep learning;Emotion recognition;Recurrent neural networks;Logic gates;Feature extraction;Electroencephalography","","15","","19","IEEE","9 Feb 2023","","","IEEE","IEEE Conferences"
"Investigating the Effect of EEG Channel Selection on Inter-subject Emotion Classification","K. Sharma; A. Dash; D. Kumar","Department of Biotechnology, Dr. D. Y.Patil Biotechnology and Bioinformatics Institute, Pune, India; Yong Siew Toh Conservatory of Music, National University of Singapore, Singapore; School of Biomedical Engineering, Indian Insttitute of Technology (BHU) Varanasi, Varanasi, India","2023 13th International Conference on Cloud Computing, Data Science & Engineering (Confluence)","22 Feb 2023","2023","","","312","316","Inter-subject emotion classification from physiological brain signal (electroencephalogram (EEG)) is often challenging due to lack of generalizability in computed features across the subjects. A great deal of research effort has been devoted to EEG-based emotion recognition. However, the effect of channel selection and relative efficacy of different domains of EEG features on multilevel emotion classification in the inter-subject scenario is still unclear. This work aims to investigate the effect of channel selection/reduction on inter-subject multilevel emotion classification. The analysis is performed on a publicly available DEAP dataset and four groups of channels are selected from the literature. Furthermore, we compute 6 number of time and frequency domain EEG features, and 3 number of non-linear EEG features and study the relative efficacy of these features towards capturing the generalizable component in EEG signals to classify emotions in inter-subject scenarios. The results indicate that the emotions are better classifiable after 20 seconds time from the beginning of the stimulus. Also, the inter-subject emotion classification accuracy increases significantly with increasing the number of channels beyond 10. Furthermore, between the time and frequency domain features, and nonlinear EEG features, the prior shows better efficacy in classifying multilevel emotions in the inter-subject domain.","","978-1-6654-6263-1","10.1109/Confluence56041.2023.10048851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10048851","Arousal;DEAP;Emotion classification;EEG;Inter-subject;PCA;SVM;Valance","Emotion recognition;Frequency-domain analysis;Electroencephalography;Physiology","","4","","19","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
"Deep learninig of EEG signals for emotion recognition","Y. Gao; H. J. Lee; R. M. Mehmood","Division of Computer Science and Engineering, Chonbuk National University, Jeonju, Korea; Center for Advanced Image and Information Technology, Chonbuk National University, Jeonju, Korea; Division of Computer Science and Engineering, Chonbuk National University, Jeonju, Korea",2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),"30 Jul 2015","2015","","","1","5","Emotion recognition is an important task for computer to understand the human status in brain computer interface (BCI) systems. It is difficult to perceive the emotion of some disabled people through their facial expression, such as functional autism patient. EEG signal provides us a non-invasive way to recognize the emotion of these disable people through EEG headset electrodes placed on their scalp. In this paper, we propose a deep learning algorithm to simultaneously learn the features and classify the emotions of EEG signals. It differs from the conventional methods as we apply deep learning on the raw signal without explicit hand-crafted feature extraction. Because the EEG signal has subject dependency, it is better to train the emotion model subject-wise, while there is not much epochs available for each subject. Deep learning algorithm provides a solution with a pre-training way using three layers of restricted Boltzmann machines (RBMs). Thus, we can use epochs of all subjects to pre-training the deep network, and use back-propagation to fine tuning the network subject by subject. Experiment results show that our proposed framework achieves better recognition accuracy than conventional algorithms.","","978-1-4799-7079-7","10.1109/ICMEW.2015.7169796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169796","EEG;Emotion Recognition;Deep learning;RBM","Electroencephalography;Support vector machines;Brain modeling;Feature extraction;Biomedical imaging;Image recognition;Artificial neural networks","","54","","21","IEEE","30 Jul 2015","","","IEEE","IEEE Conferences"
"Minimal Electrode EEG for BCI Emotion Detection","Y. Li; H. Fang; W. Liu; C. Cheng; H. Chen","School of Advanced Technology, Xi'an Jiaotong-Liverpool University, Suzhou, China; Institute for Imaging, Data and Communications, School of Engineering, The University of Edinburgh, Edinburgh, UK; School of Advanced Technology, Xi'an Jiaotong-Liverpool University, Suzhou, China; State Key Laboratory on Integrated Optoelectronics, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; Brain Machine Fusion Intelligence Institute, Jiangsu Industrial Technology Research Institute, Suzhou, China","2024 4th International Conference on Neural Networks, Information and Communication Engineering (NNICE)","22 Apr 2024","2024","","","379","383","Electroencephalography (EEG)-based emotion recognition is a potential research direction in the field of brain-computer interfaces (BCIs). However, its deployment on wearable devices still suffers from the challenges of low accuracy, heavy computation, and complex electrode placement. This study focuses on advancing the efficiency and cost-effectiveness of EEG-based BCIs for emotion recognition. Our approach begins with an investigation of electrode placement in relation to emotion detection, leveraging the SEED dataset to identify an optimal configuration that uses a minimal number of electrodes while maintaining high recognition accuracy. Employing a variety of machine learning and deep learning algorithms, we compare detection accuracy across different electrode combinations. Through these experiments and subsequent analysis, we identify an effective combination of two electrodes, T7 and T8, with the SVM method achieving an impressive 92.8 % accuracy. This finding laid the foundation for the design of our wearable, closed-loop BCI device with EEG-based emotion recognition capability.","","979-8-3503-9437-5","10.1109/NNICE61279.2024.10499167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499167","Electroencephalography;Brain-Computer Interfaces;Emotion Recognition;Channel Selection;Machine Learning;Deep Learning","Electrodes;Support vector machines;Deep learning;Emotion recognition;Computational modeling;Brain modeling;Electroencephalography","","","","12","IEEE","22 Apr 2024","","","IEEE","IEEE Conferences"
"BrainSense: A Sequential Emotion Classification Model","H. R. Chauhan; S. Dwivedi; R. D. Biswas; M. Prakash; D. Kumar","Computer Science and Engineering Department, Motilal Nehru National Institute of Technology, Allahabad; Computer Science and Engineering Department, Motilal Nehru National Institute of Technology, Allahabad; Computer Science and Engineering Department, Motilal Nehru National Institute of Technology, Allahabad; Computer Science and Engineering Department, Motilal Nehru National Institute of Technology, Allahabad; Computer Science and Engineering, MNNIT",2020 IEEE International Conference for Innovation in Technology (INOCON),"1 Jan 2021","2020","","","1","5","This paper explores the task of Emotion Classification in Humans through a Deep Learning model by analysing Electroencephalogram (EEG) waves captured by a 32-electrodes and 16-channel BCI device which measures these signal amplitudes. Our purpose is two-fold - firstly, to provide an annotated dataset: BrainSense for furthering research in this domain, and secondly, to propose a novel sequential deep learning approach for creating a binary classifier that would classify a continuous EEG data stream as “Joyous” or “Sad”. We call our model - Continuous Sequential Emotion (CSEM) Classifier. We model the EEG data in such a manner, that future and past EEG waves are inculcated in deciding the emotion of a particular tuple of signal amplitudes. Keeping in mind the latter, our approach uses a many-to-one LSTM and also Bidirectional LSTM to encode the EEG signals in order to match the continuous nature of these signals. Classification is achieved through a dense network based on the EEG encodings. We get a maximum accuracy of 96.95% on the Bidirectional-CSEM. We train and test the model on our data, and discuss the detailed results and methodology.","","978-1-7281-9744-9","10.1109/INOCON50539.2020.9298256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9298256","LSTM;Sequence Models;EEG;Emotion Classification;Brain-Computer Interface;Deep Learning;EEG Data set","Electroencephalography;Brain modeling;Data models;Videos;Predictive models;Analytical models;Feature extraction","","","","19","IEEE","1 Jan 2021","","","IEEE","IEEE Conferences"
"Electroencephalogram Emotion Recognition Using Variational Modal Decomposition Based Dispersion Entropy Feature Extraction","S. -J. Hu; Z. -T. Liu; X. -W. Ding","School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China",2021 40th Chinese Control Conference (CCC),"6 Oct 2021","2021","","","3323","3326","Electroencephalogram (EEG) emotion recognition has gained considerable attention due to its ability to reflect people’s inner emotional states objectively and naturally. Feature extraction is a critical step in EEG emotion recognition because of non-stationarity and irregularity of EEG signals. A feature extraction method using Variational Modal Decomposition (VMD) to extract Dispersion Entropy (DispEn) is proposed in this paper. Raw EEG signal is decomposed into several components, and DispEn of each component is extracted in eight emotion-related channels. Our method was tested on DEAP dataset in which the EEG emotional states are accessed in Valence-Arousal emotional space. Four emotional states (i.e., HVHA, HVLA, LVHA, LVLA) are classified by Support Vector Machine (SVM). The experimental results show that the accuracy of emotion recognition is 77.87%, which demonstrates its effectiveness.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549884","National Natural Science Foundation of China; Wuhan Science and Technology Project; China University of Geosciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549884","EEG Emotion recognition;VMD;Dispersion Entropy;Feature Extraction;SVM","Support vector machines;Emotion recognition;Temporal lobe;Feature extraction;Brain modeling;Electroencephalography;Real-time systems","","1","","20","","6 Oct 2021","","","IEEE","IEEE Conferences"
"EEG-based emotion recognition using LSTM-RNN machine learning algorithm","R. K. Jeevan; V. M. R. S.P.; P. Shiva Kumar; M. Srivikas","ECE SNIST, Hyderabad, India; ECE SNIST, Hyderabad, India; ECE SNIST, Hyderabad, India; ECE SNIST, Hyderabad, India",2019 1st International Conference on Innovations in Information and Communication Technology (ICIICT),"21 Jun 2019","2019","","","1","4","In recent days the knowledge in the Brain Machine Interface is manifesting emotion recognition and classification. There are many studies indicating potential evidence in identifying emotions using EEG brain waves. This paper investigates and proposes a new machine learning technology in identifying the emotions through the use of latest machine learning concepts using LSTN ( Long short term memory) recurring neural networks. The acquired brain wave signals are processed for classification using discrete wavelet transform and then given to the proposed algorithm for specific emotion recognition.","","978-1-7281-1604-4","10.1109/ICIICT1.2019.8741506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741506","EEG;LSTM;RNN;DWT","Electroencephalography;Brain modeling;Emotion recognition;Convolution;Logic gates;Computational modeling;Brain-computer interfaces","","26","","11","IEEE","21 Jun 2019","","","IEEE","IEEE Conferences"
"Advancements in EEG-Based Emotion and Consciousness Recognition: Techniques, Challenges, and Innovations","R. B. Jadekar; P. B; G. B H; P. S P","Dept. of Computer Science and Engineering, Data Science, Bapuji Institute of Engineering and Technology, Visvesvaraya Technological University, Belavavi, Karnataka, India; Dept. of Information Science and Engineering, Bapuji Institute of Engineering and Technology, Visvesvaraya Technological University, Belavavi, Karnataka, India; Dept. of Computer Science and Engineering, Data Science, Bapuji Institute of Engineering and Technology, Visvesvaraya Technological University, Belavavi, Karnataka, India; Dept. of Information Science and Engineering, Bapuji Institute of Engineering and Technology, Visvesvaraya Technological University, Belavavi, Karnataka, India","2024 First International Conference on Innovations in Communications, Electrical and Computer Engineering (ICICEC)","30 Dec 2024","2024","","","1","8","This study delves into the application of Electroencephalogram (EEG)-based techniques for emotion and consciousness recognition, focusing on disorders of consciousness (DOC). By evaluating existing approaches in terms of techniques, methodologies, and accuracies, the research highlights the efficacy of both machine learning and deep learning models. The key challenges were identified such as noise removal due to artifacts and variability in patient responses. The findings underscore the potential of advanced computational methods in enhancing the reliability and precision of EEG-based diagnostic tools, paving the way for better clinical interventions and deeper understanding of human emotions. Also, the findings show that using deep learning, the current works have achieved better results for identification of emotion.","","979-8-3503-7651-7","10.1109/ICICEC62498.2024.10808403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10808403","EEG;Emotion Recognition;Consciousness Detection;Machine Learning;Deep Learning;Disorder of Consciousness","Deep learning;Technological innovation;Emotion recognition;Accuracy;Computational modeling;Noise;Noise reduction;Brain modeling;Electroencephalography;Reliability","","","","51","IEEE","30 Dec 2024","","","IEEE","IEEE Conferences"
"Human Emotion Recognition Models Using Machine Learning Techniques","A. Alam; S. Urooj; A. Q. Ansari","Department of Electrical Engineering, Jamia Millia Islamia, New Delhi, India; Department of Electrical Engineering, College of Engineering, Princess Nourah Bint Abdulrahman University, Riyadh-KSA; Department of Electrical Engineering, Jamia Millia Islamia, New Delhi, India","2023 International Conference on Recent Advances in Electrical, Electronics & Digital Healthcare Technologies (REEDCON)","16 Jun 2023","2023","","","329","334","Researchers have always been curious if a computer can detect human emotions precisely and accurately. Many research publications have been reported on human-machine interaction systems. The emotion classifiers using machine learning techniques are developed using the feature dataset extracted from physiological and non-physiological parameters. Emotion recognition can be done either by using facial, speech or audio-visual data paths or using physiological signals like ECG, EEG, EMG, GSR and Respiration signals. Many have explored facial recognition techniques for emotion recognition but facial expressions can be masked. A sad person can pretend to have a smiling face and vice-versa. Physiological signals like ECG, EEG, GSR and respiration signals are non-maskable due to their involuntary source of generation. There are many datasets available publicly for researchers to use and develop an efficient emotion classifier system. In this work, the publicly available datasets of EEG, ECG and GSR recorded while watching emotional video are utilized to develop emotion classifiers using machine learning techniques. Here three physiological feature datasets named LUMED-2 (EEG+ GSR), SWELL (HRV), and YAAD (ECG+ GSR) are used to train models and classify emotions. The deep learning classifiers used are Random Forest, SVM, KNN, and/or Decision Tree. The maximum average classification accuracy achieved is close to 100% at least for one classifier in each dataset. It is observed that physiological signals like EEG, ECG, and GSR possess differentiable emotional features which can be used to detect the emotional state of a person precisely using the trained machine learning models.","","978-1-6654-9382-6","10.1109/REEDCON57544.2023.10151406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10151406","Human-Computer Interaction;Electrocardiogram (ECG);Electroencephalography (EEG);Galvanic Skin Response (GSR);Emotion Recognition;Machine Learning Techniques","Emotion recognition;Face recognition;Wearable computers;Electrocardiography;Brain modeling;Feature extraction;Physiology","","5","","14","IEEE","16 Jun 2023","","","IEEE","IEEE Conferences"
"Human Emotion Classification for Valence and Arousal via EEG-Based Machine Learning","R. Raman; V. Kumar; B. G. Pillai; A. Verma; S. Rastogi; D. Pandey","Symbiosis International(Deemed University); SRV Media Pvt. Ltd; Faculty of Management IT & Business Analytics, Sri Balaji University, Pune (SBUP); Computer science and Engineering Faculty of Engineering and Technology, Parul institute of Technology, Parul University; Symbiosis Institute of Business Management Nagpur Symbiosis International (Deemed University), Pune; Symbiosis International(Deemed University)",2024 Asian Conference on Intelligent Technologies (ACOIT),"2 Apr 2025","2024","","","1","5","In the realm of affective computing, the accurate classification of human emotions through physiological signals, particularly electroencephalogram (EEG), presents a significant opportunity to influence practical outcomes in neuroscience, psychology, and human-computer interaction. This paper introduces a novel machine learning approach to emotion classification that adeptly decodes the emotional dimensions of valence and arousal from EEG signals. Our methodology is comprehensive, involving meticulous data preprocessing, feature extraction, and the implementation of an ensemble of sophisticated algorithms, such as support vector machines and neural networks. The effectiveness of our model is demonstrated through rigorous testing, where it significantly surpasses existing benchmarks in accuracy. Importantly, the results reveal the potential of EEG-based models to revolutionize our understanding of emotional states, enhancing applications in adaptive user interfaces and emotional health monitoring. These applications are crucial for developing technologies that adjust to user emotions in real-time, offering substantial benefits in personalized digital interactions and therapeutic settings, thereby extending the research’s relevance beyond academia to directly benefit industry practitioners and clinicians.","","979-8-3503-7495-7","10.1109/ACOIT62457.2024.10939645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10939645","Electroencephalogram (EEG);Emotion Classification;Machine Learning;Valence and Arousal;Affective Computing","Support vector machines;Affective computing;Accuracy;Refining;Brain modeling;Feature extraction;Electroencephalography;Convolutional neural networks;Long short term memory;Monitoring","","","","18","IEEE","2 Apr 2025","","","IEEE","IEEE Conferences"
"MUSIC Model based Neural Information Processing for Emotion Recognition from Multichannel EEG Signal","M. Sakib Abrar Hossain; M. Asadur Rahman; A. Chakrabarty","Department of Computer Science and Engineering (CSE), BRAC University, Dhaka, Bangladesh; Department of Biomedical Engineering, Military Institute of Science and Technology (MIST), Dhaka, Bangladesh; Department of Computer Science and Engineering (CSE), BRAC University, Dhaka, Bangladesh",2021 8th International Conference on Signal Processing and Integrated Networks (SPIN),"19 Oct 2021","2021","","","955","960","Emotion recognition from neuro-signal is a computationally challenging issue in the field of medical data science that has several interesting applications in mental state revelation. Usually, electroencephalogram (EEG) based neuro-signal is widely popular for its temporal resolution, portability, easy to use, and non-invasive features. Emotion recognition from multichannel EEG signal processing technique customarily depends on non-parametric frequency spectrum estimation. These methods often fail to achieve a significant assortment in frequency estimation of different emotional EEG signals due to the non-stationary behavior. In this work, MUSIC (Multiple Signal Classification), a parametric-based frequency spectrum estimation technique is proposed to extract features from multichannel EEG signals for emotional state classification. The proposed work utilized the SEED EEG dataset of three class emotional states for feature extraction and classified it with Multi-Layer Perceptron (MLP) network. The research analyzes the characteristics of the extracted MUSIC feature space through intensive visualization and compares the quality of the extracted feature space with conventional parametric model-based feature space. The average classification accuracy of the proposed method is 90%.","2688-769X","978-1-6654-3564-2","10.1109/SPIN52536.2021.9565974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565974","EEG signal;multiple signal classification (MUSIC) algorithm;parametric-based frequency spectrum;multilayer perceptron network;feature extraction;classification;emotion recognition","Emotion recognition;Computational modeling;Signal processing algorithms;Feature extraction;Brain modeling;Electroencephalography;Frequency estimation","","2","","17","IEEE","19 Oct 2021","","","IEEE","IEEE Conferences"
"Interpretable SincNet-based Deep Learning for Emotion Recognition from EEG brain activity","J. M. Mayor-Torres; M. Ravanelli; S. E. Medina-DeVilliers; M. D. Lerner; G. Riccardi","Department of Information Engineering and Computer Science (DISI), University of Trento, Trento; Mila -Quebec Artifical Intelligence Institute, Montreal, QC, Canada; Department of Psychology, StonyBrook University, NY, USA; Department of Psychology, StonyBrook University, NY, USA; Department of Information Engineering and Computer Science (DISI), University of Trento, Trento",2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"9 Dec 2021","2021","","","412","415","Machine learning methods, such as deep learning, show promising results in the medical domain. However, the lack of interpretability of these algorithms may hinder their applicability to medical decision support systems. This paper studies an interpretable deep learning technique, called SincNet. SincNet is a convolutional neural network that efficiently learns customized band-pass filters through trainable sinc-functions. In this study, we use SincNet to analyze the neural activity of individuals with Autism Spectrum Disorder (ASD), who experience characteristic differences in neural oscillatory activity. In particular, we propose a novel SincNet-based neural network for detecting emotions in ASD patients using EEG signals. The learned filters can be easily inspected to detect which part of the EEG spectrum is used for predicting emotions. We found that our system automatically learns the high-α (9-13 Hz) and β (13-30 Hz) band suppression often present in individuals with ASD. This result is consistent with recent neuroscience studies on emotion recognition, which found an association between these band suppressions and the behavioral deficits observed in individuals with ASD. The improved interpretability of SincNet is achieved without sacrificing performance in emotion recognition.","2694-0604","978-1-7281-1179-7","10.1109/EMBC46164.2021.9630427","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9630427","","Deep learning;Band-pass filters;Emotion recognition;Neuroscience;Machine learning algorithms;Neural activity;Brain modeling","Autism Spectrum Disorder;Brain;Deep Learning;Electroencephalography;Emotions;Humans","14","","24","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"An Emotion Recognition Method Based on Selective Gated Recurrent Unit","Q. Yang; J. Zhou; C. Cheng; X. Wei; S. Chu","College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China",2018 IEEE International Conference on Progress in Informatics and Computing (PIC),"6 May 2019","2018","","","33","37","Electroencephalogram (EEG) signals can intuitively reflect the slight variations in human emotions. Consequently, they are the first choice for emotion recognition media. However, EEG signals at different time steps have different emotion representing abilities. By filtering out EEG signals with low representing abilities, the efficacy of extracted EEG features will increase. Thus emotion recognition accuracy can be improved. Therefore, a new feature extraction method called Selective Gated Recurrent Unit (SGRU) is proposed in this paper. From SGRU, we design a new method for emotion recognition. Firstly, SGRU is constructed to extract features from EEG signals. Secondly, a Fully Connected Neural Network (FCNN) is built to classify emotions with the features obtained by SGRU. Finally, the experiment results on DEAP dataset indicate that the method proposed can achieve better performance on emotion recognition compared with other similar methods.","","978-1-5386-7672-1","10.1109/PIC.2018.8706140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8706140","EEG signals;Gated Recurrent Unit;Feature Extraction;Emotion Recognition","Feature extraction;Electroencephalography;Emotion recognition;Logic gates;Neural networks;Training;Data mining","","3","","12","IEEE","6 May 2019","","","IEEE","IEEE Conferences"
"EEG Feature Selection for Emotion Recognition Based on Cross-subject Recursive Feature Elimination","W. Zhang; Z. Yin","Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai, P. R. China; Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai, P. R. China",2020 39th Chinese Control Conference (CCC),"9 Sep 2020","2020","","","6256","6261","The application of machine learning approaches to deal with the emotion recognition of physiological signals has received much attention on account of the objectivity of the electroencephalography (EEG) signals. However, the traditional feature selection methods are insufficient when building affective computing models between different subjects. In this paper, we propose a novel feature selection method termed as cross-subject recursive feature elimination (C-RFE) based on least square support vector machine to cope with this issue. This method is implemented through the absolute value of the component of the norm vector of the classification margin for all pairs of two subjects. The features are ranked in descending order of the importance via eliminating feature with the minimal contribution. The specific number of EEG feature subsets and emotion categories are operated in four machine learning models. The binary classification accuracy and F1-score of arousal and valence recognitions are achieved 0.6521, 0.6245, 0.6299 and 0.6295, respectively, for MAHNOB-HCI database, and 0.6461, 0.6176, 0.6529 and 0.6399, respectively for DEAP database.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9188573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9188573","Emotion recognition;machine learning;physiological signal;EEG;recursive feature elimination","Support vector machines;Emotion recognition;Databases;Computational modeling;Buildings;Machine learning;Feature extraction","","6","","18","","9 Sep 2020","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Model Using Windowing Techniques","K. K; D. A. Z; H. M. V; B. S. Begum","Department of Computer Science and Engineering, National Institute of Technology, Tiruchirappalli, Tamil Nadu, India; Department of Electronics and Communication Engineering, National Institute of Technology, Tiruchirappalli, Tamil Nadu, India; Department of Computer Science and Engineering, National Institute of Technology, Tiruchirappalli, Tamil Nadu, India; Department of Computer Science and Engineering, National Institute of Technology, Tiruchirappalli, Tamil Nadu, India",IECON 2023- 49th Annual Conference of the IEEE Industrial Electronics Society,"16 Nov 2023","2023","","","1","6","With increased human-machine interactions, computing systems are designed based on context and content for seamless user interactions. Nowadays, for efficient and personalized interactions, the user's emotional state plays a vital role in the design phase of the systems. Over the past decade, emotion recognition has garnered significant interest among researchers, with electroencephalogram (EEG) signals identified as a promising modality for distinguishing emotions. Among emotion recognition research, electroencephalogram (EEG) signals are found to be a potential modality to distinguish one's emotions. Several attempts have been made by researchers to increase the performance of the emotion recognition models. Hence, in this work, we proposed an EEG-based emotion recognition model that utilizes windowing techniques to classify emotions in the valence and arousal dimensions. We employed both non-overlapping and overlapping windowing techniques to segment the EEG signals. From these segmented EEG trials, a set of temporal and spatial features is extracted and used to train the classifier models. The proposed model achieves classification accuracies of 98.3 % and 98.2 % for valence and arousal dimensions, respectively. The experimental results and analysis reveal that the proposed model outperformed the state-of-the-art EEG-based emotion recognition studies on the AMIGOS dataset. Further, the proposed model can be employed in smart industries for effective human-machine interactions.","2577-1647","979-8-3503-3182-0","10.1109/IECON51785.2023.10312330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10312330","Emotion recognition;Electroencephalogram (EEG);Machine learning;AMIGOS dataset;Windowing technique","Industries;Human computer interaction;Industrial electronics;Emotion recognition;Analytical models;Electric potential;Machine learning","","","","20","IEEE","16 Nov 2023","","","IEEE","IEEE Conferences"
"A review on EEG based Emotion Analysis using Machine Learning approaches","T. Sharma; M. Diwakar; P. Singh; C. Arya; S. Lamba; P. Kumar","Department of CSE, Graphic Era Deemed to be University, Dehradun, India; Department of CSE, Graphic Era Deemed to be University, Dehradun, India; Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, India; Department of School of Computing, Graphic Era Hill University, Dehradun, Uttarakhand, India; Ajay Kumar Garg Engineering College; Department of CSE, Krishna Engineering College, Ghaziabad, UP, India","2021 IEEE 8th Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)","10 Jan 2022","2021","","","1","6","Brisk advancement of Machine Learning Algorithm with information fusion made the computers/machine able to understand human emotion, so that they can recognize and analyses the emotions. There are few ways to recognize human emotions from facial expression, behavior, speech and from physiological signals. This paper presents a review on emotion categorization, use of EEG signal for emotion recognition and use of ML techniques such as SVM, CNN, NB, LR and deep learning models such as CNN, RNN etc. for performing emotion recognition on different types of data such as visual data, audio data and text data. This paper also discusses the classical and modern methods used for processing EEG signal for emotion recognition.","2687-7767","978-1-6654-0962-9","10.1109/UPCON52273.2021.9667588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667588","Emotion recognition;EEG signals;emotion models;emotion categorization and Machine learning","Support vector machines;Deep learning;Emotion recognition;Visualization;Computational modeling;Speech recognition;Brain modeling","","3","","43","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Emotion Classification Using EEG Data in a Brain-Inspired Spiking Neural Network","Y. He; C. Li; X. Ju","College of Electronic and Information Engineering, Southwest University, Chongqing, China; College of Electronic and Information Engineering, Southwest University, Chongqing, China; College of Electronic and Information Engineering, Southwest University, Chongqing, China",2021 11th International Conference on Intelligent Control and Information Processing (ICICIP),"16 Dec 2021","2021","","","433","437","As an important application of emotion artificial intelligence, emotion classification provides the basis for the realization of affective brain-computer interface (aBCI). In this study, the NeuCube is used to learn and classify Electroencephalogram (EEG) data from the DEAP dataset. NeuCube is a type of spiking neural network (SNN) framework developed based on the real human brain. It is very suitable for analyzing and processing spatio-temporal data. Based on the 10-fold cross-validation method, we obtain a mean accuracy of 68.91 % in the emotional binary valence classification problem. Meanwhile, the EEG data recorded from F3 and F4 electrode channels provide more information compared with Fp1 and Fp2. The results prove that the spiking neural network can be applied to the task of emotion classification effectively.","","978-1-6654-2515-5","10.1109/ICICIP53388.2021.9642186","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9642186","emotion classification;EEG;spiking neural networks;NeuCube","Electrodes;Machine learning;Information processing;Electroencephalography;Brain-computer interfaces;Biology;Task analysis","","3","","19","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Dataset-Independent EEG Channel Selection for Emotion Recognition","S. Y. Dharia; S. G. Camorlinga; C. E. Valderrama; M. Hojjati","Applied Computer Science, University of Winnipeg, Winnipeg, Canada; Applied Computer Science, University of Winnipeg, Winnipeg, Canada; Applied Computer Science, University of Winnipeg, Winnipeg, Canada; Applied Computer Science, University of Winnipeg, Winnipeg, Canada",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","Electroencephalography (EEG) stands as a noninvasive and cost-effective method for recording neural activity, holding potential for applications such as identifying neural processes underlying human emotions. This paper delves into the transferability and generalizability of EEG channel selection in emotion recognition, adopting a dataset-independent approach. By leveraging Power Spectral Density (PSD), we identify high-contributing EEG channels in the SEED V dataset and validate our approach on the independent SEED IV dataset using a Convolutional Neural Network (CNN) model. The channel selection method helped in eliminating insignificant EEG channels, which can improve the applicability of developing more efficient EEG devices for daily use to monitor emotions, as well as in individuals suffering from various neurodegenerative diseases. Through extensive experiments varying the number of channels and features, our model achieves classification accuracies of 77.02%, 75.42%, 71.31%, and 64.31% with 62, 30, 20, and 10 EEG channels, accompanied by 310, 90, 60, and 30 Differential Entropy (DE) features respectively. Further, the proposed approach is tested by introducing Gaussian noise to the training set and evaluating its sensitivity to signal noise. Finally, results are compared with state-of-the-art models highlighting the potential of our dataset-independent channel selection method.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782444","Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782444","Emotion Recognition;Electroencephalography (EEG);EEG Channel Selection;Deep Learning","Training;Emotion recognition;Accuracy;Sensitivity;Computational modeling;Neural activity;Brain modeling;Electroencephalography;Recording;Convolutional neural networks","Electroencephalography;Humans;Emotions;Neural Networks, Computer;Signal Processing, Computer-Assisted;Algorithms;Entropy","","","17","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Multimodal Emotion Recognition based on Convolutional Neural Networks and Long Short-Term Memory Networks","Y. Liu; D. Geng; X. Wu; Y. Liu","School Of Mechanical And Electrical Engineering, Guilin University Of Electronic Technology, Guilin, China; School Of Mechanical And Electrical Engineering, Guilin University Of Electronic Technology, Guilin, China; School Of Mechanical And Electrical Engineering, Guilin University Of Electronic Technology, Guilin, China; School Of Mechanical And Electrical Engineering, Guilin University Of Electronic Technology, Guilin, China",2024 2nd International Conference on Signal Processing and Intelligent Computing (SPIC),"1 Oct 2024","2024","","","69","73","Due to the rapid advancements in big data and high-performance computing technologies, the application of deep learning models in the fields of computer vision and signal image processing has become increasingly widespread. To address the low efficiency and accuracy in the recognition of multimodal emotional EEG signals, this study integrates the latest deep learning network models, leveraging the advantages of convolutional neural networks (CNNs) in EEG feature extraction and the strengths of long short-term memory (LSTM) networks in processing dynamic temporal information. Therefore, this study employs CNN models to extract features from multimodal EEG signals, followed by feature vector fusion using LSTM, and classification using softmax. The network model constructed in this study achieved an accuracy of 92% in the three-class emotion recognition. Compared with traditional classification methods, the CNN-LSTM network model exhibits higher recognition efficiency due to its combined advantages in feature extraction and dynamic temporal information processing. Compared to using CNN or LSTM models alone, this model also shows a significant improvement in recognition accuracy.","","979-8-3503-6888-8","10.1109/SPIC62469.2024.10691536","Middle-aged and Young Teachers' Basic Ability Promotion Project of Guangxi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691536","Deep learning;multimodal signals;EEG emotion recognition;CNN-LSTM","Deep learning;Emotion recognition;Accuracy;Computational modeling;Brain modeling;Feature extraction;Electroencephalography;Vectors;Convolutional neural networks;Long short term memory","","","","16","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"DrivEmo: A Novel Approach for EEG-Based Emotion Classification for Drivers","T. A. Gamage; E. R. C. Sandamali; P. Kalansooriya","Department of Computer Science, General Sir John Kotelawala Defence University, Sri Lanka; Department of Computational Mathematics, General Sir John Kotelawala Defence University, Sri Lanka; Department of Computer Science, General Sir John Kotelawala Defence University, Sri Lanka",2023 International Research Conference on Smart Computing and Systems Engineering (SCSE),"17 Aug 2023","2023","6","","1","6","Electroencephalogram (EEG) based emotion recognition approaches have proven to be successful with the latest technologies, and therefore, driver emotion recognition is also being widely discussed for enhancing road safety. This paper reveals a unique approach to driver emotion recognition for the calm, fear, sad, and anger emotional states where calm is the desired state of mind while driving. Emotiv EPOC X 14 channel EEG headset is utilised for the EEG collection, and ten subjects are involved in the experiment. EEG preprocessing of the collected EEG data is done using the EEGLAB toolbox in Matlab. EEG feature extraction is performed using Matlab, and feature selection and classification model training is done using the Classification Learner app in Matlab. ANOVA and ReliefF are employed as the feature selection algorithms, and Support Vector Machine (SVM) and Naïve Bayes classifiers are utilised for the emotion classification. The outcomes reveal that the highest mean accuracy of 95% is achieved from the Coarse Gaussian SVM classifier, while the lowest mean accuracy of 85% is obtained from the Fine Gaussian SVM classifier detecting the calm, fear, sad, and anger emotional states. In addition, all the other trained classifier models have an accuracy between 85% and 95%. Therefore, the findings suggest that the proposed EEG-based implementation approach of an emotion classification model for drivers is highly successful and can be employed in future research in the paradigm of driver emotion recognition as well. Besides, this research presents a critical literature review concerning critical aspects of EEG-based emotion recognition research.","2613-8662","979-8-3503-4145-4","10.1109/SCSE59836.2023.10215028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215028","EEG;emotion recognition;feature extraction;feature selection;road safety","Support vector machines;Training;Emotion recognition;Feature extraction;Brain modeling;Electroencephalography;Mathematical models","","1","","48","IEEE","17 Aug 2023","","","IEEE","IEEE Conferences"
"Enhancing Emotion Detection through EEG Signal Processing with Machine Learning Techniques","D. Bura; M. Singh; P. Nandal; S. Jindal","Computer Science and Engineering, Manav Rachna International Institute of Research and Studies, Faridabad, India; Computer Science and Engineering, Manav Rachna International Institute of Research and Studies, Faridabad, India; Computer Science and Engineering, Manav Rachna International Institute of Research and Studies, Faridabad, India; Computer Science and Engineering, Manav Rachna International Institute of Research and Studies, Faridabad, India","2024 International Conference on Communication, Computing and Energy Efficient Technologies (I3CEET)","13 May 2025","2024","","","541","545","Brain-Computer Interfaces (BCIs) serve as a communication bridge between an individual's brain activity and external devices. Focused on noninvasive techniques, our study delves into various methods for feature selection and extraction from EEG signals to facilitate emotion detection. The paper investigates the acquisition and preprocessing of these signals, extracting essential characteristics to reduce dimensionality. Subsequently, machine learning algorithms, including Support Vector Machine (SVM), Linear Discriminant Analysis (LDA), Fuzzy Classifiers, Regression Trees, and k-Nearest Neighbor, are examined and compared in terms of complexity and performance.","","979-8-3315-4158-3","10.1109/I3CEET61722.2024.10993865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10993865","Machine Learning;Feature classification;Electroencephalography;Signal Processing","Support vector machines;Emotion recognition;Machine learning;Transforms;Signal processing;Nearest neighbor methods;Feature extraction;Electroencephalography;Linear discriminant analysis;Regression tree analysis","","","","20","IEEE","13 May 2025","","","IEEE","IEEE Conferences"
"An Emotion Recognition Scheme Based on EEG Signal and Visual Information","Y. Wang; L. Xin; Z. Liu; X. Li","Henan University, School of Computer and Information Engineering, Kaifeng City, Henan Province, China; Henan University, School of Computer and Information Engineering, Kaifeng City, Henan Province, China; Henan University, School of Computer and Information Engineering, Kaifeng City, Henan Province, China; Henan University, School of Computer and Information Engineering, Kaifeng City, Henan Province, China",2024 6th International Conference on Electronic Engineering and Informatics (EEI),"8 Oct 2024","2024","","","1423","1426","Comprehensive analysis of EEG signals and visual information provides a more precise method for identifying human emotional states. Utilizing discrete wavelet transform technology, key features in EEG signals can be effectively extracted, while feature category selection is conducted based on the visual content viewed by the tester. By combining the EEG signal features with the visual information features, a more comprehensive feature set is formed. Compared to traditional methods that rely solely on EEG signals for emotional recognition, the strategy of fusing EEG signals with visual information exhibits higher recognition accuracy. Experimental analysis results on the DEAP dataset and MAHNOB-HCI dataset show that, in terms of valence dimension, this scheme achieved the highest average recognition accuracy of 0.8573; in terms of arousal dimension, it achieved the highest average recognition accuracy of 0.7183, significantly outperforming the recognition performance using only EEG signals.","","979-8-3503-5359-4","10.1109/EEI63073.2024.10696613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696613","emotion recognition;discrete wavelet transform;EEG signal;machine learning","Support vector machines;Emotion recognition;Visualization;Accuracy;Nearest neighbor methods;Feature extraction;Electroencephalography;Stability analysis;Discrete wavelet transforms;Informatics","","","","14","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Deciphering Feelings: Exploring EEG Data for Human Emotion Recognition","P. Muthunayagom; A. C. Subhajini","Department of Computer Applications, Noorul Islam Centre for Higher Education, TamilNadu, India; Department of Computer Applications, Noorul Islam Centre for Higher Education, TamilNadu, India",2024 International Conference on Sustainable Communication Networks and Application (ICSCNA),"10 Feb 2025","2024","","","1181","1187","The ability to decipher human emotions through mind reading techniques has long been a subject of fascination, both in science fiction and scientific research. In recent years, advancements in neuroscience, artificial intelligence, and brain-computer interfaces have brought us closer to understanding and recognizing human emotions from brain activity. Recognizing and understanding human emotions has evolved significant attention in fields like psychology, technology, and healthcare. Electroencephalogram (EEG) signals, reflecting brain activity, offer valuable insights into the neural correlates of emotions. This paper reviews recent advancements in human emotion recognition from EEG signals, focusing on both deep learning and machine learning approaches. Deep learning methods, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), have shown promise in automatically extracting features from EEG data and accurately classifying emotions. This study explores methods to recognize human emotions using EEG data, focusing on the use of deep learning models to classify emotional states with high accuracy. Meanwhile, machine learning techniques such as support vector machines (SVMs) and ensemble methods have also been employed for emotion recognition tasks. Various studies utilizing different datasets and methodologies are discussed, highlighting the effectiveness of these approaches in emotion categorization. The review also discusses research gap like minimizing noise, extracting relevant features, and achieving real-time processing in EEG-based emotion recognition. It highlights potential avenues for future research in this area. The proposed approach offers insights into the applicability of EEG for real-time emotion analysis. Moreover, the review emphasizes the significance of EEG in deepening our comprehension of human emotions and its wide-ranging applications in fields such as human-computer interaction, healthcare, and education.","","979-8-3315-3001-3","10.1109/ICSCNA63714.2024.10863917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10863917","Brain-Computer Interface;Neurophysiological Emotion Analysis;Affective Computing;Electroencephalography;EEG Signal Processing;Emotion Recognition;Deep Learning","Deep learning;Human computer interaction;Emotion recognition;Accuracy;Reviews;Medical services;Feature extraction;Electroencephalography;Real-time systems;Data models","","","","21","IEEE","10 Feb 2025","","","IEEE","IEEE Conferences"
"EEG-based emotion classification using deep neural network","M. Sallout; E. Mattar","College of Engineering, University of Bahrain, Sukhair Campus, P.O. Box 32038, Bahrain; College of Engineering, University of Bahrain, Sukhair Campus, P.O. Box 32038, Bahrain",4th Smart Cities Symposium (SCS 2021),"9 May 2022","2021","2021","","325","332","The interest in automation and intelligent application increases day by day, especially with new challenges facing humanities and unexpected global situations such as COVID and other pandemics. Most of those applications have a shared focus on technology related to gathering data or applying reactions based on it, which we usually call IoT, robotics, or something in between. On the health part, heart and brain signals play a fundamental role in providing data to smart platforms that may utilize this information in many ways to identify and treat some health conditions. This paper will cover emotional health, which grabs more attention because of social distancing that impacts the morality of the human being on a large scale. However, emotion recognition has a limitation based on the nature of emotion varieties due to the objects' personal, cultural, and psychological differences. Many researchers reported different cheap and practical methodologies of analysing Electroencephalogram (EEG) signals to detect different insights of emotional states. This research will critically analyse multiple Deep Neural Network techniques, namely convolutional neural network (CNN) and recurrent neural network (RNN).","","978-1-83953-658-8","10.1049/icp.2022.0364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770717","","","","1","","","","9 May 2022","","","IET","IET Conferences"
"TWEN: EEG Emotion Recognition Model Based on Weakly Supervised Learning Framework with Two-Phase Multitask Autoencoder","T. Kim; C. Jin; S. -E. Kim","Department of Applied Artificial Intelligence, Seoul National University of Science and Technology, Seoul, South Korea; Department of Applied Artificial Intelligence, Seoul National University of Science and Technology, Seoul, South Korea; Department of Applied Artificial Intelligence, Seoul National University of Science and Technology, Seoul, South Korea",2024 15th International Conference on Information and Communication Technology Convergence (ICTC),"14 Jan 2025","2024","","","1494","1498","Emotions significantly influence human cognition, behavior, and social interactions, making accurate recognition essential in Human-Computer Interaction (HCI) applications. This study addresses challenges in EEG-based emotion recognition, particularly inter-subject variability and label noise, which hinder the development of robust and generalized models. We propose a robust Two-phase Weakly Supervised Emotion Network (TWEN), a novel deep learning model designed to enhance emotion recognition. TWEN incorporates a Two-phase Multitask Autoencoder to mitigate inter-subject variability and a Top-k Selection method to reduce label noise. The model captures both local and global temporal features of EEG signals through an innovative fusion of attention mechanisms, ensuring accurate classification of emotions over varying durations. Evaluations on the THU-EP dataset demonstrate that TWEN outperforms state-of-the-art models, achieving a classification accuracy of 60.8%, with a standard deviation of 4.07%.","2162-1241","979-8-3503-6463-7","10.1109/ICTC62082.2024.10827151","National Research Foundation of Korea; Ministry of Science and ICT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10827151","THU-EP;EEG;Emotion recognition;Weakly supervised learning;Two-phase Multitask Autoencoder","Deep learning;Emotion recognition;Accuracy;Weak supervision;Attention mechanisms;Autoencoders;Noise;Brain modeling;Feature extraction;Electroencephalography","","","","8","IEEE","14 Jan 2025","","","IEEE","IEEE Conferences"
"EEG-Based on Emotion Recognition Using Machine Learning","Y. Zhang","Huazhong Agricultural University, Wuhan, China",2023 IEEE International Conference on Image Processing and Computer Applications (ICIPCA),"27 Sep 2023","2023","","","226","232","Emotion is of great significance in human's daily interaction. Contemporarily, emotion recognition systems are beneficial in various areas, ranging from human-computer interaction (HCI) system to healthcare safety. So far, the Electroencephalogram (EEG) has received much attention because of its sensitivity to mood changes. However, the relationship hasn't been fully researched. To investigate the relationship among emotions, bands, channels and find a better classifier, this experiment organize the framework based on the DEAP dataset. After the data preprocessing, EEG data is reduced by principal component analysis (PCA). Then to create the emotion feature vectors, the discrete wavelet transform (DWT) is chosen as the parameter. Lastly, to categorize the EEG signal, support vector machine (SVM), random forest (RF), k-nearest neighbor (KNN), XGboost model is created. According to the experiment, this paper obtained the following relevant conclusions: 1) RF, SVM, KNN and XGboost classifier, performed with the accuracy of 77.01%, 80.44%, 66.75% and 77.75%, respectively; 2) In temperal and whole brain regions, gamma has the highest recognition ability, reaching 76.77% and 77.02%. The beta performs best in occipital, parietal, and central regions with 78.00%, 77.26%, and 78.00%, respectively; 3) The higher the frequency, the greater the oscillatory dynamics of brain activity on positive / negative emotions; 4) The AF3, AF4 area has a greater effect in reflecting emotions because of less irrelevant influences.","","979-8-3503-1467-0","10.1109/ICIPCA59209.2023.10257784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10257784","EEG;emotion recognition;DEAP;machine learning","Support vector machines;Human computer interaction;Emotion recognition;Sensitivity;Transforms;Brain modeling;Electroencephalography;Discrete wavelet transforms;Classification tree analysis;Principal component analysis","","","","11","IEEE","27 Sep 2023","","","IEEE","IEEE Conferences"
"Deep Neural Classifiers For Eeg-Based Emotion Recognition In Immersive Environments","J. Teo; J. T. Chia","Faculty of Computing & Informatics, Universiti Malaysia Sabah, Kota Kinabalu, Sabah, Malaysia; Faculty of Computing & Informatics, Universiti Malaysia Sabah, Kota Kinabalu, Sabah, Malaysia",2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE),"18 Nov 2018","2018","","","1","6","Emotion recognition has become a major endeavor in artificial general intelligence applications in recent years. Although significant progress has been made in emotion recognition for music, image and video stimuli, it remains largely unexplored for immersive virtual stimuli. Our main objective for this line of investigation is to enable consistently reliable emotion recognition for virtual reality stimuli using only cheap, commercial-off-the-shelf electroencephalography (EEG) headsets which have significantly less recording channels and far lower signal resolution commonly called “Wearable EEG” as opposed to medical-grade EEG headsets with the ultimate goal of applying EEG-based emotion prediction to procedurally-generated affective content such as immersive computer games and virtual learning environments through machine learning. Our prior preliminary study has found that the use of a 4-channel, 256-Hz was indeed able to perform the required emotion recognition tasks from VR stimuli albeit at classification rates of between 65-89% classification accuracy only using Support Vector Machines (SVMs) and K-Nearest Neighbor (KNN) classifiers. For this particular study, we attempt to improve the classification rates to above 95% by conducting a comprehensive investigation into the use of various deep neural-based learning architectures for this domain. By tuning the deep neural classifiers in terms of the number of hidden layers, number of hidden nodes and the nodal dropout ratio, the emotion prediction accuracy was able to be improved to over 96%. This shows the continued promise of the application of wearable EEG for emotion prediction as a cost-effective and userfriendly approach for consistent and reliable prediction deployment in virtual reality-related content and environments through deep learning approaches.","","978-1-5386-4838-4","10.1109/ICSCEE.2018.8538382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538382","electroencephalography;emotion recognition;machine learning;virtual reality;wearable EEG.","Electroencephalography;Virtual reality;Emotion recognition;Headphones;Electrodes;Feature extraction;Reliability","","6","","17","IEEE","18 Nov 2018","","","IEEE","IEEE Conferences"
"3DCANN: A Spatio-Temporal Convolution Attention Neural Network for EEG Emotion Recognition","S. Liu; X. Wang; L. Zhao; B. Li; W. Hu; J. Yu; Y. -D. Zhang","College of Electronic and Information Engineering, Hebei University, Machine Vision Engineering Research Center of Hebei Province, Baoding, China; College of Electronic and Information Engineering, Hebei University, Key Laboratory of Digital Medical Engineering of Hebei Province, Baoding, China; College of Electronic and Information Engineering, Hebei University, Key Laboratory of Digital Medical Engineering of Hebei Province, Baoding, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Interventional Ultrasound, PLA Medical College & Chinese PLA General Hospital, Beijing, China; School of Informatics, University of Leicester, Leicester, U.K.",IEEE Journal of Biomedical and Health Informatics,"10 Nov 2022","2022","26","11","5321","5331","Since electroencephalogram (EEG) signals can truly reflect human emotional state, emotion recognition based on EEG has turned into a critical branch in the field of artificial intelligence. Aiming at the disparity of EEG signals in various emotional states, we propose a new deep learning model named three-dimension convolution attention neural network (3DCANN) for EEG emotion recognition in this paper. The 3DCANN model is composed of spatio-temporal feature extraction module and EEG channel attention weight learning module, which can extract the dynamic relation well among multi-channel EEG signals and the internal spatial relation of multi-channel EEG signals during continuous period time. In this model, the spatio-temporal features are fused with the weights of dual attention learning, and the fused features are input into the softmax classifier for emotion classification. In addition, we utilize SJTU Emotion EEG Dataset (SEED) to appraise the feasibility and effectiveness of the proposed algorithm. Finally, experimental results display that the 3DCANN method has superior performance over the state-of-the-art models in EEG emotion recognition.","2168-2208","","10.1109/JBHI.2021.3083525","National Natural Science Foundation of China(grant numbers:61572063); Natural Science Foundation of Hebei Province(grant numbers:F2020201025,F2019201151); Science Research Project of Hebei Province(grant numbers:BJ2020030); Open Foundation of Guangdong Key Laboratory of Digital Signal and Image Processing(grant numbers:2020GDDSIPL-04); High-Performance Computing Center of HBU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440794","3D convolution attention neural network;dual attention learning;EEG emotion recognition;spatio-temporal feature","Electroencephalography;Feature extraction;Emotion recognition;Convolution;Brain modeling;Deep learning;Neural networks","Humans;Artificial Intelligence;Electroencephalography;Neural Networks, Computer;Emotions;Attention","89","","47","IEEE","25 May 2021","","","IEEE","IEEE Journals"
"Human Emotion Recognition from EEG-brain Signals using Enhanced Machine Learning Method","G. Ghous; S. Najam; A. Jalal","Dept. of Electrical & Computer Engineering, Riphah International University, Islamabad, Pakistan; Dept. of Electrical & Computer Engineering, Riphah International University, Islamabad, Pakistan; Dept. of Computer Science, Air University, Islamabad, Pakistan",2025 6th International Conference on Advancements in Computational Sciences (ICACS),"27 Mar 2025","2025","","","1","7","This study explores the application of advanced machine learning techniques to EEG data for detecting emotions in individuals with cognitive disabilities. Utilizing the SEED-IV dataset, we analyze EEG recordings from 15 participants across multiple sessions to identify emotional states such as happiness, sadness, fear, and neutrality. To ensure data quality, we employ preprocessing techniques including bandpass filtering and downsampling. Our proposed model integrates multi-class Support Vector Machine (SVM) with AAFST, an innovative feature selection and transformation mechanism. The effectiveness of this approach is demonstrated by an SVM accuracy of 85%, showcasing its capability to extract subtle emotional responses from EEG data. This research contributes to the growing field of affective computing, emphasizing the advantages of combining machine learning with EEG analysis to enhance the detection and understanding of emotions in individuals with cognitive disabilities.","2616-3330","979-8-3315-3303-8","10.1109/ICACS64902.2025.10937822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937822","EEG;emotion recognition;machine learning;Support Vector Machine (SVM);SEED-IV dataset","Support vector machines;Emotion recognition;Accuracy;Filtering;Machine learning;Feature extraction;Electroencephalography;Physiology;Recording;Emotional responses","","","","43","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"Hybridization of Wavelet Decomposition and Machine Learning for Brain Waves based Emotion Recognition","M. Ali; S. M. Qaisar; T. Anurulafchar","Electrical and Computer Engineering Department, Effat University, Jeddah, Saudi Arabia; Electrical and Computer Engineering Department, Effat University, Jeddah, Saudi Arabia; Computer science Department, Effat University, Jeddah, Saudi Arabia",2023 1st International Conference on Advanced Innovations in Smart Cities (ICAISC),"3 Apr 2023","2023","","","1","5","Emotion recognition has sparked the interest of researchers from a variety of disciplines. Studies have demonstrated that brain signals may be utilized to characterize a wide range of emotional states. Electroencephalogram (EEG) measures the cerebral activity. Therefore, by exploiting the EEG signals the emotion states can be determined. In this study the EEG signals undergoes through filtering, segmentation, Wavelet Packet Decomposition (WPD), feature mining, and classification. The machine learning algorithms used for classifications are “Decision Tree” (DT), “Support Vector Machine” (SVM), and K-Nearest Neighbor” (K-NN) algorithms are used for categorization. Their performance is compared for automatically identifying the emotion state. It is determined that the best performer is SVM. It has attained 98.2% accuracy, 97.3% precision, 97.3% recall, 98.7% specificity, 97.3% F1, 97.3% kappa, and 99.3% AUC.","","978-1-6654-7275-3","10.1109/ICAISC56366.2023.10085288","Effat University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10085288","Electroencephalography (EEG);Machine learning;Emotion categorization;Wavelet Decomposition;Feature extraction;Emotion recognition","Emotion recognition;Technological innovation;Machine learning algorithms;Smart cities;Signal processing algorithms;Support vector machine classification;Signal processing","","","","28","IEEE","3 Apr 2023","","","IEEE","IEEE Conferences"
"A Novel EEG-Based Emotion Recognition Framework Using DQN-DenseNet for Real-Time Applications","Y. Jia; S. Wang","Xinjiang College of Science and Technology, Korla, China; Shaanxi University of International Trade & Commerce, Xi'an, China","2024 IEEE 7th International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)","13 Feb 2025","2024","","","741","745","Emotion recognition using EEG signals has gained significant attention due to its potential applications in mental health diagnostics and human-computer interaction. However, existing algorithms often struggle with low accuracy and high computational time. This study introduces the DQN-DenseN et algorithm, which combines the sequential data processing capability of Deep Q-Learning Networks (DQN) with the efficient feature propagation of DenseNet. By leveraging EEG spectrograms processed with Fast Fourier Transform (FFT), the algorithm extracts and classifies features with enhanced accuracy. Experiments using the SEED dataset demonstrate that the proposed approach achieves a mean recognition accuracy of 99.04%, outperforming traditional CNN, DQN, and DenseNet models. The DQN-DenseNet algorithm offers improved adaptability and efficiency, paving the way for future advancements in EEG-based emotion recognition across dynamic real-world applications.","2831-4549","979-8-3503-7703-3","10.1109/AUTEEE62881.2024.10869652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10869652","EEG;emotion recognition;deep learning;DQN-DenseNet","Deep learning;Emotion recognition;Accuracy;Fast Fourier transforms;Heuristic algorithms;Mental health;Feature extraction;Electroencephalography;Classification algorithms;Streams","","","","25","IEEE","13 Feb 2025","","","IEEE","IEEE Conferences"
"A Review on EEG Based Emotion Classification","Y. Zhao; W. Zhao; C. Jin; Z. Chen","College of Information and Communication Engineering, Communication University of China, Beijing, China; College of Information and Communication Engineering, Communication University of China, Beijing, China; College of Information and Communication Engineering, Communication University of China, Beijing, China; College of Information and Communication Engineering, Communication University of China, Beijing, China","2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","13 Feb 2020","2019","","","1959","1963","In recent years, emotion recognition has broad application prospects in the development of artificial intelligence and human-computer interaction related design of human health detection, and has gradually developed into a new research field of interdisciplinary research. Researchers conduct expansion studies on different stimuli, different characteristics, and different classification models. This paper uses 40 articles from the past two years as a reference to systematically analyze the main algorithms and research trends of the emotional recognition technology in the process of EEG signal processing in 2017-2019, summarizes the innovation of the research, looks forward to the future development direction.","2381-0947","978-1-7281-1907-6","10.1109/IAEAC47372.2019.8997704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8997704","EEG;feature reduction;feature Extraction;emotion classification;Neural network","Feature extraction;Electroencephalography;Emotion recognition;Frequency-domain analysis;Brain modeling;Time-domain analysis;Wavelet transforms","","5","","42","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Recent Survey on Emotion Recognition Using Physiological Signals","E. Joy; R. B. Joseph; M. B. Lakshmi; W. Joseph; M. Rajeswari","Computer Science and Engineering Department, Sahrdaya College of Engineering and Technology, Thrissur, Kerala, India; Computer Science and Engineering Department, Sahrdaya College of Engineering and Technology, Thrissur, Kerala, India; Computer Science and Engineering Department, Sahrdaya College of Engineering and Technology, Thrissur, Kerala, India; Computer Science and Engineering Department, Sahrdaya College of Engineering and Technology, Thrissur, Kerala, India; Computer Science and Engineering Department, Sahrdaya College of Engineering and Technology, Thrissur, Kerala, India",2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS),"3 Jun 2021","2021","1","","1858","1863","Emotion Recognition has an important role in human-computer intercommunication, the medical field, etc. So that chance of Emotion Computing is gradually increased. Mainly six primary emotions are there anger, disgust, fear, happy, sad, and surprise. To recognize these emotions, many types of emotion recognition methods are available. But they are mainly attention to facial expressions, speech, and gestures. By using visible signs of emotions cannot acquire the actual emotions of people. To gain true emotions, Emotion Recognition using Physiological signals is becoming a crucial thing. The physiological signals mainly involve the electroencephalogram (EEG), electrocardiogram (ECG), galvanic skin response (GSR), etc. This can obtain real-time emotions at any time. This paper discusses different methods used to recognize the emotion using physiological signals by machine learning techniques, data acquisition methods, and their databases. It also includes the experimental results and the accuracy of each method. By this mainly focus to build a machine without emotions can attain brilliance.","2575-7288","978-1-6654-0521-8","10.1109/ICACCS51430.2021.9441999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9441999","Emotion Recognition;Feature extraction;Machine learning;Physiological signals;support vector machine (SVM)","Support vector machines;Emotion recognition;Databases;Data acquisition;Machine learning;Electrocardiography;Physiology","","9","","17","IEEE","3 Jun 2021","","","IEEE","IEEE Conferences"
"Fourier–Bessel Domain Adaptive Wavelet Transform-Based Method for Emotion Identification From EEG Signals","A. Nalwaya; R. B. Pachori","Department of Electrical Engineering, Indian Institute of Technology Indore, Indore, India; Department of Electrical Engineering, Indian Institute of Technology Indore, Indore, India",IEEE Sensors Letters,"11 Jan 2024","2024","8","2","1","4","The letter presents a novel approach for analyzing a multi-sensor EEG signal with the aim of accurately identifying the emotional state of individuals. Identifying multiple classes of emotion using nonstationary EEG signals with good accuracy and efficiency is still an issue to address. The Fourier–Bessel domain adaptive wavelet transform is used to decompose EEG signals into various modes or components. To analyze the dynamics of modes Lyapunov exponent-based features are extracted from each mode. To classify feature values among different emotional classes namely, happy, sad, fear, and neutral, machine learning models have been used. To evaluate the performance of the proposed framework, EEG signals are recorded using ten distinct scalp sensors. EEG signals of 39 (20 males and 19 females) subjects were recorded. The proposed framework achieves an average classification accuracy of 96.91%. By incorporating emotion identification, human–system interaction can greatly enhance the user experience, as it improves engagement and contextual relevance.","2475-1472","","10.1109/LSENS.2023.3347648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375087","Sensor signal processing;affective computing;electroencephalogram (EEG);emotion recognition;machine learning (ML)","Electroencephalography;Feature extraction;Sensors;Wavelet transforms;Affective computing;Emotion recognition;Machine learning;Signal processing","","4","","22","IEEE","27 Dec 2023","","","IEEE","IEEE Journals"
"Multi feature fusion EEG emotion recognition","G. Guodong; G. Yahan","Medical Instrumentation College, University of Shanghai for Science and Technology, Shanghai; Medical Instrumentation College, Shanghai University of Medicine Health Sciences, Shanghai",2021 7th International Conference on Big Data and Information Analytics (BigDIA),"6 Dec 2021","2021","","","280","284","In recent years, the research on emotion recognition of EEG signals has attracted much attention. It is an important task to realize the advanced stage of artificial intelligence. How to realize real-time and efficient human-computer interaction has become an important direction of EEG signal research. This study aims to improve the accuracy of emotion recognition of EEG signals, and proposes a binary classification a nd emotion EEG recognition method based on feature fusion is carried out after multi feature extraction to improve the recognition rate. For the preprocessed EEG signals, the eigenvalues extracted from time-frequency, spatial domain, nonlinear dynamics and convolution neural network are used as the initial eigenvectors, and the dimension is reduced by principal component analysis. Finally, the long short-term memory neural network is used for classification. T he emotion recognition experiment w as carried out on the EEG emotion data set deap. The accuracy of two classification i n pleasure a nd arousal w as 8 4.42% a nd 85.61% respectively. The recognition rate is higher than that under single feature and other combined features. The experimental results show that compared with single feature extraction, multi feature fusion has better characteristics of emotional EEG signals, and high classification accuracy c an b e achieved b y using t he long short-term memory neural network. The performance of the emotion recognition method of EEG signals proposed in this paper is better than other methods based on traditional artificial design features and SVM or DBM, It is verified that the method proposed in this paper is feasible.","","978-1-6654-2466-0","10.1109/BigDIA53151.2021.9619674","Natural Science Foundation of Shanghai(grant numbers:21ZR1428300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619674","Multi feature fusion;EEG;Feature extraction;LSTM;Emotion recognition","Support vector machines;Emotion recognition;Time-frequency analysis;Neural networks;Feature extraction;Electroencephalography;Real-time systems","","2","","18","IEEE","6 Dec 2021","","","IEEE","IEEE Conferences"
"Subject-generic EEG feature selection for emotion classification via transfer recursive feature elimination","Y. Zhong; Z. Jianhua","Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai, China; Department of Automation, East China University of Science and Technology, Shanghai, China",2017 36th Chinese Control Conference (CCC),"11 Sep 2017","2017","","","11005","11010","The machine-learning based data-mining approaches become increasingly attractive in EEG-data-based human emotion recognition since the physiological data possesses the objectivity. In particular, those learning principles can effectively model emotion classifiers via heterogeneous features. By exploring the exciting literature, the subject-specific emotion estimator has a significant disadvantage, i.e., it induces additional burdens to a single user because of the preparation of long-time, multiple-session EEG data as reliable training sets. In this paper, we propose a new subject generic EEG feature selection method called transfer recursive feature elimination (T-RFE), to find the optimal feature subset that consists of the most robust EEG markers of stable distributions among multiple training subjects and a single testing subject. We adopt the DEAP database to validate the effectiveness of the T-RFE algorithm. The subject-generic emotion classification paradigm is also constructed and investigated. By implementing a linear least square support vector machine model, the T-RFE performance is compared with several conventional feature selection methods. We also found the statistical significance in the improvement of the classification accuracy. The classification rate and F-score outperform several recent reported works on the same database.","1934-1768","978-988-15639-3-4","10.23919/ChiCC.2017.8029114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029114","Emotion recognition;affective computing;physiological signals;recursive feature elimination;EEG","","","12","","21","","11 Sep 2017","","","IEEE","IEEE Conferences"
"Face and Emotion Recognition using Deep Learning with Convolutional Neural Networks in Industry Application","D. S. O; S. K. S; E. B. Edwin; S. P; M. R. Thanka; V. Ebenezer","Division of CSE, Karunya Institute of Technology and Sciences, Coimbatore, Tamil Nadu, India; Division of CSE, Karunya Institute of Technology and Sciences, Coimbatore, Tamil Nadu, India; Division of CSE, Karunya Institute of Technology and Sciences, Coimbatore, Tamil Nadu, India; Division of DS&CS, Karunya Institute of Technology and Sciences, Coimbatore, Tamil Nadu, India; Division of CSE, Karunya Institute of Technology and Sciences, Coimbatore, Tamil Nadu, India; Division of DS&CS, Karunya Institute of Technology and Sciences, Coimbatore, Tamil Nadu, India",2024 3rd International Conference on Sentiment Analysis and Deep Learning (ICSADL),"25 Jul 2024","2024","","","438","443","Today's industry areas have more employees since the field is currently the most popular in the world. Everyone wants to be employed in one of the well-known industries and contribute to humanity's advancement by developing as many new things as possible. However, all of these activities and jobs impose both mental and physical stress. Employees who are unable to maintain their health need to take rest and avoid working extra hours. However, there are many ways in which people may work in a stable environment while still progressing and maintaining a healthy work-life balance. The concept of emotion recognition is not new, and it is applied in a variety of industries. This study analyzes how it helps in the industrial environment. This research investigates how emotion detection may be applied in MNCs and other sectors and discusses how the proposed CNN model helps in the classification of emotions. Here, the emotion is captured and detected using OpenCV, and an Email is sent to the identified recipient over SMTP based on the emotion recognized. Thus, the proposed approach has the potential to benefit people.","","979-8-3503-9615-7","10.1109/ICSADL61749.2024.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601460","Deep Learning;Image Classification;smtplib;CNN;TensorFlow;Keras;OpenCV;Mimetext;Jupyter Notebook","Deep learning;Industries;Emotion recognition;Analytical models;Sentiment analysis;Solids;Data models","","","","13","IEEE","25 Jul 2024","","","IEEE","IEEE Conferences"
"Classification of emotion from EEG using hybrid radial basis function networks with elitist PSO","Sreeshakthy M.; Preethi J.","Dept. of CSE, Anna University Regional Centre, Coimbatore, India; Dept. of CSE, Anna University Regional Centre, Coimbatore, India",2015 IEEE 9th International Conference on Intelligent Systems and Control (ISCO),"1 Oct 2015","2015","","","1","4","Emotion play an important role at several activities in the present world. Human decision making, cognitive process and interaction between human & machine all the activities depends on human emotions. Facial expression, musical activities and several approaches used to find the human emotions. In this paper EEG is used to find the accurate emotion. Emotion classification is the huge task. Classification of the human emotion is a process that merges the feature selection and provides the class labels for the data. The proposed work has four stages which include preprocessing, feature extraction, feature selection and classification. This paper uses a Radial Basis Function Network with trained by Evolution algorithm and particle Swarm Optimization is used to select the particular features in the feature selection process. The result of the network will classify the human emotions into arousal and valence emotion. Based on the classification, different emotion level accuracy has to be validated.","","978-1-4799-6480-2","10.1109/ISCO.2015.7282340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7282340","EEG;Classification;Particle Swarm Optimization;Radial Basis Function Network;Arousal and Valence emotion","Feature extraction;Electroencephalography;Radial basis function networks;Accuracy;Computer architecture;Conferences;Classification algorithms","","10","","24","IEEE","1 Oct 2015","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Optimized Deep-Learning Techniques","A. K. Dwivedi; O. P. Verma; S. Taran","Department of Electronics and Communication, Delhi Technological University Rohini, New Delhi, India; Department of Electronics and Communication, Delhi Technological University Rohini, New Delhi, India; Department of Electronics and Communication, Delhi Technological University Rohini, New Delhi, India",2024 11th International Conference on Signal Processing and Integrated Networks (SPIN),"9 May 2024","2024","","","372","377","Emotions are crucial in identifying the current mental state. Electroencephalography (EEG) signals can accurately determine the current mental state. The automatic identification and analysis of human emotions using EEG data is vital in the treatment of psychiatric diseases due to the influence that emotions have on interactions, interpretations, and decisions. However, one major problem with EEG recorders is their low spatial resolution. EEG signals are nonlinear and complicated. Advanced signal processing techniques are required to analyze and extract useful characteristics. Emotion recognition using EEG can improve human-machine interactions. A convolutional neural network (CNN) is suggested in this article for automatic feature extraction and categorization of emotions. First, EEG signals are converted into pictures by applying time-frequency representation techniques. Following this, these pictures are fed into CNN and optimized CNN models for training. The optimized CNN model produces better accuracy for classifying emotions. Deep learning (DL) techniques are popular for determining complex patterns. Deep learning methods have shown good results in EEG-based emotion identification. It can automatically extract high-level information. DL’s performance depends on hyperparameters. However, optimizing the performance of a deep-learning model requires accurate hyperparameter tuning. The manual hyperparameter tuning method is tiresome, costly, computationally expensive, and time-consuming. As a result, an automated method is required to determine the best hyperparameters to maximize DL’s efficacy. To automate the hyperparameter optimization process, the proposed work presents a novel framework based on Bayesian optimization. Bayesian optimization (BO) is used to automate hyperparameter selection to predict emotional states.","2688-769X","979-8-3503-0843-3","10.1109/SPIN60856.2024.10512074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10512074","EEG;Emotion Recognition;Deep-Learning Methods;Bayesian Optimization","Deep learning;Time-frequency analysis;Emotion recognition;Computational modeling;Brain modeling;Feature extraction;Electroencephalography","","3","","31","IEEE","9 May 2024","","","IEEE","IEEE Conferences"
"EMD-Based Feature Extraction Toward Real-Time Fear Emotion Recognition Application Using EEG","S. Ishizuka; Y. Kurebayashi; Y. Tobe","Department of Integrated Information Technology, College of Science and Technology, Aoyama Gakuin University, Sagamihara, Kanagawa, Japan; Department of Integrated Information Technology, College of Science and Technology, Aoyama Gakuin University, Sagamihara, Kanagawa, Japan; Department of Integrated Information Technology, College of Science and Technology, Aoyama Gakuin University, Sagamihara, Kanagawa, Japan",2024 IEEE/SICE International Symposium on System Integration (SII),"9 Feb 2024","2024","","","278","283","In recent years, many researchers have shown interests in EEG-based emotion recognition for the application of Brain Computer Interface devices. Therefore, this study investigates the applicability of Empirical Mode Decomposition (EMD)-based feature extraction method for real-time EEG fear emotion recognition. In this study, instead of relying on publicly available datasets such as the DEAP dataset, the EEG data are collected independently by utilizing video clips available on the Internet to elicit fearful emotions. The algorithm mainly consists of two parts: feature extraction and fear emotion recognition. In the feature extraction stage, the acquired EEG signals are divided into five seconds segments and decomposed into several Intrinsic Mode Functions (IMFs) using EMD. Subsequently, the mean and Differential Entropy are extracted from the first five IMFs. These features are then classified by Support Vector Machine. To investigate the applicability of EMD, the EMD-based feature extraction method is compared to conventional methods, namely Short-time Fourier Transform and Wavelet Transform. As a result, the EMD-based method has demonstrated superior accuracy in both subject-dependent and subject-independent classification compared to the other two methods.","2474-2325","979-8-3503-1207-2","10.1109/SII58957.2024.10417245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10417245","","Support vector machines;Emotion recognition;System integration;Streaming media;Feature extraction;Electroencephalography;Real-time systems","","","","23","IEEE","9 Feb 2024","","","IEEE","IEEE Conferences"
"An Efficient Approach to EEG-Based Emotion Recognition using LSTM Network","Anubhav; D. Nath; M. Singh; D. Sethia; D. Kalra; S. Indu","Department of Computer Science and Engineering, Delhi Technological University, Delhi, India; Department of Computer Science and Engineering, Delhi Technological University, Delhi, India; Department of Computer Science and Engineering, Delhi Technological University, Delhi, India; Department of Computer Science and Engineering, Delhi Technological University, Delhi, India; Department of Electronics and Communication Engineering, Delhi Technological University, Delhi, India; Department of Electronics and Communication Engineering, Delhi Technological University, Delhi, India",2020 16th IEEE International Colloquium on Signal Processing & Its Applications (CSPA),"16 Apr 2020","2020","","","88","92","This work aims to investigate the performance of the Long Short-Term Memory (LSTM) Model for EEG-Based Emotion Recognition. For the experimentation, we use the publicly available DEAP dataset, which consists of preprocessed EEG and physiological signals. Our work limits itself to the study of only the EEG signals to have a scope for developing an efficient headgear model for real-time monitoring of emotions. In this study, we extract the band power, a frequency-domain feature, from the EEG signals and compare the classification accuracies for Valence and Arousal domain for different classifiers. The proposed Long Short-Term Memory (LSTM) model achieves the best classification accuracy of 94.69% and 93.13% for Valence and Arousal scales, respectively, illustrating a significant average increment of 16% in valence and 18% in arousal in comparison to other classifiers.","","978-1-7281-5310-0","10.1109/CSPA48992.2020.9068691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068691","EEG Data;Emotion;Emotion Recognition;DEAP dataset;Band power;LSTM Network","Brain modeling;Electroencephalography;Emotion recognition;Feature extraction;Frequency-domain analysis;Support vector machines;Signal processing","","24","","19","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Mental Stress Detection using EEG and Recurrent Deep Learning","A. Patel; D. Nariani; A. Rai","School of Engineering and Applied Science, Ahmedabad University, Ahmedabad, Gujarat, India; School of Engineering and Applied Science, Ahmedabad University, Ahmedabad, Gujarat, India; School of Engineering and Applied Science, Ahmedabad University, Ahmedabad, Gujarat, India",2023 IEEE Applied Sensing Conference (APSCON),"17 Apr 2023","2023","","","1","3","Mental stress is a major health problem and affects the individual’s capability to perform in day-to-day life. This paper proposes a novel deep-learning (DL)-based-artificial intelligence (AI)-approach that uses electroencephalogram (EEG) data to build an emotional stress state detection model. The EEG data are first processed to extract time and frequency-domain features, which are then supplied to train DL algorithms and identify emotional stress state. Three different DL methods, namely, one-dimensional convolutional neural network (CONVID), bidirectional long-short term memory network (BiLSTM) and bidirectional gated recurrent unit (BiGRU) networks are considered for constructing stressdetection models. The proposed approach is validated using the benchmark dataset-Database for Emotion Analysis using Physiological Signals (DEAP) available freely in the public domain. The DEAP dataset consists of EEG data of 32 participants recorded by exposing them to 40 one-minute-long expressive music video samples along with the respective emotion-ratings. The results showed that among the different developed DL models, the CONVlD+BiLSTM provided the highest emotion detection accuracy of 88.03 % and outperformed the conventional shallow learning approaches.","","978-1-6654-6163-4","10.1109/APSCON56343.2023.10100977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10100977","Mental Stress;Emotion classification;EEG;Deep learning;Bi LSTM;BiGRU","Radio frequency;Emotion recognition;Human factors;Brain modeling;Feature extraction;Prediction algorithms;Electroencephalography","","6","","11","IEEE","17 Apr 2023","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Classification Using Wavelet Decomposition and K-Nearest Neighbor","A. E. Putra; C. Atmaji; F. Ghaleb","Dept. of Computer Science and Electronics, Universitas Gadjah Mada, Yogyakarta, Indonesia; Dept. of Computer Science and Electronics, Universitas Gadjah Mada, Yogyakarta, Indonesia; Electronics and Instrumentations Study Program, Universitates Gadjah Mada, Yogyakarta, Indonesia",2018 4th International Conference on Science and Technology (ICST),"11 Nov 2018","2018","","","1","4","The following topics are dealt with: regression analysis; feature extraction; Global Positioning System; computational fluid dynamics; statistical analysis; remote sensing; support vector machines; organisational aspects; geophysical image processing; learning (artificial intelligence).","","978-1-5386-5813-0","10.1109/ICSTC.2018.8528652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528652","k-NN;EEG;emotion classification","Electroencephalography;Feature extraction;Hidden Markov models;Error analysis;Training;Emotion recognition;Correlation","","10","","13","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Exploring day-to-day variability in EEG-based emotion classification","Y. -P. Lin; T. -P. Jung","Institute for Neural Computation and Institute of Engineering in Medicine, University of California San Diego, La Jolla, USA; Institute for Neural Computation and Institute of Engineering in Medicine, University of California San Diego, La Jolla, USA","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","4 Dec 2014","2014","","","2226","2229","The research of electroencephalography (EEG)-based emotion classification has gained much popularity in the past few years. Researchers continue to seek an optimal machine learning-based pipeline to characterize the associations between complex spatio-spectral EEG dynamics and implicit emotional responses. However, toward a real-life application, addressing the inherent day-to-day variability in EEG signals is also of urgent importance, yet was less concerned in the literature. This study explored the day-to-day EEG variability and tested the feasibility of developing an emotion-classification pipeline that can account for such variability. The empirical results of this study showed that the use of a proper feature extraction, e.g., band-power asymmetries over the fronto-posterior regions, in conjunction with an effective artifact removal method, e.g., independent component analysis, could alleviate the impacts of inter-day variability and improve the classification performance.","1062-922X","978-1-4799-3840-7","10.1109/SMC.2014.6974255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974255","EEG-based emotion classification;independent component analysis;day-to-day variability","Electroencephalography;Feature extraction;Brain modeling;Pipelines;Independent component analysis;Training data;Music","","9","","13","IEEE","4 Dec 2014","","","IEEE","IEEE Conferences"
"AGCN-SAT: Adaptive Graph Convolutional Network with Spatial Attention and Transformer for EEG Emotion Recognition","B. Qian; Z. Qian; H. Liang; Q. Luo; L. Xu","Department of Mechanical Engineering, East China University of Science and Technology, Shanghai, China; Department of Mechanical Engineering, East China University of Science and Technology, Shanghai, China; Department of Mechanical Engineering, East China University of Science and Technology, Shanghai, China; Department of Mechanical Engineering, East China University of Science and Technology, Shanghai, China; Department of Clinical Medical Engineering, Anting Hospital, Shanghai, China","2023 IEEE 3rd International Conference on Electronic Technology, Communication and Information (ICETCI)","17 Jul 2023","2023","","","418","423","EEG is the most commonly used input signal in emotion recognition tasks. Since EEG has topological structure, using graph convolutional network is a more appropriate approach. However, how to capture the dynamic changes of dependency information between EEG channels in different emotional states is still a challenge. In addition, graph convolution only captures the functional connectivity relationships between local channels, and misses many useful global spatial information. To solve the above problems, this paper proposes an Adaptive Graph Convolutional Network with Spatial Attention and Transformer (AGCN-SAT) for EEG emotion recognition. The model constructs an adaptive learning adjacency matrix to facilitate better mining of local spatial features by the graph convolutional network. Meanwhile, the Transformer module is applied to extract global spatial features and concatenate them with local spatial features to form the complete spatial features. Also, spatial attention mechanism is introduced to make model focus more attention on EEG channels related to target emotion, so as to obtain more discriminative features. We conduct experiments on SEED dataset, and the experimental results show that the proposed method achieves excellent performance.","","979-8-3503-9841-0","10.1109/ICETCI57876.2023.10176392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10176392","Emotion Recognition;Graph Convolutional Network;Transformer;Spatial Attention;Adaptive Learning","Emotion recognition;Adaptation models;Adaptive learning;Adaptive systems;Convolution;Feature extraction;Transformers","","1","","10","IEEE","17 Jul 2023","","","IEEE","IEEE Conferences"
"Emotion Recognition Based on EEG and DE-CNN-RNN","Q. Zhao; Y. Dong; W. Yin","College of Engineering, Qufu Normal University, Rizhao, China; College of Engineering, Qufu Normal University, Rizhao, China; School of Electrical and Electronic Engineering, University of Manchester, Manchester, UK",2023 China Automation Congress (CAC),"19 Mar 2024","2023","","","3376","3380","Convolutional neural networks (CNNs) can extract local spatial features from data well, but it cannot capture long-term dependencies contained in serial data. Recurrent neural networks (RNNs) can compensate for this defect. In the study, according to the differential entropy (DE) feature of electroencephalogram (EEG), DE-CNN-RNN model is proposed to explore emotion recognition based on EEG signals. Experimental tests were conducted with the SEED emotion dataset to compare the accuracy of emotion classification of DE-CNN, DE-RNN, CNN-RNN, and DE-CNN-RNN models. The proposed DE-CNN-RNN model could improve the accuracy of emotion classification by 10% and effectively identify emotion types.","2688-0938","979-8-3503-0375-9","10.1109/CAC59555.2023.10450238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450238","emotion recognition;EEG;CNN;RNN;DE","Emotion recognition;Recurrent neural networks;Feature extraction;Brain modeling;Electroencephalography;Spatial databases;Convolutional neural networks","","1","","16","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"A WeChat Mini-program System with LSTM for The Emotional EEG Signal Recognition","L. Wang; X. Deng; X. Lv; K. Liu; Q. Yang; C. Long","Chongqing University of Posts and Telecommunications, China; Chongqing University of Posts and Telecommunications, College of Computer Science and Technology, Chongqing, China; Chongqing University of Posts and Telecommunications, China; Chongqing University of Posts and Telecommunications, College of Computer Science and Technology, Chongqing, China; Chongqing University of Posts and Telecommunications, China; Chongqing University of Posts and Telecommunications, China",2020 2nd International Conference on Industrial Artificial Intelligence (IAI),"30 Nov 2020","2020","","","1","5","As one of the advanced functions for human being, the emotion has a great influence on people's personality and mental health. EEG serves as a rapid measure method for neural signals that becomes an important way to evaluate different emotions. Some traditional machine learning techniques do not take into account the crucial temporal dynamic information in the EEG signals. However, with the recursive structure in time, the long and short time memory (LSTM) network in deep learning technology can solve this problem well. In this paper, a LSTM is designed and trained well to classify the emotional EEG, and then a WeChat mini-program system is constructed. The mini-program system incorporates with the LSTM to perform the EEG preprocessing, feature extraction, emotion classifying, and user management functions and so on. It can give feedback to the users about the emotional changes degree of pleasure and sobriety according to their EEG, which could serve as the emotion inspector as well as the entertainment tool for personal use.","","978-1-7281-8216-2","10.1109/IAI50351.2020.9262189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9262189","","Electroencephalography;Feature extraction;Brain modeling;Videos;Social networking (online);Message service;Emotion recognition","","1","","15","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"A method of EEG emotion recognition based on residual contraction network","N. Su; F. Liu","School of Information Science and Engineering, Shenyang Ligong University, Shen Yang, China; School of Information Science and Engineering, Shenyang Ligong University, Shen Yang, China",2024 2nd International Conference on Signal Processing and Intelligent Computing (SPIC),"1 Oct 2024","2024","","","141","144","The emotional constantly affects human body, mind and behavior. The accurate recognition of emotion has a broad application prospect in the medical field. Although deep learning algorithms have made some achievements in emotion recognition research, there are still some problems such as insufficient extraction of EEG (Electroencephalogram) frequency features, spatial features and time features, noise interference and complex model parameters. In this thesis, the 4D-RSC-BiLSTM (Four-Dimensional Residual Shrinkage Convolutional Bidirectional Long Short Term Memory Neural Network) is constructed, and the convolutional layer is inserted into the deep separable convolutional network to complete the improvement of network structure and parameter adjustment. Then the residual shrinkage module is combined with the improved deep separable convolutional network to remove redundant features and reduce the network computing cost while solving the problem of model gradient disappearance. The experimental results show that the proposed model is feasible in emotion recognition.","","979-8-3503-6888-8","10.1109/SPIC62469.2024.10691605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691605","EEG signal;Residual shrinkage module;Emotion recognition","Deep learning;Emotion recognition;Costs;Convolution;Computational modeling;Signal processing algorithms;Feature extraction;Brain modeling;Electroencephalography;Convolutional neural networks","","","","10","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"Comparison of EEG- Based Deep Neural Network Classifiers for Emotion Recognition using Selected Electrodes","A. N. A. Gul; A. Altuntas","Department of Mechatronics Engineering, Faculty of Technology, Sakarya University of Applied Sciences, Sakarya, Turkey; Department of Mechatronics Engineering, Faculty of Technology, Sakarya University of Applied Sciences, Sakarya, Turkey",2023 Medical Technologies Congress (TIPTEKNO),"19 Dec 2023","2023","","","1","4","In this study, SVM and DNN models were employed to classify participants’ emotional states using the publicly available SEED dataset, achieving impressive accuracy rates of 79.8% for DNN and 79.4% for SVM, surpassing the accuracies of previous models. Furthermore, our electrode reduction study, which optimized electrode placement by focusing on emotionally relevant brain regions, yielded high accuracy. The use of EL further enhanced model accuracy to 81.3%. These classification results suggest that future research with more extensive datasets could lead to even more robust models. Consequently, this work has the potential to provide guidance for personalized recommendations in areas such as music, movies, books, and other choices tailored to individuals’ emotional states.","2687-7783","979-8-3503-2896-7","10.1109/TIPTEKNO59875.2023.10359196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10359196","EEG;emotion recognition;machine learning;deep learning;electrode reduction;ensemble learning","Electrodes;Support vector machines;Emotion recognition;Electric potential;Focusing;Artificial neural networks;Propulsion","","","","14","IEEE","19 Dec 2023","","","IEEE","IEEE Conferences"
"DFSMDA: A Domain Adaptation Algorithm with Domain Feature Extraction for EEG Emotion Recognition","X. Wu; X. Ju; S. Dai; M. Li","College of Intelligence Science and Technology, National University of Defense Technology, NUDT, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, NUDT, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, NUDT, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, NUDT, Changsha, China","2024 4th International Conference on Artificial Intelligence, Virtual Reality and Visualization","4 Feb 2025","2024","","","120","124","In the realm of EEG-based emotion recognition, individual differences present significant challenges for cross-subject recognition, making it an essential area of research. While many existing methods utilize unsupervised multi-source domain adaptation techniques to mitigate these differences, they often fall short in achieving effective recognition performance. To alleviate this issue, this paper introduces a domain feature-aware semi-supervised multi-source domain adaptation emotion recognition model, named DFSMDA. The proposed approach employs a multi-branch network to extract both domain-invariant and domain-specific features while applying domain adaptation methods to minimize distribution discrepancies between source and target domains. Additionally, during training, we utilize a limited number of labeled samples from the target domain to enhance model suitability for recognition tasks. We validated the model through cross-subject emotion recognition experiments on the publicly available SEED dataset. The results demonstrate that our method achieves outstanding performance, with an average recognition accuracy of 92.76% across three sessions.","","979-8-3315-2874-4","10.1109/AIVRV63595.2024.10860141","Natural Science Foundation of China(grant numbers:62076248); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10860141","electroencephalogram;domain adaptation;semi-supervised;emotion recognition","Training;Emotion recognition;Solid modeling;Adaptation models;Visualization;Accuracy;Virtual reality;Feature extraction;Brain modeling;Electroencephalography","","","","20","IEEE","4 Feb 2025","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition on Unstable Patterns over Time","G. Wu; S. Qin; Y. Luo; Q. Fu; J. Zeng; S. Yang; J. Liu","Guangxi Key Lab of Brain-inspired Computing and Intelligent Chips, School of Electronic and Information Engineering, Guangxi Normal University, Guilin, China; Guangxi Key Lab of Brain-inspired Computing and Intelligent Chips, School of Electronic and Information Engineering, Guangxi Normal University, Guilin, China; Guangxi Key Lab of Brain-inspired Computing and Intelligent Chips, School of Electronic and Information Engineering, Guangxi Normal University, Guilin, China; Guangxi Key Lab of Brain-inspired Computing and Intelligent Chips, School of Electronic and Information Engineering, Guangxi Normal University, Guilin, China; Guangxi Key Lab of Brain-inspired Computing and Intelligent Chips, School of Electronic and Information Engineering, Guangxi Normal University, Guilin, China; Department of Computer Science, Swansea University, UK; Guangxi Key Lab of Brain-inspired Computing and Intelligent Chips, School of Electronic and Information Engineering, Guangxi Normal University, Guilin, China",2024 International Symposium on Digital Home (ISDH),"5 Mar 2025","2024","","","227","234","In brain-computer interface (BCI) systems, users' emotion can be recognized by using electroencephalography (EEG) data. Recent researches proposed different methods for feature extraction and EEG-based emotion classification. However, EEG data collected over time is not always stable and accuracies of emotion recognition are found not robust. In this paper, a novel EEG emotion recognition system is proposed by combining ReliefF for extracting features, and Long Short-Term Memory Network coupled with Support Vector Machine for classification. A public EEG emotion dataset of SEED which contains three different recordings of EEG data is used in the experiments. It is shown that the proposed network is effective for mitigating the phenomenon of unstable accuracies, and achieves higher emotion classification accuracies compared to other approaches using the same dataset.","2769-8823","979-8-3315-0987-3","10.1109/ISDH64927.2024.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10907988","Brain-Computer Interface;Emotion Classification;Deep Learning","Support vector machines;Emotion recognition;Accuracy;Feature extraction;Electroencephalography;Brain-computer interfaces;Recording;Classification algorithms;Long short term memory;Biological neural networks","","","","40","IEEE","5 Mar 2025","","","IEEE","IEEE Conferences"
"From Data Exploration to Classification: A Framework for EEG-Based Emotion Detection","S. I. Ali; R. Betala; M. Z. I. Khan; S. K. Gupta","University of Technology and Applied Sciences, Al Musannah, Oman; University of Technology and Applied Sciences, Al Musannah, Oman; University of Technology and Applied Sciences, Nizwa, Oman; Government Polytechnic Daman, Daman, India",2024 International Conference on Emerging Trends in Networks and Computer Communications (ETNCC),"4 Dec 2024","2024","","","1","6","Emotion detection from electroencephalogram (EEG) signals holds immense potential for understanding human affective states and has wide-ranging applications in various domains. This implementation-based research paper presents a comprehensive approach to EEG-based emotion detection. Utilizing the EEG Brainwave Dataset, comprising positive, negative, and neutral emotions, our methodology encompasses exploratory analyses and advanced preprocessing techniques. Incorporating progressive preprocessing steps enhances the quality and reliability of EEG data. We leverage ensemble learning methodologies, including random forest, support vector machine, and k-nearest neighbours classifiers, to classify emotional states from EEG signals. By leveraging ensemble learning techniques, we overcome the limitations of individual classifiers and achieve superior classification accuracy, underscoring the effectiveness of our approach. Our study focuses on accuracy assessment and decision boundary visualization to shed light on the discriminative features utilized by the classifiers. The high classification accuracy achieved underscores the effectiveness of our approach, offering a holistic framework for EEG-based emotion detection with implications for mental health assessment, personalized learning environments, and human-computer interaction.","","979-8-3503-5326-6","10.1109/ETNCC63262.2024.10767480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767480","Electroencephalogram;Emotion Classification;Data Exploration;Ensemble Learning;Feature Analysis;Exploratory Data Analysis;Decision Boundaries","Support vector machines;Human computer interaction;Emotion recognition;Accuracy;Mental health;Market research;Electroencephalography;Ensemble learning;Reliability;Random forests","","","","35","IEEE","4 Dec 2024","","","IEEE","IEEE Conferences"
"PNN for EEG-based Emotion Recognition","Jianhai Zhang; Ming Chen; Sanqing Hu; Yu Cao; R. Kozma","College of Computer Science, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou, China; Department of Computer Science, The University of Massachusetts, Lowell, MA, USA; Center for Large-Scale Intelligent Optimization and Networks, University of Memphis, Memphis, USA","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","9 Feb 2017","2016","","","002319","002323","The effort to integrate emotions into human-computer interaction (HCI) system has attracted broad attentions. Automatic emotion recognition enables the HCI to become more intelligent and user friendly. Although numerous studies have been performed in this field, emotion recognition is still an extremely challenging task, especially in real-world practice usage. In this work, probabilistic neural network (PNN), with advantage of simple, efficient, and easy to train, was employed to recognize emotions elicited by watching music videos from scalp EEG. The publicly available DEAP emotion database was used to validate our algorithms. The powers of 4 frequency bands of EEG were extracted as features. The results show that the mean classification accuracy of PNN is 81.21% for valence(≥5 and <;5) and 81.26% for arousal(≥5 and <;5) across 32 subjects, similar with the results of SVM. In addition, they demonstrate that higher frequency bands (beta and gamma) play more important role in emotion classification than lower ones (theta and alpha). For the purpose of practical emotion recognition system, we proposed a ReliefF-based channel selection algorithm to reduce the number of used channels for convenience in practical usage. The results show that while using PNN, the 98% of the maximum classification accuracy can be obtained with only 9 (for valence) and 8 (for arousal) best channels, however, 19 (for valence) and 14 (for arousal) channels are needed while using SVM.","","978-1-5090-1897-0","10.1109/SMC.2016.7844584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844584","Emotion Recognition;Electroencephalogram (EEG);Probabilistic Neural Network (PNN);ReliefF;Channel Selection","Emotion recognition;Electroencephalography;Feature extraction;Support vector machines;Neurons;Training;Probabilistic logic","","24","","19","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"A Novel Solution for EEG-based Emotion Recognition","Z. Xie; M. Zhou; H. Sun","School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China",2021 IEEE 21st International Conference on Communication Technology (ICCT),"4 Jan 2022","2021","","","1134","1138","Electroencephalogram (EEG) is widely utilized in emotion recognition because of its exceptional stability and high detection accuracy. However, large amounts of labeled EEG data are difficult to come by. Self-supervised representation learning with multi-transformation tasks is presented as an innovative solution for emotion recognition. The solution consists of two tasks: self-supervised representation learning and emotion recognition. Self-supervised learning is applied to learn high-level EEG representation from unlabeled data. Representation learning contains six different transformations to learn the high-level EEG representations comprehensively: noising, scaling, negating, horizontally flipping, permuting, and time-warping. Then the self-supervised network can recognize different EEG representations, after that the weights of convolutional layers are frozen and transferred to the emotion recognition network, and the ability to distinguish EEG is transferred too. This is the first work that self-supervised learning that has been used for emotion recognition using EEG signals to the best of our knowledge. The accuracy we achieved is 98.64% that higher than all known fully supervised methods, and self-supervised learning saves a tremendous amount of time for labeling data. This result is state-of-the-art until now. Our experiments prove that the application of self-supervised learning in EEG-based emotion recognition is feasible and effective.","2576-7828","978-1-6654-3206-1","10.1109/ICCT52962.2021.9657922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9657922","EEG;Emotion Recognition;Self-supervised Learning;Transfer Learning","Representation learning;Training;Emotion recognition;Convolution;Conferences;Manuals;Feature extraction","","5","","19","IEEE","4 Jan 2022","","","IEEE","IEEE Conferences"
"Convolutional Neural Network Approach for Eeg-Based Emotion Recognition Using Brain Connectivity and its Spatial Information","S. -E. Moon; S. Jang; J. -S. Lee","School of Integrated Technology, Yonsei University, Republic of Korea; School of Integrated Technology, Yonsei University, Republic of Korea; School of Integrated Technology, Yonsei University, Republic of Korea","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","2556","2560","Emotion recognition based on electroencephalography (EEG) has received attention as a way to implement human-centric services. However, there is still much room for improvement, particularly in terms of the recognition accuracy. In this paper, we propose a novel deep learning approach using convolutional neural networks (CNNs) for EEG-based emotion recognition. In particular, we employ brain connectivity features that have not been used with deep learning models in previous studies, which can account for synchronous activations of different brain regions. In addition, we develop a method to effectively capture asymmetric brain activity patterns that are important for emotion recognition. Experimental results confirm the effectiveness of our approach.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8461315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461315","electroencephalography (EEG);convolutional neural network (CNN);brain connectivity","Electroencephalography;Electrodes;Emotion recognition;Feature extraction;Machine learning;Brain modeling;Convolution","","92","","16","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"A Wearable BTE-EEG Embedded Device for Emotion Monitoring with Quantum Machine Learning","N. -D. Mai; H. Barki; W. -Y. Chung","Dept. Artificial Intelligence Convergence, Pukyong National University, Busan, Korea; Dept. Artificial Intelligence Convergence, Pukyong National University, Busan, Korea; Dept. Artificial Intelligence Convergence, Pukyong National University, Busan, Korea",2024 Tenth International Conference on Communications and Electronics (ICCE),"21 Aug 2024","2024","","","562","566","Emotional disorders present significant challenges in both mental health research and clinical practice, underscoring the critical need for effective recognition and intervention strategies. This study introduces a groundbreaking approach that employs a behind-the-ear (BTE) EEG system for emotion recognition, utilizing quantum machine learning techniques. The research encompasses a comprehensive design framework addressing both hardware and software aspects of the BTE-EEG emotion recognition system for data acquisition and monitoring. By utilizing BTE-EEG technology, this study aims to improve comfort, mobility, and signal quality compared to traditional scalp EEGs, thus enabling continuous monitoring in real-world settings. Furthermore, the integration of quantum computing and machine learning represents a paradigm shift in emotion recognition and cognitive tasks, offering exponential speedups and superior pattern recognition capabilities compared to classical approaches. The findings underscore the efficacy of the proposed BTE EEG system in accurately detecting emotional states (positive and negative). This advancement shows promise for enhancing our understanding of emotional disorders and improving clinical interventions tailored to individual needs.","2836-4392","979-8-3503-7979-2","10.1109/ICCE62051.2024.10634608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10634608","Emotion recognition;Ear-EEG;Quantum machine learning;Variational quantum classifier","Emotion recognition;Quantum computing;Scalp;Machine learning;Electroencephalography;Software;Hardware","","","","16","IEEE","21 Aug 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition Using Time-frequency Analysis of EEG Signals and Machine Learning","J. Zhang; P. Chen; S. Nichele; A. Yazidi","Department of Computer Science, Oslo Metropolitan University, Oslo, Norway; Department of Automation, East China U. of Sci. & Tech., Shanghai, P.R. China; Department of Computer Science, Oslo Metropolitan University, Oslo, Norway; Department of Computer Science, Oslo Metropolitan University, Oslo, Norway",2019 IEEE Symposium Series on Computational Intelligence (SSCI),"20 Feb 2020","2019","","","404","409","In recent years, emotion recognition has drawn intense interest from researchers in various fields. Because of their intrinsic correlation with emotions, physiological signals based emotion recognition method is objective and insusceptible to intentional disguise of the subject under study. In particular, electroencephalogram (EEG) signals are known to be responsive and sensitive to variations in emotional state. In this paper, a 4- class emotion classification problem is investigated. Firstly, a clustering algorithm is used to determine the target class of each emotion-related data. Then wavelet analysis is used to extract features from 32-channel EEG signals. Finally, we compare five feature dimensionality reduction or feature selection algorithms and four types of machine learning based classifiers. The comprehensive comparative results show the effectiveness of the combination of EEG feature dimensionality reduction algorithm and random forest (RF) for multi-class emotion recognition problem under study.","","978-1-7281-2485-8","10.1109/SSCI44817.2019.9003057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003057","emotion recognition;affective computing;electroencephalogram (EEG);wavelet transform;dimensionality reduction;machine learning","Handheld computers;Electroencephalography;Feature extraction;Machine learning;Computational intelligence;Affective computing;Wavelet transforms","","8","","29","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"EEG-based subject-dependent emotion recognition algorithm using fractal dimension","Y. Liu; O. Sourina","Fraunhofer IDM@NTU, Nanyang Technological University, Singapore; Fraunhofer IDM@NTU, Nanyang Technological University, Singapore","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","4 Dec 2014","2014","","","3166","3171","In this paper, a real-time Electroencephalogram (EEG)-based emotion recognition algorithm using Higuchi Fractal Dimension (FD) Spectrum is proposed. As EEG is a nonlinear and multi-fractal signal, its FD spectrum can give a better understanding of the nonlinear property of EEG. Three values are selected from the whole spectrum and are combined with the other features such as statistical and Higher Order Crossings ones. The Support Vector Machine is used as the classifier. The proposed algorithm is validated on both benchmark database DEAP with video stimuli and our own dataset which used visual stimuli to evoke emotions. Up to 8 emotions can be recognized with only 4 channels. The experiment analysis results show that using FD spectrum features it is possible to improve classification accuracy.","1062-922X","978-1-4799-3840-7","10.1109/SMC.2014.6974415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974415","EEG;affective computing;emotion recognition;valence-arousal-dominance emotion model;Emotiv Epoch","Electroencephalography;Emotion recognition;Fractals;Accuracy;Classification algorithms;Feature extraction;Databases","","57","","44","IEEE","4 Dec 2014","","","IEEE","IEEE Conferences"
"EEG-based Emotion Detection Using Unsupervised Transfer Learning","H. A. Gonzalez; J. Yoo; I. M. Elfadel","Chair for Highly-Parallel VLSI-Systems and Neuromorphic Circuits, Technische Universität Dresden, Dresden, Germany; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore; Department of Electrical and Computer Engineering, Khalifa University, Abu Dhabi, United Arab Emirates",2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"7 Oct 2019","2019","","","694","697","Emotion classification using EEG signal processing has the potential of significantly improving the social integration of patients suffering from neurological disorders such as Amyotrophic Lateral Sclerosis (ALS) or the acute stages of Alzheimer's disease. One important challenge to the implementation of high-fidelity emotion recognition systems is the inadequacy of EEG data in terms of Signal-to-noise ratio (SNR), duration, and subject-to-subject variability. In this paper, we present a novel, integrated framework for semi-generic emotion detection using (1) independent component analysis for EEG preprocessing, (2) EEG subject clustering by unsupervised learning, and (3) a convolutional neural network (CNN) for EEG-based emotion recognition. The training and testing data was built using the combination of two publicly available repositories (DEAP and DREAMER), and a local dataset collected at Khalifa University using the standard International Affective Picture System (IAPS). The CNN classifier with the proposed transfer learning approach achieves an average accuracy of 70.26% for valence and 72.42% for arousal, which are superior to the reported accuracies of all generic (subject-independent) emotion classifiers.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8857248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8857248","","Electroencephalography;Training;Task analysis;Manuals;Unsupervised learning;Feature extraction","Arousal;Electroencephalography;Emotions;Humans;Machine Learning;Signal Processing, Computer-Assisted","31","","13","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"Emotion Recognition from EEG with Normalized Mutual Information and Convolutional Neural Network","M. A. Maria; M. A. H. Akhand; T. Shimamura","Department of Computer Science and Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Graduate School of Science and Engineeringment, Saitama University, Saitama, Japan",2022 12th International Conference on Electrical and Computer Engineering (ICECE),"4 Apr 2023","2022","","","372","375","Emotion is the fundamental trait of human beings, and brain signals are prospectus for emotion recognition (ER). Electroencephalography (EEG) is a preferable brain signal for ER as it is non-invasive, fast, portable and easy to use. Automatic ER from EEG is a challenging computational intelligence or machine learning task due to the inherited complexity of EEG signals. The aim of this study is to analyze different sub-bands (e.g., Alpha, Beta, and Gamma) of the EEG signals to find the most appropriate band to classify emotions using deep learning. At a glance, this study investigated main EEG signals (i.e., full frequency spectrum) and its sub-bands for connectivity feature map (CFM) construction using normalized mutual information (NMI); and convolutional neural network (CNN) was used for emotion classification from NMI CFMs. Experimental results identified that NMI CFMs from Gamma band showed relatively better emotion classification accuracy by CNN than CFMs with other bands. Finally, the proposed ER method with NMI and CNN is revealed as a potential EEG based ER method showing better than or competitive to existing related methods.","2771-7917","979-8-3503-9879-3","10.1109/ICECE57408.2022.10088920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10088920","convolutional neural network;emotion;electroencephalography;normalized mutual information","Deep learning;Emotion recognition;Computational modeling;Feature extraction;Electroencephalography;Complexity theory;Convolutional neural networks","","1","","20","IEEE","4 Apr 2023","","","IEEE","IEEE Conferences"
"WeDea: A New EEG-Based Framework for Emotion Recognition","S. -H. Kim; H. -J. Yang; N. A. T. Nguyen; S. K. Prabhakar; S. -W. Lee","Department of Brain and Cognitive Engineering, Korea University, Anam-dong, Seongbuk-ku, Seoul, Korea; Department of Computer Science, Chonnam National University, Gwangju, Korea; Department of Faculty of Information Technology, University of DaNang-University of Science and Education, Đà Năng, Vietnam; Department of Brain and Cognitive Engineering, Korea University, Anam-dong, Seongbuk-ku, Seoul, Korea; Department of Artificial Intelligence, Korea University, Anam-dong, Seongbuk-ku, Seoul, Korea",IEEE Journal of Biomedical and Health Informatics,"17 Jan 2022","2022","26","1","264","275","With the development of sensing technologies and machine learning, techniques that can identify emotions and inner states of a human through physiological signals, known as electroencephalography (EEG), have been actively developed and applied to various domains, such as automobiles, robotics, healthcare, and customer-support services. Thus, the demand for acquiring and analyzing EEG signals in real-time is increasing. In this paper, we aimed to acquire a new EEG dataset based on the discrete emotion theory, termed as WeDea (Wireless-based eeg Data for emotion analysis), and propose a new combination for WeDea analysis. For the collected WeDea dataset, we used video clips as emotional stimulants that were selected by 15 volunteers. Consequently, WeDea is a multi-way dataset measured while 30 subjects are watching the selected 79 video clips under five different emotional states using a convenient portable headset device. Furthermore, we designed a framework for recognizing human emotional state using this new database. The practical results for different types of emotions have proven that WeDea is a promising resource for emotion analysis and can be applied to the field of neuroscience.","2168-2208","","10.1109/JBHI.2021.3091187","National Research Foundation of Korea(grant numbers:NRF-2018R1A2B6006046); National Foundation for Science and Technology Development(grant numbers:102.01-2020.27); National Research Foundation of Korea(grant numbers:NRF-2020R1A4A1019191); Department of Artificial Intelligence, Korea University(grant numbers:2019-0-00079); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462344","Electroencephalography;emotion recognition;wireless devices;artifact removal;feature extraction;deep learning","Electroencephalography;Emotion recognition;Feature extraction;Wireless communication;Electrodes;Physiology;Wireless sensor networks","Databases, Factual;Electroencephalography;Emotions;Humans;Machine Learning","33","","50","IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"Emotion Classification Using Ensemble of Convolutional Neural Networks and Support Vector Machine","A. Mishra; A. Singh; P. Ranjan; A. Ujlayan","Amity School of Engineering & Technology, Amity University Uttar Pradesh, Noida, India; Amity School of Engineering & Technology, Amity University Uttar Pradesh, Noida, India; School of Engineering & Applied Sciences, SRM University, Amarawathi, A. P, India; School of Vocational Studies and Applied Sciences, Gautam Buddha University, Greater Noida, India",2020 7th International Conference on Signal Processing and Integrated Networks (SPIN),"20 Apr 2020","2020","","","1006","1010","This paper presents an ensemble of convolutional neural networks (CNNs) and support vector machine (SVM) for classifying emotions from electroencephalogram (EEG) patterns. We used popular deep learning models for feature extraction and a support vector machine classifier is employed to classify the EEG patterns into suitable emotion classes. The main contribution of this work is to investigate on the following points: creating an ensemble of pre-trained deep learning networks with support vector machine classifier (SVM) for classifying emotional states of person for single and multiple emotional attributes. Finding out the best ensemble network, extracting suitable layer and robust features to improve the classification accuracy of support vector machine and finally to compare the performance of ensemble of networks with stand-alone deep learning networks. Two popular convolutional neural networks are used for experiments: Alex Net and GoogLeNet. All experiments are carried out on database for emotion analysis using physiological signals (DEAP). A thorough analysis of experimental results revealed that classification accuracy of 87.5% is achieved by ensemble of Alex Net and SVM for single attribute (valance) classification while for two attributes (arousal and valance) the accuracy achieved is 62.5%. Similarly, accuracy of 100% and 62.5% are achieved for single and two attributes classification respectively using ensemble of GoogLeNet and SVM.","2688-769X","978-1-7281-5475-6","10.1109/SPIN48934.2020.9071399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9071399","emotion classification;EEG;convolutional neural network;deep learning;affect classification","Feature extraction;Support vector machines;Machine learning;Electroencephalography;Convolutional neural networks;Training;Physiology","","6","","20","IEEE","20 Apr 2020","","","IEEE","IEEE Conferences"
"Analysis of EEG Signals in the DEAP Dataset for Emotion Recognition using Deep Learning Algortihms","R. D. Gaddanakeri; M. M. Naik; S. Kulkarni; P. Patil","Electrical and Electronics Engineering, KLE Technological University, Hubballi, India; Electrical and Electronics Engineering, KLE Technological University, Hubballi, India; MCA Department, KLE Technological University, Hubballi, India; MCA Department, KLE Technological University, Hubballi, India",2024 IEEE 9th International Conference for Convergence in Technology (I2CT),"10 Jun 2024","2024","","","1","7","Emotions are the behavioral responses representing mental state of a person. It is crucial to recognize the emotions of a person for human-computer interaction, to understand and respond to one’s mental health. EEG signal provides a clear-sighted analysis of emotional state. It is a challenging task to recognize the patterns of multi channel EEG signals for emotion recognition using traditional Machine Learning approach. So, the Deep Learning algorithms are preferred more as they are successful in learning features and patterns to classify the the data. In this paper we have proposed Deep Learning models, CNN and LSTM Neural Networks to recognize the emotional states: Arousal, Valence and Dominance for the subjects 01-22 whose frontal videos were recorded and 23-32 whose frontal videos were non recorded in the DEAP dataset. CNN and LSTM Neural Networks are built with significant layers and the model is trained with required number of epochs and batch size after a number of trials to improve the accuracy of the emotion recognition. Finally, performance of the proposed models is evaluated with different evaluation metrics and the accuracy of both the Deep Learning models are compared for the subjects 01-22 and 23-32.","","979-8-3503-9447-4","10.1109/I2CT61223.2024.10543369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543369","Emotion Recognition;EEG Signals;DEAP dataset;CNN (Convolutional Neural Network);LSTM (Long Short-Term Memory)","Deep learning;Emotion recognition;Neural networks;Brain modeling;Electroencephalography;Convolutional neural networks;Task analysis","","1","","24","IEEE","10 Jun 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition and Understanding Using EEG Data in A Brain-Inspired Spiking Neural Network Architecture","W. Alzhrani; M. Doborjeh; Z. Doborjeh; N. Kasabov","School of Engineering, Computing and Mathematical Sciences, Auckland University of Technology (AUT), Auckland, New Zealand; School of Engineering, Computing and Mathematical Sciences, Auckland University of Technology (AUT), Auckland, New Zealand; Audiology Department, School of Population Health, The University of Auckland, Faculty of Medical and Health Sciences, New Zealand; School of Engineering, Computing and Mathematical Sciences, Auckland University of Technology (AUT), Auckland, New Zealand",2021 International Joint Conference on Neural Networks (IJCNN),"20 Sep 2021","2021","","","1","9","This paper is in the scope of emotion recognition by employing a recurrent spiking neural network (BI-SNN) architecture for modelling, mapping, learning, classifying, visualising, and understanding of spatio-temporal Electroencephalogram (EEG) data related to different emotional states. It further explores, develops, and applies a methodology based on the NeuCube BI-SNN, that includes methods for EEG data encoding, data mapping into a 3-dimensional BI-SNN model, unsupervised learning using spike-timing dependent plasticity (STDP) rule, spike-driven supervised learning, output classification, network analysis, and model visualisation and interpretation. The research conducted to model different emotional subtypes through mapping both space (brain regions) and time (brain dynamics) components of EEG brain data into SNN architecture. Here, a benchmark EEG dataset was used to design an empirical study that consisted of different experiments for classification of emotions. The obtained accuracy of 94.83% for EEG classification of four types of emotions was superior when compared with traditional machine learning techniques. The BI-SNN models not only detected the brain activity patterns related to positive and negative emotions with a high accuracy, but also revealed new knowledge about the brain areas activated in relation to different emotions. The research confirmed that neural activation increased in the frontal sites of brain (F7, F3, AF4) associated with positive emotions, while in the case of the negative emotions, connectivity strength was concentrated in the frontal (F4, AF3, F7, F8) and parietal sites of the brain (P7, P8).","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533368","EEG;Emotion recognition;Classification;Spiking Neural Networks","Emotion recognition;Analytical models;Supervised learning;Data visualization;Machine learning;Brain modeling;Electroencephalography","","6","","62","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"A Review on Face Emotion Recognition using EEG Features and Facial Features","S. A. Bhosale; S. R. Chougule","Department of Electronics Engineering, KITCOE Research Center, Shivaji Univesity, Kolhapur, India; Department of Electronics Engineering, KITCOE Research Center, Shivaji Univesity, Kolhapur, India",2023 1st International Conference on Cognitive Computing and Engineering Education (ICCCEE),"12 Feb 2024","2023","","","1","5","Emotions recognition using feature extraction from face, Speech and EEG signal have become emerging research field. It has contributed in different research areas like safety, biomedical sector, industrial automation and Human Computer Interface. Deep Learning approach using Convolution Neural Networks (CNN) has provided better results in obtaining an accuracy in Facial Emotion Recognition. Researcher are trying to get better results by using a novel approach along with CNN’s to obtain better results. This paper will help the researchers to understand the novel approach in Deep Learning used for Facial Emotion Recognition (FER) with different techniques and Electroencephalogram (EEG) based emotion recognition classified on basis of Valence, Arousal and Dominance with different datasets like DEAP (Database of Emotion Analysis using Physiological signals), SEED (SJTU Emotion EEG Dataset). The review provided use of different dataset and the accuracy of models while using those dataset along with the novel approaches in Emotion Recognition. This paper also reviews different algorithms, architectures and recent work carried by different researchers.","","979-8-3503-3280-3","10.1109/ICCCEE55951.2023.10424432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10424432","Facial Emotion Recognition;CNN;Deep Learning;EEG and database","Deep learning;Emotion recognition;Databases;Face recognition;Computer architecture;Electroencephalography;Facial features","","1","","39","IEEE","12 Feb 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Classification and Personal Response Optimization Using SVM","R. Pandey; S. Barde","Computer Science Maharaja Agrasen International College, Raipur, India; CSE PIET, Parul University, Vadodara (G.J.), India",2025 International Conference on Ambient Intelligence in Health Care (ICAIHC),"14 Apr 2025","2025","","","1","7","Humans experience a range of emotions during connection and communication, and these feelings differ in complexity, meaning, and intensity. Although behavioral emotions can also be expressed through body language, tone of voice, facial expressions, and other means, it can be challenging to identify the emotions in people who are unable to do so. In these cases, brain signals can be used to identify the emotions. In this research, we used a signal that the human brain creates based on an emotion's state to classify the emotions. To identify emotions, we employed EEG Eight frequency band signals. Different emotions, including joyful, sad, angry, fear, surprise, neutral, and disgust, were explained and recognized through expressions. by playing five minutes of these seven Indian films that focus on emotions, measuring the intensity of each emotion using brain signals, and comparing the results to the subject's remark. For this, we selected 35 Indian movie clips based on seven emotions and 50 individuals between the ages of 18 and 50. In order to improve accuracy, we used two techniques for emotion detection: one for answering the questions and another for classifying brain signals using support vector machines.","","979-8-3315-3025-9","10.1109/ICAIHC64101.2025.10957206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10957206","Emotion;EEG;SVM;Classifier","Support vector machines;Emotion recognition;Accuracy;Films;Medical services;Motion pictures;Electroencephalography;Complexity theory;Ambient intelligence;Optimization","","","","13","IEEE","14 Apr 2025","","","IEEE","IEEE Conferences"
"A Novel Deep Learning based Improved Cluster based Region Classifier Algorithm to Recognize and Categorize Emotions using EEG Signals","G. K. Chakravarthy; M. Suchithra","Department of CT, SRM Institute of Science and Technology, Kattankulathur, Tamil Nadu, India; Department of CT, SRM Institute of Science and Technology, Kattankulathur, Tamil Nadu, India",2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS),"8 Jun 2023","2023","","","242","248","Nowadays deep learning plays vital role in emotion recognition. It distinguishes emotions as easy or multi-models for visual capturing. This works to provide an automatic version for identifying feelings primarily based on EEG signals. The proposed version specializes in developing an effective model, which combines the basic ranges of EEG signal handling and feature extraction. A system is developed based on Independent component analysis (ICA) algorithm to overcome the recognition task which removes noise object and to extract the independent components, for the obtaining components. The channels were selected based on the threshold average activity value. K-Nearest Neighbor(KNN) and Artificial Neural Network (ANN) are used to categorize emotional states and extracted the features, together with the unconventional improved Cluster-based region Classifier (ICBRC). Based on EEG signals Average recognition rate up to 94% for three emotional states and 95% for binary states can be achieved with this system.","2768-5330","979-8-3503-9725-3","10.1109/ICICCS56967.2023.10142369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10142369","Independent component analysis (ICA);electroencephalogram (EEG);Deep Learning;Artificial Neural Network;ICBRC","Deep learning;Emotion recognition;Visualization;Independent component analysis;Artificial neural networks;Feature extraction;Data processing","","","","24","IEEE","8 Jun 2023","","","IEEE","IEEE Conferences"
"A Multidimensional Feature Extraction Method Based on MSTBN and EEMD-WPT for Emotion Recognition from EEG Signals","S. Zhang; Q. Zhang","College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Technology, Hainan University, Haikou, China",2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"2 Jan 2023","2022","","","2042","2048","Emotion recognition is an important component of human-computer interaction (HCI) systems. However, current emotion recognition methods have some drawbacks such as inconsistency in brain network size, lack of effective mining of features in different dimensions. In this paper, we propose a multidimensional feature extraction method based on MSTBN and EEMD-WPT for emotion recognition. Firstly, the wavelet packet transform (WPT) is utilized to decompose the pre-processed electroencephalography (EEG) signals into four frequency bands ($\theta,\alpha,\beta$, and $\gamma$), and phase locking value (PLV) is used to construct multi-band connectivity matrix. Secondly, to remove redundant information, the minimum spanning tree based brain network (MSTBN) is established and MSTBN features are extracted including global features and local features. Thirdly, ensemble empirical mode decomposition (EEMD) and WPT (EEMD-WPT) are applied to EEG signals for a more refined decomposition of modes and bands. Then, the modified multi-scale sample entropy (MMSE) and fractal dimension (FD) are extracted to capture the neural activity processes in the brain. Finally, the MSTBN features are fused with the nonlinear features MMSE and FD, which are input into random forest (RF) to identify emotions. Experimental results on DEAP dataset indicate that the accuracy is 87.24% and 89.84% for valance and arousal. Experimental analysis reveals that MSTBN of negative emotions is more divergent and emotional information is transmitted more rapidly in the brain. Women are more susceptible to emotional perception than men. The proposed multidimensional feature extraction method has potential to be applied to HCI systems.","","978-1-6654-6819-0","10.1109/BIBM55620.2022.9995251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9995251","Emotion recognition;EEG;Brain network;EEMD;MMSE","Human computer interaction;Radio frequency;Emotion recognition;Neural activity;Feature extraction;Electroencephalography;Wavelet packets","","","","15","IEEE","2 Jan 2023","","","IEEE","IEEE Conferences"
"EmoClass: Subject-Independent Emotion Classification for BCI Systems Using EEG Signals","A. R. Reddy; K. K; H. M V; V. Sridevi; B. S. Begum","Department of CSE, NIT, Tiruchirappalli, Tamilnadu, India; Department of CSE, NIT, Tiruchirappalli, Tamilnadu, India; Department of CSE, NIT, Tiruchirappalli, Tamilnadu, India; Department of ICE, NIT, Tiruchirappalli, Tamilnadu, India; Department of CSE, NIT, Tiruchirappalli, Tamilnadu, India","2024 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES)","13 Dec 2024","2024","","","1","6","Human-Computer Interaction (HCI) explores the interaction between humans and technology, considering emotional responses as an integral part of human life. Emotions are significant influencers of decision-making, and public well-being, intricately linked to mental health, detectable through BrainComputer Interface (BCI) technologies. BCI provides insights into neural correlates of emotional states and aids in mental health assessment and intervention strategies. Electroencephalography (EEG) based BCI systems play a significant role in classifying emotions to enhance human interaction. The proposed models were applied to a PhyMER dataset for emotion classification, encompassing both binary and multi-class models using a subject-independent approach. The features are extracted from EEG signals using Fast Fourier Transform (FFT) and Peak-to-Peak (P2P) analysis. The proposed binary and multiclass models were analyzed to classify the emotions using a variety of machine learning (ML) and deep learning (DL) algorithms. The performance metrics of the binary and multiclass ML models have maximum F1 scores of 62.04 and 28.48 respectively. Additionally, the binary and multiclass DL model achieved a maximum F1 score of 61.31 and 29.2 respectively.","","979-8-3503-7613-5","10.1109/SPICES62143.2024.10779770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10779770","Emotions;EEG Signals Classification;PhyMER Dataset;Time-Frequency Analysis;Subject-Independent;Multi-Class Models","Measurement;Analytical models;Fast Fourier transforms;Signal processing algorithms;Mental health;Signal processing;Brain modeling;Feature extraction;SPICE;Electroencephalography","","","","13","IEEE","13 Dec 2024","","","IEEE","IEEE Conferences"
"HMT: An EEG Signal Classification Method Based on CNN Architecture","M. He; Y. Wu; Z. Li; S. Wang; W. Li; W. Zhou; H. Rong; J. Wang","School of Medical Instruments Shanghai University of Medicine & Health Sciences, Shanghai, China; School of Medical Imaging Shanghai University of Medicine & Health Sciences, Shanghai, China; School of Medical Technology Shanghai University of Medicine & Health Sciences, Shanghai, China; School of mathematics and statistics Fuyang Normal University, Fuyang, China; School of Nursing and Health Management Shanghai University of Medicine & Health Sciences, Shanghai, China; College of Information Engineering Nanjing University of Finance & Economics, Nanjing, China; Information Engineering College China Jiliang University, Hangzhou, China; School of Business and Management Shanghai International Studies University, Shanghai, China","2023 5th International Conference on Intelligent Control, Measurement and Signal Processing (ICMSP)","12 Jul 2023","2023","","","1015","1018","Electroencephalogram (EEG) is a non-invasive measurement method commonly used to measure human brain electrical activity,It detects weak electrical signals generated by neurons in the cerebral cortex by placing electrodes on the scalp. These signals reflect the bioelectricity activity of the brain when thinking, perception, emotion, behavior and other activities.It has been used to measure the electrical activity of the human brain. The EEG signal can reflect the changes in the brain waves of the human brain, it is widely used to help doctors diagnose neurological diseases such as motor neurons, Parkinson's disease, and spasms. Low signal-to-noise ratio and high nonlinearity are the characteristics of EEG signals, which bring great challenges and difficulties to the analysis and recognition of EEG signals. Convolutional neural network is a great breakthrough technology in deep learning, which can deal with highly nonlinear and noisy data, and can automatically learn the distribution characteristics of EEG data and the classification of EEG signals. Compared with traditional EEG signal classification methods, the convolutional neural network has great potential in dealing with complex EEG signals. In this paper, a deep learning algorithm model HMT based on CNN architecture is proposed to process EEG signals, and its application in EEG signal classification and recognition is discussed.","","979-8-3503-3603-0","10.1109/ICMSP58539.2023.10170904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10170904","EEG;deep learning;classification;convolutional neural network","Deep learning;Neurons;Scalp;Pattern classification;Electric variables measurement;Signal processing algorithms;Signal processing","","2","","14","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Robust EEG emotion classification using segment level decision fusion","V. Rozgić; S. N. Vitaladevuni; R. Prasad","Speech Language and Multimedia Technologies, Raytheon BBN Technologies, USA; Speech Language and Multimedia Technologies, Raytheon BBN Technologies, USA; Speech Language and Multimedia Technologies, Raytheon BBN Technologies, USA","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","1286","1290","In this paper we address single-trial binary classification of emotion dimensions (arousal, valence, dominance and liking) using electroencephalogram (EEG) signals that represent responses to audio-visual stimuli. We propose an innovative three step solution to this problem: (1) in contrast to the typical feature extraction on the response-level, we represent the EEG signal as a sequence of overlapping segments and extract feature vectors on the segment level; (2) transform segment level features to the response level features using projections based on a novel non-parametric nearest neighbor model; and (3) perform classification on the obtained response-level features. We demonstrate the efficacy of our approach by performing binary classification of emotion dimensions on DEAP (Dataset for Emotion Analysis using electroencephalogram, Physiological and Video Signals) and report state-of-the-art classification accuracies for all emotional dimensions.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6637858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637858","EEG;emotion recognition","Support vector machine classification;Feature extraction;Electroencephalography;Accuracy;Emotion recognition;Vectors;Kernel","","78","","19","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"A review of the classification of neuroscience problems with the help of deep learning framework","D. Pathak; R. Kashyap","Department of Computer Science & Engineering, Amity School of Engineering and Technology, Raipur, India; Department of Computer Science & Engineering, Amity School of Engineering and Technology, Raipur, India",2021 5th International Conference on Information Systems and Computer Networks (ISCON),"14 Feb 2022","2021","","","1","6","Electroencephalographic signals (EEG signals) processing has become very popular nowadays due to its effectiveness in dealing with and treating various disorders associated with the field of neuroscience. As per the recent trends, deep learning has shown many promising results as compared to machine learning due to its ability to extract end-to-end features automatically from the raw input data and subsequently providing better performance in classification results. This paper has reviewed research papers that implemented various deep learning methodologies i.e., CNN, R-CNN, LSTM, GAN, etc. for the classification of EEG signals specific to epilepsy, sleep stage, and mental stages disorders. The review has been carried out considering various parameters i.e., objectives, datasets, models, results, etc. This paper has also discussed the common challenges associated with EEG signals classification related to neuroscience problems and proposed a framework to overcome the same for future studies.","","978-1-6654-0341-2","10.1109/ISCON52037.2021.9702485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9702485","CNN;Deep Learning;EEG Data;Feature extraction;Machine Learning;Neuroscience","Deep learning;Training;Neuroscience;Sleep;Pattern classification;Feature extraction;Brain modeling","","4","","26","IEEE","14 Feb 2022","","","IEEE","IEEE Conferences"
"EEG Data Enhancement and Emotion Recognition Based on Generative Adversarial Networks","T. Wang; J. Sun; Z. Li; Z. Zhang","dept. School of Information and Control, Engineering Qingdao University of Technology, Qingdao, China; dept. School of Information and Control, Engineering Qingdao University of Technology, Qingdao, China; dept. School of Information and Control, Engineering Qingdao University of Technology, Qingdao, China; dept. School of Information and Control, Engineering Qingdao University of Technology, Qingdao, China",2023 IEEE 6th International Conference on Electronic Information and Communication Technology (ICEICT),"22 Sep 2023","2023","","","1051","1054","In emotion recognition, the requirement for authenticity and real-time of the medium under study has always been a primary consideration. Thus electroencephalogram (EEG) is most suitable as the primary medium for emotion recognition research. However, there needs to be more high-quality training data for building EEG-based emotion recognition models through machine learning or deep learning methods, and manual generation of high-quality data is an effective way to overcome this problem. Generative adversarial networks (GAN) have recently succeeded in generative applications involving images. They are beginning to be applied to EEG data enhancement, so this paper generates high-quality artificial data through generative adversarial networks.","2836-7782","979-8-3503-9905-9","10.1109/ICEICT57916.2023.10245428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10245428","Data enhancement;Generating adversarial networks;EEG signals","Deep learning;Emotion recognition;Buildings;Training data;Manuals;Generative adversarial networks;Electroencephalography","","","","21","IEEE","22 Sep 2023","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition by Using Machine Learning and Deep Learning","W. Tong; L. Yang; Y. Qin; Y. Che; C. Han","Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China","2022 15th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","21 Dec 2022","2022","","","1","5","At present, there are many classification methods for emotion recognition. Based on the SEED dataset, this paper explored the recognition performance of three emotion recognition models: support vector machine(SVM), random forest(RF) and convolutional neural network(CNN). Firstly, we compared the accuracy of the five features of power spectral density (PSD), differential entropy (DE), differential asymmetry (DASM), rational asymmetry (RASM), and the differences between DE features of 23 pairs of frontal-posterior electrodes (DCAU) in total frequency bands. It is proved that the DE feature is more conducive to emotion recognition. Secondly, we also compared the accuracy of DE feature in five different frequency bands (Delta, Theta, Alpha, Beta and Gamma). Here we verified that the performance of the Gamma and Beta frequency bands were better than the others. Then, we traversed the recognition accuracy of each channel through the RF method, and chose four different channel combinations for 4, 6, 9 and 12 channels. Finally, we compared the DE features of these four combined channels with the performance of all 62 channels, and it is demonstrated that the accuracy of the four combined channels were comparatively steady, and the best recognition rate was 89.20%, which was similar to the recognition accuracy of the original 62 channels.","","978-1-6654-8887-7","10.1109/CISP-BMEI56279.2022.9979849","Natural Science Foundation of Tianjin, China(grant numbers:18JCYBJC88200); National Natural Science Foundation of China(grant numbers:62103301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9979849","EEG;emotion;support vector machine(SVM);random forest(RF);convolutional neural network(CNN)","Deep learning;Radio frequency;Emotion recognition;Machine learning algorithms;Support vector machine classification;Signal processing algorithms;Forestry","","","","15","IEEE","21 Dec 2022","","","IEEE","IEEE Conferences"
"Recurrent Neural Network Based Human Emotion Recognition using EEG brain signals","Q. Nawaz; M. Younas; I. Hamid; S. M. M. Gilani","Department of Computer Science, University of Agriculture Faisalabad, Faisalabad, Pakistan; Department of Computer Science, University of Agriculture Faisalabad, Faisalabad, Pakistan; Department of Computeer Science, National Textile University, Faisalabad, Pakistan; Department of Computer Science, University of Agriculture Faisalabad, Faisalabad, Pakistan",2023 3rd International Conference on Computing and Information Technology (ICCIT),"10 Oct 2023","2023","","","117","122","In daily lives, human experience a different emotion i.e.- sadness, angry, happiness. and rage that are very closely related tits behavior and play an important role in their daily routines. Emotion recognition means that identify and classify the human emotional mind state using their EEG brain signal, because EEG is very convenient and gives necessary information about brain emotion state. Since, emotion recognition through EEG signals is an active and challenging research area due to the non-stationary behavior of the signal. Therefore, Fastest, and accurate method is essential for automatically recognize and classify the human emotions. In this paper, we proposed RNN based Emotion Recognition model which consist of different layers i.e.- input layer, Dense layer, flatten layer, ReLU layer, SoftMax layer, and dropout layer that are used for feature extraction and model classification. ADAM optimization algorithm is used for model training which update the parameters of network after each iteration of epoch. EEG signals are used to train, validate and to predict the brain state of the human. The training accuracy of proposed model is 96.25% with 0.01 learning rate. To show the robustness and efficiency of proposed model, we compared with similar studies that shows performance of the proposed algorithm is better than the existing one in terms of better classification and model accuracy.","","979-8-3503-2148-7","10.1109/ICCIT58132.2023.10273944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10273944","EEG Signals;Emotion Recognition;RRN;Classification;Deep Leaning;ADAM optimizer","Training;Emotion recognition;Recurrent neural networks;Computational modeling;Brain modeling;Prediction algorithms;Electroencephalography","","2","","21","IEEE","10 Oct 2023","","","IEEE","IEEE Conferences"
"Multi-Task Improve Domain Generalization in EEG-based Emotion Classification using Feature Fusion Learning Model","S. Menaka; R. S. Joshi; V. M","Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India; Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India; Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India",2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),"23 Nov 2023","2023","","","1","9","The use of Electroencephalography (EEG) signals for emotion identification tasks is common. However, the domain shift issue may cause EEG-based emotion detection models' performance to decline when used in new domains. In this article, we present a feature fusion learning model-based multi-task learning strategy to enhance domain generalization in EEG-based emotion classification. By simultaneously learning a shared feature extractor and numerous domain-specific classifiers, we train a single model to categorize emotions across various domains. Additionally, we present a feature fusion module that combines time-domain, frequency-domain, and wavelet-domain EEG characteristics. We run trials on the DEAP and SEED EEG emotion recognition datasets to assess our suggested methodology. The experimental findings show that our suggested strategy outperforms a number of cutting-edge techniques in terms of classification accuracy, both within- and across-domains.","2473-7674","979-8-3503-3509-5","10.1109/ICCCNT56998.2023.10307526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307526","EEG;emotion identification;feature fusion;cutting-edge techniques","Emotion recognition;Fuses;Frequency-domain analysis;Brain modeling;Multitasking;Feature extraction;Electroencephalography","","","","20","IEEE","23 Nov 2023","","","IEEE","IEEE Conferences"
"Emotional reaction recognition from EEG","K. Dhindsa; S. Becker","School of Scientific Computing and Engineering, McMaster University, Hamilton, Ontario, Canada; Department of Psychology Neuroscience and Behaviour, McMaster University, Hamilton, Ontario, Canada",2017 International Workshop on Pattern Recognition in Neuroimaging (PRNI),"20 Jul 2017","2017","","","1","4","In this study we explore the application of pattern recognition models for recognizing emotional reactions elicited by videos from electroencephalography (EEG). We show that both the presence and magnitude of each emotion can be predicted above chance levels with up to 88% accuracy. Furthermore, we show that there are differences in classifiability for different emotions and participants, but whether a participant's data can be classified with respect to different emotions can itself be predicted from their EEG.","","978-1-5386-3159-1","10.1109/PRNI.2017.7981501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7981501","Emotion recognition;electroenecephalography (EEG);pattern recognition;classification;regression;individual differences;affective computing applied","Electroencephalography;Videos;Feature extraction;Pattern recognition;Couplings;Emotion recognition;Regression analysis","","3","","19","IEEE","20 Jul 2017","","","IEEE","IEEE Conferences"
"Research on Emotion Recognition Based on Facial Expression and EEG","N. Yan; X. Zeng; L. Chen","Institutes of Physical Science and Information Technology, Anhui University, Hefei, China; Academy for Engineering and Technology, Fudan University, Shanghai, China; Institute of Intelligent Machines, HFIPS Chinese Academy of Sciences, Hefei, China",2021 International Conference on Networking Systems of AI (INSAI),"19 Apr 2022","2021","","","118","122","With the development of artificial intelligence technology, emotion recognition has become an increasingly important research topic. Recognizing emotions only from the data with a single modality has its drawbacks. In this paper, the two modalities of facial expressions and EEG are integrated to realize the recognition of five types of emotions such as happiness, and the accuracy rate has reached a relatively satisfactory result. For facial expression modalities, this paper uses histogram equalization for preprocessing, then use LBP algorithm to extract facial expression features, and finally use SVM for expression recognition; for EEG modalities, this paper uses wavelet threshold denoising for preprocessing, and then use fractal dimension and multi-scale entropy algorithm to extract EEG signal features. This paper classifies EEG signals in the DEAP data set for emotion classification. Under the condition of using only one EEG channel FP1, the accuracy of SVM classification can reach 75.0%.","","978-1-6654-0859-2","10.1109/INSAI54028.2021.00031","Science and Technology Commission of Shanghai Municipality; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757969","multi-modality;facial expression;EEG signal;emotion recognition","Support vector machines;Emotion recognition;Histograms;Face recognition;Noise reduction;Feature extraction;Electroencephalography","","","","10","IEEE","19 Apr 2022","","","IEEE","IEEE Conferences"
"Automated Human Emotion Recognition System Using TQWT-Based EEG Subbands","D. Pachori; T. K. Gandhi","Department of Electronics and Communication Engineering, Indian Institute of Information Technology, Nagpur, India; Department of Electrical Engineering, Indian Institute of Technology Delhi, New Delhi, India",IEEE Sensors Letters,"19 Nov 2024","2024","8","12","1","4","This letter presents a new framework for the identification of human emotion states, namely, positive, neutral, and negative, by using the electroencephalogram (EEG) signals. The methodology comprises advanced signal processing techniques and machine learning algorithms. The EEG signals were decomposed to various subbands by using the tunable-Q wavelet transform (TQWT). Further, from each subband, features, such as TQWT energy, total Shannon energy, Rényi entropy, Tsallis entropy, and fractal dimension, were extracted. The obtained features were combined and tested on various machine learning classifiers. The proposed method has been validated on the publicly available SJTU Emotion EEG Dataset. The accuracy obtained for human emotion recognition was 86.67% for subject-independent analysis and 88.87% for subject-dependent analysis. Also, we concluded that human emotions could be recognized more efficiently by both audio and visual stimuli as compared to individual audio or visual stimuli based on the channels selection method.","2475-1472","","10.1109/LSENS.2024.3486708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10736203","Sensor signal processing;electroencephalogram (EEG) signals;human emotion recognition;machine learning;signal processing","Electroencephalography;Emotion recognition;Visualization;Feature extraction;Entropy;Machine learning;Accuracy;Measurement;Fractals;Electrostatic discharges","","5","","26","IEEE","25 Oct 2024","","","IEEE","IEEE Journals"
"Classification of Emotional States from Multi-channel EEG Signals by Convolutional Neural Networks","H. Donmez; N. Ozkurt","Department of Electrical and Electronics Engineering, Yaş ar University, Izmir, Turkey; Department of Electrical and Electronics Engineering, Yaş ar University, Izmir, Turkey",2022 Innovations in Intelligent Systems and Applications Conference (ASYU),"1 Nov 2022","2022","","","1","6","Assistive technologies have great importance in providing comfortable life by today. Perception of the emotions by the interfaces is also an important concern, especially for Human Machine Interaction (HMI) studies; however, the performance of the recent tools is not sufficient. Hence, these studies need to be improved. For this goal, it was aimed to provide trustable and efficient emotion classification methods to improve the previous studies' performances. In the literature, there are several feature extraction and classification algorithms used due to the non-linear characteristics of electroencephalogram (EEG) signals. In this study, one of the performance achiever models of the literature, which is Convolutional Neural Networks (CNN), selected and basic signal analysis methods were applied as preprocessing techniques. To examine the performance of the classifier, the DEAP dataset, which is one of the commonly preferred and reliable datasets, was chosen, and valance/ar ousal states were classified with CNN by using raw EEG signals, Fourier, and wavelet features. It was observed that CNN provides a successful performance even for raw signals where wavelet features transform features outperform raw and Fourier features.","2770-7946","978-1-6654-8894-5","10.1109/ASYU56188.2022.9925308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925308","EEG;DEAP;Emotion;CNN;HMI","Wavelet transforms;Human computer interaction;Technological innovation;Feature extraction;Electroencephalography;Classification algorithms;Convolutional neural networks","","2","","17","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"EEG emotion recognition based on LMD fuzzy entropy","Z. Wang; B. Guo; Z. Cui; J. Zhang","Electronic Information Engineering, Changchun University of Science and Technology, Changchun, China; Electronic Information Engineering, Changchun University of Science and Technology, Changchun, China; Electronic Information Engineering, Changchun University of Science and Technology, Changchun, China; Electronic Information Engineering, Changchun University of Science and Technology, Changchun, China",2022 4th International Academic Exchange Conference on Science and Technology Innovation (IAECST),"17 Mar 2023","2022","","","1092","1095","In order to improve the accuracy and reliability of EEG emotion recognition, an algorithm based on local mean decomposition (LMD) fuzzy entropy is proposed, and support vector machine (SVM) classification is performed on this basis. The LMD decomposition of the rhythm signal of the emotional EEG signal is performed to filter out the useless components of the EEG signal, and the PF components selected by the local mean decomposition algorithm are used as the input vector of the fuzzy entropy algorithm to extract the EEG signal feature parameters. Maximum correlation minimum redundancy is used for feature selection. Finally, the feature matrix is fed into the SVM classifier for binary classification in terms of validity and arousal, respectively, to achieve EEG emotion recognition. The method achieves 91.87% classification accuracy on the DEAP dataset, which can extract EEG emotion features more effectively and provides a new method for the study of EEG emotion recognition.","","979-8-3503-2000-8","10.1109/IAECST57965.2022.10061937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061937","electroencephalographic emotion recognition;local mean decomposition;fuzzy entropy;support vector machine","Support vector machines;Emotion recognition;Technological innovation;Redundancy;Feature extraction;Rhythm;Electroencephalography","","1","","8","IEEE","17 Mar 2023","","","IEEE","IEEE Conferences"
"Multimodal Fusion for EEG Emotion Recognition in Music with a Multi-Task Learning Framework","S. Huang; Z. Jin; D. Li; J. Han; X. Tao","Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","2","This paper proposes a novel EEG-based emotion recognition approach for music, employing a two-stage training framework that integrates emotion representations from music, lyrics, and EEG. First, a modality-specific feature extraction strategy fine-tunes encoders for music and lyrics to extract emotion-related features, while the EEG encoder is fine-tuned to capture identity-related features. The second stage applies a local-to-global fusion strategy, merging EEG and music features for detailed, time-aligned modeling, and integrating global semantic representations from lyrics. Additionally, a multi-task learning framework is utilized to disentangle emotion-related and identity-related information in EEG, enabling robust, subject-independent emotion recognition. Experimental results on the EREMUS dataset show a significant improvement over baseline models, achieving a total score of 51.97%, demonstrating the effectiveness of multimodal integration and our proposed framework for emotion recognition in music.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890727","EEG;emotion recognition;multimodal fusion;multi-task learning","Training;Emotion recognition;Soft sensors;Speech recognition;Signal processing;Feature extraction;Brain modeling;Multitasking;Electroencephalography;Speech processing","","","","9","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Emotion Recognition from EEG Signals Based on GoogleNet and Multi-Dimensional Fusion","K. Cheng; H. Bo; L. Ren; W. Kong","College of Information Engineering, Shanghai Maritime University, Shanghai, China; College of Information Engineering, Shanghai Maritime University, Shanghai, China; College of Information Engineering, Shanghai Maritime University, Shanghai, China; College of Information Engineering, Shanghai Maritime University, Shanghai, China",2023 International Conference on Intelligent Management and Software Engineering (IMSE),"2 Apr 2024","2023","","","76","80","Emotion recognition has received great attention in the fields of emotional computing and human-computer interaction. Due to the limited number of EEG signal samples and individual differences, efficient feature extraction and fusion of brain dynamic information have become key issues. Inspired by Google Network, a new convolutional neural network called TFSNet (Time Frequency Space Network) has been proposed. The network includes a dynamic temporal layer, a frequency domain layer, a spatial block layer, and a fusion optimization layer. Unlike traditional networks, in order to learn the activities within and between different functional areas of the brain, EEG is divided into functional blocks, and Inception modules are used to learn time and frequency features. Complex relationships within and between brain functional areas are modeled, and features are fused in the fusion optimization layer. Compared with other advanced methods on the public dataset DEAP, the results showed that TFSNet performed the best in classification accuracy and F1 score, with an average improvement of about 5.28% in accuracy and a maximum improvement of about 9.55%.","","979-8-3503-9414-6","10.1109/IMSE61332.2023.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483350","EEG;Emotional recognition;Deep learning;Convolutional neural network","Human computer interaction;Emotion recognition;Time-frequency analysis;Brain modeling;Electroencephalography;Convolutional neural networks;Task analysis","","","","12","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Knowledge-guided Network Pruning for EEG-based Emotion Recognition","W. Rao; S. -H. Zhong; Z. Zhang; Y. Liu","College of Computer Science and Software, Shenzhen University Guangdong Laboratory of Artificial Intelligence and Digital Economy, Shenzhen, China; College of Computer Science and Software, Shenzhen University Guangdong Laboratory of Artificial Intelligence and Digital Economy, Shenzhen, China; Department of Computer Science, Hong Kong Polytechnic University, China; Department of Computer Science, Hong Kong Polytechnic University, China",2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Jan 2024","2023","","","2187","2190","With the development of deep learning in EEG-related tasks, the complexity of learning models has gradually increased. These complex models often result in long inference times, high energy consumption, and an increased risk of overfitting. Therefore, model compression has become an important consideration. Although some EEG models have used lightweight techniques, such as separable convolution, no existing work has directly attempted to compress EEG models to reduce their complexity. In this paper, we integrate neuroscience knowledge into EEG model pruning recovery, and innovatively propose two loss functions in the learning process, the knowledge-guided region-wise loss that enforces the classification evidence consistent with the importance of the prefrontal lobe, and the knowledge-guided sample-wise loss that constrains the learning process by distinguishing the importance of different samples.","2156-1133","979-8-3503-3748-8","10.1109/BIBM58861.2023.10385325","Natural Science Foundation of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385325","EEG-based emotion recognition;model compression;knowledge-guided network pruning","Deep learning;Emotion recognition;Energy consumption;Neuroscience;Convolution;Brain modeling;Electroencephalography","","1","","12","IEEE","18 Jan 2024","","","IEEE","IEEE Conferences"
"Human Emotion Classification Based On the EEG Signals and Music Recommendation","S. S. V. Kasa; S. Kolangiammal; G. Praneethi; S. S. Kumar","Dept of Electronics and Communication Engineering, SRM Institute of Science & Technology SRM Nagar, Kattankulathur; Dept of Electronics and Communication Engineering, SRM Institute of Science & Technology SRM Nagar, Kattankulathur; Dept of Electronics and Communication Engineering, SRM Institute of Science & Technology SRM Nagar, Kattankulathur; Dept of Electronics and Communication Engineering, SRM Institute of Science & Technology SRM Nagar, Kattankulathur",2024 10th International Conference on Communication and Signal Processing (ICCSP),"6 Jun 2024","2024","","","1605","1610","This study examines the interaction between human emotions, electroencephalogram (EEG) data, and music inorder to develop a novel method to emotion identification and tailored music selection. Furthermore, music has a strong influence on emotions. making it a perfect medium for emotional control. The proposed method classifies human emotions using powerful machine learning algorithms that analyze EEG data. EEG data is obtained from subjects while they respond to various emotional stimuli. Feature extraction and selection approaches are used to discover meaningful patterns in EEG data that correspond to distinct emotions. In addition, the study includes a music recommendation system that tailors music playlists depending on observed emotional states.","2836-1873","979-8-3503-5306-8","10.1109/ICCSP60870.2024.10543385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543385","Eectroencephalogram (EEG);music recommendation;machine learning;Feature extraction","Privacy;Ethics;Machine learning algorithms;Machine learning;Signal processing;Feature extraction;Electroencephalography;User experience;Recommender systems","","4","","9","IEEE","6 Jun 2024","","","IEEE","IEEE Conferences"
"Brain Signal-Based Emotion Classification Using DEAP Corpus","B. Kumar; M. A. J. Pasha; N. Apoorva; K. S. Dheeraj; S. Sivanand; A. K. Varma","Department of Electronics and Communication Engineering, Koneru Lakshmaiah Education Foundation, Hyderabad, India; Department of Electronics and Communication Engineering, Koneru Lakshmaiah Education Foundation, Hyderabad, India; Department of Electronics and Communication Engineering, Koneru Lakshmaiah Education Foundation, Hyderabad, India; Department of Electronics and Communication Engineering, Koneru Lakshmaiah Education Foundation, Hyderabad, India; Department of Electronics and Communication Engineering, Koneru Lakshmaiah Education Foundation, Hyderabad, India; Department of Electronics and Communication Engineering, Koneru Lakshmaiah Education Foundation, Hyderabad, India","2024 First International Conference on Electronics, Communication and Signal Processing (ICECSP)","8 Oct 2024","2024","","","1","6","The research paper presents a study on utilizing deep learning models, particularly Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNNs), to classify emotions based on Electroencephalography (EEG) signals captured from the human brain using electrodes placed on the scalp. The study primarily analyses the DeapData corpus, employing advanced deep-learning techniques to interpret and classify emotional states from EEG readings. The LSTM model is utilized for its capability to capture long-term dependencies in sequential data, while the CNN model is harnessed for its effectiveness in extracting features from EEG brain signals. Experimental results demonstrate the efficiency of the proposed approach in accurately categorizing emotions from EEG brain signal data, emphasizing the potential of deep learning techniques in neuroimaging and affective computing applications.","","979-8-3503-6459-0","10.1109/ICECSP61809.2024.10698526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10698526","LSTM;CNN;Deep Learning;DEAP dataset;EEG data;Emotion recognition","Deep learning;Emotion recognition;Accuracy;Computational modeling;Brain modeling;Feature extraction;Electroencephalography;Data models;Convolutional neural networks;Long short term memory","","1","","15","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Insights of 3D Input CNN in EEG-based Emotion Recognition","K. van Noord; W. Wang; H. Jiao","Eindhoven University of Technology, The Netherlands; Eindhoven University of Technology, The Netherlands; China and Eindhoven University of Technology, Peking University, The Netherlands",2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"9 Dec 2021","2021","","","212","215","Electroencephalogram (EEG) signals have shown to be a good source of information for emotion recognition algorithms in Human-Brain interaction applications. In this paper, a reproducible framework is proposed for classifying human emotions based on EEG signals. The framework consists of extracting frequency-dependent features from raw EEG signals to form a three-dimensional EEG image which is classified by a convolutional neural network (CNN). The framework is used to show that the 3D input CNN outperforms conventional methods with two-dimensional input, using a public dataset. The implementation of the framework is publicly available to facilitate further work on this topic: https://github.com/KvanNoord/3D-CNN-EEG-Emotion-Classification.","2694-0604","978-1-7281-1179-7","10.1109/EMBC46164.2021.9631042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9631042","","Electrodes;Emotion recognition;Three-dimensional displays;Biological system modeling;Feature extraction;Brain modeling;Electroencephalography","Algorithms;Brain;Electroencephalography;Emotions;Humans;Neural Networks, Computer","5","","9","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"EEG Emotion Recognition Based on a Novel 4D Feature Fusion and 3D Convolutional Networks","X. Gong; J. Gu; T. Li","School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China","2022 3rd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)","23 Dec 2022","2022","","","359","362","Emotion recognition based on electroencephalogram (EEG) signals has become an important research field in recent years due to its application in human-computer interaction. Feature fusion is the solution strategy in many pattern recognition works. However, there are few related works on EEG feature fusion. This research field has much room for development. This paper proposes a 4-dimensional representation method of multiple features across channels and frequency bands and 3D convolution neural network (3D-CNN) for EEG emotion recognition. Specifically, we perform two-dimensional electrode position relationship mapping on the extracted one-dimensional sequences of different features. The obtained 2D features are fused to the three-dimensional array to achieve complementarity between features. The 3D features of different frequency bands are then formed into the four-dimensional feature to capture cross-band information. We adopt the subject-independent approach to conduct experiments on the SEED dataset. Compared with the state-of-the-art emotion classification method, the classification performance is improved with an average accuracy of 98.64%. The 4-dimensional representation method combining different features proposed in this paper is proved to be significant in feature extraction and fusion.","","978-1-6654-5160-4","10.1109/ICBAIE56435.2022.9985838","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985838","Emotion recognition;Multi-feature;3D-CNN;electroencephalogram (EEG)","Deep learning;Emotion recognition;Solid modeling;Three-dimensional displays;Convolution;Feature extraction;Brain modeling","","","","16","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"Cross-Subject EEG Emotion Recognition Using Domain Adaptive Few-Shot Learning Networks","R. Ning; C. L. Philip Chen; T. Zhang","Sch. of Computer Science&Engineering South China University of Technology, Guangzhou, China; Sch. of Computer Science&Engineering South China University of Technology Pazhou Lab, Guangzhou, China; Sch. of Computer Science&Engineering South China University of Technology Pazhou Lab, Guangzhou, China",2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"14 Jan 2022","2021","","","1468","1472","Due to the individual differences and nonstationary of EEG signals, it is difficult to classify EEG emotions with traditional machine methods, which assume that the training and testing set come from the same data distribution, but this assumption is usually not true in the EEG field, therefore the accuracy of emotion recognition is very poor. In this paper, a Single-Source Domain Adaptive Few-Shot Learning Networks (SDA-FSL) was proposed for cross-subject EEG emotion recognition. This is the first time that domain adaptation method with few-shot learning has been used in the field of EEG emotion recognition. A CBAM-based feature mapping module was designed to extract the common features of the two domains, and the domain adaptation module was used to align the data distribution of two domains. In addition, Prototypical Networks with instance-attention mechanism is introduced to preserve domain-specific information. The proposed method was evaluated on DEAP and SEED datasets in within-dataset and cross-dataset experiments under various N-way k-shot settings. Experimental results show that the performance of SDA-FSL outperforms other comparison methods and has superior generalization performance on cross-dataset experiments.","","978-1-6654-0126-5","10.1109/BIBM52615.2021.9669542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669542","EEG emotion recognition;cross-subject;domain adaptation;few-shot learning","Training;Electrodes;Emotion recognition;Adaptive systems;Conferences;Feature extraction;Electroencephalography","","10","","15","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"Analysis of Emotion EEG Classification Based on GA-Fisher Classifier","S. Zhang; J. Gao; Z. Chen","College of Mathematics, Physics and Information Engine, Zhejiang Normal University, Jinhua, China; College of Mathematics, Physics and Information Engine, Zhejiang Normal University, Jinhua, China; College of Mathematics, Physics and Information Engine, Zhejiang Normal University, Jinhua, China",2011 First International Workshop on Complexity and Data Mining,"12 Jan 2012","2011","","","24","27","Emotion classification is a research hotspot in fields such as psychology and physiology. The categorical scales used recently need to be further researched for their subjective factors and accuracy influence. This paper presents an effective method which integrates GA-Fisher classifier and EEG, and we have got good classification effects by doing experiments on four emotions: excitement, fear, oscitancy, awaking. The results show that: (1) The classification accuracy rate between excitement and fear is 88.45%; and among excitement, fear and oscitancy for 86.71%; excitement, fear, oscitancy, and awaking for 84.70%; (2) the classifier introduced here is obviously better than the classical PCA-Fisher classifier (whose accuracy rate is 79.82% - 82.74%).","","978-1-4577-2007-9","10.1109/IWCDM.2011.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128409","Emotion Classification;EEG;GA-Fisher Classifier","Electroencephalography;Accuracy;Videos;Physiology;Educational institutions;Feature extraction;Materials","","6","","7","IEEE","12 Jan 2012","","","IEEE","IEEE Conferences"
"Exploring Emotions in EEG: Deep Learning Approach with Feature Fusion","D. T. Mridula; A. Ahmed Ferdaus; T. S. Pias","Computer Science and Engineering, University of Dhaka, Dhaka, Bangladesh; Computer Science and Engineering, University of Dhaka, Dhaka, Bangladesh; Computer Science, Virginia Tech, Blacksburg, USA",2023 26th International Conference on Computer and Information Technology (ICCIT),"27 Feb 2024","2023","","","1","6","Emotion is an intricate physiological response that plays a crucial role in how we respond and cooperate with others in our daily affairs. Numerous experiments have been evolved to recognize emotion, however still require exploration to intensify the performance. To enhance the performance of effective emotion recognition, this study proposes a subject-dependent robust end-to-end emotion recognition system based on a 1D convolutional neural network (1D-CNN). We evaluate the SJTU Emotion EEG Dataset SEED-V with five emotions (happy, sad, neural, fear, and disgust). To begin with, we utilize the Fast Fourier Transform (FFT) to decompose the raw EEG signals into six frequency bands and extract the power spectrum feature from the frequency bands. After that, we combine the extracted power spectrum feature with eye movement and differential entropy (DE) features. Finally, for classification, we apply the combined data to our proposed system. Consequently, it attains 99.80% accuracy which surpasses each prior state-of-the-art system.","","979-8-3503-5901-5","10.1109/ICCIT60459.2023.10441204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441204","Emotion Recognition System;EEG Signal;EYE Movement Signal;1D-CNN;Deep Learning;Subject-Dependant Emotion Recognition;SEED-V Emotion Dataset","Emotion recognition;Codes;Feature extraction;Brain modeling;Electroencephalography;Entropy;Software development management","","2","","40","IEEE","27 Feb 2024","","","IEEE","IEEE Conferences"
"Three class emotions recognition based on deep learning using staked autoencoder","B. Yang; X. Han; J. Tang","Department of Automation, Shanghai University, Shanghai, China; Department of Automation, Shanghai University, Shanghai, China; Department of Automation, Shanghai University, Shanghai, China","2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","26 Feb 2018","2017","","","1","5","Emotion recognition is a hot spot in advanced humancomputer interaction system, which is of great significance in artificial intelligence, health care, distance education, military field and so on. The paper builds a stacked autoencoder deep learning classification network consist of an input layer, two autoencoder hidden layers and a softmax classifier output layer based on SJTU Emotion EEG Dataset (SEED). Pretrain the first autoencoder employed L-BFGS to optimize the cost function. Then pretrain the second autoencoder with the output of first autoencoder. Finally send to the softmax classifier. Pretrain each autoencoder in forward propagation, then fine-tuning the whole network in back propagation. The well-trained network is used to classify three emotion states including happy, neural and grief. The raw inputs are differential entropy of EEG signal in five rhythmic frequencies band and the differential entropy of whole EEG signal. Fourteen experiments are performed with 5-fold cross validation, the average classification accuracy of three class emotion states is 59.6%, 66.27%, 71.97%, 78.48%, 82.56% and 85.5%. The result shows the higher frequency band differential entropy like Gamma band is more relative to emotion reaction.","","978-1-5386-1937-7","10.1109/CISP-BMEI.2017.8302098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302098","emotion recognition;sacked autoencoder;SEED;differential entropy;deep learning","Electroencephalography;Emotion recognition;Entropy;Machine learning;Training;Cost function;Feature extraction","","12","","19","IEEE","26 Feb 2018","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Classification with Wavelet Entropy Feature","X. Song; Q. Kang; Z. Tian; Y. Yang; S. Yang; Q. Gao; Y. Song","Engineering Training Center, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China; National Demonstration Center for Experimental Mechanical an Electrical Engineering Education, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China",2020 Chinese Automation Congress (CAC),"29 Jan 2021","2020","","","5685","5689","Traditional emotion recognition methods are mainly based on voice, expression and body movement. These physiological signals or facial expressions may hardly reveal inner emotions. In this paper, the wavelet entropy (WE) was utilized to represent the characteristics associated with emotional states. The average classification accuracies of positive, neutral and negative emotions are 70.65%, 70.53% and 70.28%, respectively. In order to demonstrate the effectiveness of the proposed method, the comparison experiments were carried out by the power spectrum density (PSD) feature and approximate entropy (ApEn) feature. The average classification accuracies are 70.49% (WE), 66.93% (PSD) and 64.44% (ApEn), respectively. The results indicate that the wavelet entropy feature performs better than the other two features for Electroencephalogram (EEG) based emotion recognition.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9326323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326323","emotion recognition;wavelet entropy;classification accuracies","Electroencephalography;Feature extraction;Entropy;Fourier transforms;Emotion recognition;Wavelet coefficients;Wavelet analysis","","4","","18","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Decoding Olfactory EEG Signals Using Multi-Domain Features and Machine Learning","S. P. Akbugday; B. Akbugday; F. Yeganli; A. Akan; R. Sadikzade","Dept. of Biomedical Eng, Izmir University of Economics, Izmir, TURKEY; Dept. of Electrical and Electronics Eng, Izmir University of Economics, Izmir, TURKEY; Dept. of Electrical and Electronics Eng, Izmir University of Economics, Izmir, TURKEY; Dept. of Electrical and Electronics Eng, Izmir University of Economics, Izmir, TURKEY; Dept. of Business Administration, Izmir University of Economics, Izmir, TURKEY",2024 Medical Technologies Congress (TIPTEKNO),"20 Nov 2024","2024","","","1","4","Accurate detection of human emotion is an important topic for affective computing. Especially with the rise of artificial intelligence in the marketing industry, the tools available are subjective and often heavily dependent on sample sizes and demographics. This study explores the neural responses to olfactory stimuli by analyzing EEG data collected from 57 participants exposed to a perfume scent in correlation with self-reported survey results. The electroencephalogram (EEG) signals were processed to extract time-domain, spectral-domain, and nonlinear features, which were subsequently classified using various machine learning algorithms. The classification outcomes were mapped onto a two-dimensional pleasure-arousal plane, with the Medium Gaussian support vector machine (SVM) achieving the highest performance, including $99.8 \%$ validation accuracy and $100 \%$ test accuracy. These results highlight the significant potential of EEG-based approaches in decoding the neural underpinnings of sensory experiences, with implications for applications in neuromarketing and therapeutic contexts.","2687-7783","979-8-3315-2981-9","10.1109/TIPTEKNO63488.2024.10755366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10755366","olfactory EEG;EEG;machine learning;emotion recognition;arousal-valence plane;neuromarketing","Support vector machines;Surveys;Emotion recognition;Accuracy;Olfactory;Feature extraction;Electroencephalography;Decoding;Time-domain analysis;Spectral analysis","","","","17","IEEE","20 Nov 2024","","","IEEE","IEEE Conferences"
"EEG Image Features for Classification of Emotional States","F. N. Feradov","Department of Electronic Equipment and Microelectronics, Faculty of Computer Sciences and Automation Technical University of Varna Studentsa 1, Varna, Bulgaria",2021 XXX International Scientific Conference Electronics (ET),"4 Nov 2021","2021","","","1","4","The following paper presents an examination of EEG image features designed for the purposes of emotion classification. A technique for transformation of EEG recordings into images of spectral activity over different areas of the brain is applied and histograms of the obtained images are obtained. The use of the differences of percentile values as features is proposed and experimentally evaluated using kNN, polynomial and rbf SVM and REPTree classifiers and mean classification results of >99% are obtained.","","978-1-6654-4518-4","10.1109/ET52713.2021.9579600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9579600","Negative emotion recognition;EEG Images;Feature extraction;Percentiles","Support vector machines;Histograms;Emotion recognition;Image recognition;Electroencephalography;Classification algorithms;Task analysis","","","","14","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"Pattern Analysis of Stimuli-Induced and Self-Induced Emotions Based on EEG","Z. Li; M. Li; C. He; H. Chen; Y. Wang; Z. Li","School of Information Engineering, Nanchang Hangkong University, Nanchang, China; School of Information Engineering, Nanchang Hangkong University, Nanchang, China; College Of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Information Engineering, Nanchang Hangkong University, Nanchang, China; School of Information Engineering, Nanchang Hangkong University, Nanchang, China; School of Information Engineering, Nanchang Hangkong University, Nanchang, China",2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI),"4 Oct 2022","2022","","","870","874","Up to now, various findings of brain region localization associated with different emotions have been reported. However, whether these key brain regions apply to all emotional induction methods has not been fully investigated yet. Emotions are divided into self-induced and stimuli-induced according to the induction mode, and there are few data sets about self-induced. In this paper, we developed a new dataset named MSI which includes two emotional induction methods. And we focus on identifying stability across subjects and critical brain areas' consistency in the two ways of emotion extraction. We systematically evaluate the performance of popular feature extraction and pattern classification methods with the newly developed dataset called MSI for this study. Random Forest with differential entropy features achieves the average accuracies of 81.15% in stimuli-induced emotions and 81.10% in self-induced emotions on the MSI dataset. The performance of our model shows that self-induced emotions and stimuli-induced emotions have stable recognition patterns across subjects. Further, we used the mRMR algorithm to sort each dimensional feature on the electrodes and localize the brain regions associated with emotion production. We find that the prefrontal, temporal, and occipital lobes are most associated with emotion production in both self-induced and stimulus-induced emotions. This indicates that stimuli-induced and self-induced stimuli have similarities in the physiological mechanism of emotion production.","","978-1-6654-9916-3","10.1109/PRAI55851.2022.9904010","Research and Development; National Natural Science Foundation of China; Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9904010","emotional induction;EEG;classification;electrodes;brain regions","Electrodes;Emotion recognition;Pattern classification;Production;Feature extraction;Physiology;Stability analysis","","1","","20","IEEE","4 Oct 2022","","","IEEE","IEEE Conferences"
"Wavelet based Emotion Detection from Multi-channel EEG using a Hybrid CNN-LSTM Model","M. Islam; T. Lee","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong",TENCON 2022 - 2022 IEEE Region 10 Conference (TENCON),"20 Dec 2022","2022","","","1","6","In this study, the continuous wavelet transform (CWT) is applied on multi-channel EEG signal to obtain wavelet coefficients in the time-frequency domain. The coefficients are considered to yield the spectral energy as input feature for detection of emotional states. Scalogram of each EEG signal covers a range of frequency components which indicates the percentage of total energy of the signal carried by each of those component. The useful spectral features obtained by wavelet scalogram of each EEG channel are aggregated to constitute the input frame of a combined neural network model comprising convolutional neural network (CNN) along with Long-short-term memory (LSTM). The spatial and temporal sequence based features are extracted simultaneously by the combined two dimensional (2D) CNN-LSTM network. This hybrid deep neural network showed notable performance on emotion detection for both binary and multi-class classification with the modeled spatial-temporal-spectral features. Performance of our proposed approach is evaluated on the publicly available DEAP and SEED dataset. On binary classification of valence and arousal state (high versus low level), the obtained accuracies are 94.36 % and 94.07 % respectively, meanwhile accuracy of 94.41 % is attained on 4-class classification with DEAP dataset. Additionally, our model achieved 97.40 % accuracy for multi-class emotion detection with SEED dataset, which significantly outperform the reported state-of-the-art systems with CNN/LSTM and/or conventional temporal and spectral features.","2159-3450","978-1-6654-5095-9","10.1109/TENCON55691.2022.9978122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978122","EEG;emotion detection;scalogram;CWT;2D CNN-LSTM model","Deep learning;Emotion recognition;Continuous wavelet transforms;Wavelet domain;Feature extraction;Brain modeling;Wavelet analysis","","1","","29","IEEE","20 Dec 2022","","","IEEE","IEEE Conferences"
"EEG-Based Tension Recognition Annotated with Electrodermal Activity","W. Gu; Y. -H. Li; L. -M. Zhao; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University; Shanghai Emotionhelper Technology Co., LTD; Shanghai Emotionhelper Technology Co., LTD; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","The precise annotation of emotions is crucial for constructing emotion EEG datasets, where videos are the dominant emotion-inducing tools. However, existing annotation methodologies are often limited, assigning a uniform label to the entire video and neglecting subjects' emotional arousal variations during viewing. This paper proposes a novel approach to address this issue by integrating electrodermal activity (EDA), a psychophysiological marker of arousal, with EEG data. We introduce a new dataset that captures both tension and calmness, utilizing EDA to annotate EEG data with high and low arousal. The method is systematically tested in subject-specific paradigms, employing a suite of machine learning and deep learning algorithms. Our results demonstrate that models trained solely on highly induced EEG data, comprising 71.75% of the initial training set, yield equal or superior performance on test sets, regardless of their arousal levels. This underscores the potential of EDA in enhancing emotion recognition accuracy in EEG studies.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782145","National Natural Science Foundation of China; Shanghai Jiao Tong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782145","EEG;EDA;continuous label;emotion classification;tension","Training;Emotion recognition;Electric potential;Machine learning algorithms;Annotations;Brain modeling;Electroencephalography;Data models;Engineering in medicine and biology;Videos","Humans;Electroencephalography;Galvanic Skin Response;Emotions;Algorithms;Arousal;Signal Processing, Computer-Assisted;Machine Learning;Male;Adult;Female","","","19","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Study on EEG Classification Characteristics Induced by Emotional Music","Y. Qiao; J. Mu; G. Liu","Southwest University, College of Electronic and Information Engineering, Chongqing, China; Southwest University, College of Electronic and Information Engineering, Chongqing, China; Southwest University, College of Electronic and Information Engineering, Chongqing, China","2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","11 Jul 2024","2024","","","1631","1635","Music is one of the main ways of arousing human emotions. However, the feeling of music is subjective, making it difficult to determine which emotions music triggers in a particular individual. To correctly identify music-induced emotional problems and the effects of music on brain activity, we first created a music-induced EEG dataset. Then, channel and feature selection algorithms are introduced into emotion recognition tasks, RFECV algorithm is used for feature screening, and the optimal channel combination is determined. Meanwhile, the distribution of optimal channel subsets in brain regions under different emotions is compared. Then random forest model is used to evaluate the classification effect of valence and arousal tasks. In addition, we further investigated the effects of different EEG bands and multi-band combinations on musical emotion recognition, and the results confirmed relevant neuroscience studies. By applying our method to a publicly available EEG dataset DEAP, we evaluated the generalization and reliability of our method. This method has good classification performance and provides a promising framework for the future research of emotion recognition systems based on brain-computer interface.","","979-8-3503-8555-7","10.1109/AINIT61980.2024.10581690","National Natural Science Foundation of China(grant numbers:61872301,61472330); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581690","component;EEG;emotion recognition;musical induction;machine learning","Seminars;Emotion recognition;Brain modeling;Electroencephalography;Brain-computer interfaces;Classification algorithms;Reliability","","","","12","IEEE","11 Jul 2024","","","IEEE","IEEE Conferences"
"An Affective Service based on Multi-Modal Emotion Recognition, using EEG enabled Emotion Tracking and Speech Emotion Recognition","D. S. Moschona","National Taipei University of Technology Interaction Design and Innovation, Taipei, Taiwan",2020 IEEE International Conference on Consumer Electronics - Asia (ICCE-Asia),"9 Dec 2020","2020","","","1","3","From 1964 to 1966 Joseph Weizenbaum created ELIZA - a natural language processing computer program. ELIZA engaged and simulated conversations with users, that gave them the illusion of being understood by the program, without in fact having any built in framework to do so. One of ELIZA's most famous script “DOCTOR” simulated the method of “Person-centered therapy” by responding with non-directional questions to user inputs. Weizenbaum was surprised by the number of individuals who attributed human-like feelings to the computer program. While ELIZA was capable of engaging in discourse, it could not converse with true understanding [1].","","978-1-7281-6164-8","10.1109/ICCE-Asia49877.2020.9277291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277291","Emotion recognition;Emotion detection;Electroencephalography (EEG);EEG enabled Emotion Tracking;Brain waves;Brain-Computer Interface (BCI);Speech Recognition;Speech Emotion Recognition (SER);Multi-Modal Emotion Recognition;Human-Computer Interaction (HCI);Affective Service;Affective Computing","Speech recognition;Electroencephalography;Emotion recognition;Brain modeling;Databases;Feature extraction;Computational modeling","","26","","15","IEEE","9 Dec 2020","","","IEEE","IEEE Conferences"
"EEG-EmoNet: A Deep Learning Framework for Emotion Recognition from EEG Data","S. Arya; R. Chaudhary; A. Agarwal; S. Samant; A. Goel; H. Tyagi","Department of AIML, COER University, Roorkee, India; Department of Mathematics, COER, Roorkee, Roorkee, India; Department of Sciences, Quntum University, Roorkee, India; Department of AIML, COER University, Roorkee, India; Department of AIML, COER University, Roorkee, India; Department of CSE, Quantum University, Roorkee, India","2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)","23 Jul 2024","2024","","","1921","1925","Emotion recognition is a basic aspect of human interaction and understanding In recent years, there has been a growing interest in developing automated systems capable of accurately detecting and classifying human emotions using electroencephalography (EEG) data. EEG, a non-invasive technique for measuring electrical activity in the brain, offers valuable insights into the neural correlates of emotional states. This paper aims to explore and develop a robust emotion recognition system utilizing EEG data. An EEG dataset comprising recordings from individuals experiencing different emotional stimuli is collected, ensuring diversity and representativeness of emotions. The collected data is then pre-processed to remove noise, artifacts, and other unwanted signals, enhancing the quality and reliability of the EEG signals. The paper also explores the potential for real-time emotion recognition from EEG data, aiming to develop an efficient system that can provide timely and accurate emotion detection. We start by providing an overview of EEG and its use in emotion detection. We then review various machine learning algorithms that have been used for emotion detection using EEG data, including Gated Recurrent Unit (GRU), Attention Model etc, thus giving a test accuracy of 97.50 percent","","979-8-3503-6684-6","10.1109/IC3SE62002.2024.10593650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10593650","Electroencephalography (EEG);human-computer interaction;mental health monitoring;real-time emotion recognition","Emotion recognition;Machine learning algorithms;Accuracy;Reviews;Logic gates;Brain modeling;Electroencephalography","","","","21","IEEE","23 Jul 2024","","","IEEE","IEEE Conferences"
"Button Press Detection from EEG Signals Using Deep Learning","E. E. Kuzucu","Mathematics and Mechanics Faculty, Novosibirsk State University, Novosibirsk, Russia",2022 IEEE 23rd International Conference of Young Professionals in Electron Devices and Materials (EDM),"19 Aug 2022","2022","","","538","542","Electroencephalogram (EEG) signals are mainly used for neuroscience studies and for brain-computer interface applications with a rising trend. Majority of studies include conducting experiments with different paradigms, and studying the results of experiments for scientific discoveries or various practical applications. It is likely that in the near future, all EEG related scientific discoveries will combine into one comprehensive automated EEG analysis framework where different paradigms can be analyzed in terms of detection of events, markers and signals of interest. In this study we created an application which estimates time of button press actions from stop-signal paradigm EEG recordings, using deep learning methods. Current results indicate that the proposed solution has low intra-subject and inter-subject variability and the work can be further investigated with different applications as well as different paradigms. We are hoping that this work will be the first step to a complete paradigm analysis framework, which aims to automatically extract experiment related information from single-trial EEG signals and facilitates the overall research and analysis process as well being used as a part of various brain-computer interface applications.","2325-419X","978-1-6654-9804-3","10.1109/EDM55285.2022.9855191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9855191","EEG;deep learning;stop-signal;button press detection;convolutional neural networks","Deep learning;Presses;Neuroscience;Market research;Electroencephalography;Brain-computer interfaces;Recording","","","","17","IEEE","19 Aug 2022","","","IEEE","IEEE Conferences"
"Dual-Branch Residual CNN-LSTM Model for Emotion Classification using PPG and GSR","M. Saleem; S. Ullah; D. -H. Kim","Department of Electrical and Computer Engineering, Inha University, Incheon, South Korea; Department of Electrical and Computer Engineering, Inha University, Incheon, South Korea; Department of Electrical and Computer Engineering, Inha University, Incheon, South Korea",2025 IEEE International Conference on Big Data and Smart Computing (BigComp),"31 Mar 2025","2025","","","252","255","Emotion recognition using bio-signals such as photoplethysmography (PPG) and galvanic skin response (GSR) is a practical and efficient approach compared to other bio-signals like EEG and EMG. In this paper, we propose a novel deep learning model with a dual-branch architecture for emotion classification, specifically designed to extract both spatial and temporal features from PPG and GSR signals. Each branch of the model comprises 1D-CNN-based residual blocks for spatial feature extraction, followed by LSTM layers to capture temporal dependencies. The left branch processes PPG signals, while the right branch processes GSR signals. To enhance feature representation, we concatenate the outputs from both branches and pass them through fully connected layers, leveraging the “Swish” activation function. The self-gating behavior of Swish helps to suppress less relevant neural nodes while emphasizing critical ones, thereby improving model performance. Our model outperforms state-of-the-art methods on two publicly available datasets, MERTI-Apps and DEAP, achieving higher accuracy in both valence and arousal classification. Detailed results and analyses demonstrate the effectiveness of our approach for bio-signal-based emotion recognition.","2375-9356","979-8-3315-2902-4","10.1109/BigComp64353.2025.00055","National Research Foundation of Korea; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10936886","Emotion Classification;Photoplethysmography (PPG);Galvanic Skin Response (GSR);Residual Block;1D CNN, LSTM","Deep learning;Emotion recognition;Accuracy;Biological system modeling;Computer architecture;Feature extraction;Brain modeling;Photoplethysmography;Skin;Long short term memory","","","","18","IEEE","31 Mar 2025","","","IEEE","IEEE Conferences"
"Subject-independent emotion recognition of EEG signals using graph attention-based spatial-temporal pattern learning","Y. Zhu; Y. Guo; W. Zhu; L. Di; Z. Yin","School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, P.R. China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, P.R. China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, P.R. China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, P.R. China; Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai, P. R. China",2022 41st Chinese Control Conference (CCC),"11 Oct 2022","2022","","","7070","7075","Electroencephalography (EEG) reveals human brain activities and becomes an essential solution for exploring human intrinsic emotional states. In this study, we proposed a graph attention-based spatial-temporal pattern learning method called TAGAT to take full advantage of spatial structure of EEG channels and take the nonstationary of emotions into consideration. The attention mechanism is applied to compute weight coefficients of different spatial and temporal patterns. To alleviate differences between subjects, a domain discriminator is added to the model based on domain adaptation to tackle subject-independent EEG emotion recognition tasks. Based on the DEAP database, our method achieves the accuracy of 56.56% and 58.91% of arousal and valence dimensions for subject-independent emotion classification. The effectiveness of T-AGAT method has been demonstrated when compared to other existing common methods.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9901838","National Natural Science Foundation of China(grant numbers:61703277); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901838","Emotion recognition;electroencephalography;deep learning;transfer learning;pattern learning","Learning systems;Deep learning;Emotion recognition;Adaptation models;Databases;Computational modeling;Brain modeling","","","","20","","11 Oct 2022","","","IEEE","IEEE Conferences"
"CROSS-CORPUS EEG-BASED EMOTION RECOGNITION","S. Rayatdoost; M. Soleymani","Swiss Center for Affective Sciences and Computer Science Department, University of Geneva, Switzerland; USC Institute for Creative Technologies, University of Southern California, USA",2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP),"1 Nov 2018","2018","","","1","6","Lack of generalization is a common problem in automatic emotion recognition. The present study aims to explore the suitability of the existing EEG features for emotion recognition and investigate the performance of emotion recognition methods across different corpora. We introduce a novel dataset which includes spontaneous emotions and was analyzed in addition to the existing datasets for cross-corpus evaluation. We demonstrate that the performance of the existing methods significantly decreases when evaluated across different corpora. The best results are obtained by a convolutional neural network fed by spectral topography maps from different bands. We provide some evidence that stimuli-related sensory information is learned by machine learning models for emotion recognition using EEG signals.","1551-2541","978-1-5386-5477-4","10.1109/MLSP.2018.8517037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517037","EEG signals;emotion recognition;crosscorpus evaluation;deep learning","Electroencephalography;Emotion recognition;Feature extraction;Databases;Machine learning;Brain modeling;Frequency-domain analysis","","32","","25","IEEE","1 Nov 2018","","","IEEE","IEEE Conferences"
"Contrastive Self-supervised EEG Representation Learning for Emotion Classification","K. Hu; R. -J. Dai; W. -T. Chen; H. -L. Yin; B. -L. Lu; W. -L. Zheng","Department of Computer Science and Engineering, Shanghai Jiao Tong University; School of Software Engineering, Tongji University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","Self-supervised learning provides an effective approach to leverage a large amount of unlabeled data. Numerous previous studies have indicated that applying self-supervision to physiological signals can yield better representations of the signals. In the paper, we aim to apply this method to the crucial field of emotion recognition. We perform the experiment with several state-of-the-art contrastive self-supervised methods to explore their effectiveness in pre-training feature encoders on raw electroencephalography (EEG) signals and fine-tuning the pre-trained encoders on the downstream emotion classification tasks. We attempt to vary the proportion of labeled data used during fine-tuning and find that the improvement from self-supervised methods is more pronounced when the proportion of labeled data is small. Additionally, we explore the transferability of the feature encoders pre-trained on various datasets and observe that most self-supervised methods exhibit a certain degree of transferability. Methods that effectively utilize the temporal information in EEG signals show superior stability, accuracy, and transferability.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10781579","National Natural Science Foundation of China; Shanghai Jiao Tong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781579","Self-supervised learning;EEG emotion classification;Affective computing","Training;Representation learning;Emotion recognition;Accuracy;Time series analysis;Training data;Feature extraction;Electroencephalography;Physiology;Engineering in medicine and biology","Electroencephalography;Humans;Emotions;Algorithms;Signal Processing, Computer-Assisted;Supervised Machine Learning","","","17","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Classifying Human Emotions through EEG data with Machine Learning","G. Kaur; M. Gupta; R. Kumar","Department of Computer Science and Engineering, Chandigarh University, Punjab, India; Department of Computer Science and Engineering, Chandigarh University, Punjab, India; Department of Computer Science and Engineering, Chandigarh University, Punjab, India",2025 International Conference on Intelligent Systems and Computational Networks (ICISCN),"27 Mar 2025","2025","","","1","6","The Psychological and neurobiological phenomenon is called an emotion. The capability of human beings to present different expressions clashes with the emotional state of mind. Therefore, it is challenging to identify a human’s real emotional state by observing physical appearance. Nowadays, using emotional engineering to predict human emotions is one of the major attempts of Machine Learning (ML). This study aims to investigate the usage of techniques of machine learning for efficient emotion prediction that relies on electroencephalography (EEG) signals. EEG dataset is complex and sometimes noisy, making it difficult to interpret using traditional methods. So, in this research the Kaggle dataset collected through a Muse headband, five distinct ML methods are applied to classify emotions. Performance metrics such as precision, recall, F1 score, support, and accuracy are employed to differentiate between different classification techniques. Random Forest turned out to be the best method due to its high accuracy 99%. Three emotions are predicted based on the results of performance metrics. This study benefits mental health monitoring, neurofeedback therapy, stress management, human-computer interaction, etc. This study lays the groundwork for exploring this field and helps develop more sophisticated control systems in the future.","","979-8-3315-2924-6","10.1109/ICISCN64258.2025.10934662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10934662","EEG;BCI;Emotion detection;classification;LR;DT;RF;SVM;and KNN","Support vector machines;Radio frequency;Accuracy;Nearest neighbor methods;Electroencephalography;Object recognition;Noise measurement;Random forests;Neurofeedback;Monitoring","","","","21","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"E-EmotiConNet: EEG-based Emotion Recognition with Context Information","L. Jin; E. Y. Kim","Department of Computer Science and Engineering, Konkuk University, Seoul, South Korea; Department of Computer Science and Engineering, Konkuk University, Seoul, South Korea",2022 International Joint Conference on Neural Networks (IJCNN),"30 Sep 2022","2022","","","1","8","Electroencephalogram (EEG) based emotion recognition has received considerable research attention for brain-computer interface (BCI) and health care. The main challenge is to find an effective representation that is invariant to within and inter-subject differences associated with raw EEG data. In this paper, we propose a novel deep learning architecture for subject-independent emotion recognition, which is named E-EmotiConNet. The proposed model is a two-stream CNN architecture that effectively incorporates multi-channel EEG-time series information and spatial interactions. It comprises EEG-Emotion and EEG-Context networks, which can find emotion related features and subject oriented ones from raw EEG data, respectively. By fusing the interaction model with emotional features, the proposed model can improve emotion recognition accuracy. Experiments were performed on two publicly available datasets, DEAP and SEED, and verified the proposed E-EmotiConNet achieved classification accuracy of 93.39% and 93.69% on DEAP and SEED, respectively, compared with current state-of-the-art approach.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892017","National Foundation of Korea(NRF)(grant numbers:NRF-2020R1A2C1102081); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892017","Channel-wise feature;EEG emotion recognition;E-EmotiConNet;Multi-task learning","Emotion recognition;Neural networks;Knowledge based systems;Medical services;Brain modeling;Electroencephalography;Neuropsychology","","4","","28","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Automated Electroencephalogram-Based Emotion Detection: A Review","N. Gudikandula; R. C. Janapati; R. Sengupta; S. Chintala","School of Engineering, SR University, Warangal, India; School of Engineering, SR University, Warangal, India; Center of Creative Cognition, SR University, Warangal, India; School of CS &AI, SR University, Warangal, India",2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI),"4 Oct 2024","2024","","","572","577","Automatic emotion recognition has emerged as a significant research area in recent years, integrating new methods and technologies applicable to recommender systems and cognitive load detection. An emotion recognition system improves the exchange of information between machines and people. A branch within artificial intelligence devoted to identifying, analyzing, interpreting, and simulating emotional states—affective computing-focuses on human emotions. Utilizing physiological signals provides an improved scientific and reliable method to emotion detection. EEG signals are particularly sensitive to changes in emotional states, providing real-time insights influenced directly by the central nervous system. This sensitivity makes Electroencephalography (EEG) pivotal in Brain-Computer Interaction (BCI) systems and essential for understanding emotions across various contexts. This survey marks a critical advancement for researchers aiming to refine EEG- based emotion identification technologies. Such advancements are especially valuable for individuals who cannot verbally or physically express their emotions. By capturing and preprocessing EEG signals from the scalp, researchers extract key features crucial for distinguishing emotional states. This involves steps like feature extraction, selection, and reduction to optimize machine learning and deep learning algorithms for emotion classification tasks. Such comprehensive approaches not only deepen our comprehension of emotional states but also highlight EEG's practical applications in real-world scenarios.","","979-8-3315-4066-1","10.1109/ICoICI62503.2024.10696472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696472","Electroencephalogram;Brain-Computer Interaction;Emotions;Pre-processing;Feature Extraction;Machine learning and Deep learning","Deep learning;Surveys;Training;Emotion recognition;Time-frequency analysis;Sensitivity;Scalp;Feature extraction;Electroencephalography;Brain-computer interfaces","","","","45","IEEE","4 Oct 2024","","","IEEE","IEEE Conferences"
"Hardware Acceleration of EEG-Based Emotion Classification Systems: A Comprehensive Survey","H. A. Gonzalez; R. George; S. Muzaffar; J. Acevedo; S. Höppner; C. Mayr; J. Yoo; F. H. P. Fitzek; I. M. Elfadel","Chair for Highly-Parallel VLSI-Systems and Neuromorphic Circuits, Technische Universität Dresden, Dresden, Germany; Chair for Highly-Parallel VLSI-Systems and Neuromorphic Circuits, Technische Universität Dresden, Dresden, Germany; Department of Electrical Engineering and Computer Science, and the Center for Cyber Physical Systems (C2PS), Khalifa University, Abu Dhabi, United Arab Emirates; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Chair for Highly-Parallel VLSI-Systems and Neuromorphic Circuits, Technische Universität Dresden, Dresden, Germany; Chair for Highly-Parallel VLSI-Systems and Neuromorphic Circuits, Technische Universität Dresden, Dresden, Germany; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore; Deutsche Telekom Chair of Communication Networks, Technische Universität Dresden, Dresden, Germany; Department of Electrical Engineering and Computer Science, and the Center for Cyber Physical Systems (C2PS), Khalifa University, Abu Dhabi, United Arab Emirates",IEEE Transactions on Biomedical Circuits and Systems,"12 Aug 2021","2021","15","3","412","442","Recent years have witnessed a growing interest in EEG-based wearable classifiers of emotions, which could enable the real-time monitoring of patients suffering from neurological disorders such as Amyotrophic Lateral Sclerosis (ALS), Autism Spectrum Disorder (ASD), or Alzheimer's. The hope is that such wearable emotion classifiers would facilitate the patients’ social integration and lead to improved healthcare outcomes for them and their loved ones. Yet in spite of their direct relevance to neuro-medicine, the hardware platforms for emotion classification have yet to fill up some important gaps in their various approaches to emotion classification in a healthcare context. In this paper, we present the first hardware-focused critical review of EEG-based wearable classifiers of emotions and survey their implementation perspectives, their algorithmic foundations, and their feature extraction methodologies. We further provide a neuroscience-based analysis of current hardware accelerators of emotion classifiers and use it to map out several research opportunities, including multi-modal hardware platforms, accelerators with tightly-coupled cores operating robustly in the near/supra-threshold region, and pre-processing libraries for universal EEG-based datasets.","1940-9990","","10.1109/TBCAS.2021.3089132","Khalifa University Center for Cyber Physical Systems(grant numbers:C2PS); Deutsche Forschungsgemeinschaft; Germany's Excellence Strategy(grant numbers:EXC 2050/1,390696704); Centre for Tactile Internet with Human-in-the-Loop; Technische Universität Dresden; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9454320","Emotion detection and classification;EEG;hardware acceleration;machine learning;monitoring of neurological disorders","Hardware;Software;Feature extraction;Electroencephalography;Biomedical monitoring;Training;Software algorithms","Acceleration;Autism Spectrum Disorder;Computers;Electroencephalography;Emotions;Humans","18","","279","IEEE","14 Jun 2021","","","IEEE","IEEE Journals"
"Emotion Recognition with Multi-Channel EEG Signals Using Visual Stimulus","T. Ergin; M. A. Ozdemir; A. Akan","Department of Biomedical Engineering, Electrophysiological Signals Laboratory Izmir Katip Celebi University, Izmir, Turkey; Department of Biomedical Engineering, Electrophysiological Signals Laboratory Izmir Katip Celebi University, Izmir, Turkey; Department of Biomedical Engineering, Electrophysiological Signals Laboratory Izmir Katip Celebi University, Izmir, Turkey",2019 Medical Technologies Congress (TIPTEKNO),"11 Nov 2019","2019","","","1","4","Emotions are complex and may vary from person to person in a situation. The purpose of this study is to perform emotion analysis by using specific signal processing algorithms, to find the features and channels that are effective in the emotion recognition by using 60 visual stimuli with obtained EEG signals from the 32-channel EEG device that is branded Brain Products from 25 volunteers. The graphical user interface (GUI) has been designed to display visual stimuli at certain time intervals. Empirical mode decomposition (EMD) method has been applied to EEG signals and Intrinsic Mode Functions (IMFs) have been obtained. The most meaningful IMF has been investigated by using signal analysis methods such as Power Spectrum Density (PSD) using Periodogram and Welch's method. Feature extractions have been performed for filtered signal, IMF1, IMF2, IMF3 and average of first three IMFs. The classification has been done by the support vector machine (SVM). Different channels and features have been classified. It has been found that IMF1 and IMF2 are the most meaningful IMFs, Hjorth parameters have highest success rate as features and F4, T7, T8 EEG channels were the most effective channels with a success rate of 81.80%, 77.13%, 76% in emotions recognition.","2687-7783","978-1-7281-2420-9","10.1109/TIPTEKNO.2019.8895242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8895242","EEG Signal;Emotion Recognition;Empirical Mode Decomposition;Feature Extraction;Graphical User Interface;Intrinsic Mode Functions;Support Vector Machine","Support vector machines;Performance evaluation;Emotion recognition;Visualization;Signal processing algorithms;Feature extraction;Electroencephalography","","8","","21","IEEE","11 Nov 2019","","","IEEE","IEEE Conferences"
"An Analytical Approach of EEG Analysis for Emotion Recognition","I. Mazumder","Department of Electronics & Communication Engineering, Brainware University, Barasat, India",2019 Devices for Integrated Circuit (DevIC),"1 Aug 2019","2019","","","256","260","Emotion is the fundamental behavioral attributes of humans. To identify emotional variations from Electroencephalogram signals have currently expanded consideration amid BCI researchers. In this work, emotion recognition from EEG is performed using 21channel EEG acquisition device employing 10-20 method of electrode placement. The experiment being performed on issues of the peer group of 20-25 years of 16 university students (eight females and eight males). Audio-visual stimuli are used for bringing four dissimilar emotions (Happy, Sad, Fear and Relaxed) and corresponding signals are processed for emotion classification. At first EEG signals are filtered using Butterworth 4th order filter which is band limited by 0.5-60 Hz after that smoothened with the help of Surface Laplacian filter. Filtered EEG signals are feature extracted using Power Spectral Density, Wavelet Decomposition, Hjorth Parameter and AR parameter. After that Linear SVM classifier is used. Support Vector Machine classifier generates the best result when used with Wavelet coefficient feature extraction technique (96.81%). The experimental result also shows the diminutive interval EEG can be used for sensing the emotional thought variations effectively. We found that the EEG signals contained adequate information to separate four different emotion classes.","","978-1-5386-6722-4","10.1109/DEVIC.2019.8783331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8783331","EEG;PSD;Wavelet decomposition;Hjorth parameter;AR parameter;SVM","Electroencephalography;Feature extraction;Support vector machines;Multiresolution analysis;Time-frequency analysis;Emotion recognition;Correlation","","11","","12","IEEE","1 Aug 2019","","","IEEE","IEEE Conferences"
"Emotion recognition based on CNN-SVM from EEG signals","Y. Jia; C. Han; W. Tong; Y. Pei","Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China",2023 Asia Symposium on Image Processing (ASIP),"28 Dec 2023","2023","","","15","18","Emotion recognition is an important research directions in the field of artificial intelligence. In this paper, we adopted deep learning methods to try to improve the performance of emotion recognition. First, we designed an experiment to induce three different emotions by video footage and collected corresponding EEG signals. Then, CNN-SVM algorithms were adopted to classify positive, neutral and negative emotions. Finally, the recognition performance of CNN-SVM for three-class emotion recognition was verified. It is found that CNN-SVM achieves a highest recognition accuracy of 99.98% and an average recognition accuracy of 98.53%, which is better than CNN alone.","","979-8-3503-2342-9","10.1109/ASIP58895.2023.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10371689","EEG;emotion;CNN;CNN-SVM","Deep learning;Emotion recognition;Image processing;Asia;Electroencephalography;Classification algorithms;Artificial intelligence","","","","15","IEEE","28 Dec 2023","","","IEEE","IEEE Conferences"
"Wavelet ELM-AE Based Data Augmentation and Deep Learning for Efficient Emotion Recognition Using EEG Recordings","B. Ari; K. Siddique; Ö. F. Alçin; M. Aslan; A. Şengür; R. M. Mehmood","Department of Electrical and Electronics Engineering, Firat University, Elazıǧ, Turkey; Information and Communication Technology Department, School of Computing and Data Science, Xiamen University Malaysia, Sepang, Malaysia; Department of Electrical and Electronics Engineering, Malatya Turgut Özal University, Malatya, Turkey; Department of Electrical and Electronics Engineering, Bingol University, Bingol, Turkey; Department of Electrical and Electronics Engineering, Firat University, Elazıǧ, Turkey; Information and Communication Technology Department, School of Computing and Data Science, Xiamen University Malaysia, Sepang, Malaysia",IEEE Access,"14 Jul 2022","2022","10","","72171","72181","Emotion perception is critical for behavior prediction. There are many ways to capture emotional states by observing the body and copying actions. Physiological markers such as electroencephalography (EEG) have gained popularity, as facial emotions may not always adequately convey true emotion. This study has two main aims. The first is to measure four emotion categories using deep learning architectures and EEG data. The second purpose is to increase the number of samples in the dataset. To this end, a novel data augmentation approach namely the Extreme Learning Machine Wavelet Auto Encoder (ELM-W-AE) is proposed for data augmentation. The proposed data augmentation approach is both simple and faster than the other synthetic data augmentation approaches. For deep architectures, large datasets are important for performance. For this reason, data multiplexing approaches with classical and synthetic methods have become popular recently. The proposed synthetic data augmentation is the ELM-W-AE because of its efficiency and detail reproduction. The ELM-AE structure uses wavelet activation functions such as Gaussian, groove gap waveguide (GGW), Mexican, Meyer, Morlet, and Shannon. Deep convolutional architectures classify EEG signals as images. EEG waves are scalograms using Continuous Wavelet Transform (CWT). The ResNet18 architecture recognizes emotions. The proposed technique uses GAMEEMO data collected during gameplay. Each of these states is represented in the GAMEEMO data collection. The visual data set created from the signal was divided into two groups 70% training and 30% testing. ResNet18 has been fine-tuned with augmented photos, training images only. It achieved 99.6% classification accuracy in tests. The proposed method is compared with the other approaches on the same dataset, and an approximately 22% performance improvement is achieved.","2169-3536","","10.1109/ACCESS.2022.3181887","Xiamen University Malaysia Research Fund (XMUMRF)(grant numbers:XMUMRF/2022-C9/IECE/0035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810814","Auto-encoder;data augmentation;deep learning;emotion recognition;emotion measurement","Electroencephalography;Brain modeling;Emotion recognition;Feature extraction;Support vector machines;Deep learning;Data models","","16","","48","CCBY","29 Jun 2022","","","IEEE","IEEE Journals"
"EEG-based emotion recognition using hybrid filtering and higher order crossings","P. C. Petrantonakis; L. J. Hadjileontiadis","Department of Electrical & Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Electrical & Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece",2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops,"8 Dec 2009","2009","","","1","6","EEG-based emotion recognition is a relatively new research field in the human computer interaction area and its aim is the implementation of new algorithms that would identify and recognize emotions from EEG (electroencephalogram) signals. Towards that, a novel method is presented in this paper that employs an optimized hybrid filter, using empirical mode decomposition (EMD) and genetic algorithms (GA), in order to isolate the intrinsic mode functions (IMFs) corresponding to the plurality of the energy content of the initial signal for classification. The filtered signal is constructed by the selected IMFs and is subjected to higher order crossings (HOC) analysis for feature extraction. The final feature vector is classified into six emotion classes, i.e., happiness, anger, fear, disgust, sadness, and surprise, using quadratic discriminant analysis. The high classification performance (84.72% maximum mean classification rate) justifies the efficiency of the proposed EEG-based emotion recognition approach.","2156-8111","978-1-4244-4800-5","10.1109/ACII.2009.5349513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349513","","Emotion recognition;Filtering;Feature extraction;Human computer interaction;Electroencephalography;Signal processing;Optimization methods;Filters;Genetic algorithms;Signal analysis","","19","","23","IEEE","8 Dec 2009","","","IEEE","IEEE Conferences"
"A Secure Authentication System based on Emotion Analysis of EEG Signals using Deep Learning Technique","R. Mathumitha; A. Maryposonia","Department of Computer Science and Engineering, Sathyabama Institute of Science and Technology, Chennai, TamilNadu; Department of Computer Science and Engineering, Sathyabama Institute of Science and Technology, Chennai, TamilNadu",2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS),"8 Jun 2022","2022","","","1232","1237","In recent years, researchers take more attention to utilize electroencephalogram (EEG) signals as a biometric feature to secure numerous applications. Many personal and financial applications generally utilize biometric features like fingerprints and Iris for authentication. In a few cases, for additional security, one-time passwords are also used. However, there is no such research that uses EEG as a biometric feature for authentication and application security. Considering this as a research objective, in this research, an EEG-based emotion analysis has been presented to authenticate medical database access. This could identify the mental stability of the physicians so that human errors in the diagnosis due to stress can be reduced. Based on the emotion, either the user will get access to the database or be redirected to settle down the personal emotions. A deep learning-based emotion analysis has been presented to efficiently classify the benchmark DEAP and SEED datasets, which have numerous emotion data. Experimental results of the proposed work are compared with conventional techniques like support vector machines and artificial neural networks in terms of precision, recall, sensitivity, specificity, and accuracy. The proposed model attains maximum accuracy of 87.11% compared to existing methods.","2768-5330","978-1-6654-1035-9","10.1109/ICICCS53718.2022.9788367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9788367","Electroencephalogram (EEG);Emotion Analysis;Authentication;Deep Learning;Convolutional Neural Network (CNN)","Deep learning;Support vector machines;Databases;Computational modeling;Authentication;Medical services;Brain modeling","","2","","18","IEEE","8 Jun 2022","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition Using Multi-scale Window Deep Forest","H. Yao; H. He; S. Wang; Z. Xie","College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, China; College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, China; College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, China; College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, China",2019 IEEE Symposium Series on Computational Intelligence (SSCI),"20 Feb 2020","2019","","","381","386","With the fast development of human-machine interface technology, emotion recognition has attracted more and more attentions in recent years. Compared to other physiological experimental signals frequently used in emotion recognition, EEG signals are easy to record but not easy to disguise. However, because of high dimensionality of EEG data and the diversity of human emotions, feature extraction and classification of EEG signals are still difficult. In this paper, we propose deep forest with multi-scale window (MSWDF) to identify EEG emotions. Deep Forest is an integrated method of decision trees. In the MSWDF, features can be extracted by multi-granularity scanning with multi-scale windows. Compared with deep neural network, the MSWDF not only has less parameters to adjust, but also can realize the classification of the dataset with small samples. In the MSWDF, raw EEG signals were firstly filtered and segmented into samples. Regarding EEG signals as multivariate time series, a new multi-granularity scanning strategy with variable windows is proposed to extract features from EEG samples. After classifying EEG features by the cascade forest, the recognition results are compared with these of Nearest Neighbor algorithm (KNN), Naive Bayes, Decision Tree (DT), Random Forest (RF), Support Vector Machine (SVM). We found that the average classification accuracy of three emotions reaches to 84.90%, which is better than those of five compared methods.","","978-1-7281-2485-8","10.1109/SSCI44817.2019.9003164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003164","emotion recognition;EEG signal;deep forest;multi-scale window","Forestry;Electroencephalography;Emotion recognition;Decision trees;Feature extraction;Brain modeling;Microsoft Windows","","6","","26","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Multimodal Deep Learning Model for Subject-Independent EEG-based Emotion Recognition","S. Y. Dharia; C. E. Valderrama; S. G. Camorlinga","Applied Computer Science, University of Winnipeg, Winnipeg, Canada; Applied Computer Science, University of Winnipeg, Winnipeg, Canada; Applied Computer Science, University of Winnipeg, Winnipeg, Canada",2023 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),"26 Oct 2023","2023","","","105","110","Regulating emotion is crucial for maintaining well-being and social relationships. However, as we age, the volume of the frontal lobes is reduced, which can cause difficulties in regulating emotions. Electroencephalography (EEG)-based emotion recognition has the potential to understand the complexity of human emotions and the atrophy of the frontal lobes that leads to cognitive impairment. In this study, we investigated a multimodal deep learning approach for subject-independent emotion recognition using EEG and eye movement data. To that end, we proposed an attention mechanism layer to fuse features extracted from the EEG and eye movement data. We tested our approach in two benchmarking emotion recognition datasets: SEED-IV and SEED-V. Our approach achieved an average accuracy of 67.3% and 72.3% for SEED-IV and SEED-V, respectively. Our results demonstrate the potential of multimodal deep learning models for subject-independent emotion recognition using EEG and eye movement data, which can have important implications for assessing emotional regulation in clinical and research settings.","2576-7046","979-8-3503-2397-9","10.1109/CCECE58730.2023.10289007","Health; University of Winnipeg; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10289007","Emotion Recognition;Deep Learning;Electroencephalography (EEG)","Deep learning;Emotion recognition;Frontal lobe;Gaze tracking;Brain modeling;Feature extraction;Electroencephalography","","4","","21","IEEE","26 Oct 2023","","","IEEE","IEEE Conferences"
"Optimal Feature Selection and Deep Learning Ensembles Method for Emotion Recognition From Human Brain EEG Sensors","R. Majid Mehmood; R. Du; H. J. Lee","Division of Computer Science and Engineering, Chonbuk National University, Jeonju, South Korea; College of Geographic and Biologic Information, Nanjing University of Posts and Telecommunications, Nanjing, China; Center for Advanced Image and Information Technology, Chonbuk National University, Jeonju, South Korea",IEEE Access,"9 Aug 2017","2017","5","","14797","14806","Recent advancements in human-computer interaction research have led to the possibility of emotional communication via brain-computer interface systems for patients with neuropsychiatric disorders or disabilities. In this paper, we efficiently recognize emotional states by analyzing the features of electroencephalography (EEG) signals, which are generated from EEG sensors that noninvasively measure the electrical activity of neurons inside the human brain, and select the optimal combination of these features for recognition. In this paper, the scalp EEG data of 21 healthy subjects (12-14 years old) were recorded using a 14-channel EEG machine while the subjects watched images with four types of emotional stimuli (happy, calm, sad, or scared). After preprocessing, the Hjorth parameters (activity, mobility, and complexity) were used to measure the signal activity of the time series data. We selected the optimal EEG features using a balanced one-way ANOVA after calculating the Hjorth parameters for different frequency ranges. Features selected by this statistical method outperformed univariate and multivariate features. The optimal features were further processed for emotion classification using support vector machine, k-nearest neighbor, linear discriminant analysis, Naive Bayes, random forest, deep learning, and four ensembles methods (bagging, boosting, stacking, and voting). The results show that the proposed method substantially improves the emotion recognition rate with respect to the commonly used spectral power band method.","2169-3536","","10.1109/ACCESS.2017.2724555","Brain Korea 21 PLUS Project; National Research Foundation (NRF) of Korea; Ministry of Science, ICT and Future Planning, Korea, under the Information Technology Research Center, supervised by the Institute for Information and Communications Technology Promotion(grant numbers:IITP-2016-R0992-16-1023); Basic Science Research Program through the NRF of South Korea, through the Ministry of Education(grant numbers:GR 2016R1D1A3B03931911); National Natural Science Foundation for Young Scholars of China(grant numbers:61603198); Natural Science Foundation for Young Scholars of Jiangsu Province(grant numbers:BK20160918); NUPTSF(grant numbers:NY214194); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7997991","EEG pattern recognition;Hjorth parameter;EEG feature extraction;EEG emotion recognition","Electroencephalography;Feature extraction;Emotion recognition;Electrodes;Sensors;Support vector machines;Medical services","","155","","46","OAPA","31 Jul 2017","","","IEEE","IEEE Journals"
"EEG-based automatic emotion recognition: Feature extraction, selection and classification methods","P. Ackermann; C. Kohlschein; J. Á. Bitsch; K. Wehrle; S. Jeschke","Institute of Information Management in Mechanical Engineering (IMA), RWTH Aachen University, Germany; Institute of Information Management in Mechanical Engineering (IMA), RWTH Aachen University, Germany; Chair of Communication and Distributed Systems (COMSYS), RWTH Aachen University, Germany; Chair of Communication and Distributed Systems (COMSYS), RWTH Aachen University, Germany; Institute of Information Management in Mechanical Engineering (IMA), RWTH Aachen University, Germany","2016 IEEE 18th International Conference on e-Health Networking, Applications and Services (Healthcom)","21 Nov 2016","2016","","","1","6","Automatic emotion recognition is an interdisciplinary research field which deals with the algorithmic detection of human affect, e.g. anger or sadness, from a variety of sources, such as speech or facial gestures. Apart from the obvious usage for industry applications in human-robot interaction, acquiring the emotional state of a person automatically also is of great potential for the health domain, especially in psychology and psychiatry. Here, evaluation of human emotion is often done using oral feedback or questionnaires during doctor-patient sessions. However, this can be perceived as intrusive by the patient. Furthermore, the evaluation can only be done in a noncontinuous manner, e.g. once a week during therapy sessions. In contrast, using automatic emotion detection, the affect state of a person can be evaluated in a continuous non-intrusive manner, for example to detect early on-sets of depression. An additional benefit of automatic emotion recognition is the objectivity of such an approach, which is not influenced by the perception of the patient and the doctor. To reach the goal of objectivity, it is important, that the source of the emotion is not easily manipulable, e.g. as in the speech modality. To circumvent this caveat, novel approaches in emotion detection research the potential of using physiological measures, such as galvanic skin sensors or pulse meters. In this paper we outline a way of detecting emotion from brain waves, i.e., EEG data. While EEG allows for a continuous, real-time automatic emotion recognition, it furthermore has the charm of measuring the affect close to the point of emergence: the brain. Using EEG data for emotion detection is nevertheless a challenging task: Which features, EEG channel locations and frequency bands are best suited for is an issue of ongoing research. In this paper we evaluate the use of state of the art feature extraction, feature selection and classification algorithms for EEG emotion classification using data from the de facto standard dataset, DEAP. Moreover, we present results that help choose methods to enhance classification performance while simultaneously reducing computational complexity.","","978-1-5090-3370-6","10.1109/HealthCom.2016.7749447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749447","","Electroencephalography;Feature extraction;Emotion recognition;Support vector machines;MATLAB;Physiology;Radio frequency","","88","","28","IEEE","21 Nov 2016","","","IEEE","IEEE Conferences"
"Subject-Independent EEG-based Emotion Recognition using Adversarial Learning","S. Hwang; M. Ki; K. Hong; H. Byun","Department of Computer Science, Yonsei University, Seoul, Korea; Department of Computer Science, Yonsei University, Seoul, Korea; Department of Computer Science, Yonsei University, Seoul, Korea; Department of Computer Science, Yonsei University, Seoul, Korea",2020 8th International Winter Conference on Brain-Computer Interface (BCI),"9 Apr 2020","2020","","","1","4","Electroencephalography (EEG) based emotion recognition studies have been conducted in recent years. Most prior researches are based on subject-dependent models since EEG signals have a large variation between individuals. In this paper, we propose a novel EEG-based emotion recognition approach that addresses the challenging issue of the subject-dependency. To solve the problem, we design a multi-task deep neural network, which consists of two objectives. The first one is to classify subject-independent emotional labels, and the second is to make the model cannot distinguish the subject labels. To achieve the latter purpose, we adversarially learn the proposed model, which has three components: 1) Emotion classification module, 2) Subject classification module, 3) Adversarial module. To make the model confuse the subject labels, we apply the randomization function to the subject classification module for adversarial learning. For the experiment, we evaluate the proposed method to classify EEG emotional labels with a leave-one-subject-out scheme on SEED dataset, which has recorded EEG from 15 participants and contains three emotional labels: positive, negative, and neutral. We compare the proposed method with a single-task deep neural network and multi-task model that classify emotional labels with subject labels. Our experimental results show that the proposed method achieves better results than the others with an average accuracy of 75.31%. Moreover, the standard deviation of our model was 7.33%, which is the lowest with the compared models.","2572-7672","978-1-7281-4707-9","10.1109/BCI48061.2020.9061624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9061624","subject independent model;EEG emotion recognition;adversarial learning;multi-task learning;deep neural network","Brain modeling;Electroencephalography;Emotion recognition;Standards;Task analysis;Biological neural networks","","35","","19","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Locally Robust Feature Selection of EEG Signals for the Inter-subject Emotion Recognition","Z. Yin; W. Zhang; Z. Zheng","Engineering Research Center of Optical Instrument and System, University of Shanghai for Science and Technology, Shanghai, P. R. China; Engineering Research Center of Optical Instrument and System, University of Shanghai for Science and Technology, Shanghai, P. R. China; Engineering Research Center of Optical Instrument and System, University of Shanghai for Science and Technology, Shanghai, P. R. China",2020 39th Chinese Control Conference (CCC),"9 Sep 2020","2020","","","6250","6255","Brain computer interface (BCI) systems has the capability to extract cortical affective activities into interpretable features to achieve the human-computer interaction based on the emotional clues. However, individual differences of cortical responses from the BCI users has become an obstacle to inter-subject emotion recognition. In this study, a locally-robust feature selection (LRFS) method has been proposed to find generic feature subsets of the electroencephalography (EEG) within a group of accessible subjects. The LRFS first modeled EEG features by using different probability densities. By quantitatively measuring the similarity of the estimated density functions, inter-intersubject consistency of each EEG feature is represented. By ordering the values of the consistency, the locally-robust EEG features can be properly selected. To fuse the selected features, we follow the ensemble learning principle to build a classification committee. Based on the public DEAP database, the inter-subject classification accuracy of arousal and valence dimensions for the LRFS-based classifier is achieved by 0.65 and 0.68, respectively. The competiveness of the LRFS has been demonstrated when we compared it with the several existing feature selection methods.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9189239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189239","Emotion recognition;physiological signals;machine learning;feature selection;pattern recognition","Electroencephalography;Feature extraction;Emotion recognition;Probability density function;Robustness;Databases;Brain-computer interfaces","","2","","15","","9 Sep 2020","","","IEEE","IEEE Conferences"
"EEG Correlation Analysis-guided Graph Local Enhanced Feature Learning For Emotion Recognition","X. Li; G. Zhuang; M. Wu; Z. Lv","School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","EEG-based emotion recognition is a key technology in brain-computer interfaces. Many previous studies have applied deep learning methods to mine emotion-related features in EEG to decode emotions. However, they overlooked the importance of electrode correlations and varying brain region activation during emotional processes, which are critical for emotion recognition. In this paper, we propose a method named EEG correlation analysis-guided graph local enhanced feature learning network (CAGLE-net). In CAGLE-net, we use the correlation analysis to guide the learning of the dynamic directed connection matrix to capture topological features, which are then fed into the locally enhanced embedding layer to generate enhanced features for each brain region. Subsequently, the cross-attention fusion mechanism is employed to fully leverage these locally enhanced features, yielding more discriminative representations. Experiment results on the SEED dataset show that CAGLE-net outperforms existing baseline methods. This study offers a promising solution for EEG-based emotion recognition.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890374","National Natural Science Foundation of China; Youth Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890374","Emotion recognition;electroencephalography (EEG);graph convolution Network;cross-attention mechanism;brain-computer interfaces(BCI)","Representation learning;Electrodes;Deep learning;Emotion recognition;Correlation;Convolution;Speech recognition;Electroencephalography;Brain-computer interfaces;Speech processing","","","","39","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Emotion Recognition using Machine Learning Models on Electroencephalogram (EEG) Data","S. Asha; R. S; S. S. S; V. K","Department of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, Tamilnadu, India; Department of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, Tamilnadu, India; Department of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, Tamilnadu, India; Department of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, Tamilnadu, India",2023 Second International Conference on Augmented Intelligence and Sustainable Systems (ICAISS),"22 Sep 2023","2023","","","657","662","Analyzing Electroencephalogram (EEG) data using cutting-edge machine learning techniques to comprehend emotional states. EEGs are non-invasive and have been shown to illuminate the complex neural mechanisms at play during affective experiences in the human brain. Everything is addressed in detail, from data collection and organization to feature extraction for model training and evaluation. It has the potential to increase the accuracy of diagnosing mental health disorders and has additional applications in psychology, human-computer interaction, and healthcare. The proposed machine learning algorithms and refined data processing strategies are being investigated to improve the accuracy and applicability of EEG-based emotion identification. This is the case despite the challenges posed by factors such as the signal's complexity, individual variation, and the need for robust feature extraction. Due to the interdisciplinary nature of the discipline and the ongoing collaboration of its practitioners, EEG analysis has the potential to shed light on underexplored aspects of the human emotional experience.","","979-8-3503-2579-9","10.1109/ICAISS58487.2023.10250578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10250578","Signal complexity;Human-computer interaction;Electroencephalogram (EEG);Machine Learning;Emotion-aware technologies","Training;Human computer interaction;Emotion recognition;Machine learning algorithms;Machine learning;Medical services;Brain modeling","","","","15","IEEE","22 Sep 2023","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Temporal Convolutional Network","L. Yang; J. Liu","School of Automation and Electrical Engineering, University of Science and Technology, Beijing, P. R. China; School of Automation and Electrical Engineering, University of Science and Technology, Beijing, P. R. China",2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS),"25 Nov 2019","2019","","","437","442","Emotion recognition based on physiological signal can be used in many applications such as, intelligent human-computer interface design, emotional disorder diagnoses. For traditional approaches, the prior knowledge is required to design and extract a range of features from physiological signal. The generalization ability of traditional methods is poor because of the lack of high-level features. Using deep-learning methodologies to analyze physiological signal, i.e. eeg, becomes increasingly attractive for recognizing emotions. In this paper, we design a sequence model based on deep-learning that uses Temporal Convolutional Network(TCN) to extract high-level features in consideration of the time dependence of physiological signals for EEG emotion recognition. Specifically, we extract the differential entropy feature in seconds and construct a time series with fixed-length time window data as the input to TCN, and then using softmax to classify. Furthermore, in order to get reliable results, we divide the samples according to the trials, avoiding the testing set samples and training set samples from the same trial. Specifically, we first divide the samples according to the trials as the testing set and the training set, and then segment the trials in the testing set and training set with fixed time window length to obtain more samples respectively. To evaluate the performance of the proposed model, we conduct the emotion classification experiments on DEAP database. The experimental results show the effectiveness of our proposed model for EEG emotion recognition.","","978-1-7281-1454-5","10.1109/DDCLS.2019.8908839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908839","EEG;TCN;emotion recognition;deep-learning","Feature extraction;Electroencephalography;Convolution;Emotion recognition;Brain modeling;Entropy;Physiology","","13","","22","IEEE","25 Nov 2019","","","IEEE","IEEE Conferences"
"A Comparative Study between Supervised and Unsupervised Techniques for Two Class Emotion Recognition using EEG","P. Kar; J. Hazarika; M. R. Sethi","Department of Electronics and Instrumentation Engineering, NIT, Silchar, Silchar, India; Department of Electronics and Instrumentation Engineering, NIT, Silchar, Silchar, India; Department of Electronics and Instrumentation Engineering, NIT, Silchar, Silchar, India",2023 IEEE 8th International Conference for Convergence in Technology (I2CT),"23 May 2023","2023","","","1","6","Brain-Computer Interface (BCI) based emotion recognition is mostly motivated by the supervised technique and may get biased by the emotion category representation. An alternative approach to emotion recognition could be employing an unsupervised technique. We compared a supervised and an unsupervised technique for two-class emotion recognition using time-frequency-based EEG features. Results obtained from the unsupervised technique reveal incongruency between the actual physiological response and the established ground truth (emotion category), whereas an average accuracy of 62.76% is observed for different emotion categories while employing the supervised technique.","","979-8-3503-3401-2","10.1109/I2CT57861.2023.10126336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10126336","BCI;Emotion recognition;supervised technique;unsupervised technique;EEG","Emotion recognition;Electroencephalography;Physiology;Brain-computer interfaces;Convergence","","1","","22","IEEE","23 May 2023","","","IEEE","IEEE Conferences"
"eRAD-Fe: Emotion Recognition-Assisted Deep Learning Framework","S. -H. Kim; N. A. T. Nguyen; H. -J. Yang; S. -W. Lee","Department of the Artificial Intelligence, Korea University, Seoul, South Korea; Faculty of Information Technology, The University of Danang—University of Science and Education, Da Nang, Vietnam; Department of Computer Science, Chonnam National University, Gwangju, South Korea; Department of the Artificial Intelligence, Korea University, Seoul, South Korea",IEEE Transactions on Instrumentation and Measurement,"7 Oct 2021","2021","70","","1","12","With recent advancements in artificial intelligence technologies and human–computer interaction, strategies to identify the inner emotional states of humans through physiological signals such as electroencephalography (EEG) have been actively investigated and applied in various fields. Thus, there is an increasing demand for emotion analysis and recognition via EEG signals in real time. In this article, we proposed a new framework, emotion recognition-assisted deep learning framework from eeg signal (eRAD-Fe), to achieve the best recognition result from EEG signals. eRAD-Fe integrates three aspects by exploiting sliding-window segmentation method to enlarge the size of the training dataset, configuring the energy threshold-based multiclass common spatial patterns to extract the prominent features, and improving emotional state recognition performance based on a long short-term memory model. With our proposed recognition-assisted framework, the emotional classification accuracies were 82%, 72%, and 81% on three publicly available EEG datasets, such as SEED, DEAP, and DREAMER, respectively.","1557-9662","","10.1109/TIM.2021.3115195","Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Education(grant numbers:NRF-2021R1I1A1A01048455); Vietnam National Foundation for Science and Technology Development (NAFOS-TED)(grant numbers:102.01-2020.27); National Research Foundation of Korea (NRF) Grant; Korean Government (MSIT)(grant numbers:NRF-2020R1A4A1019191); Institute for Information and Communications Technology Promotion (IITP) Grant; Government of South Korea (Development of BCI-Based Brain and Cognitive Computing Technology for Recognizing User’s Intentions using Deep Learning; Artificial Intelligence Graduate School Program, Korea University)(grant numbers:2017-0-00451,2019-0- 00079); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547276","Deep learning;electroencephalogram;emotion recognition;energy threshold;feature extraction;long short-term memory;multiclass common spatial pattern (CSP)","Feature extraction;Electroencephalography;Emotion recognition;Deep learning;Data mining;Covariance matrices;Speech recognition","","10","","61","IEEE","23 Sep 2021","","","IEEE","IEEE Journals"
"Emotion recognition system based on MAPL","H. Jiang; X. Song; Z. Zhou","MinZu University of China, Beijing, China; MinZu University of China, Beijing, China; MinZu University of China, Beijing, China",2014 IEEE 5th International Conference on Software Engineering and Service Science,"23 Oct 2014","2014","","","581","584","A EEG signal-based emotion recognition system was designed. The system was developed to operate as a user-independent system, based on MAPL (minority affective picture library), EEG-signal database obtained from multiple ethnic objections. The system consisted of preprocessing, feature extraction and pattern classification stages. Preprocessing and feature extraction methods were devised so that emotion-specific characteristics could be extracted. a simple experiment was carried out, and the classification result is about 56.4%, which indicated that minorities emotion problems can be studied based MPAL emotion recognition system.","2327-0594","978-1-4799-3279-5","10.1109/ICSESS.2014.6933635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933635","MAPL(minority affective picture library);Ethnic emotion;Pattern recognition","Emotion recognition;Electroencephalography;Feature extraction;Physiology;Computers;Classification algorithms;Support vector machines","","","","7","IEEE","23 Oct 2014","","","IEEE","IEEE Conferences"
"Decoding Emotions Using Deep Learning Approach to EEG-Based Emotion Recognition","R. Rajesh Immanuel; S. K. B. Sangeetha","Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India; Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India",2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS),"19 Mar 2024","2023","","","1","6","In disciplines including medicine, psychology, and human-computer interaction, understanding and interpreting human emotions is crucial. Emotion analysis that is accurate and real-time has the potential to transform these fields. Deep learning methods combined with Electroencephalography (EEG) inputs have become a viable approach for enhancing emotion identification systems in recent years. With EEG data and a cutting-edge Deep CNN algorithm, this study introduces a novel method for emotion analysis that achieves a remarkable accuracy rate of 96.24% and model loss as 0.89. We gathered EEG data from a variety of participants, drepresenting a spectrum of emotional reactions. EEG data are a useful source for emotion research because they provide clear insights into the electrical activity of the brain.In our study, EEG data is processed using a Deep Convolutional Neural Network (CNN) architecture, which is well-known for its efficiency in image-related tasks. This deep learning algorithm improves the precision of emotion identification by learning complex spatial and temporal patterns from EEG data.The creation of a novel feature extraction technique specifically designed for EEG data is a key breakthrough in our strategy. By capturing both spectral and temporal properties, this technique gives the model additional discriminative data for classifying emotions.A great accuracy rate of 96.24% was attained by the suggested technique while classifying emotions. This extraordinary precision indicates the dependability and strength of our method. Real-time emotion detection is made possible by our model's quick analysis of EEG data, which has implications for virtual reality, human-computer interaction, and mental health monitoring, among other fields. In order to further our understanding of how the human brain reacts to emotional inputs, we offer insights into the neural patterns connected to various emotions.This study illustrates the application of deep learning, more especially the Deep CNN algorithm, to emotion analysis using EEG data. With the help of this cutting-edge architecture and our innovative feature extraction technique, accuracy has greatly increased, making it a useful tool for situations where accurate emotion identification is essential. This discovery has ramifications across a range of fields, including psychology, healthcare, and human-computer interaction, eventually improving our capacity to recognize and react to emotional states in others.","","979-8-3503-9458-0","10.1109/ICCEBS58601.2023.10449107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449107","EEG Signal;Stress;CNN;Deep Learning;Feature Extraction;Emotion Recognition;Accuracy","Human computer interaction;Deep learning;Solid modeling;Brain modeling;Electroencephalography;Real-time systems;Convolutional neural networks","","","","28","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"CFBM: Cubic Filtered Brain Map Creation Method and Models for Eeg-Based Emotion Recognition","X. Gao; D. Wang; Y. Zhao; X. Wang","State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences",2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI),"12 May 2025","2025","","","1","5","Emotion recognition based on electroencephalogram (EEG) is one of the most prominent areas within biomedical re-search. With the advancement of deep learning, many studies have focused on developing more complex models to im-prove recognition performance. However, these approaches can not resol ve the challenges posed by the low discrim-inability of EEG data. In this paper, we propose a Cubic Filtered Brain Map (CFBM) creation method for EEG-based emotion recognition, which introduces an innovative use of Gaussian filtering to enhance feature extraction, thereby improving the discriminability of the EEG data. We theo-retically demonstrate the effectiveness of CFBM in reducing noise, enhancing spatial continuity, and improving feature discriminability. Additionally, we propose convolutional neural network (CNN) models that incorporate instance nor-malization (IN) and batch normalization (BN). Extensive experiments conducted on the DEAP dataset demonstrate the effectiveness of CFBM in enhancing EEG data discrim-inability across various model architectures. The recognition accuracies for valence and arousal reach 99.34% and 99.29%, achieving state-of-the-art performance. Our code is available at https://github.com/Gloria-G2021/CFBM.","1945-8452","979-8-3315-2052-6","10.1109/ISBI60581.2025.10981248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10981248","EEG;Emotion Recognition;Gaussian Filter;Deep Learning;Differential Entropy","Emotion recognition;Accuracy;Filtering;Biological system modeling;Noise;Brain modeling;Feature extraction;Electroencephalography;Entropy;Convolutional neural networks","","","","16","IEEE","12 May 2025","","","IEEE","IEEE Conferences"
"Multi-Channel EEG-based Multi-Class Emotion Recognition From Multiple Frequency Bands","B. Revanth; S. Gupta; P. Dubey; B. Choudhury; K. Kamble; J. Sengupta","Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India","2023 2nd International Conference on Paradigm Shifts in Communications Embedded Systems, Machine Learning and Signal Processing (PCEMS)","2 Jun 2023","2023","","","1","5","Electroencephalogram (EEG)-based emotion recognition has demonstrated encouraging results using machine learning (ML)-based algorithms. This study compares the performance of different frequency bands using four ML-based classifiers for the recognition of multi-class human emotions from EEG signals. Initially, the raw EEG signals are divided into five frequency bands such as delta, theta, alpha, beta, and gamma bands. Secondly, the statistical, time and frequency domain features are extracted. To classify emotions into positive, negative and neutral classes from the SEED dataset, these features are fed to four ML-based classifiers. This study shows the efficacy of an ensemble ML-based classifier over traditional classifiers. The best highest average classification accuracy reported by the random forest (RF) classifier for the delta band is 95.71%. The second highest average accuracy was reported by KNN with 80.32% for the theta band. A similar trend was also followed by other frequency bands. In conclusion, our study demonstrated the value of the proposed ML-based model for multi-class emotion recognition.","","979-8-3503-1071-9","10.1109/PCEMS58491.2023.10136120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136120","EEG;multi-class emotions recognition;statistical features;frequency-domain features;machine-learning classifiers","Support vector machines;Emotion recognition;Machine learning algorithms;Frequency-domain analysis;Signal processing;Feature extraction;Market research","","5","","18","IEEE","2 Jun 2023","","","IEEE","IEEE Conferences"
"Emotion Recognition Classification Using an EEG- based Brain Computer Interface System based on Different Machine Learning Models","N. Pandey; O. Sharma","Computer Science and Engineering, SRMIST, Ghaziabad, India; Computer Science and Engineering, SRMIST, Ghaziabad, India",2022 2nd International Conference on Innovative Sustainable Computational Technologies (CISCT),"22 Feb 2023","2022","","","1","5","Brain Computer Interface (BCI) is a communication channel between brain and machine which helps to control the instruction over the machine. The BCI helps in both the situations where patients are not able to move a bit and utter a word. There are many devices made that are being used by the people with disabilities and helping them in doing their work. Brain computer interfaces have been made useful for the patients with mental disorders like strokes and also for the other physical disorders. BCI first acquires and analyzes the brain Electroencephalography (EEG) signals, translating them into the desired actions that are needed to fulfill the command given by the brain. The recent advancement in the brain computer interface technology is exciting for the scientists, engineers and clinical persons which further enlighten the rapid development and growing research. The classification and detection on BCI systems for the patients have been made. This study proposes a novel stochastic model that classifies the emotions as: positive, negative and neutral with the accuracy of 96% using the ensemble model (Multiclass Logistic Regression, Light Gradient Boosting Machine (LGBM), Random Forest Classifier and Decision Tree Classifier).","","978-1-6654-7416-0","10.1109/CISCT55310.2022.10046601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046601","EEG;BCI;ensemble learning;random forest;positive;negative;neutral;machine learning;LGBM;Logistic regression","Emotion recognition;Computational modeling;Mental disorders;Stochastic processes;Forestry;Stroke (medical condition);Brain modeling","","1","","28","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
"A study on Frequency Domain Microstate Feature Fusion for EEG Emotion Recognition","D. Xiao; Z. Lv; S. Hu","Sch. of Computer Science and Technology, Anhui University, Hefei, China; Sch. of Computer Science and Technology, Anhui University, Hefei, China; Sch. of Computer Science and Technology, Anhui University, Hefei, China","2022 7th International Conference on Communication, Image and Signal Processing (CCISP)","19 Dec 2022","2022","","","428","433","The microstate analysis of EEG signals makes full use of the spatial information of the brain topographic map, and reflects the active association of different brain regions. Different from the traditional EEG features that mostly focus on single-channel information, the microstate feature contains the spatio-temporal information of EEG signals. Unlike microstate studies that mostly focus on dimensional emotions, the experiments classify positive, neutral, and negative discrete emotions using the SEED database. This work filters the data of a single subject into five frequency bands and calculates the microstate topographic maps of EEG signals in different frequency bands, respectively. The extracted features of microstate classes are coverage, duration, occurrence, and transition probability between microstates. The gender difference as to the dominant microstate pattern for emotions and the comparison between microstates, we found that the brain activity of males in three emotional states and females in negative emotions were related to the frontal-occipital pattern, the females of positive and neutral emotional states were associated with the left and right brain areas. We also investigated the traditional power spectra features, these features which be fused over frequency bands or not fused were fed into the classifiers such as the K-Nearest Neighbor (KNN) and the the Support Vector Machine(SVM) to classify discrete emotional labels in SEED. The average classification accuracy of 15 subjects was 97.67±1.4% and 92.58±3.24%, respectively.","","978-1-6654-5959-4","10.1109/CCISP55629.2022.9974553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9974553","EEG signals;microstate;discrete emotional lables","Emotion recognition;Brain;Frequency-domain analysis;Support vector machine classification;Signal processing;Feature extraction;Electroencephalography","","2","","25","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Attention-Temporal Convolutional Networks for EEG-based Emotion Recognition","D. Kim; H. Kim; C. Jin; S. -E. Kim",Seoul National University of Science and Technology; Seoul National University of Science and Technology; Seoul National University of Science and Technology; Seoul National University of Science and Technology,2023 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia),"27 Nov 2023","2023","","","1","4","Emotion recognition is an emerging technology that employs various types of data modalities to detect and interpret human emotional states. Among them, electroencephalogram (EEG) has gained attention for its ability to capture underlying emotions related to specific brain activities. In this study, we adopt the attention-temporal convolutional network (ATCNet), previously recognized for its efficacy in decoding motor imagery EEG data, to develop a high-performance emotion recognition method. Our modified ATCNet achieved an average accuracy of 95.71% using the SEED dataset, which has three emotional classes, including positive, negative, and neutral.","","979-8-3503-4431-8","10.1109/ICCE-Asia59966.2023.10326369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10326369","ATCNet;Classification;Deep learning;EEG;Emotion recognition;SEED","Deep learning;Emotion recognition;Brain;Image recognition;Electroencephalography;Decoding;Convolutional neural networks","","","","3","IEEE","27 Nov 2023","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition while Listening to Quran Recitation Compared with Relaxing Music Using Valence-Arousal Model","S. A. Y. Al-Galal; I. F. T. Alshaikhli; A. W. bin Abdul Rahman; M. A. Dzulkifli","Dept. of Comput. Sci., International Islamic University Malaysia, Kuala Lumpur, Wilayah Persekutuan, MY; Dept. of Comput. Sci., International Islamic University Malaysia, Kuala Lumpur, Wilayah Persekutuan, MY; Dept. of Comput. Sci., International Islamic University Malaysia, Kuala Lumpur, Wilayah Persekutuan, MY; Dept. of Comput. Sci., International Islamic University Malaysia, Kuala Lumpur, Wilayah Persekutuan, MY",2015 4th International Conference on Advanced Computer Science Applications and Technologies (ACSAT),"26 May 2016","2015","","","245","250","Relaxation and calmness are two emotions that people always seek for. One popular method people used to do in order to reduce their level of tension and pressure is listening to some types of relaxing music. On the other hand, Quran is Allah's words that are ultimately given to us human to benefit of. Although, Muslims are strongly believed that listening to Quran or reading it brings them to comfort, pleasure and confidence. Scientific evidence is still required to prove that scientifically. Human emotion can be recognized from voice, text, facial expression or body language. But those methods are susceptible to change and are not really accurate. Recently, electroencephalograms (EEG) allowed researchers to evoke the inner emotions. This paper aims to study human emotions while listening to Quran recitation compared with listening to relaxing music. To evoke emotions, some stimuli should be used, in this research we implemented International Affective Picture System (IAPS) database. And for the emotion classification technique we followed two-dimensional Arousal-Valence emotion model. Finally the emotion model was implemented to recognize four basic emotions Happy, Fear, Sad and Calm with an average accuracy of 76.81 %. The data collected while listening to Quran and music were tested and the result generally showed that both Quran and Music are classified more into positive valence.","","978-1-5090-0424-9","10.1109/ACSAT.2015.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478752","Quran recitation;Relaxing music;EEG;emotion;Valence;Arousal","Music;Electroencephalography;Brain modeling;Classification algorithms;Emotion recognition;Feature extraction;Finite impulse response filters","","8","","19","IEEE","26 May 2016","","","IEEE","IEEE Conferences"
"A Novel Multidimensional Feature Extraction Method Based on VMD and WPD for Emotion Recognition","M. Zhang; B. Hu; X. Zheng; T. Li","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Education, Shandong Normal University, Jinan, China",2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"13 Jan 2021","2020","","","1216","1220","Emotion plays an indispensable role in the process of human cognition and decision-making, but it is often neglected in human-computer interaction (HCI). Although many researchers have applied some decomposition algorithms to extract features from EEG, there are several shortcomings including the modal aliasing, a high computational cost and so on. In this paper, we introduce variational mode decomposition (VMD) and wavelet packet decomposition (WPD) algorithms and propose a multidimensional feature extraction method based on VMD and WPD for emotion recognition. Firstly, we apply VMD to decompose the EEG signal into a specific number of variational mode functions (VMF). Secondly, WPD is executed to generate an emotional frequency band. Then, we continue to extract the wavelet packet entropy (WPE), modified multi-scale sample entropy (MMSE), fractal dimension (FD) and first difference (1ST) of each emotional VMF to construct a new feature form. Finally, the random forest (RF) is utilized to classify the extracted emotional states. The experiment results demonstrate that the proposed method is more competitive and universal for emotion recognition, which provides a novel idea for the application of multidimensional feature extraction.","","978-1-7281-6215-7","10.1109/BIBM49941.2020.9313220","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9313220","EEG;Emotion recognition;Feature extraction;Variational Mode Decomposition;Wavelet Packet Decomposition","Feature extraction;Electroencephalography;Emotion recognition;Wavelet packets;Time-frequency analysis;Time series analysis;Entropy","","8","","18","IEEE","13 Jan 2021","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition using Graph Attention Network with Dual-Branch Attention Module","C. Li; S. H. Pun; J. W. Li; F. Chen","State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau, China; State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau, China; School of Computer Science, Guangdong Polytechnic Normal University, Guangzhou, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","EEG reveals human brain activities for emotion and becomes an important aspect of affective computing. In this study, we developed a novel approach, namely DAM-GAT, which incorporated a dual-branch attention module (DAM) into a graph attention network (GAT) for EEG-based emotion recognition. This method used the GAT to capture the local features of emotional EEG signals. To enhance the important EEG features for emotion recognition, the proposed method also included a DAM that calculated weights considering both channel and frequency information. Additionally, the relationship between EEG channels was determined using the phase-locking value (PLV) connectivity of corresponding EEG signals. Based on the SEED datasets, the proposed approach provided an accuracy of up to 94.63% for emotion recognition, demonstrating its impressive performance compared with other existing methods.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782334","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782334","EEG emotion recognition;graph attention network;convolutional block attention;phase-locking value","Emotion recognition;Accuracy;Dams;Brain modeling;Feature extraction;Electroencephalography;Stability analysis;Resource management;Engineering in medicine and biology;Radio spectrum management","Humans;Electroencephalography;Emotions;Algorithms;Attention;Signal Processing, Computer-Assisted","","","14","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Emotion Estimation Using EEG with Deep learning Networks","M. Vynatheya; S. D. P.","Dept. of Electrical Engineering, National Institute of Technology Calicut, Kerala, India; Dept. of Electrical Engineering, National Institute of Technology Calicut, Kerala, India",2022 IEEE 19th India Council International Conference (INDICON),"16 Feb 2023","2022","","","1","5","Emotions play an important role in people’s daily life and are expressed using voice, facial expressions, body language to interact with the environment. In this regard, it is necessary to better understand the emotions to interpret it. Multitude of approaches based on facial expressions, voice and physiological signals exist for emotion estimation. EEG signals are preferred as a more trustworthy and long-lasting technique as the other approaches could lead to inaccurate results since the subjects can manipulate them easily. Much emphasis has been placed on the recognition of human emotions using EEG signals based on deep learning techniques. This paper is an attempt to distinguish between positive and negative emotional states using deep learning models namely AlexNet, VGG-16, and Inception-v3. The efficiency of the deep learning models was tested on an available GAMEEMO dataset. Spectrogram generated from the EEG signals are input to the deep convolutional neural network (DNN) based categorization models. The models are evaluated using the performance indices such as accuracy, precision, recall and F1-score. Inception-v3 performed the best in classifying emotions from the three DL models with an accuracy of 97.68%. The efficiency of these models indicate the great potential of DL models in emotion estimation.","2325-9418","978-1-6654-7350-7","10.1109/INDICON56171.2022.10040132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040132","Alexnet;VGG-16;Inception-v3;Emotion recognition;Electreoncephalograph","Deep learning;Emotion recognition;Estimation;Brain modeling;Electroencephalography;Physiology;Convolutional neural networks","","","","16","IEEE","16 Feb 2023","","","IEEE","IEEE Conferences"
"An Emotion Classification Model for Driver Emotion Recognition Using Electroencephalography (EEG)","T. A. Gamage; L. P. Kalansooriya; E. R. C. Sandamali","Department of Computer Science, General Sir John Kotelawala Defence University, Sri Lanka; Department of Computer Science, General Sir John Kotelawala Defence University, Sri Lanka; Dept. of Computational Mathematics, General Sir John Kotelawala Defence University, Sri Lanka",2022 International Research Conference on Smart Computing and Systems Engineering (SCSE),"4 Oct 2022","2022","5","","76","82","Road accidents have been a critical issue that has resulted in fatal injuries, disabilities, and deaths for many individuals worldwide. The notion of Human-Computer Interaction (HCI) is widely considered in monitoring drivers to safeguard their lives on roads. As a solution to the issue of the higher rate of road accidents, driver emotion recognition approaches have gained much attention, and the involvement of biological signals in detecting the emotional states of drivers is also significant. The authors have conducted a comprehensive literature review that concerns contemporary literature on the driver emotion recognition paradigm and comes up with four emotional states in this research to monitor the drivers' affective states. This paper presents a novel approach to detecting sad, angry, fearful, and calm emotional states of drivers with an emotion classification model using Electroencephalography (EEG) signals where the EEG data acquisition for the research is done using the Emotiv EPOC X device. The collected EEG data are preprocessed using the EEGLAB toolbox in Matlab, and feature extraction, selection, and emotion classification model training are done using Matlab. EEG acquisition and preprocessing have already been achieved, and as further work, the authors are to train the proposed emotion classification model as laid out in this paper. The findings of this research encourage the authors to continue towards the completion and provide further insights into enhancing research in the driver emotion recognition paradigm.","2613-8662","978-1-6654-7375-0","10.1109/SCSE56529.2022.9905108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9905108","driver emotion recognition;EEG;EEGLAB;emotion classification;road safety","Human computer interaction;Emotion recognition;Road accidents;Brain modeling;Electroencephalography;Mathematical models;Data models","","7","","34","IEEE","4 Oct 2022","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Dynamic Spatial-Spectral-Temporal Network","L. Cao; Z. Luo; B. Sun","School of Electrical and Information Engineering, Tianjin University, Tianjin, P. R. China; School of Electrical and Information Engineering, Tianjin University, Tianjin, P. R. China; School of Electrical and Information Engineering, Tianjin University, Tianjin, P. R. China",2024 43rd Chinese Control Conference (CCC),"17 Sep 2024","2024","","","8340","8344","EEG-based emotion recognition has demonstrated significant potential in various domains, for example, emotion detection of patients with disorders of consciousness, cognitive load assessment, emotion detection of drivers and so on. In the realm of brain-computer interface research and development, the ability to accurately and effectively recognize emotions is of significant importance. In this paper, we propose a multi-channel EEG emotion recognition method called dynamic spatial-spectral-temporal network (DSSTNet). It comprises three components: (1) Spatial features extractor consists of a graph converter and a graph convolutional network. It first converts the EEG signal into graph-structured data. Then, the graph convolutional network is used to dynamically optimize the adjacency matrix to obtain the spatial features between the channels. (2) Band attention module composed of semi-global pooling, local cross-band interaction, and adaptive weighting for further extracting spectral information. (3) Temporal features extractor that extracts deep temporal information by stacking multiple one-dimensional convolution layers. The results of comparative experiments show that the classification accuracy of the proposed method is better than that of a series of comparison methods, and the robustness is relatively strong. The results of ablation experiments validate the effectiveness of each module proposed in the method.","1934-1768","978-9-8875-8158-1","10.23919/CCC63176.2024.10661675","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10661675","EEG classification;adjacency matrix;graph convolutional network (GCN);subject-dependent;deep learning","Emotion recognition;Accuracy;Graph convolutional networks;Stacking;Feature extraction;Electroencephalography;Robustness","","","","23","","17 Sep 2024","","","IEEE","IEEE Conferences"
"Spatiotemporal Feature Extraction of Dynamic Brain Networks and its Application in EEG-Based Emotion Recognition","J. Han; Q. Zhong; R. Lin; P. Zhu; N. Qiu; P. Li","Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing University of Posts and Telecommunications, Chongqing, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; Chongqing University of Posts and Telecommunications, Chongqing, China",2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),"5 May 2025","2024","","","575","580","Effectively extracting discriminative and neural-interpretative features from time-varying brain networks promotes the development of affective brain-computer interface (BCI). However, traditional methods such as the convolutional neural networks (CNNs) have limits in capturing the dynamic evolution patterns of those network structures, which restricts their applications in the field of emotion recognition. To address this issue, we proposed a novel deep-learning model based on CNN-transformer framework. This model learns both spatial and temporal features from time-varying brain networks and integrates them for emotion recognition. To verify its efficiency, we applied it to both the electroencephalogram (EEG) of emotions recorded in our lab and one public dataset, i.e., SEED, and compared it with other commonly used feature extraction methods. Through experimental results, we found that the proposed method is superior to the comparative methods and achieves the highest classification accuracy. In addition, it also holds the capability of extracting neural interpretative spatiotemporal features to enhance the evolution differences of brain networks between different emotions. These results indicate that our method has great potential in decoding emotional states through neural networks, which may offer an alternative for both the diagnosis and treatment of emotional disorders.","","979-8-3315-0494-6","10.1109/WI-IAT62293.2024.00092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10973396","CNN-transformer framework;Dynamic brain network;Spatiotemporal feature extraction;Emotion recognition;EEG decoding","Support vector machines;Emotion recognition;Feature extraction;Brain modeling;Transformers;Electroencephalography;Spatiotemporal phenomena;Decoding;Convolutional neural networks;Long short term memory","","","","17","IEEE","5 May 2025","","","IEEE","IEEE Conferences"
"Prediction of Attention Deficit Hyperactivity Disorder and Autism Spectrum Disorder Patients Emotion Recognition using Machine Learning Models","S. Parameswaran; P. Arulmozhi; K. Indumathi; D. M. Rodax; G. Pradeep; J. R. Devi","Department of Biomedical Engineering, KSR College of Engineering, Tiruchengode, India; Department of Biomedical Engineering, KSR College of Engineering, Tiruchengode, India; Department of Biomedical Engineering, KSR College of Engineering, Tiruchengode, India; Department of Biomedical Engineering, KSR College of Engineering, Tiruchengode, India; Department of Biomedical Engineering, KSR College of Engineering, Tiruchengode, India; Department of Biomedical Engineering, KSR College of Engineering, Tiruchengode, India",2025 International Conference on Intelligent Computing and Control Systems (ICICCS),"12 May 2025","2025","","","1314","1319","To design a machine learning-based approach to predict and differentiate emotional recognition patterns in patients with ASD and ADHD, thus aiding in precise diagnosis and personalized care strategies. Current approaches of group 1 emotional recognition in ADHD and ASD rely mainly on traditional statistical methods and a relatively small number of datasets. Such approaches group 2 often consider only simple facial expression analysis and vocal tone evaluations without integrating the multi-modal features, leading to lower classification accuracy. Conversely, the method under this study employed machine learning algorithms, namely Random Forests and Deep Neural Networks, to analyze multi-modal data including facial expressions, vocal tones, and physiological signals of 200 samples from ADHD and ASD patients. The Random Forest model was 92 % accurate in classification compared to existing methods, and the Deep Neural Networks further improved detection of subtle variations in emotions and proved to be a more robust and reliable method for distinguishing the patterns of emotional recognition between both groups. The proposed machine learning framework is thus a strong, data-driven approach to improving diagnostic accuracy and personalizing interventions for ASD and ADHD. This is, therefore, promising in clinical application in the management of these neurodevelopmental conditions on time and effectively.","","979-8-3315-1208-8","10.1109/ICICCS65191.2025.10984719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10984719","Machine Learning;ADHD;ASD;Emotion Recognition;Deep Learning;EEG;CNN;RNN;Accuracy;Physiological Signals","Deep learning;Emotion recognition;Accuracy;Scalability;Predictive models;Brain modeling;Electroencephalography;Real-time systems;Reliability;Random forests","","","","21","IEEE","12 May 2025","","","IEEE","IEEE Conferences"
"Emotion Classification from EEG Signals in Convolutional Neural Networks","H. Donmez; N. Ozkurt","Department of Electrical and Electronics Engineering, Yaşar University, Izmir, Turkey; Department of Electrical and Electronics Engineering, Yaşar University, Izmir, Turkey",2019 Innovations in Intelligent Systems and Applications Conference (ASYU),"2 Jan 2020","2019","","","1","6","The objective of this research is to classify EEG (electroencephalography) signal recordings of the subjects evoked by visual stimulus by using CNN (Convolutional Neural Networks). EEG records the electrical activity of brain signals. In medicine, EEG is used to diagnose some neurological disorders but moreover the classification of the emotions is also possible from EEG recordings. Emotion recognition is an important task for the computers in machine perception. Therefore, in this study the participants are presented with a video containing funny, scary and sad excerpts and simultaneously EEG signal is measured by Neurosky Mindwave EEG Headset. The spectrogram of EEG signals is supplied to CNN and three emotions are classified using brain signal spectrogram images.","","978-1-7281-2868-9","10.1109/ASYU48272.2019.8946364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946364","EEG;CNN;Deep Learning;Emotion Classification","Electroencephalography;Spectrogram;Deep learning;Convolutional neural networks;Electrodes;Matlab;Visualization","","34","","20","IEEE","2 Jan 2020","","","IEEE","IEEE Conferences"
"Eye Fixation Versus Pupil Diameter as Eye-Tracking Features for Virtual Reality Emotion Classification","L. J. Zheng; J. Mountstephens; J. Teo","Evolutionary Computing Laboratory Faculty of Computing and Informatics, Universiti Malaysia Sabah, Kota Kinabalu, Sabah, Malaysia; Evolutionary Computing Laboratory Faculty of Computing and Informatics, Universiti Malaysia Sabah, Kota Kinabalu, Sabah, Malaysia; Evolutionary Computing Laboratory Faculty of Computing and Informatics, Universiti Malaysia Sabah, Kota Kinabalu, Sabah, Malaysia",2021 IEEE International Conference on Computing (ICOCO),"19 Jan 2022","2021","","","315","319","The usage of eye-tracking technology is becoming increasingly popular in machine learning applications, particularly in the area of affective computing and emotion recognition. Typically, emotion recognition studies utilize popular physiological signals such as electroencephalography (EEG), while the research on emotion detection that relies solely on eye-tracking data is limited. In this study, an empirical comparison of the accuracy of eye-tracking-based emotion recognition in a virtual reality (VR) environment using eye fixation versus pupil diameter as the classification feature is performed. We classified emotions into four distinct classes according to Russell's four-quadrant Circumplex Model of Affect. 360° videos are presented as emotional stimuli to participants in a VR environment to evoke the user's emotions. Three separate experiments were conducted using Support Vector Machines (SVMs) as the classification algorithm for the two chosen eye features. The results showed that emotion classification using fixation position obtained an accuracy of 75% while pupil diameter obtained an accuracy of 57%. For four-quadrant emotion recognition, eye fixation as a learning feature produces better classification accuracy compared to pupil diameter. Therefore, this empirical study has shown that eye-tracking-based emotion recognition systems would benefit from using features based on eye fixation data rather than pupil size.","","978-1-6654-3689-2","10.1109/ICOCO53166.2021.9673503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9673503","emotion classification;eye-tracking;fixation;pupil diameter;virtual reality;support vector machines","Emotion recognition;Solid modeling;Conferences;Support vector machine classification;Virtual reality;Machine learning;Electroencephalography","","5","","15","IEEE","19 Jan 2022","","","IEEE","IEEE Conferences"
"EEG Emotion Recognition using Multisource Instance Transfer Learning Framework","R. Ren; Y. Yang; H. Ren","Software College, Xi’an Jiaotong University, Xian, China; Software College, Xi’an Jiaotong University, Xian, China; Department of Computer Science and Technology, Northeast Agricultural University, Haerbin, China","2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)","12 Jan 2023","2022","","","192","196","In recent years, EEG has been widely used in the field of emotion recognition. However, the instability of EEG signals leads to great differences not only between different subjects, but also the same subjects in different sessions, so the same emotion recognition models cannot be shared with each other, resulting in new participants need to collect new label data to train new models. This is time-consuming and labor-intensive. In order to solve this problem, we propose a multi-source instance transfer learning framework based on ensemble learning, which uses the instance-transfer method to reduce the dependence on the target domain labeled data, and combines ensemble learning to improve the accuracy of emotion recognition. The framework first filters the source domain data using a base classifier trained on existing label data. The filtered multi-source domain data is then combined with the existing target domain labeled data to train the corresponding classifier. Finally, all classifiers are integrated together for a test experiment on the unlabeled data of the target domain. Our method has been experimented on the SEED dataset and the SEED-IV dataset, and the classification accuracy of the experimental results has been significantly improved compared with the base classifier, and the cross-subject and cross-session scenarios have been significantly improved. Among them, the cross-session increased (5%-10%), and the cross-subject increased (2%-3%). This is a simple, fast, and flexible framework that has experimentally demonstrated the transfer learning framework’s effectiveness for multi-source EEG emotion recognition (different emotion can achieve different brain signals) problems.","","978-1-6654-6468-0","10.1109/ICICML57342.2022.10009876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009876","EEG;emotion recognition;MSIT;ensemble learning","Emotion recognition;Computer vision;Costs;Image recognition;Transfer learning;Brain modeling;Electroencephalography","","2","","21","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Real-Time Emotion Detection System using Emotive and ESP-32","R. Janapati; S. Bhattacharya; S. R. Gorre; K. Sreedhar; B. K. Verma; C. Adupa; U. Desai; K. Bikshalu","Department of ECE, SR University, Warangal, India; Department of ECE, SR University, Warangal, India; Department of ECE, SR University, Warangal, India; Department of ECE, SR University, Warangal, India; Cognitive Science Department, SR University, Warangal, India; Department of ECE, SR University, Warangal, India; Department of ECE, SEA College of Engg & Tech, Bengaluru, India; Department of ECE, Kakatiya University, Warangal, India",2023 IEEE 20th India Council International Conference (INDICON),"27 Feb 2024","2023","","","1211","1215","This paper aims to develop an emotion detection system using Emotive and ESP-32. The system will use Emotive, an EEG headset that measures brain activity, to detect emotions in real-time. The data obtained from the Emotive will be transmitted wirelessly to an ESP-32 microcontroller, which will process the data and classify the emotion using machine learning algorithms. The ESP-32 will be programmed using the Arduino IDE and will use a wireless communication protocol such as Wi-Fi or Bluetooth to transmit the data. The machine learning algorithms will be trained using a dataset of EEG signals and corresponding emotions and will be optimized to classify emotions accurately. The system will have various applications in fields such as mental health, education, and entertainment. It could be used to detect the emotional state of a person and provide appropriate support or feedback. For example, in a classroom setting, the system could be used to detect when a student is feeling frustrated or confused and provide additional resources to help them understand the material better. Overall, this work has the potential to create a more empathetic and responsive environment, by detecting and responding to human emotions in real-time.","2325-9418","979-8-3503-0559-3","10.1109/INDICON59947.2023.10440945","Science and Engineering Research Board; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440945","Brain Computer Interface (BCI);Brain activity;EEG headset;Emotion detection(ED);ESP","Wireless communication;Emotion recognition;Machine learning algorithms;Protocols;Real-time systems;Electroencephalography;Wireless fidelity","","2","","15","IEEE","27 Feb 2024","","","IEEE","IEEE Conferences"
"A Transformer Framework Based on Self-Supervised Contrastive Learning for EEG-based Emotion Recognition","H. Wen; T. Yu","Coll. of Automation Sci. & Eng., South China University of Technology, Guangzhou, China; Coll. of Automation Sci. & Eng., South China University of Technology, Guangzhou, China",2024 7th International Conference on Advanced Algorithms and Control Engineering (ICAACE),"18 Jun 2024","2024","","","313","317","Deep learning has achieved a great success in various fields, such as image classification and so on. But its excellent performance tends to rely on a large amount of labeled data that are hard to collect and manual annotation, especially for EEG signals. Self-supervised learning (SSL), as a solution, can discover structure in unlabeled data and learn global representations. Therefore, for emotion recognition, we propose a Transformer Framework based on Self-supervised Contrastive Learning (TFSCL). We design positive and negative pairs based on temporal and spatial characteristic of EEG to pre-train a general model for crossing subjects. We conduct a convolution-based patch embedding framework and a self-attention Transformer encoder to learn local and global representations of EEG signals. We adopt experiments on our proposed TFSCL using the DEAP and SEED emotion datasets, and perform better accuracy in cross-subject tasks. Experimental results suggest that self-supervised learning may play a greater role in deep learning models of EEG signals.","","979-8-3503-6144-5","10.1109/ICAACE61206.2024.10548104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548104","EEG classification;emotion recognition;contrastive learning","Deep learning;Emotion recognition;Control engineering;Convolution;Manuals;Transformers;Brain modeling","","","","20","IEEE","18 Jun 2024","","","IEEE","IEEE Conferences"
"A comparison study on the relationship between the selection of EEG electrode channels and frequency bands used in classification for emotion recognition","A. Chatchinarat; K. W. Wong; C. C. Fung","School of Engineering and Information Technology, Murdoch University, WA, Australia; School of Engineering and Information Technology, Murdoch University, WA, Australia; School of Engineering and Information Technology, Murdoch University, WA, Australia",2016 International Conference on Machine Learning and Cybernetics (ICMLC),"23 Feb 2017","2016","1","","251","256","It has been established that it is possible to reveal human emotions using electroencephalogram (EEG) signals. Most studies used a wide variety of data sets and methods, therefore a comparison between the performances of their approaches is difficult. This paper reports a study on the effects of the number of electrode channels and frequency bands for emotion classification based on a database for emotion analysis using physiological signals (DEAP). Discrete wavelet transform (DWT) was used for feature extraction and support vector machine (SVM) was applied as the classifier. From experimental results, it is found that (a) using more electrodes channels did not guarantee better accuracy, (b) comparing with all frequency bands, using a few of them did not reduce the accuracy dramatically and some results revealed that only two bands produced better results, and, (c) the more emotions to be classified, the lower accuracy was achieved based on the same method.","2160-1348","978-1-5090-0390-7","10.1109/ICMLC.2016.7860909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860909","Electroencephalogram;Emotion classification;Affective computing;Discrete wavelet transform;Support vector machine","Electrodes;Electroencephalography;Feature extraction;Emotion recognition;Support vector machines;Brain models","","13","","29","IEEE","23 Feb 2017","","","IEEE","IEEE Conferences"
"Emotion Recognition Using Frontal EEG in VR Affective Scenes","T. Xu; R. Yin; L. Shu; X. Xu",South China University of Technology; South China University of Technology; South China University of Technology; South China University of Technology,2019 IEEE MTT-S International Microwave Biomedical Conference (IMBioC),"29 Jul 2019","2019","1","","1","4","Frontal EEG has been widely used for human emotion recognition since its convenience. However, many relevant studies used traditional wet electrodes to collect EEG signals and the stimulation ways were restricted as music, videos and pictures. This paper provides a new framework for emotion recognition using frontal EEG and VR affective scenes. An experiment about VR stimuli EEG data collection was conducted among 19 subjects. The EEG data were collected using textile dry electrodes. EEG features were extracted from time, frequency and space domain in the collected data. Model stacking method were applied in the experiment to ensemble 3 models including GBDT, RF and SVM. The mean accuracy of our framework achieved about 81.30%, which exhibited better performance compared with relevant studies. The framework proposed in this work can be well applied to wearable device for EEG emotion recognition in VR scenes.","","978-1-5386-7395-9","10.1109/IMBIOC.2019.8777843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8777843","EEG;emotion recognition;VR;machine learning","Electroencephalography;Brain modeling;Emotion recognition;Electrodes;Feature extraction;Stacking;Support vector machines","","22","","16","IEEE","29 Jul 2019","","","IEEE","IEEE Conferences"
"A Supervised Information Enhanced Multi-Granularity Contrastive Learning Framework for EEG Based Emotion Recognition","X. Li; J. Song; Z. Zhao; C. Wang; D. Song; B. Hu","Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Institute of Engineering Medicine, Beijing Institute of Technology, Beijing, China; Institute of Engineering Medicine, Beijing Institute of Technology, Beijing, China","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","2325","2329","This study introduces a novel Supervised Info-enhanced Contrastive Learning framework for EEG based Emotion Recognition (SI-CLEER). SI-CLEER employs multi-granularity contrastive learning to create robust EEG contextual representations, potentially improving emotion recognition effectiveness. Unlike existing methods solely guided by classification loss, we propose a joint learning model combining self-supervised contrastive learning loss and supervised classification loss. This model optimizes both loss functions, capturing subtle EEG signal differences specific to emotion detection. Extensive experiments demonstrate SI-CLEER’s robustness and superior accuracy on the SEED dataset compared to state-of-the-art methods. Furthermore, we analyze electrode performance, highlighting the significance of central frontal and temporal brain region EEGs in emotion detection. This study offers an universally applicable approach with potential benefits for diverse EEG classification tasks.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10447740","Ministry of Science and Technology; Shandong Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10447740","Emotion Recognition;EEG;Contrastive Learning","Training;Emotion recognition;Source coding;Self-supervised learning;Speech recognition;Benchmark testing;Brain modeling","","1","","18","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"EEG evoked automated emotion recognition using deep convolutional neural network","A. Abgeena; S. Garg","Department of Computer Science and Engineering, Birla Institute of Technology, Mesra, Ranchi, INDIA; Department of Computer Science and Engineering, Birla Institute of Technology, Mesra, Ranchi, INDIA","2023 Fifth International Conference on Electrical, Computer and Communication Technologies (ICECCT)","21 Jul 2023","2023","","","1","7","As life continues to change in the digital era, it is crucial to perceive a person's emotional state. Affective computing is receiving more attention with the increase in the human-computer interface (HCI). Human emotion recognition employing electroen-cephalogram (EEG) signals has been studied to obtain a person's emotional status for different stimuli. However, it is difficult to identify clear patterns in EEG signals because they have low electrical impulses and are highly sensitive to noise. A deep convolutional neural network (DCNN) was employed in the present study to recognize emotions in EEG signals. For this purpose, a publicly available dataset, DREAMER, was utilized in this study to assess the applicability of the model for emotion classification. The dataset consisted of three-dimensional emotions, that is, valence, arousal, and dominance (VAD). 2D emotions arousal and valence were the most-recognized emotions in existing research. The present study identified the 3D emotions present in the above-mentioned dataset. In this study, raw EEG signals from the DREAMER dataset were pre-processed. Subsequently, three EEG rhythms, theta, alpha, and beta, were extracted using a bandpass filter. The power spectral density (PSD) was computed using fast Fourier transform (FFT) in the feature extraction. Finally, a 1D CNN model is applied to the classification of emotions. In addition, the performance of the proposed model was compared with two machine learning (ML) classifiers: random forest (RF) and extreme Gradient Boosting (XGBoost) classifiers. The highest accuracy (ACC) of 97.6% was obtained using the proposed model in the dominance dimension. The working principles were compared and discussed to determine the suitability of the model for emotion recognition applications.","","978-1-6654-9360-4","10.1109/ICECCT56650.2023.10179711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179711","Affective Computing;EEG;Emotion Recognition;Deep Learning;Machine Learning","Radio frequency;Emotion recognition;Solid modeling;Three-dimensional displays;Computational modeling;Brain modeling;Feature extraction","","","","27","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Research on Classification and Recognition of Emotional EEG Signal Based on Extreme Learning Machine","F. Guo; L. Zhao; Y. Bian; Z. Zhang","Tianjin University of Technology and Education, Tianjin, China; Tianjin University of Technology and Education, Tianjin, China; Tianjin University of Technology and Education, Tianjin, China; Tianjin University of Technology and Education, Tianjin, China","2021 IEEE 5th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC)","4 Nov 2021","2021","5","","1221","1227","There are many research methods of emotion recognition, and the emotion recognition of EEG signals is one of the important researches in the field of emotion computing. Extreme learning machine can recognize more generalized features with fewer training parameters. Therefore, this paper constructs a power spectral density-extreme learning machine classification model and applies it to the classification and recognition of emotional EEG signals. The experimental paradigm was designed and the emotion-related EEG signals of 10 subjects were collected, and the PSD characteristics of the corresponding frequency bands of their EEG signals in the three emotions were extracted, and the model was used to achieve specific and cross-subject evaluation of emotional EEG signals. Positive, neutral, and negative three categories are distinguished. Among them, the accuracy of single body recognition is up to 96.17%, and the average accuracy is 81.72%; the cross-subject recognition accuracy is 76.82%, which verifies the combination of this feature and the classification method. Effectiveness in the field of emotional EEG signal recognition.","2693-3128","978-1-6654-1599-6","10.1109/ITNEC52019.2021.9586996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586996","Emotion Recognition;Emotion;EEG;Power Spectral Density;Extreme Learning Machine","Training;Emotion recognition;Adaptation models;Extreme learning machines;Face recognition;Brain modeling;Feature extraction","","2","","15","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"Affective computing for emotion identification using dual-stage filtered multi-channel EEG signals","K. Kamble; J. Sengupta","Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India","2023 2nd International Conference on Paradigm Shifts in Communications Embedded Systems, Machine Learning and Signal Processing (PCEMS)","2 Jun 2023","2023","","","1","5","The dual-stage correlation and instantaneous frequency (CIF) thresholding approach for retrieval of noise-free desired frequency band of EEG signal is proposed for affective emotion identification task. Initially, the raw electroencephalogram (EEG) signals are breakdown applying the empirical mode decomposition technique to produce intrinsic mode functions (IMFs). The noisy IMFs are eliminated by applying correlation thresholding. Secondly, these noise-free EEG signals are divided into several modes using a non-linear chirp variational mode decomposition approach to retrieve desired frequency bands (4-30Hz) by applying the IF-based filtering method on the modes. The power spectral densities extracted from filtered modes are fed to ML-based classifiers to classify emotions into arousal, valence, and dominance groups. This study also shows the efficacy of ensemble ML (EML): random forest (RF) and bagging over conventional ML (CML): support vector machine and logistic regression classifiers. The RF reported the highest average F1-scores using 10-fold cross-validation for arousal, valence, and dominance are 83.99%,75.94%, and 88.86% respectively. Similarly, the respective average accuracies of two-EML are~1.47%, ~1.27%, and~0.3% higher compared to two-CML classifiers. To summarize, the proposed CIF-based filtering approach is useful for affective emotion identification under the framework of EML classifiers.","","979-8-3503-1071-9","10.1109/PCEMS58491.2023.10136088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136088","EEG;Affective emotion identification;Empirical mode decomposition;Non-linear chirp variational mode decomposition;Ensemble learning","Radio frequency;Support vector machines;Emotion recognition;Correlation;Filtering;Chirp;Electroencephalography","","3","","22","IEEE","2 Jun 2023","","","IEEE","IEEE Conferences"
"Semi-Skipping Layered Gated Unit and Efficient Network: Hybrid Deep Feature Selection Method for Edge Computing in EEG-Based Emotion Classification","M. A. Asghar; M. J. Khan; H. Shahid; M. Shorfuzzaman; N. N. Xiong; R. M. Mehmood","Telecommunication Engineering Department, University of Engineering and Technology Taxila, Taxila, Pakistan; Telecommunication Engineering Department, University of Engineering and Technology Taxila, Taxila, Pakistan; Telecommunication Engineering Department, University of Engineering and Technology Taxila, Taxila, Pakistan; Department of Computer Science, College of Computers and Information Technology, Taif University, Taif, Saudi Arabia; Department of Mathematics and Computer Science, Northeastern State University, Tahlequah, OK, USA; Information and Communication Technology Department, School of Electrical and Computer Engineering, Xiamen University Malaysia, Sepang, Malaysia",IEEE Access,"22 Jan 2021","2021","9","","13378","13389","In this article, we propose a novel feature selection method based on hybrid neural networks for emotional classification avoiding expensive computation. With the development of neural networks for machine learning tools, the Electroencephalogram (EEG)-based classification of human emotions is increasingly important in providing health-care for Edge Computing in support of multiple Internet of Things (IoT) applications. However, classifying emotions through EEG signals is very challenging due to the low temporal boundaries and non-linear nature of EEG signals. Regarding non-linearity, we propose a hierarchical Semi-skipping Layered Gated Unit (SLGU) besides Efficient Network (ENet) for feature extraction. For faster processing, we have introduced a semi-skipping layer for Gated Recurrent Units (GRU) in Recurrent Neural Networks (RNN). The entered layer automatically skips the divergent factor during network training. Preprocessed EEG signals are sent to the hybrid SLGU-ENet model for deep feature extraction. To overcome the computational cost, an optimal function reduction method called the Bag of Visualized Characteristics (BoVC) is used. The entire facility is verified against two publicly available datasets. The results show that the classification performance of the proposed model achieves superior classification precision in a short processing time compared to the state-of-the-art models. The proposed algorithm required around 1.2-5 seconds, suitable for real-time IoT applications.","2169-3536","","10.1109/ACCESS.2021.3051808","Xiamen University Malaysia Research Fund (XMUMRF)(grant numbers:XMUMRF/2019-C3/IECE/0007); Taif University Researchers Supporting Project(grant numbers:TURSP-2020/79); Taif University, Taif, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323045","Edge computing;efficient network;electroencephalogram (EEG);emotion classification;Internet of Things;recurrent neural network","Electroencephalography;Feature extraction;Brain modeling;Internet of Things;Human computer interaction;Entropy;Emotion recognition","","14","","44","CCBY","14 Jan 2021","","","IEEE","IEEE Journals"
"EmHM: A Novel Hybrid Model for the Emotion Recognition based on EEG Signals","R. Sharma; H. K. Meena","Electrical Engineering Department, Malaviya National Institute of Technology, Jaipur, India; Electrical Engineering Department, Malaviya National Institute of Technology, Jaipur, India",2023 19th IEEE International Colloquium on Signal Processing & Its Applications (CSPA),"5 Apr 2023","2023","","","75","80","Emotion is a fundamental aspect of daily life and is crucial for human interactions. This study suggests a unique electroencephalogram (EEG)-based technique for identifying human emotions. For EEG-based emotion analysis, the proposed model is tested on the SEED and DEAP datasets. For the DEAP dataset, we consider valence and arousal emotions for classification purposes, and for the SEED dataset, three emotions, neutral, positive, and negative, have been considered. The differential entropy (DE) is used for the SEED dataset, and for the DEAP dataset, the power spectral density (PSD) is used as a feature. For precise emotion recognition, an EmHM (Emotion Hybrid Model) based on long short-term memory (LSTM) and a convolutional neural network (CNN) are constructed. Furthermore, we applied the CNN, LSTM, and EmHM models, and all three models for emotion recognition are fed with the retrieved information. Various methods improved on already-existing models to accurately classify human emotion. To get better accuracy than the existing techniques, we suggested a model that uses a different approach known as EmHM. By applying all three models such as CNN, LSTM and EmHM, we got the highest accuracy 86.50%, 87.98% and 91.56% respectively on dataset. To improve prediction results, the CNN and the LSTM models are combined to make the EmHM hybrid model.","","978-1-6654-7692-8","10.1109/CSPA57446.2023.10087500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10087500","Emotion recognition;DEAP dataset;CNN;LSTM;SEED datatset;EmHM;power spectral density (PSD)","Emotion recognition;Analytical models;Predictive models;Brain modeling;Feature extraction;Electroencephalography;Entropy","","2","","27","IEEE","5 Apr 2023","","","IEEE","IEEE Conferences"
"Emotion Recognition from EEG Signal Using CA-GCN","R. Gao; Y. Li; X. Liu; B. Liu; J. Tao; Z. Lv","Sch. of Computer Science and Technology, Anhui University, Hefei, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Qiyuan Lab, Beijing, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Sch. of Computer Science and Technology, Anhui University, Hefei, China",2022 16th IEEE International Conference on Signal Processing (ICSP),"2 Dec 2022","2022","1","","134","138","Electroencephalography(EEG)-based emotion recognition has become an important task in the field of brain computer interface(BCI). Recently, deep learning methods have achieved excellent results in the field of EEG emotion recognition. However, these methods either ignore the relations between channels or ignore the contributions of different channels, which makes it difficult to improve the recognition performance. In this paper, an end-to-end EEG emotion recognition method that graph convolutional neural network with channel-wise attention(CA-GCN) is proposed. First, we utilize a GCN layer to extract EEG signals spatial information for expressing intrinsic relations between different EEG channels. Furthermore, a channel-wise attention pooling layer in CA-GCN was adopted to extract the contribution of different EEG channels. Sufficient experiments have been carried out on the DEAP database. Experimental results demonstrate that the proposed method outperforms the previous methods, and the recognition accuracy of the proposed method achieve 93.69% and 94.59% in valence and arousal, respectively.","2164-5221","978-1-6654-6056-9","10.1109/ICSP56322.2022.9965334","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965334","emotion recognition;channel-wise attention;Graph Convolutional Neural Networks(GCNN);Electroencephalography(EEG);DEAP","Deep learning;Emotion recognition;Adaptation models;Databases;Signal processing;Feature extraction;Brain modeling","","1","","25","IEEE","2 Dec 2022","","","IEEE","IEEE Conferences"
"EEG-based Confusion Recognition Using Different Machine Learning Methods","S. He; Y. Xu; L. Zhong","School of Electronic Engineering Xidian University, Xian, China; College of Medicine and Biological Information Engineering Northeastern University, Shenyang, China; International School Beijing University of Posts and Telecommunications, Beijing, China",2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE),"24 Jun 2022","2021","","","826","831","Massive Open Online Course (MOOC) has emerged as a key trend. As a way of teaching online, the main shortcoming of MOOC is lacking feedback because there is a distance in both time and space between teachers and students. This study proposes the confusion recognition system based on Electroencephalography(EEG). We apply machine learning methods, including Naive Bayes, KNN, Random Forest, XGBoost, and also a deep learning method, LSTM, on the EEG data set respectively to detect whether a student feel confused. We find that LSTM shows better performance than any machine learning methods we use. The average accuracy of LSTM classifier is 78.1%. This study shows the significance of detecting confusion through EEG and helping students in improving learning efficiency.","","978-1-6654-2186-7","10.1109/ICAICE54393.2021.00160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797647","EEG;Brain-computer interface (BCI);Emotion recognition;Classification;Machine learning;LSTM","Deep learning;Computer aided instruction;Electronic learning;Education;Market research;Electroencephalography;Random forests","","6","","17","IEEE","24 Jun 2022","","","IEEE","IEEE Conferences"
"Subject-independent Emotion recognition based on Entropy of EEG Signals","H. Yang; P. Rong; G. Sun","Department of Automation, Heilongjiang University, Harbin; Department of Automation, Heilongjiang University, Harbin; Department of Automation, Heilongjiang University, Harbin",2021 33rd Chinese Control and Decision Conference (CCDC),"30 Nov 2021","2021","","","1513","1518","Emotion recognition is always an academic focus in the research of human-computer interaction (HCI). Due to poor generalizability of electroencephalogram (EEG) features from EEG different subjects, the aim of this paper is to further explore different characteristics in EEG signals to improve the accuracy of subject-independent emotion recognition. We extract the features from time domain and frequency domain, sample entropy and wavelet entropy and so on, and use SVM to evaluate the recognition performance. Results show the accuracy of public DEAP dataset is up to 70.1% in the valence dimension and 64.2% in the arousal dimension. This analysis is significant because we verify that the sample entropy and wavelet entropy are effective in subject-independent emotion recognition.","1948-9447","978-1-6654-4089-9","10.1109/CCDC52312.2021.9602439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9602439","EEG;Emotion recognition;Subject-independent;Entropy;SVM","Human computer interaction;Support vector machines;Emotion recognition;Wavelet domain;Wavelet analysis;Feature extraction;Entropy","","3","","19","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Fast emotion detection from EEG using asymmetric spatial filtering","D. Huang; H. Zhang; K. Ang; C. Guan; Y. Pan; C. Wang; J. Yu","Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","589","592","The injection of emotional intelligence in human-computer interfaces is necessary for computer applications to appear intelligent when interacting with people. With the recent development of brain imaging techniques and brain-computer interfaces, computers can actually take a look inside users' head to observe their emotional states. This paper presents an EEG-based emotion detection system which detects emotional states based on short EEG segments of 1s. A novel feature extraction algorithm termed asymmetric spatial filtering is proposed to extract features from high dimensional EEG data. The effectiveness of the proposed method is tested for two types of emotion detection problems on data from five subjects.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6287952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6287952","emotion detection;BCI;EEG;feature extraction","Electroencephalography;Feature extraction;Videos;Accuracy;Time frequency analysis;Labeling;Emotion recognition","","12","","9","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Detection: A Machine Learning Approach with Multiple Classifiers","S. K; J. Thejasree; B. V. Rushitha","Department of Computer Science and Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Chennai, India; Department of Computer Science and Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Chennai, India; Department of Computer Science and Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Chennai, India",2024 International Conference on Electronic Systems and Intelligent Computing (ICESIC),"22 Jan 2025","2024","","","93","98","The present work sought to evaluate the ability of three classification methodologies Decision Tree, Support Vector Machine (SVM), and Naive Bayes in classifying emotional states using EEG data. The models were trained and tested in a dataset that covers a broad range of emotional labels. The performance of both models was measured using quantitative measures such as accuracy, precision, recall, and F1-score. A Decision Tree Classifier that achieved 95.78% accuracy with a good set of precision and recall in all emotion categories, where particularly the Negative emotions are detected better than other classifiers. The SVM model presented improved classification ability and overall robustness regarding categorizing whether an emotion was a Neutral, Negative, or Positive category compared with the Decision Tree model doing so with 96.02% accuracy (Table 1). However, the Naive Bayes Classifier demonstrated lower overall performance, achieving an accuracy of 69.79%. It tended to over-allocate to non-positive categories, which impacted its precision in correctly predicting the exact category. Upon evaluating the confusion matrix, it is found that Decision Tree and SVM models managed to significantly reduce misclassifications. According to the result, Ensemble models are more effective in EEG-based emotion classification tasks compared with SVM and Decision Tree. On the other hand, Naive Bayes might not be an ideal choice for these kinds of tasks because it assumes a very simple model. The paper emphasizes the demand for careful selection of machine learning models to be used in recognizing emotions from EEG data. It also points to future possible directions, such as optimization of models and rich feature engineering improvements using deep learning approaches. Overall, Support Vector Machines (SVM) and Decision Trees show strong potential for practical application in emotional processing to offer fresh insights into improving the accuracy/ reliability of classification in EEG-oriented systems.","","979-8-3315-2298-8","10.1109/ICESIC61777.2024.10846557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10846557","EEG;Emotion Recognition;Machine Learning;Classifier Comparison;Decision Tree;SVM","Support vector machines;Hands;Deep learning;Emotion recognition;Accuracy;Brain modeling;Electroencephalography;Naive Bayes methods;Decision trees;Optimization","","","","21","IEEE","22 Jan 2025","","","IEEE","IEEE Conferences"
"Combination of EOG and EEG for emotion recognition over different window sizes","H. Cai; X. Liu; A. Jiang; R. Ni; X. Zhou; A. Cangelosi","College of IoT Engineering, Hohai University, Changzhou, P. R. China; College of IoT Engineering, Hohai University, Changzhou, P. R. China; College of IoT Engineering, Hohai University, Changzhou, P. R. China; College of IoT Engineering, Hohai University, Changzhou, P. R. China; College of IoT Engineering, Hohai University, Changzhou, P. R. China; School of Computer Science, The University of Manchester, Manchester, M13 9RL, United Kingdom",2021 IEEE 2nd International Conference on Human-Machine Systems (ICHMS),"27 Oct 2021","2021","","","1","6","Considering the use of a multi-modal framework to enhance emotion recognition, we propose to combine electroencephalography (EEG) and electrooculogram (EOG) through decision level fusion(DLF) and feature level fusion(FLF) for emotion recognition. By using different temporal window sizes to segment the signal, we explore the duration of the emotion of the EOG signal and the EEG signal. Then, some temporal window sizes that are friendly to both EOG signal and EEG signal are selected for segmentation and emotion recognition. According to the different degree of dependence of subjects, the accuracy of the proposed algorithm on subject-dependent and subject-independent is verified on the DEAP dataset. For subject-dependent, using feature level fusion strategy with a window size of 6 seconds, the accuracy is 0.9562 in terms of arousal, and 0.9558 in terms of valence. For subject-independent, using feature level fusion strategy with a window size of 5 seconds, the accuracy is 0.8638 in terms of arousal, and 0.8542 in terms of valence. The experimental results show that the proposed algorithm can better enhance emotion recognition.","","978-1-6654-0170-8","10.1109/ICHMS53169.2021.9582628","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582628","EEG;EOG;Window selection;DFL;FLF","Emotion recognition;Electrooculography;Conferences;Interference;Position measurement;Feature extraction;Electroencephalography","","6","","24","IEEE","27 Oct 2021","","","IEEE","IEEE Conferences"
"Graph Convolutional Neural Network for EEG Emotion Recognition","Q. Li; Y. Liu; Q. Zhang; J. Liu; T. Sui","Changchun University of Science and Technology, Changchun, China; Changchun University of Science and Technology, Changchun, China; Daqing Oilfield Drilling Engineering Company, Daqing, China; Chunliang Oil Production Plant of Shengli Oilfield, Binzhou, China; Zhangjiagang Zhongmei Ultra thin Belt Technology Co., Ltd, Zhangjiagang, China",2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC),"27 Mar 2023","2022","","","784","787","Because emotion recognition is vital in improving human-computer interaction, emotion recognition is becoming increasingly critical in artificial intelligence. Deep learning is widely used to analyze frequency or time domain features in emotion recognition research. Few researchers consider the spatial features of Electroencephalogram (EEG) and the correlation between EEG channels. In this paper, the graph convolutional neural network is used to analyze the emotion of EEG signal. Firstly, the edge and point features of EEG were extracted to construct the graph structure, which made full use of the spatial and frequency features of EEG. Then, the time-series information of EEG was extracted and classified by using the long short-term memory neural network. In the DEAP dataset, the accuracy of the proposed method in the valence and arousal dimension are 83.23% and 85.82%, respectively. The research can be applied to the emotional brain computer interface system.","","979-8-3503-2195-1","10.1109/ICFTIC57696.2022.10075315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075315","Graph Convolutional Network;Electroencephalogram;Long Short-term Memory","Emotion recognition;Correlation;Convolution;Feature extraction;Brain modeling;Electroencephalography;Data models","","","","10","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"An Efficient Bidirectional LSTM-Based Deep Neural Network for Automatic Emotion Recognition Using EEG Signal","M. S. Mahmud; O. Saha; S. A. Fattah","Department of Electrical and Electronic Engineering (EEE), Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh",2022 12th International Conference on Electrical and Computer Engineering (ICECE),"4 Apr 2023","2022","","","417","420","Since the variations in brain activity provide a pathway for different emotional states, emotion recognition using electroencephalogram (EEG) has embraced a vast research area in the realm of human-computer interaction. This paper proposes a novel deep neural architecture based on bidirectional long short-term memory (BiLSTM) for automated emotion classification. In the proposed scheme, the long-short-term memory (LSTM) blocks effectively capture important information throughout time steps. The deep-stacked bidirectional mechanism attributes essential features in the forward and backward directions for time-series data. Finally, the acquired feature vector is applied to the dense classifier to categorize different emotion classes. In contrast to the conventional method, an additional feature extraction step is eliminated, resulting in a substantially reduced computational complexity. In this work, extensive and detailed experiments are conducted on a widely available dataset, and satisfactory results are obtained for the valence and arousal domains, considering the performance of all subjects. In binary classification performance, the average accuracy for the valence domain is 82.36%, and the average for the arousal domain is 83.10%.","2771-7917","979-8-3503-9879-3","10.1109/ICECE57408.2022.10088864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10088864","Electroencephalogram (EEG);Emotion Recognition;Bidirectional Long-Short Term Memory (BiLSTM);Deep Learning","Human computer interaction;Deep learning;Emotion recognition;Brain;Neural networks;Computer architecture;Feature extraction","","2","","22","IEEE","4 Apr 2023","","","IEEE","IEEE Conferences"
"Decoding Emotions: Emotion Classification from EEG brain signals using AI","U. Dudeja; S. K. Dubey","CSE-Department, Amity University, Noida, Uttar Pradesh, India; CSE-Department, Amity University, Noida, Uttar Pradesh, India","2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)","26 Feb 2024","2023","10","","1204","1208","Emotion classification using EEG (Electroencephalogram) brain signals has recently garnered significant interest due to its potential applications in fields such as mental health, human-computer interaction, and affective computing. The three prominent recurrent neural network (RNN) architectures GRU (Gated Recurrent Unit), LSTM (Long Short-Term Memory), and BiLSTM (Bidirectional Long Short-Term Memory) are thoroughly compared in this study with a focus on classifying emotions. The study examines how well these RNN variations capture temporal dependencies and patterns within EEG data to precisely discriminate between various emotional states. Based on classification accuracy and training effectiveness, these model's performance will be assessed. The results will reveal the most suitable architecture in terms of accuracy and robustness for EEG-based emotion classification. The findings provide valuable insights into choosing the appropriate RNN architecture for emotion classification tasks, advancing the understanding of AI's potential in interpreting human emotions through EEG signals.","2687-7767","979-8-3503-8247-1","10.1109/UPCON59197.2023.10434423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434423","Accuracy;RNN;GRU;LSTM;BiLSTM","Deep learning;Recurrent neural networks;Computer architecture;Mental health;Brain modeling;Electroencephalography;Long short term memory","","","","17","IEEE","26 Feb 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition Using Three-Dimensional Feature and Convolutional Neural Network from Multichannel EEG Signals","H. Chao; L. Dong","Data Mining and Pattern Recognition Laboratory, School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo, China; Data Mining and Pattern Recognition Laboratory, School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo, China",IEEE Sensors Journal,"17 Dec 2020","2021","21","2","2024","2034","Using electroencephalogram (EEG) signal to recognize emotional states has become a research hotspot of affective computing. Previous emotion recognition methods almost ignored the correlation and interaction among multichannel EEG signals, which may provide salient information related to emotional states. This article proposes a novel approach based on rearranged EEG features and deep learning algorithm. In particular, each channel EEG signal is first processed in time domain to get time-domain features. Then, features of all channels are treated as a three-dimensional (3D) feature matrix, according to positions of electrode sensors. This makes the features closer to the real response of the cerebral cortex. Subsequently, an advanced convolutional neural network (CNN) designed with univariate convolution layer and multivariate convolution layer is employed to deal with the 3D feature matrix for emotion recognition. A benchmark dataset for emotion analysis using physiological signal is employed to evaluate this method. The experimental results proved that the 3D feature matrix can effectively represent the emotion-related features in multichannel EEG signals and the proposed CNN can efficaciously mine the unique features of each channel and the correlation among channels for emotion recognition.","1558-1748","","10.1109/JSEN.2020.3020828","National Natural Science Foundation of China(grant numbers:61502150); Fundamental Research Funds for the Universities of Henan Province(grant numbers:NSFRF1616); Foundation for Scientific and Technological Project of Henan Province(grant numbers:172102210279); Key Scientific Research Projects of Universities in Henan(grant numbers:19A520004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184077","Emotion recognition;multichannel EEG signal;three-dimensional feature;CNN;deep learning","Electroencephalography;Feature extraction;Emotion recognition;Time-domain analysis;Three-dimensional displays;Electrodes;Machine learning","","70","","43","IEEE","1 Sep 2020","","","IEEE","IEEE Journals"
"Cross-Subject Channel Selection Using Modified Relief and Simplified CNN-Based Deep Learning for EEG-Based Emotion Recognition","L. Farokhah; R. Sarno; C. Fatichah","Department of Informatics, Faculty of Intelligent Electrical and Informatics Technology, Institut Teknologi Sepuluh Nopember (ITS), Surabaya, Indonesia; Department of Informatics, Faculty of Intelligent Electrical and Informatics Technology, Institut Teknologi Sepuluh Nopember (ITS), Surabaya, Indonesia; Department of Informatics, Faculty of Intelligent Electrical and Informatics Technology, Institut Teknologi Sepuluh Nopember (ITS), Surabaya, Indonesia",IEEE Access,"12 Oct 2023","2023","11","","110136","110150","Emotion recognition based on EEG has been implemented in numerous studies. In most of them, there are two observations made: first, extensive implementation is negatively associated with the performed validation. Cross-subject validation is more difficult than subject-dependent validation due to the high variability between EEG recordings caused by domain shifts. Second, a large number of channels requires extensive computation. Efforts to reduce channels are impeded by decreased performance as the number of channels is decreased; therefore, an effective approach for reducing channels is required to maintain performance. In this paper, we propose collaboration on 2D EEG input in the form of scalograms, CNN, and channel selection based on power spectral density ratios coupled with the relief method. The power ratio is derived from the power band’s power spectral density. Based on the trial selection with various conditions, the collaboration of the proposed scalogram and PR-Relief (power ratio-Relief) produced a stable classification rate. For analysis, the Database for Emotion Analysis of Physiological Signals (DEAP) has been employed. Experimental results indicate that the proposed method increases the accuracy of cross-subject emotion recognition using 10 channels by 2.71% for valence and 1.96% for arousal, respectively. Using 10 channels for subject-dependent validation, the efficacy of the valence and arousal classes increased by 2.41% and 1.2%, respectively. Consequently, by pursuing collaboration between input interpretation and stable channel selection methods, the proposed collaborative method achieves a better result.","2169-3536","","10.1109/ACCESS.2023.3322294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10272583","Channel selection;emotion recognition;validation;cross-subject;scalogram","Electroencephalography;Emotion recognition;Deep learning;Brain modeling;Collaboration;Computer architecture;Support vector machines;Convolutional neural networks","","5","","67","CCBYNCND","5 Oct 2023","","","IEEE","IEEE Journals"
"Tunable Q-factor Wavelet Transform Based Features for Classification of Emotions Using EEG Signals","K. Das; M. Saha","Dept. of Electronics and Communication Engineering, National Institute of Technology Jamshedpur, Jharkhand, India; School of Electronics Engineering, Vellore Institute of Technology, Andhra Pradesh, India","2025 Fourth International Conference on Power, Control and Computing Technologies (ICPC2T)","17 Apr 2025","2025","","","1","5","Understanding human emotions is crucial for effective communication and interaction. Emotion recognition systems have gained considerable attention due to their potential applications in various fields, such as healthcare, human-computer interaction, and entertainment. Electroencephalography (EEG) has emerged as a promising modality for emotion recognition, offering insights into the underlying neural processes associated with different emotional states. This study investigates the effectiveness of the tunable Q-factor wavelet transform (TQWT) for classifying different emotions using EEG signals. TQWT decomposes the EEG signal into subbands, and some features such as variance, skewness, kurtosis, and sample entropy are calculated from these subbands. These features serve as input to the support vector machine (SVM) classifier to classify emotions like happiness, fear, sadness, neutral, and relaxation. Experimental results of this proposed method demonstrate superior performance in classifying these five emotions compared to other existing methods.","","979-8-3315-3040-2","10.1109/ICPC2T63847.2025.10958579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10958579","Electroencephalography (EEG);Emotion recognition;TQWT;SVM","Support vector machines;Wavelet transforms;Q-factor;Emotion recognition;Adaptation models;Accuracy;Feature extraction;Brain modeling;Electroencephalography;Robustness","","","","15","IEEE","17 Apr 2025","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition of Deaf Subjects by Integrated Genetic Firefly Algorithm","Z. Tian; D. Li; Y. Song; Q. Gao; Q. Kang; Y. Yang","Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical and Electronic Engineering, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical and Electronic Engineering, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical and Electronic Engineering, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Maritime College, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical and Electronic Engineering, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical and Electronic Engineering, Tianjin University of Technology, Tianjin, China",IEEE Transactions on Instrumentation and Measurement,"9 Nov 2021","2021","70","","1","11","In recent years, many researchers have explored different methods to obtain discriminative features for electroencephalogram-based (EEG-based) emotion recognition, but a few studies have been investigated on deaf subjects. In this study, we have established a deaf EEG emotion dataset, which contains three kinds of emotion (positive, neutral, and negative) with 15 subjects. Ten kinds of time–frequency domain features and eleven kinds of nonlinear dynamic system features were extracted from the EEG signals. To obtain the optimal feature combination and optimal classifier, an integrated genetic firefly algorithm (IGFA) was proposed. The multi-objective function with variable weight was utilized to balance the classification accuracy and the feature reduction ratio that are contradictory goals to find brighter fireflies in each generation. To retain the historical optimal solution and reduce the feature dimension, an optimal population protection scheme and subgroups generation scheme was carried out. The experimental results show that the averaged feature reduction rate of the proposed method is 0.959, and the averaged classification accuracy is 0.961. By investigating important brain regions, deaf subjects have common areas in the frontal and temporal lobes for EEG emotion recognition, while individual areas occur in the occipital and parietal lobes.","1557-9662","","10.1109/TIM.2021.3121473","Fundamental Research on Advanced Technology and Engineering Application Team, Tianjin, China(grant numbers:20160524); Natural Science Foundation of Tianjin(grant numbers:18JCYBJC87700); Post-Doctoral Research Projects of Hebei Province of China(grant numbers:B2020003020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580860","Deaf subjects;electroencephalogram (EEG);emotion recognition;firefly algorithm (FA);genetic algorithm","Electroencephalography;Feature extraction;Emotion recognition;Entropy;Classification algorithms;Brain modeling;Protocols","","19","","39","IEEE","19 Oct 2021","","","IEEE","IEEE Journals"
"Enhancing Emotion Recognition Performance Using EEG Signals with Deep Learning Algorithms and Hyperparameter Optimization","S. Gokalp; I. Aydin; M. S. Ercin","Computer Engineering, Fırat University, Elazığ, Türkiye; Computer Engineering, Fırat University, Elazığ, Türkiye; Computer Engineering, Hacettepe University, Ankara, Türkiye",2024 International Conference on Decision Aid Sciences and Applications (DASA),"17 Jan 2025","2024","","","1","5","Emotions play a crucial role in understanding human nature and enable us to experience and feel what we go through. Humans express their behavioral characteristics through emotions. In addition to or as an alternative to facial image-based emotion recognition, the EEG (Electroencephalogram) method is also employed in emotion recognition tasks. In this study, a novel deep learning approach has been developed to enhance emotion recognition performance using EEG signals. We applied hyperparameter optimization on SEED-IV, DEAP, and DREAMER datasets using our proposed Special Convolutional Model (SCM) and two-dimensional LSTM models. The SCM model achieved an accuracy rate of 64.35% on the SEED-IV dataset, outperforming similar studies in the literature. The two-dimensional LSTM model, on the other hand, achieved a notable accuracy of 58.8%. The success of the SCM model is attributed to hyperparameter optimization performed using the RMSprop optimizer. In tests conducted on the DREAMER dataset, the accuracy rate, which was 28% in the WEKA environment, was increased to 44% through optimization. This study demonstrates the effectiveness of optimizations in deep learning models for enhancing EEG-based emotion recognition performance. The developed approach allows for a deeper understanding of EEG signals and surpasses existing methods in the literature.","","979-8-3503-6910-6","10.1109/DASA63652.2024.10836549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10836549","EEG;SEED;DEAP;DREAMER;Convolutional Neural Network;LSTM","Deep learning;Hands;Emotion recognition;Accuracy;Convolution;Brain modeling;Hyperparameter optimization;Electroencephalography;Optimization;Long short term memory","","","","22","IEEE","17 Jan 2025","","","IEEE","IEEE Conferences"
"Emotion classification using EEG headset signals and Random Forests","R. Vasquez; J. Carrion-Jumbo; D. Riofrío-Luzcando; C. Guevara","Universidad Internacional SEK, Quito, Ecuador; Universidad Internacional SEK, Quito, Ecuador; Universidad Internacional SEK, Quito, Ecuador; Centro de Investigación en Mecatrónica y Sistemas Interactivos -MIST, Universidad Indoamérica, Quito, Ecuador",2023 18th Iberian Conference on Information Systems and Technologies (CISTI),"15 Aug 2023","2023","","","1","7","Emotions are one of the important components of the human being, thus they are a valuable part of daily activities such as interaction with people, decision making and learning. For this reason, it is important to detect, recognize and understand emotions using computational systems to improve communication between people and machines, which would facilitate the ability of computers to understand the communication between humans. This study proposes the creation of a model that allows the classification of people's emotions based on their EEG signals, for which the brain-computer interface EMOTIV EPOC was used. This allowed the collection of electroencephalographic information from 50 people, all of whom were shown audiovisual resources that helped to provoke the desired mood. The information obtained was stored in a database for the generation of the model and the corresponding classification analysis. Random Forest model was created for emotion prediction (happiness, sadness and relaxation), based on the signals of any person. The results obtained were 97.21% accurate for happiness, 76% for relaxation and 76% for sadness. Finally, the model was used to generate a real-time emotion prediction algorithm; it captures the person's EEG signals, executes the generated algorithm and displays the result on the screen with the help of images representative of each emotion.","2166-0727","978-989-33-4792-8","10.23919/CISTI58278.2023.10211789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10211789","EEG;emotion classification;emotion prediction;Machine Learning;Random Forest","Headphones;Mood;Predictive models;Brain modeling;Prediction algorithms;Electroencephalography;Real-time systems","","","","0","","15 Aug 2023","","","IEEE","IEEE Conferences"
"Improving EEG-based Emotion Recognition using DCGAN with MobileNet","S. Kaushik; G. K. Verma","Department of Information Technology, National Institute of Technology, Raipur, India; Department of Information Technology, National Institute of Technology, Raipur, India",2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"4 Nov 2024","2024","","","1","6","Emotions play a crucial role in influencing human behaviour, decision-making, and task performance. EEG signals are preferred for emotion recognition due to their non-invasive nature, cost-effectiveness, and high temporal resolution. CNNs have emerged as effective tools for emotion recognition specifically with image datasets. Their strength lies in automatically extracting intricate features from images, aiding in identifying subtle facial expression details that convey emotions. Transfer Learning (TL) allows precise parameter learning and specialized CNN configurations. We utilized the fine-tuning of a pre-trained CNN model, MobileNet, to classify four emotional states. The experimental outcomes demonstrated ${9 9. 8 9 \%, 9 8 \%}$ and $95.75 \%$ accuracy with DCGAN with MobileNet architecture which proves its usefulness in emotion recognition.","2473-7674","979-8-3503-7024-9","10.1109/ICCCNT61001.2024.10724882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10724882","Emotion Recognition;MobileNet;Deep Learning;CNN models;DCGAN;Data Augmentation","Emotion recognition;Accuracy;Image resolution;Computational modeling;Transfer learning;Computer architecture;Brain modeling;Feature extraction;Robustness;Signal resolution","","","","17","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"Internal Emotion Classification Using EEG Signal With Sparse Discriminative Ensemble","H. Ullah; M. Uzair; A. Mahmood; M. Ullah; S. D. Khan; F. A. Cheikh","College of Computer Science and Engineering, University of Hail, Ha’il, Saudi Arabia; Department of Computer Science, COMSATS University Islamabad, Wah Campus, Pakistan; Department of Computer Science, Information Technology University, Lahore 54000, Wah Cantonment, Pakistan; Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, Norway; College of Computer Science and Engineering, University of Hail, Ha’il, Saudi Arabia; Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, Norway",IEEE Access,"3 Apr 2019","2019","7","","40144","40153","Among various physiological signal acquisition methods for the study of the human brain, EEG (Electroencephalography) is more effective. EEG provides a convenient, non-intrusive, and accurate way of capturing brain signals in multiple channels at fine temporal resolution. We propose an ensemble learning algorithm for automatically computing the most discriminative subset of EEG channels for internal emotion recognition. Our method describes an EEG channel using kernel-based representations computed from the training EEG recordings. For ensemble learning, we formulate a graph embedding linear discriminant objective function using the kernel representations. The objective function is efficiently solved via sparse non-negative principal component analysis and the final classifier is learned using the sparse projection coefficients. Our algorithm is useful in reducing the amount of data while improving computational efficiency and classification accuracy at the same time. The experiments on publicly available EEG dataset demonstrate the superiority of the proposed algorithm over the compared methods.","2169-3536","","10.1109/ACCESS.2019.2904400","Norwegian University of Science and Technology through the open access journal publication fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8665857","Multiple channel EEG;emotion recognition;linear discriminant analysis;sparse PCA","Electroencephalography;Emotion recognition;Kernel;Feature extraction;Support vector machines;Training;Linear programming","","85","","59","OAPA","12 Mar 2019","","","IEEE","IEEE Journals"
"Cross-Subject EEG Emotion Recognition Based on Interconnected Dynamic Domain Adaptation","Y. An; S. Hu; S. Liu; Z. Wang; X. Wang; X. Ma","Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; College of Electronic and Information Engineering, Hebei University, Baoding, China; College of Electronic and Information Engineering, Hebei University, Baoding, China; College of Electronic and Information Engineering, Hebei University, Baoding, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","12981","12985","Electroencephalogram (EEG) is widely utilized in emotion recognition owing to its unique advantages. To achieve more optimal cross-subject emotion recognition, a cross subject emotion recognition method based on interconnection dynamic domain adaptation (IDDA) is proposed. In IDDA, dynamic graph convolution (DGC) is employed to dynamically learn the intrinsic relationships between different EEG channels and to extract domain invariant features. And dynamic domain adaptation (DDA) is employed to align the source domain and target domain, at the same time the emotional sub-domains is aligned, achieving more optimal cross subject emotion recognition. To select suitable subjects as the source domain, a multi-source selection algorithm is incorporated before dynamic adaptive computation reducing migration noise and achieving interconnection between DGC and DDA. IDDA enhances the emotion discrimination ability of domain invariant features, thereby improving the accuracy of cross-subject EEG emotion recognition. This method achieves classification results of 85.75% and 72.36% in cross subject experiments on SEED and SEED-IV.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10446957","National Natural Science Foundation of China; Natural Science Foundation of Hebei Province; Hebei University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446957","emotion recognition;dynamic graph convolution;domain adaptation;multi-source selection","Training;Emotion recognition;Convolution;Databases;Heuristic algorithms;Signal processing algorithms;Speech recognition","","3","","21","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition Using Self-Organizing Map for Boundary Detection","R. Khosrowabadi; H. C. Quek; A. Wahab; K. K. Ang","Center of Computational Intelligence, School of Computer Engineering, Nanyang Technological University, Singapore; Center of Computational Intelligence, School of Computer Engineering, Nanyang Technological University, Singapore; Center of Computational Intelligence, School of Computer Engineering, Nanyang Technological University, Singapore; Institute of Infocomm Research, Agency for Science, Technology and Research, Singapore",2010 20th International Conference on Pattern Recognition,"7 Oct 2010","2010","","","4242","4245","This paper presents an EEG-based emotion recognition system using self-organizing map for boundary detection. Features from EEG signals are classified by considering the subjects' emotional responses using scores from SAM questionnaire. The selection of appropriate threshold levels for arousal and valence is critical to the performance of the recognition system. Therefore, this paper investigates the performance of a proposed EEG-based emotion recognition system that employed self-organizing map to identify the boundaries between separable regions. A study was performed to collect 8 channels of EEG data from 26 healthy right-handed subjects in experiencing 4 emotional states while exposed to audio-visual emotional stimuli. EEG features were extracted using the magnitude squared coherence of the EEG signals. The boundaries of the EEG features were then extracted using SOM. 5-fold cross-validation was then performed using the k-nn classifier. The results showed that proposed method improved the accuracies to 84.5%.","1051-4651","978-1-4244-7541-4","10.1109/ICPR.2010.1031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597763","EEG;Emotion recognition;Self organizing map","Electroencephalography;Emotion recognition;Feature extraction;Accuracy;Protocols;Scalp;Data processing","","78","1","18","IEEE","7 Oct 2010","","","IEEE","IEEE Conferences"
"EEG-based Classification of the Intensity of Emotional Responses","V. Babushkin; W. Park; M. H. Jamil; H. Alsuradi; M. Eid","Tandon School of Engineering, New York University, NYC, USA; Engineering Division, New York University Abu Dhabi, Saadiyat Island, Abu Dhabi, United Arab Emirates; Engineering Division, New York University Abu Dhabi, Saadiyat Island, Abu Dhabi, United Arab Emirates; Tandon School of Engineering, New York University, NYC, USA; Engineering Division, New York University Abu Dhabi, Saadiyat Island, Abu Dhabi, United Arab Emirates",2021 10th International IEEE/EMBS Conference on Neural Engineering (NER),"2 Jun 2021","2021","","","218","221","Considering the fact that emotional experiences are stored in the brain, classifying emotion from brain activity measured using electroencephalography has become a trend. In most of the previous studies, the user's emotions were classified based on a stimulus. In this paper, we present a model that can classify the emotion intensity by the participants' self-report. Two machine learning classifiers are considered: support vector machine (SVM) and convolutional neural networks (CNN). Results demonstrated that both SVM and CNN models perform well with four classes of emotions (positive/negative valence high/low arousal combination) where SVM achieved an accuracy of 85% whereas CNN achieved 81%. Considering 12 classes of emotional responses (low, medium, and high intensity for positive/negative valence high/low arousal combination) by the participants' self report resulted in an accuracy of 70% for SVM and 69% for CNN. The proposed model excels in classifying emotional intensity and provides superior performance compared to the state-of-the-art emotion classification systems.","1948-3554","978-1-7281-4337-8","10.1109/NER49283.2021.9441371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9441371","Emotion recognition;Affective computing;Machine Learning;Biomedical signal processing","Support vector machines;Emotion recognition;Neural engineering;Machine learning;Signal processing;Brain modeling;Particle measurements","","1","","20","IEEE","2 Jun 2021","","","IEEE","IEEE Conferences"
"Classifying Human Emotions through EEG data with Machine Learning","G. Kaur; M. Gupta; R. Kumar","Department of Computer Science and Engineering, Chandigarh University, Punjab, India; Department of Computer Science and Engineering, Chandigarh University, Punjab, India; Department of Computer Science and Engineering, Chandigarh University, Punjab, India",2024 International Conference Automatics and Informatics (ICAI),"28 Jan 2025","2024","","","632","636","The Psychological and neurobiological phenomenon is called an emotion. Nowadays, using emotional engineering for the prediction of human emotions is one of the major attempts of Machine Learning (ML). However, the capability of human beings to present different expressions clashes with the emotional state of mind. Therefore, by observing physical appearance it is challenging to be able to identify a human’s real emotional state. In this study, the objective is to examine the usage of machine learning techniques for an efficient emotion prediction that relies on Electroencephalography (EEG) signals. Using the Kaggle dataset that was collected through a Muse headband, five distinct ML methods are applied to classify emotions. To differentiate between different classification techniques performance metrics such as precision, recall, F1 score, support, and accuracy are used, Random Forest turned out to be the best method due to its high accuracy. Three emotions are predicted based on the results of performance metrics. This study is beneficial for mental health monitoring, neurofeedback therapy, stress management, human-computer interaction, etc. This study lays the groundwork for exploration into this field and helps in the development of more sophisticated control systems in the future.","","979-8-3503-5390-7","10.1109/ICAI63388.2024.10851616","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10851616","EEG;BCI;Emotion detection;classification;LR;DT;RF;SVM;and KNN","Measurement;Accuracy;Medical treatment;Mental health;Nearest neighbor methods;Electroencephalography;Object recognition;Random forests;Neurofeedback;Monitoring","","","","18","IEEE","28 Jan 2025","","","IEEE","IEEE Conferences"
"A Transformer Convolutional Network With the Method of Image Segmentation for EEG-Based Emotion Recognition","X. Zhang; X. Cheng","School of Biomedical Engineering (Suzhou), Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China; School of Biomedical Engineering (Suzhou), Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China",IEEE Signal Processing Letters,"25 Jan 2024","2024","31","","401","405","Electroencephalogram (EEG) based emotion recognition has become an important topic in human-computer interaction and affective computing. However, existing advanced methods still have some problems. Firstly, using too many electrodes will decrease the practicality of EEG acquisition device. Secondly, transformer is not good at extracting local features. Finally, differential entropy (DE) is unsuitable for extracting features outside the 2–44 Hz frequency band. To solve these problems, we designed a neural network using 14 electrodes, utilizing differential entropy and designed spectrum sum (SS) to extract features, using convolutional neural networks and image segmentation techniques to learn local features, and transformer encoders to learn global features. The model outperformed advanced methods with classification results of 98.50% and 99.00% on the SEED-IV and SEED-V datasets.","1558-2361","","10.1109/LSP.2024.3353679","National Key R&D Program of China(grant numbers:2022YFC2405603); Key R&D Program of Jiangsu Province(grant numbers:BE2022064-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398432","Electroencephalogram (EEG);emotion recognition;transformer;image segmentation","Feature extraction;Electroencephalography;Transformers;Image segmentation;Emotion recognition;Convolutional neural networks;Tensors","","6","","36","IEEE","12 Jan 2024","","","IEEE","IEEE Journals"
"Terrain-based Gait Recognition using EEG: Comparing Machine Learning and Deep Learning Models","Nidhi; D. Joshi","Accenture Labs, Accenture, India; Centre For Biomedical Engineering, Indian Institute of Technology, Delhi, India",2021 International Conference on Computational Performance Evaluation (ComPE),"12 Apr 2022","2021","","","734","740","In the past EEG analysis has been done in many sectors, like emotion classification, standing/swing stance, letter classification. Research has shown a requisite amount of relation between signal generation at cortical and walking during locomotion. In this paper our aim is to classify different terrains like Level Walking, Ramp Descent, Ramp Ascent, Stair Descent, Stair Ascent as an application to decode gait patterns. We have used kinematics data of the toe for a generalized EEG dataset classification into different terrain. As the open sourced EEG dataset was already preprocessed using Artifacts Subspace Reconstruction (ASR) and Reliable Independent Component Analysis (RELICA) which removed any noise due to movements. Our study was evaluated on an open sourced dataset [1] of 11 healthy subjects walking on different terrains with 2 trials for each recording. The dataset was A deep learning based subject independent model was developed approach has been applied here to classify the EEG dataset with complex features into 6 classes. The average accuracy during cross-validaition was 70%,with further accuracy of 68% on the unseen data. Deep learning model showed superiority over machine learning approach using bagging regressor (accuracy – 62%). The present study concludes that deep learning models are better choice given the complexity.","","978-1-6654-3656-4","10.1109/ComPE53109.2021.9751957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751957","Brain Computer Interface;Electroencephalogram;Gait Decoding;Terrain Classification","Deep learning;Training;Legged locomotion;Stairs;Brain modeling;Electroencephalography;Electromyography","","","","17","IEEE","12 Apr 2022","","","IEEE","IEEE Conferences"
"HVFM: an Emotion Classification Model based on Horizontal and Vertical Flow Domain-adaptive","Z. Li; X. Zhao; Y. Yang; Q. Gao; Y. Song","Department of School of Electrical Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology",2022 IEEE International Conference on Mechatronics and Automation (ICMA),"22 Aug 2022","2022","","","455","460","Emotion classification has received increasing and extensive attention from researchers. Compared with conventional emotion recognition methods by facial expression, text, gesture and speech, Electroencephalogram (EEG) can reflect the real emotional state, which shows an advantage in reliability and accuracy. In this paper, a novel horizontal and vertical flow model (HVFM) was proposed to learn the flow information between the horizontal and vertical transition of EEG channels. Firstly, we constructed a two-direction flow of EEG signals by the international EEG location. Then, we employed recurrent neural networks (RNN) with two flow directions to extract the deep emotion flow characteristic from the EEG electrodes. A domain discriminator that induces the model to obtain the source and target’s invariant emotion features. What is more, to minimize the variability between the two domains, we use a similar loss function. Finally, to demonstrate the effectiveness of the proposed HVFM model, we perform emotion classification (positive, natural, negative) on the SEED dataset, the result has a state-of-art emotion recognition accuracy of 92.75%.","2152-744X","978-1-6654-0853-0","10.1109/ICMA54519.2022.9856194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856194","EEG;Emotion recognition;Domain adaption","Emotion recognition;Visualization;Recurrent neural networks;Mechatronics;Speech recognition;Brain modeling;Feature extraction","","1","","23","IEEE","22 Aug 2022","","","IEEE","IEEE Conferences"
"Spatiotemporal Graph Convolutional Networks for EEG-Based Emotion Recognition","W. Li; W. Shi; C. -H. Yeh","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","2024 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","12 Feb 2025","2024","","","1","6","Emotion recognition is a critical task in understanding human affective states and their impact on interactions with products, services, and brands. In this study, we introduce a novel spatiotemporal graph convolutional network (GCN) framework for EEG-based emotion recognition. Unlike conventional CNN and RNN models that struggle with non-Euclidean data structures, our approach leverages the spatial and temporal relationships between EEG channels, captured using advanced GCN techniques. The proposed framework includes spatial and spatiotemporal models, each further divided based on different feature inputs, including Differential Entropy (DE) and Power Spectral Density (PSD). We validate our models on the DEAP dataset, where the spatiotemporal model achieved a valence classification accuracy of 79.7% and an arousal classification accuracy of 68.2%. These results demonstrate that the optimal model configuration significantly enhances emotion classification accuracy, particularly in the recognition of both valence and arousal states. The findings suggest that incorporating GCNs into emotion recognition systems can effectively address the challenges posed by the complex, non-Euclidean structure of EEG data.","","979-8-3315-1566-9","10.1109/ICSIDP62679.2024.10869131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10869131","Emotion Recognition;EEG;GCN;Feature Extraction","Emotion recognition;Accuracy;Graph convolutional networks;Brain modeling;Feature extraction;Reliability engineering;Electroencephalography;Data models;Robustness;Spatiotemporal phenomena","","","","22","IEEE","12 Feb 2025","","","IEEE","IEEE Conferences"
"EEG-based emotion recognition in music listening: A comparison of schemes for multiclass support vector machine","Y. -P. Lin; C. -H. Wang; T. -L. Wu; S. -K. Jeng; J. -H. Chen","Department of Electrical Engineering, National Taiwan University, Taiwan; Cardinal Tien Hospital, Taiwan; Department of Electrical Engineering, National Taiwan University, Taiwan; Department of Electrical Engineering, National Taiwan University, Taiwan; Department of Electrical Engineering, National Taiwan University, Taiwan","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","489","492","Currently, how to equip machines with the ability for properly recognizing users' felt-emotion during multimedia presentation is a growing issue. In this study we focused on the approach for recognizing music-induced emotional responses from brain activity. A comparative study was conducted to testify the feasibility of using hierarchical binary classifiers to improve the classification performance as compared with nonhierarchical schemes. According to our classification results, we not only found that using one-against-one scheme of hierarchical binary classifier results in an improvement to performance, but also established an alternative solution for emotion recognition by proposed model-based scheme depending on 2D emotion model.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4959627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959627","Brain activity;Emotion recognition","Emotion recognition;Support vector machines;Electroencephalography;Brain;Neural networks;Data acquisition;Humans;Multiple signal classification;Hospitals;Multilayer perceptrons","","107","","13","IEEE","26 May 2009","","","IEEE","IEEE Conferences"
"Multichannel EEG-Based Emotion Recognition via Group Sparse Canonical Correlation Analysis","W. Zheng","Key Laboratory of Child Development and Learning Science, Ministry of Education, Research Center for Learning Science, Southeast University, Nanjing, China",IEEE Transactions on Cognitive and Developmental Systems,"7 Sep 2017","2017","9","3","281","290","In this paper, a novel group sparse canonical correlation analysis (GSCCA) method is proposed for simultaneous electroencephalogram (EEG) channel selection and emotion recognition. GSCCA is a group sparse extension of the conventional CCA method to model the linear correlationship between emotional EEG class label vectors and the corresponding EEG feature vectors. In contrast to conventional CCA method or previous GSCCA methods, a major advantage of our GSCCA method is the ability of handling the group feature selection problem from raw EEG features, which makes it very suitable for simultaneously coping with both EEG emotion recognition and automatic channel selection issues where each EEG channel is associated with a group of raw EEG features. To deal with EEG emotion recognition problem, we adopt the popularly used frequency feature to describe the EEG signal by dividing the full EEG frequency band into five parts, i.e., δ, θ, α, β, and γ frequency bands, and then extract the frequency band features from each band for GSCCA model learning and emotion recognition. Finally, we conduct extensive experiments on EEG-based emotion recognition based on the SJTU emotion EEG dataset and experimental results demonstrate that the proposed GSCCA method would outperform the state-of-the-art EEG-based emotion recognition approaches.","2379-8939","","10.1109/TCDS.2016.2587290","National Basic Research Program of China(grant numbers:2015CB351704); National Natural Science Foundation of China(grant numbers:61231002,61572009); Natural Science Foundation of Jiangsu Province(grant numbers:BK20130020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505915","Channel selection;EEG-based emotion recognition;electroencephalogram (EEG);group sparse canonical correlation analysis (GSCCA)","Electroencephalography;Emotion recognition;Feature extraction;Frequency conversion;Frequency-domain analysis;Correlation;Brain modeling","","227","","25","IEEE","6 Jul 2016","","","IEEE","IEEE Journals"
"Artificial Intelligence Based Emotion Recognition from Fuzzy EEG Signals: A Comprehensive Review","I. K; T. B. Miriyala","Department of Computer Science and Enginnering, Koneru Lakshmaiah Education Foundation, Hyderabad, Telangana, India; Department of Computer Science and Enginnering, Koneru Lakshmaiah Education Foundation, Hyderabad, Telangana, India",2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL),"27 Mar 2025","2025","","","921","926","Affective Computing, a type of artificial intelligence that can detect and interpreting human emotions with an advancement technologies like non-invasive sensing, and brain-computer interfaces. Academics have been fascinated by the idea of developing systems that can identify human emotions using various facial expressions, gestures, and physiological signals. The first three techniques are typically ineffective because people may hide what they're feeling unconsciously or consciously. Emotion recognition can be improved using physiological signals. These signals are more sensitive to the changes in the emotional states of people. Implementing emotional recognition systems face several challenges including data acquisition, feature extraction, model development, model training, and generalization. Adoption of AI techniques can help in improving accuracy and robustness using fuzzy EEG signals. Advancements in deep learning and machine learning have made it possible to recognize, analyze, and interpret various emotional states. The present work reviews the available datasets and techniques such as the extraction of features, selection/reduction, and machine learning for eliciting emotional states. It also explores different types of neural networks with long-term memory or convolutional systems. The review also describes electroencephalogram signals that are associated with certain emotions and the relationship among various parts of the brain. The review also helps future research to gain insights towards efficient emotion recognition capturing human emotions.","","979-8-3315-2392-3","10.1109/ICSADL65848.2025.10933235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10933235","Affective Computing;Brain Computer Interface;Convolutional Systems;Electroencephalogram Signal Analysis;Emotion Recognition Systems","Deep learning;Emotion recognition;Reviews;Convolution;Face recognition;Feature extraction;Electroencephalography;Physiology;Data models;Brain-computer interfaces","","","","57","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"EEG-Based Human Emotion Classification Using Combined Computational Techniques for Feature Extraction and Selection in Six Machine Learning Models","L. O. Mohutsiwa; R. S. Jamisola","Dept. of Mechanical, Energy and Industrial Engineering, Botswana International University of Science and Technology, Palapye, Botswana; Dept. of Mechanical, Energy and Industrial Engineering, Botswana International University of Science and Technology, Palapye, Botswana",2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS),"26 May 2021","2021","","","1095","1102","This work focuses on the use of electroencephalogram (EEG) signals to classify four human emotions, i.e., amused, disgust, sad, and scared that are elicited by custom-made video clips. The proposed model uses the independent component analysis (ICA) for artifact removal, band power and Hjorth parameters for feature extraction, and neighborhood component analysis (NCA) and minimum redundancy maximum relevance (mRMR) for feature selection. These computational techniques are combined because when individually used, they tend to give better accuracy results. However, they are not jointly used in many EEG-based emotion studies. A comparison has been made on the results obtained from six machine learning models, namely, decision trees, support vector machines, k-nearest neighbors, naive Bayes, random forest, and long short-term memory (LSTM) recurrent neural network (RNN). The highest accuracy attained in this study is 99.1% that used long short-term memory recurrent neural network as a machine learning model, a combined NCA and mRMR for feature selection, and a combined band power and Hjorth parameters for feature extraction.","","978-1-6654-1272-8","10.1109/ICICCS51141.2021.9432207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432207","EEG;Feature Extraction;Feature Selection;NCA;ICA;mRMR;Machine Learning;LSTM RNN","Support vector machines;Recurrent neural networks;Computational modeling;Redundancy;Feature extraction;Brain modeling;Electroencephalography","","4","","22","IEEE","26 May 2021","","","IEEE","IEEE Conferences"
"EEG-based Positive-Negative Emotion Classification Using Machine Learning Techniques","Y. Kasuga; J. Shin; M. A. M. Hasan; Y. Okuyama; Y. Tomioka","Department of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; Department of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; Department of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; Department of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; Department of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan",2021 IEEE 14th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC),"4 Feb 2022","2021","","","135","139","The aim of this study is to find useful electrodes for positive-negative emotion classification based on EEG. We collected EEG signals from 30 people aged 19-38 using 14 electrodes. We used two movies for positive and negative emotions. First, we extracted the power spectrum from the EEG data, normalized the data, and extracted frequency-domain statistical parameters therefrom. When the features were applied to Random Forests (RF), 85.4%, 83.8%, and 83.4% accuracy was obtained for P8, P7, and FC6 electrodes, respectively. This indicates that the P8, P7 and FC6 electrodes are the useful electrode in positive-negative emotion classification.","","978-1-6654-3860-5","10.1109/MCSoC51149.2021.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691947","Component;EEG;Positive-negative emotion;Svm;K-nearest neighbor;Random forest","Electrodes;Radio frequency;Multicore processing;Frequency-domain analysis;Training data;Feature extraction;Motion pictures","","3","","11","IEEE","4 Feb 2022","","","IEEE","IEEE Conferences"
"Research on EEG signal emotion recognition based on improved GAPSO-SVM","J. Peng; R. Liu","Tiangong University, Tianjin, China; Tiangong University, Tianjin, China","2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)","10 Apr 2023","2023","","","1726","1729","This study improves and combines genetic algorithm (GA) and particle swarm optimization (PSO), optimizes the penalty parameter C and kernel function parameter g of support vector machine (SVM), and proposes an emotion classification and recognition model of EEG based on improved GAPSO-SVM. The classification accuracy of γ band energy data was up to 88.8889 %, and the classification accuracy of SampEn data was up to 77.7778 %.","","978-1-6654-6253-2","10.1109/EEBDA56825.2023.10090851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10090851","EEG signals;emotion;PSO;GA;SVM","Support vector machines;Electrical engineering;Emotion recognition;Big Data;Brain modeling;Electroencephalography;Classification algorithms","","2","","10","IEEE","10 Apr 2023","","","IEEE","IEEE Conferences"
"Effective Emotion Recognition by Learning Discriminative Graph Topologies in EEG Brain Networks","C. Li; P. Li; Y. Zhang; N. Li; Y. Si; F. Li; Z. Cao; H. Chen; B. Chen; D. Yao; P. Xu","Ministry of Education (MOE) Key Laboratory for Neuroinformation, Clinical Hospital of Chengdu Brain Science Institute, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Bioinfomatics, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Computer Science and Technology, Southwest University of Science and Technology, Mianyang, China; Ministry of Education (MOE) Key Laboratory for Neuroinformation, Clinical Hospital of Chengdu Brain Science Institute, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Department of Psychology, Xinxiang Medical University, Xinxiang, China; Ministry of Education (MOE) Key Laboratory for Neuroinformation, Clinical Hospital of Chengdu Brain Science Institute, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Science, Technology, Engineering and Mathematics (STEM), University of South Australia, Adelaide, SA, Australia; Department of Radiology, First Affiliated Hospital to Army Medical University, Chongqing, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Clinical Hospital of Chengdu Brain Science Institute, Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Ministry of Education (MOE) Key Laboratory for Neuroinformation, Clinical Hospital of Chengdu Brain Science Institute, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China",IEEE Transactions on Neural Networks and Learning Systems,"5 Aug 2024","2024","35","8","10258","10272","Multichannel electroencephalogram (EEG) is an array signal that represents brain neural networks and can be applied to characterize information propagation patterns for different emotional states. To reveal these inherent spatial graph features and increase the stability of emotion recognition, we propose an effective emotion recognition model that performs multicategory emotion recognition with multiple emotion-related spatial network topology patterns (MESNPs) by learning discriminative graph topologies in EEG brain networks. To evaluate the performance of our proposed MESNP model, we conducted single-subject and multisubject four-class classification experiments on two public datasets, MAHNOB-HCI and DEAP. Compared with existing feature extraction methods, the MESNP model significantly enhances the multiclass emotional classification performance in the single-subject and multisubject conditions. To evaluate the online version of the proposed MESNP model, we designed an online emotion monitoring system. We recruited 14 participants to conduct the online emotion decoding experiments. The average online experimental accuracy of the 14 participants was 84.56%, indicating that our model can be applied in affective brain–computer interface (aBCI) systems. The offline and online experimental results demonstrate that the proposed MESNP model effectively captures discriminative graph topology patterns and significantly improves emotion classification performance. Moreover, the proposed MESNP model provides a new scheme for extracting features from strongly coupled array signals.","2162-2388","","10.1109/TNNLS.2023.3238519","STI 2030-Major Projects(grant numbers:2022ZD0208500,2022ZD02114000); National Natural Science Foundation of China(grant numbers:U19A2082,61961160705,61901077,62076209,62103085); Dr. Cao’s Australian Research Council (ARC) DECRA Fellowship(grant numbers:DE220100265); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035420","Brain neural network;emotion recognition;emotional intelligence;graph topology;multiple emotion-related spatial network topology pattern (MESNP)","Emotion recognition;Brain modeling;Electroencephalography;Feature extraction;Network topology;Topology;Monitoring","Humans;Emotions;Electroencephalography;Brain;Neural Networks, Computer;Brain-Computer Interfaces;Male;Adult;Female;Young Adult;Algorithms;Pattern Recognition, Automated;Machine Learning","23","","68","IEEE","2 Feb 2023","","","IEEE","IEEE Journals"
"Wearable Wireless Dual-Channel EEG System for Emotion Recognition Based on Machine Learning","Y. Wang; W. Tian; J. Xu; Y. Tian; C. Xu; B. Ma; Q. Hao; C. Zhao; H. Liu","State Key Laboratory of Digital Medical Engineering, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China",IEEE Sensors Journal,"14 Sep 2023","2023","23","18","21767","21775","Emotion recognition is critical for promoting mental health, as too much negative emotions may cause mental illness, especially in the era of COVID-19. EEG is the dominating modality to study brain dynamics. However, most of the current EEG devices were designed for as much applications as possible with unnecessary electrodes for emotion recognition applications. In this article, a wearable and wireless EEG device with only two channels were specifically designed for emotion recognition. The device is minimized and could be embedded in a headband. Novel preprocessing algorithm to remove ocular artifacts, features selection, and optimization, comparison between the four machine learning methods were studied to demonstrate a high classification accuracy of emotion valence on 20 subjects. As our wearable EEG system achieved high accuracy with only two channels, it would broaden the application perspective of emotion recognition, and could be applied in outdoor environments or other scenarios.","1558-1748","","10.1109/JSEN.2023.3303441","National Natural Science Foundation of China(grant numbers:62001104,22005312); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200357); Key Research and Development Program of Jiangsu Province(grant numbers:BE2021700); Science and Technology Development Program of Suzhou(grant numbers:SYG202117); Key Project and Open Research Fund of State Key Laboratory of Bioelectronics; Fundamental Research Funds for the Central Universities(grant numbers:2242022R10052,2242022R20044); Zhishan Young Scholars of Southeast University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215346","EEG;emotion recognition;machine learning;wearable device","Electroencephalography;Emotion recognition;Electrodes;Sensors;Wireless communication;Feature extraction;Wireless sensor networks","","6","","42","IEEE","11 Aug 2023","","","IEEE","IEEE Journals"
"EEG Emotion Recognition Via Ensemble Learning Representations","B. Taha; D. Y. Hwang; D. Hatzinakos","Electrical and Computer Engineering Dept, University of Toronto, Canada; Electrical and Computer Engineering Dept, University of Toronto, Canada; Electrical and Computer Engineering Dept, University of Toronto, Canada","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Electroencephalography (EEG) based emotion recognition is gaining substantial interest because of its strong association with the area of brain-computer interface. Even though several works exist in the literature, it is still challenging to find discriminative features that can generalize well to different EEG datasets. In this work, we focus on developing a deep learning model that makes use of the spatial and temporal representations of the EEG signal to generate EEG embeddings for emotion recognition. The proposed model uses a self-attention mechanism along with a feature fusion approach to improve the discrimination power of the learned EEG embeddings. Comprehensive experiments are conducted on the DEAP dataset, which demonstrates the superiority of the proposed work, where the attained accuracies for the arousal and valence classification are 91.17% and 90.73% , respectively.","2379-190X","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10094939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094939","Emotion Recognition;EEG signal;Feature Fusion;Learning Representations","Deep learning;Emotion recognition;Speech recognition;Signal processing;Brain modeling;Feature extraction;Electroencephalography;Brain-computer interfaces;Ensemble learning;Speech processing","","1","","24","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Emotion Classification Based On EEG Signals In a Stable Environment","P. K. Sahu; R. kumar Sahoo; N. Sethi; S. Sethi","Department of Computer Science and Engineering, GIET University, Gunupur, Odisha, India; Department of Computer Science and Engineering, IGIT, Saranga, Odisha, India; Department of Computer Science and Engineering, GIET University, Gunupur, Odisha, India; Department of Computer Science and Engineering, IGIT, Saranga, Odisha, India","2020 International Conference on Computer Science, Engineering and Applications (ICCSEA)","3 Jul 2020","2020","","","1","5","Emotion defines as human behavior used to know the feeling, expression sentiment of humans inside the brain. So Emotion can explain its statics through voice interaction, words, facial expression, and body language. The main objective of this paper is to analyze the accuracy of the responded data by electroencephalograph (EEG) through machine learning algorithms. After predicting the accuracy we are trying to classify results with classification learning techniques. So here we have considered different lobes points of a human brain by using the NMX32 device for converting EEG signal to raw data format. From different types of lobes point we are using two pairs of points(P3-O1, T4-T6). We used Naive Bayes and Decision trees to classify and accuracy of our data in a stable environment with different situations. The above techniques give better accuracy on the EEG signal data source as 95.4% with ROC, 83.2% with Precision, 83.1% with Recall.","","978-1-7281-5830-3","10.1109/ICCSEA49143.2020.9132966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9132966","EEG;Emotion;Classification;Machine learning","Computer science;Machine learning algorithms;Soft sensors;Electroencephalography;Decision trees","","1","","13","IEEE","3 Jul 2020","","","IEEE","IEEE Conferences"
"Self-Weighted Semi-Supervised Classification for Joint EEG-Based Emotion Recognition and Affective Activation Patterns Mining","Y. Peng; W. Kong; F. Qin; F. Nie; J. Fang; B. -L. Lu; A. Cichocki","School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Artificial Intelligence, OPtics and ElectroNics (iOPEN), Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Center for Computational and Data-Intensive Science and Engineering, Skolkovo Institute of Science and Technology, Moscow, Russia",IEEE Transactions on Instrumentation and Measurement,"15 Nov 2021","2021","70","","1","11","In electroencephalography (EEG)-based affective brain–computer interfaces (aBCIs), there is a consensus that EEG features extracted from different frequency bands and channels have different abilities in emotion expression. Besides, EEG is so weak and non-stationary that easily causes distribution discrepancies for EEG data collected at different times; therefore, it is necessary to explore the affective activation patterns in cross-session emotion recognition. To address these two problems, we propose a self-weighted semi-supervised classification (SWSC) model in this article for joint EEG-based cross-session emotion recognition and affective activation patterns mining, whose merits include: 1) using both the labeled and unlabeled samples from different sessions for better capturing data characteristics; 2) introducing a self-weighted variable to learn the importance of EEG features adaptively and quantitatively; and 3) mining the activation patterns including the critical EEG frequency bands and channels automatically based on the learned self-weighted variable. Extensive experiments are conducted on the benchmark SEED_IV emotional dataset and SWSC obtained excellent average accuracies of 77.40%, 79.55%, and 81.52% in three cross-session emotion recognition tasks. Moreover, SWSC identifies that the Gamma frequency band contributes the most and the EEG channels in prefrontal, left/right temporal, and (central) parietal lobes are more important for cross-session emotion recognition.","1557-9662","","10.1109/TIM.2021.3124056","National Natural Science Foundation of China(grant numbers:61971173,61972121,U20B2074); National Key Research and Development Program of China for the Intergovernmental International Science and Technology Innovation Cooperation Project(grant numbers:2017YFE0116800); Natural Science Foundation of Zhejiang Province(grant numbers:LY21F030005,LY21F020015); National Social Science Foundation of China(grant numbers:19ZDA348); Fundamental Research Funds for the Provincial Universities of Zhejiang(grant numbers:GK209907299001-008); China Postdoctoral Science Foundation(grant numbers:2017M620470); MoE Key Laboratory of Advanced Perception and Intelligent Control of High-end Equipment (Anhui Polytechnic University)(grant numbers:GDSC202015); Guangxi Key Laboratory of Optoelectronic Information Processing (Guilin University of Electronic Technology)(grant numbers:GD21202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9597546","Affective patterns mining;electroencephalography (EEG);emotion recognition;feature self-weighting;semi-supervised classification","Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Time-frequency analysis;Physiology;Task analysis","","14","","40","IEEE","1 Nov 2021","","","IEEE","IEEE Journals"
"Emotion Recognition with the Feature extracted from brain Networks","C. Li; P. Li; L. Jiang; X. Zhu; Y. Si; Y. Zeng; D. Yao; P. Xu","The Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, China; The School of Bioinfomatics, Chongqing University of Posts and Tele communications, Chongqing, China; The Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, China; The Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, China; The Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, China; National Digital Switching, System Engineering and Technological, Research Center, Zhengzhou, China; The Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, China; The Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, China",2019 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA),"20 Apr 2020","2019","","","1","4","Emotion plays a crucial role in humans' daily life, which affects the decision and communication of human. Moreover, the effective recognition of emotion is essential to establish the affective Human-Computer Interaction (aHCI) systems. In this work, we mainly focus on feature extraction from the brain networks constructed with EEG to perform the emotion recognition. The analysis based on the public emotion dataset MAHNOB-HCI reveals that the proposed approach could achieved 100.00%, 99.95% and 99.99% for Negative-Neutral, Negative-Positive, and Positive-Neutral paired emotion states, respectively. Compared with the previous work for MAHNOB-HCI dataset, the proposed approach achieved the better classification results, and the experiment results have indicated that the feature extracted from brain networks is promising for the emotion classification.","2377-9322","978-1-5386-8344-6","10.1109/CIVEMSA45640.2019.9071616","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9071616","emotion classification;feature extraction;brain network;EEG","Feature extraction;Electroencephalography;Emotion recognition;Hospitals;Support vector machines;Brain;Human computer interaction","","2","","9","IEEE","20 Apr 2020","","","IEEE","IEEE Conferences"
"Accurate EEG-Based Emotion Recognition on Combined Features Using Deep Convolutional Neural Networks","J. X. Chen; P. W. Zhang; Z. J. Mao; Y. F. Huang; D. M. Jiang; Y. N. Zhang","School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; Department of Electronical and Information Engineering, Shaanxi University of Science and Technology, Xi’an, China; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Epidemiology and Biostatistics, University of Texas Health Science Center at San Antonio, San Antonio, TX, USA; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China",IEEE Access,"12 Apr 2019","2019","7","","44317","44328","In order to improve the accuracy of emotional recognition by end-to-end automatic learning of emotional features in spatial and temporal dimensions of electroencephalogram (EEG), an EEG emotional feature learning and classification method using deep convolution neural network (CNN) was proposed based on temporal features, frequential features, and their combinations of EEG signals in DEAP dataset. The shallow machine learning models including bagging tree (BT), support vector machine (SVM), linear discriminant analysis (LDA), and Bayesian linear discriminant analysis (BLDA) models and deep CNN models were used to make emotional binary classification experiments on DEAP datasets in valence and arousal dimensions. The experimental results showed that the deep CNN models which require no feature engineering achieved the best recognition performance on temporal and frequency combined features in both valence and arousal dimensions, which is 3.58% higher than the performance of the best traditional BT classifier in valence dimension and 3.29% higher than that of BT classifier in arousal dimension.","2169-3536","","10.1109/ACCESS.2019.2908285","National Natural Science Foundation of China(grant numbers:61806118 (Project name: Research on EEG based emotion recognition with semi-supervised deep generative adversarial network, Project principal: J. X. Chen),61806144 (Project name: Interpretation convolutional network based motor-cognitive joint brain network dynamically modeling and cognitive quantitative study, Project principal: Z. J. Mao)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676231","EEG;emotion recognition;convolution neural network;combined features;deep learning","Electroencephalography;Feature extraction;Brain modeling;Emotion recognition;Frequency-domain analysis;Videos;Convolutional neural networks","","181","","32","OAPA","29 Mar 2019","","","IEEE","IEEE Journals"
"Feature extraction of emotional states for EEG-based rage control","S. -H. Kim; N. A. Nguyen Thi","Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; Department of Computer Science, Chonnam National University, Gwangju, Jeollanam-do, KR",2016 39th International Conference on Telecommunications and Signal Processing (TSP),"1 Dec 2016","2016","","","361","364","We conduct an emotion recognition study to understand and classify the emotional states of human since the social problems caused by impulsive rage explosions such as the revenge driving, noise floor, and violent crimes. Recently, the emotion recognition technology using electroencephalogram (EEG) has become the foremost consideration of researchers compared with others using voice, facial image, bio-signal, and etc. However, the recognition and assumption of an emotion condition based on direct EEG face a challenge due to its characteristics, namely noise, non-stationary, and non-linear. Therefore, a further appropriate algorithm for feature extraction is more desirable to classify the emotion state from EEG data. In this paper, a multiclass-common spatial patterns (Multi-CSP) is proposed to extract the features with respect to each emotional state. Then, support vector machine (SVM) model is set up to classify these features of emotional state for the rage control. The experimental results show that the proposed feature extraction method, Multi-CSP, performs higher classification accuracy than the popular feature extraction methods.","","978-1-5090-1288-6","10.1109/TSP.2016.7760897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760897","Common spatial patterns;EEG;Emotional states;Feature extraction;Rage","Feature extraction;Electroencephalography;Covariance matrices;Emotion recognition;Principal component analysis;Support vector machines;Eigenvalues and eigenfunctions","","2","","15","IEEE","1 Dec 2016","","","IEEE","IEEE Conferences"
"Emotion Recognition with Machine Learning Using EEG Signals","O. Bazgir; Z. Mohammadi; S. A. H. Habibi","Department of Electrical and Computer Engineering, Texas Tech University Lubbock, Texas, USA; Department of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Electrical and Computer Engineering, Texas Tech University Lubbock, Texas, USA",2018 25th National and 3rd International Iranian Conference on Biomedical Engineering (ICBME),"2 May 2019","2018","","","1","5","In this research, an emotion recognition system is developed based on valence/arousal model using electroencephalography (EEG) signals. EEG signals are decomposed into the gamma, beta, alpha and theta frequency bands using discrete wavelet transform (DWT), and spectral features are extracted from each frequency band. Principle component analysis (PCA) is applied to the extracted features by preserving the same dimensionality, as a transform, to make the features mutually uncorrelated. Support vector machine (SVM), K-nearest neighbor (KNN) and artificial neural network (ANN) are used to classify emotional states. The cross- validated SVM with radial basis function (RBF) kernel using extracted features of 10 EEG channels, performs with 91.3% accuracy for arousal and 91.1% accuracy for valence, both in the beta frequency band. Our approach shows better performance compared to existing algorithms applied to the ""DEAP"" dataset.","","978-1-5386-7952-4","10.1109/ICBME.2018.8703559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703559","Emotion;Machine Learning;Valence-arousal;EEG;DWT;PCA;SVM;KNN","Feature extraction;Electroencephalography;Support vector machines;Brain modeling;Principal component analysis;Entropy;Emotion recognition","","85","","18","IEEE","2 May 2019","","","IEEE","IEEE Conferences"
"Estimation of valence of emotion using two frontal EEG channels","S. Wu; X. Xu; L. Shu; B. Hu","School of Electronic and Information Engineering, South China University of Technology, Guangzhou, Guangdong Province, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, Guangdong Province, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, Guangdong Province, China; School of Information Science & Engineering, Lanzhou University, Lanzhou, Gansu province, China",2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Dec 2017","2017","","","1127","1130","Emotion recognition using EEG signals has become a hot research topic in the last few years. This paper aims at providing a novel method for emotion recognition using less channels of frontal EEG signals. By employing the asymmetry theory of frontal brain, a new method fusing spatial and frequency features was presented, which only adopted two channels of frontal EEG signals at Fp1 and Fp2. In order to estimate the efficiency of the method, a GBDT classifier was evaluated and selected, and the method was implemented on the DEAP database. The maximum and mean classification accuracy were achieved as 76.34% and 75.18% respectively, which exhibited the best result comparing with other related studies. This method is extremely suitable for wearable EEG monitoring applications in human daily life.","","978-1-5090-3050-7","10.1109/BIBM.2017.8217815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8217815","Frontal EEG;Emotion recognition;GBDT classifier;DEAP","Electroencephalography;Feature extraction;Emotion recognition;Indexes;Entropy;Biomedical monitoring","","46","","12","IEEE","18 Dec 2017","","","IEEE","IEEE Conferences"
"Human emotion modeling based on salient global features of EEG signal","T. Ahmed; M. Islam; M. Ahmad","Dept. of Electrical and Electronic Engineering, Khulna University of Engineering & Technology, Bangladesh; Dept. of Electrical and Electronic Engineering, Khulna University of Engineering & Technology, Bangladesh; Dept. of Electrical and Electronic Engineering, Khulna University of Engineering & Technology, Bangladesh",2013 2nd International Conference on Advances in Electrical Engineering (ICAEE),"3 Mar 2014","2013","","","246","251","Feature extraction and accurate classification of the emotion-related EEG-characteristics have a key role in success of emotion recognition systems. This paper proposes an emotion modeling from EEG (Electroencephalogram) signals based on both time and frequency domain features by applying some statistical measures, Fourier and wavelet transform. After collecting the EEG signals, the various kinds of EEG features are investigated to build an emotion classification system. The main objective of this work is to compare the efficacy of the extracted features for classifying five types of emotional states relax, mental task, memory related task, pleasant, and fear. For this purpose support vector machine classifier was employed to classify the five emotional states by using salient global features. In case of statistical features the overall accuracy was obtained 54.2%, which is improved for FFT features 55.00% and the highest accuracy was obtained by DWT features 60.15%.","","978-1-4799-2465-3","10.1109/ICAEE.2013.6750341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6750341","EEG emotion modeling;time;frequency and time-frequency domain features;emotional states;support vector machine","Electroencephalography;Feature extraction;Support vector machines;Accuracy;Time-frequency analysis;Brain modeling","","5","","18","IEEE","3 Mar 2014","","","IEEE","IEEE Conferences"
"Emotion Recognition Using Temporally Localized Emotional Events in EEG With Naturalistic Context: DENS# Dataset","M. Asif; S. Mishra; M. T. Vinodbhai; U. S. Tiwary","Indian Institute of Information Technology Allahabad, Allahabad, Uttar Pradesh, India; Indian Institute of Information Technology Allahabad, Allahabad, Uttar Pradesh, India; Indian Institute of Information Technology Allahabad, Allahabad, Uttar Pradesh, India; Indian Institute of Information Technology Allahabad, Allahabad, Uttar Pradesh, India",IEEE Access,"1 May 2023","2023","11","","39913","39925","Emotion recognition using EEG signals is an emerging area of research due to its broad applicability in Brain-Computer Interfaces. Emotional feelings are hard to stimulate in the lab. Emotions don’t last long, yet they need enough context to be perceived and felt. However, most EEG-related emotion databases either suffer from emotionally irrelevant details (due to prolonged duration stimulus) or have minimal context, which may not elicit enough emotion. We tried to overcome this problem by designing an experiment in which participants were free to report their emotional feelings while watching the emotional stimulus. We called these reported emotional feelings “Emotional Events” in our Dataset on Emotion with Naturalistic Stimuli (DENS), which has the recorded EEG signals during the emotional events. To compare our dataset, we classify emotional events on different combinations of Valence(V) and Arousal(A) dimensions and compared the results with benchmark datasets of DEAP and SEED. Short-Time Fourier Transform (STFT) is used for feature extraction and in the classification model consisting of CNN-LSTM hybrid layers. We achieved significantly higher accuracy with our data compared to DEAP and SEED data. We conclude that having precise information about emotional feelings improves the classification accuracy compared to long-duration recorded EEG signals which might be contaminated by mind-wandering. This dataset can be used for detailed analysis of specific experienced emotions and related brain dynamics.","2169-3536","","10.1109/ACCESS.2023.3266804","Ministry of Education, Government of India, funded by the acquisition of the EEG system; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10101783","Affective computing;CNN;DEAP;DENS;EEG;emotion dataset;emotion recognition;LSTM;SEED","Electroencephalography;Emotion recognition;Feature extraction;Brain modeling;Task analysis;Affective computing;Benchmark testing","","14","","37","CCBYNCND","13 Apr 2023","","","IEEE","IEEE Journals"
"Feature Extraction Approach Based on Statistical Methods and Wavelet Packet Decomposition for Emotion Recognition using EEG Signals","A. Abdulrahman; M. Baykara","Dept. of Information Technology, Duhok Polytechnic University, Duhok, Iraq; Dept. of Software Engineering, Firat University, Elazig, Turkey",2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA),"30 Sep 2021","2021","","","1","7","Classification of human emotions via EEG signals is a hot topic today. In this study, a method for feature extraction from EEG signals is presented. It is applied for the first time on the GAMEEMO dataset, using a combination of the wavelet packet decomposition (WPD) method with the statistical feature method (SF). The GAMEEMO dataset was classified using the discrete model (2 classes) and the dimensional model (4 classes). This study passed through three sequential main steps: In the first step, four methods were used to extract the features from EEG signals, which are the SF, the combination of the SF with the Welsh power spectral density using (PSD), the combination of the SF with the fast Fourier transform (FFT), and the combination of the SF with the WPD method. In a second step, the Decision Tree Classifier (DT), and the recurrent neural network (RNN) with the long short-term memory (LSTM) algorithm, were applied to classify both emotion models. In the third step, the model performance was evaluated by calculating accuracy, sensitivity, and specificity. The proposed method achieved 98.31% accuracy in the binary-class classification and 99.48% in the multi-class classification. The proposed method may be used in other EEG datasets.","","978-1-6654-3603-8","10.1109/INISTA52262.2021.9548406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548406","Signal processing;Emotion recognition;Wavelet packet decomposition;Fast Fourier Transform;Decision tree;RNN;LSTM","Emotion recognition;Technological innovation;Recurrent neural networks;Statistical analysis;Sensitivity and specificity;Feature extraction;Brain modeling","","7","","29","IEEE","30 Sep 2021","","","IEEE","IEEE Conferences"
"Attention-Driven Emotion Recognition in EEG: A Transformer-Based Approach With Cross-Dataset Fine-Tuning","G. Ghous; S. Najam; M. Alshehri; A. Alshahrani; Y. AlQahtani; A. Jalal; H. Liu","Department of Electrical and Computer Engineering, Riphah International University, Islamabad, Pakistan; Department of Electrical Engineering, Bahria University, Islamabad, Pakistan; Department of Computer Science, King Khalid University, Abha, Saudi Arabia; Department of Informatics and Computer Systems, King Khalid University, Abha, Saudi Arabia; Department of Informatics and Computer Systems, King Khalid University, Abha, Saudi Arabia; Faculty of Computer Science and AI, Air University, Islamabad, Pakistan; Cognitive Systems Laboratory, University of Bremen, Bremen, Germany",IEEE Access,"25 Apr 2025","2025","13","","69369","69394","Emotion recognition from EEG (electroencephalogram) signals is crucial in mental health diagnostics and human-computer interaction but is often hindered by high dimensionality, noise, and complex temporal dependencies in the data. This paper presents a novel approach that integrates transformer models, attention mechanisms, and transfer learning to enhance emotion recognition accuracy from EEG signals. The proposed methodology consists of two phases: Attention Enhanced Base Model Development (AE-BMD) and Cross-Dataset Fine Tuning Adaptation (CD-FTA). In the AE-BMD phase, the base model is developed and trained on the SEED-IV dataset (15 participants, 62 EEG channels), achieving an accuracy of 84%, with an average precision of 84.75%, recall of 84% and F1-score of 84%. This phase employs optimized feature extraction from key EEG frequency bands (Delta, Theta, Alpha, Beta, Gamma) using techniques such as MFCC, GFCC, power spectral density, and Hjorth parameters. A transformer encoder with integrated spectral and temporal attention mechanisms captures intricate patterns and long-range dependencies within the EEG signals. In the CD-FTA phase, the model undergoes fine-tuning on the SEED-V dataset (20 participants, 62 channels) leading to an improved accuracy of 90%, with an average precision of 90.6%, recall of 90.6%, and F1-score of 90.6%. The model’s generalization is further validated on the MPED dataset (23 participants, 62 channels, seven emotion classes), achieving 79%, with an average precision of 79.3%, recall of 79.3% and F1-score of 79.1% across diverse emotional states. This cross-dataset adaptation leverages transfer learning to enhance the model’s generalization across different emotional states and EEG datasets. Experimental results show that the proposed approach outperforms traditional methods, achieving superior accuracy and robustness in emotion recognition tasks. This work advances emotion recognition systems by addressing challenges in EEG signal processing, feature extraction, and model generalization, positioning it as a promising tool for applications in healthcare, affective computing, and emotional state analysis.","2169-3536","","10.1109/ACCESS.2025.3561137","Open Access Initiative of the University of Bremen and the DFG via SuUB Bremen; Deanship of Research and Graduate Studies at King Khalid University; Large Group Project(grant numbers:RGP.2/568/45); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10965678","Attention mechanisms;cross-dataset adaptation;deep learning;electroencephalography (EEG);emotion recognition;feature extraction;neurocomputing;SEED-IV dataset;SEED-V dataset;spectral attention;temporal attention;transformer models;transfer learning","Electroencephalography;Brain modeling;Emotion recognition;Feature extraction;Transformers;Accuracy;Adaptation models;Transfer learning;Computational modeling;Mental health","","","","80","CCBY","15 Apr 2025","","","IEEE","IEEE Journals"
"Unsupervised Feature Learning for EEG-based Emotion Recognition","Z. Lan; O. Sourina; L. Wang; R. Scherer; G. Müller-Putz","Fraunhofer Singapore Nanyang Technological University, Singapore, Singapore; Fraunhofer Singapore Nanyang Technological University, Singapore, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; Institute of Neural Engineering, Graz University of Technology, Graz, Austria; Institute of Neural Engineering, Graz University of Technology, Graz, Austria",2017 International Conference on Cyberworlds (CW),"30 Nov 2017","2017","","","182","185","Spectral band power features are one of the most widely used features in the studies of electroencephalogram (EEG)-based emotion recognition. The power spectral density of EEG signals is partitioned into different bands such as delta, theta, alpha and beta band etc. Though based on neuroscientific findings, the partition of frequency bands is somewhat on an ad-hoc basis, and the definition of frequency ranges of the bands of interest can vary between studies. On the other hand, it is also arguable that one definition of power bands could perform equally well on all subjects. In this paper, we propose to use autoencoder to automatically learn from each subject the salient frequency components from power spectral density estimated as periodogram by Fast Fourier Transform (FFT). We propose a network architecture especially for EEG feature extraction, one that adopts hidden unit clustering with added pooling neuron per cluster. The classification accuracy with features extracted by our proposed method is benchmarked against that with standard power features. Experimental results show that our proposed feature extraction method achieves accuracy ranging from 44% to 59% for three-emotion classification. We also see a 4-20% accuracy improvement over standard band power features.","","978-1-5386-2089-2","10.1109/CW.2017.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8120320","emotion classification;electroencephalogram (EEG);brain-computer-interface (BCI);power spectral density;unsupervised feature extraction;autoencoder","Neurons;Feature extraction;Electroencephalography;Training;Standards;Motion pictures;Training data","","5","","5","IEEE","30 Nov 2017","","","IEEE","IEEE Conferences"
"Improved Graph Convolutional Neural Networks based on Granger Causality Analysis for EEG Emotion Recognition","J. Zhang; X. Zhang; Q. Zhao","College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan, China",2022 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI),"17 Aug 2022","2022","","","684","688","How to learn the adjacency matrix of graph convolution neural network (GCNN) is one of the key issues in EEG emotion recognition. Most current GCNN models adopt the spatial position between EEG channels to define the adjacency matrix, which have ignored the effective connection characteristics among the EEG signals. Granger causality (GC) analysis can reveal the causal connectivity between EEG channels. In this paper, an improved GCNN method based on the GC analysis is proposed for EEG emotion recognition. Firstly, the causal association matrix is calculated by the GC analysis. Then, a reasonable threshold is used to adaptively converts the causal association matrix into the adjacency matrix, which makes the adjacency matrix more consistent with the cognitive law of the human brain. Finally, the DEAP emotion dataset is used to test the performance of our method. The experimental results demonstrate that the proposed GC-GCNN method achieves better recognition performance than the state-of-the-art methods, in which the average accuracies of 90.11%, 89.48% and 86.35% are respectively obtained for arousal, valence, and arousal-valence classifications.","","978-1-6654-6803-9","10.1109/ICCEAI55464.2022.00146","Natural Science Foundation of China(grant numbers:61371193); Shanxi Scholarship Council of China(grant numbers:HGKY2019025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9853467","EEG emotion recognition;granger causality;graph convolutional neural networks;adjacency matrix","Emotion recognition;Convolution;Computational modeling;Brain modeling;Feature extraction;Electroencephalography;Convolutional neural networks","","2","","13","IEEE","17 Aug 2022","","","IEEE","IEEE Conferences"
"Closed-loop Individual-specific EEG Neurofeedback for Emotion Regulation","X. Liu; J. Zhao; S. Wang; G. Pei; S. Funahashi; T. Yan","School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China; Advanced Research Institute of Multidisciplinary Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China","2022 International Conference on Automation, Robotics and Computer Engineering (ICARCE)","22 Feb 2023","2022","","","1","4","Individual difference is the main factor affecting the effect of emotion regulation neurofeedback training. An individual-specific emotion recognition model can be constructed based on machine learning. However, the current researches simply the preprocessing process to meet real-time feedback, resulting in a reduction in classification accuracy. This paper proposes a closed-loop electroencephalogram (EEG) neurofeedback processing program with high accuracy in feedback information. Artifact subspace reconstruction is used to optimize EEG processing. The positive, neutral, and negative emotion topographic maps of the 5 frequency bands verify inter-individual differences. A support vector machine with particle swarm optimization is used to construct an individual emotion recognition model based on the power spectral density features. The average classification accuracy of 5 subjects is 97.49%. The emotion facial Go/No-go task objectively demonstrates the effectiveness of neurofeedback training on emotion regulation. The closed-loop individual-specific EEG neurofeedback program provides a promising method for emotion regulation training.","","978-1-6654-7548-8","10.1109/ICARCE55724.2022.10046573","National Natural Science Foundation of China; Beijing Municipal Science and Technology Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046573","emotion regulation;closed-loop neurofeedback;individual model;artifact subspace reconstruction","Training;Support vector machines;Emotion recognition;Machine learning;Brain modeling;Regulation;Electroencephalography","","","","16","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
"Leveraging Sparse Coding for EEG Based Emotion Recognition in Shooting","Y. Wang; Y. Sun; L. Fang; C. Zhang","Department of Automation, Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems. Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, P.R.China; Department of Automation, Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems. Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, P.R.China; DataCanvas Technology Co., Ltd.; Department of Automation, Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems. Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, P.R.China","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","1421","1425","Emotion recognition in shooting is of great importance for improving athletes’ training methods. However, there is no open and high confident electroencephalography (EEG) dataset about shooting due to the difficulty of data acquisition, which made it a challenge for related studies. In this paper, we collected EEG of novice shooters and high-level shooters in different emotion states, and established two shooting datasets. Furthermore, instead of adopting the common convolutional neural network, we are the first to leverage sparse coding for EEG based emotion recognition in shooting process. Our proposed method can effectively solve the problem of low accuracy caused by data with low signal-noise ratio and small training set. The experimental results demonstrate that our method outperforms other representative deep learning based methods.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747506","Sparse coding;EEG;Shooting;Emotion Recognition","Training;Emotion recognition;Noise reduction;Speech recognition;Signal processing;Electroencephalography;Encoding","","2","","22","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Classification of Emotional States and Familiarity based on EEG Signals","F. Feradov","Faculty of Computer Sciences and Automation, Technical Unviersity of Varna, Varna, Bulgaria",2021 International Conference on Biomedical Innovations and Applications (BIA),"20 Jul 2022","2022","1","","54","57","Emotional reactions and familiarity towards the multimedia stimuli are closely connected and have a strong effect during tasks related to the classification of emotion and cognitive activity. The current study presents spectral activity-based EEG features for simultaneous classification of emotional states and level of familiarity towards multimedia stimuli. The experimental evaluation of the examined features was conducted using the SEED database and three classification algorithms – polynomial SVM, kNN and C4.5 – that were employed in a 9 class classification task. A maximum mean classification accuracy of 95.2% is reported.","","978-1-6654-4581-8","10.1109/BIA52594.2022.9831267","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831267","EEG features;emotion classification;familiarity classification;affective computing","Support vector machines;Technological innovation;Affective computing;Multimedia databases;Electroencephalography;Classification algorithms;Task analysis","","","","18","IEEE","20 Jul 2022","","","IEEE","IEEE Conferences"
"Human Brain Waves Study Using EEG and Deep Learning for Emotion Recognition","M. Priyadarshani; P. Kumar; K. Sindhuben Babulal; D. Singh Rajput; H. Patel","Department of Computer Science and Engineering, Indian Institute of Technology Gandhinagar, Palaj, India; Department of Computer Science and Engineering, Central University of Jharkhand, Ranchi, India; Department of Computer Science and Engineering, Central University of Jharkhand, Ranchi, India; School of Computer Science Engineering and Information Systems, VIT, Vellore, India; School of Computer Science Engineering and Information Systems, VIT, Vellore, India",IEEE Access,"29 Jul 2024","2024","12","","101842","101850","Emotion Recognition is a critical area of research including healthcare, human-computer interaction, and psychology. While traditional methods mainly rely on facial expressions and textual analysis, they also have inherent flaws and cannot be reliable. Facial expression-based emotion recognition assumes that it represents genuine internal emotions that may be inaccurate. Similarly, textual analysis depends on the available data and needs help accurately capturing subtle emotions in text. However, electroencephalography (EEG) has emerged as a rising alternative for objective and real-time emotion recognition. Unlike facial and textual methods, EEG directly measures brain activity and provides a reliable result. To address this researchers have used basic machine learning methods that need manual feature extraction, which might miss essential data and make the process slow and less accurate. In this study, we propose a comprehensive methodology for EEG-based emotion recognition that addresses the limitations of traditional methods and basic machine learning techniques. Our approach involves preprocessing EEG signals using a butter-worth bandpass filter to eliminate noise, followed by feature extraction techniques. We then employ Principal Component Analysis (PCA) for dimensionality reduction, ensuring efficient data representation. To further enhance the model performance we explore machine learning classifiers(GaussianNB, SVM, Random Forest) and proposed an EEG-LSTM and GRU model with an accuracy of 97% and 96% respectively, that gives better results than the basic machine learning models.","2169-3536","","10.1109/ACCESS.2024.3427822","Vellore Institute of Technology, Vellore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597545","EEG;machine learning;deep learning;FFT;neural network;PSD;spectral entropy","Electroencephalography;Emotion recognition;Face recognition;Brain modeling;Accuracy;Deep learning;Machine learning;Neural networks;Spectral analysis","","4","","23","CCBYNCND","15 Jul 2024","","","IEEE","IEEE Journals"
"Hjorth Descriptor as Feature Extraction for Classification of Familiarity in EEG Signal","H. Sanggarini; I. Wijayanto; S. Hadiyoso","School of Electrical Engineering, Telkom University, Bandung, Indonesia; School of Electrical Engineering, Telkom University, Bandung, Indonesia; School of Applied Science, Telkom University, Bandung, Indonesia",2019 International Conference on Information and Communications Technology (ICOIACT),"23 Dec 2019","2019","","","306","309","Deficiency in identifying human emotional stages occurs in most of the existing contemporary Human-Computer Interactions (HCI) systems. There are vast areas of the human stages that can be identified. One of them is the stage when a human feels familiar. Electroencephalogram (EEG) signal can be used to detect human affective stage in familiarity category. This research classifies familiarity in EEG signal using data from DEAP: A Database for Emotion Analysis Using Physiological Signals. The signal feature was extracted using Hjorth Descriptor producing three parameters. The parameters then fed to the Multilayer Perceptron as the classifier. The best accuracy achieved was 92.85% using three features combination, with 2.132 seconds of computation time.","","978-1-7281-1655-6","10.1109/ICOIACT46704.2019.8938532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938532","Familiarity EEG;Hjorth Descriptor;Multilayer Perceptron","Electroencephalography;Feature extraction;Complexity theory;Physiology;Emotion recognition;Testing;Multilayer perceptrons","","3","","19","IEEE","23 Dec 2019","","","IEEE","IEEE Conferences"
"The Effectiveness of Advance Deep Learning Architectures for Classification of Stress using Raw EEG Data","A. Tibrewal; Shikha; D. Sethia","Department of Applied Physics, Delhi Technological University, Delhi, India; Department of Computer, Science and Engineering, Delhi Technological University, Delhi, India; Department of Software Engineering, Delhi Technological University, Delhi, India",2024 Asia Pacific Conference on Innovation in Technology (APCIT),"18 Sep 2024","2024","","","1","6","Understanding and detecting stress in today’s fast-paced world remains challenging. Previous research has explored various physiological signals, including EEG (Electroencephalography), to quantify stress. However, accurately assessing stress poses a significant challenge, particularly in preprocessing and feature extraction of EEG signals. Hence, this paper employs advanced deep learning algorithms, including ResNet18-1D, DenseNet-1D, and VGG16-1D, on raw EEG signals for automated stress classification. The study utilizes the publicly available EEG-based SAM40 dataset for stress classification and further validates the results on the EEG-based MAT dataset. The results demonstrate the efficacy of the ResNet18-1D model by achieving the highest of 98.73% and 97.24% on the SAM40 and MAT datasets, respectively. The findings pave the way for further exploration and utilization of raw EEG signals across diverse research domains beyond stress classification.","","979-8-3503-6153-7","10.1109/APCIT62007.2024.10673521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10673521","Raw EEG;DeepLearning;Stress Classification;VGG16;ResNet18;DenseNet","Deep learning;Measurement;Technological innovation;Time series analysis;Brain modeling;Electroencephalography;Data models","","1","","20","IEEE","18 Sep 2024","","","IEEE","IEEE Conferences"
"Enhancing Emotion Recognition in EEG Signals using Fractional Fourier Transform","J. Chang; Z. Wang; J. Jia","School of Computer and Information Technology, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China; School of Computer and Information Technology, Shanxi University, Taiyuan, China",2024 IEEE 17th International Conference on Signal Processing (ICSP),"23 Jan 2025","2024","","","110","115","Electroencephalogram (EEG) signals are a valuable tool for emotion recognition due to their effectiveness. However, their non-linear and non-stationary nature often leads to suboptimal outcomes when using traditional single-feature extraction methods. To address these challenges, this paper employs the fractional Fourier transform (FRFT) method, which is well-suited for processing non-stationary signals, for emotion recognition in EEG signals. First, the FRFT was applied to convert EEG signals into time-frequency representations at various fractional orders. Seven features were then extracted from the EEG data at each order: maximum value, mean value, width, energy, power, variance, and differential entropy. Finally, the SVM method was used for the identification of emotions based on the EEG signals. The approach was validated using open EEG datasets (DEAP and SEED). Experimental results demonstrate that the optimal emotion recognition performance is achieved when the fractional order is set to 0.4.","2164-5221","979-8-3503-8738-4","10.1109/ICSP62129.2024.10845993","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845993","fractional Fourier transform;EEG;emotion recognition","Support vector machines;Emotion recognition;Time-frequency analysis;Fourier transforms;Refining;Focusing;Signal processing;Feature extraction;Electroencephalography;Testing","","","","14","IEEE","23 Jan 2025","","","IEEE","IEEE Conferences"
"Interpretable Emotion Recognition Using EEG Signals","C. Qing; R. Qiao; X. Xu; Y. Cheng","School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science and Technology, University of Hull, Hull, U.K.",IEEE Access,"24 Jul 2019","2019","7","","94160","94170","Electroencephalogram (EEG) signal-based emotion recognition has attracted wide interests in recent years and has been broadly adopted in medical, affective computing, and other relevant fields. However, the majority of the research reported in this field tends to focus on the accuracy of classification whilst neglecting the interpretability of emotion progression. In this paper, we propose a new interpretable emotion recognition approach with the activation mechanism by using machine learning and EEG signals. This paper innovatively proposes the emotional activation curve to demonstrate the activation process of emotions. The algorithm first extracts features from EEG signals and classifies emotions using machine learning techniques, in which different parts of a trial are used to train the proposed model and assess its impact on emotion recognition results. Second, novel activation curves of emotions are constructed based on the classification results, and two emotion coefficients, i.e., the correlation coefficients and entropy coefficients. The activation curve can not only classify emotions but also reveals to a certain extent the emotional activation mechanism. Finally, a weight coefficient is obtained from the two coefficients to improve the accuracy of emotion recognition. To validate the proposed method, experiments have been carried out on the DEAP and SEED dataset. The results support the point that emotions are progressively activated throughout the experiment, and the weighting coefficients based on the correlation coefficient and the entropy coefficient can effectively improve the EEG-based emotion recognition accuracy.","2169-3536","","10.1109/ACCESS.2019.2928691","National Natural Science Foundation of China(grant numbers:U180120050,61702192,U1636218); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762129","EEG;emotion activation;emotion recognition;machine learning","Electroencephalography;Feature extraction;Emotion recognition;Brain modeling;Physiology;Computational modeling;Human computer interaction","","104","","36","CCBY","15 Jul 2019","","","IEEE","IEEE Journals"
"Cross-Subject EEG-Based Emotion Recognition Using Deep Metric Learning and Adversarial Training","H. R. A. Alameer; P. Salehpour; S. Hadi Aghdasi; M. -R. Feizi-Derakhshi","Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran",IEEE Access,"24 Sep 2024","2024","12","","130241","130252","Nowadays, due to individual differences and the non-stationarity properties of EEG signals, developing an accurate cross-subject EEG emotion recognition method is in demand. Despite many successful attempts, the accuracy of generalized models across subjects is inferior compared to those limited to a specific individual. Moreover, most cross-subject training methods assume that the unlabeled data from target subjects is available. However, this assumption does not hold in practice. To address these issues, this paper presents a novel deep similarity learning loss specific to the emotion recognition task. This loss function minimizes intra-emotion class variations of EEG segments with different subject labels while maximizing inter-emotion class variations. Another key aspect of the proposed semantic embedding loss is that it preserves the order of emotion classes in the learned embedding. Specifically, it ensures that the embedding space maintains the semantic order of emotions. Also, we integrate the deep similarity learning module with adversarial learning, which helps to learn a subject-invariant representation of EEG signals in an end-to-end training paradigm. We conduct several experiments on three widely used datasets: SEED, SEED-GER, and DEAP. The results confirm that the proposed method effectively learns a subject invariant representation from EEG signals and consistently outperforms the state-of-the-art (SOTA) peer methods.","2169-3536","","10.1109/ACCESS.2024.3458833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10677436","EEG signals;cross-subject emotion recognition;deep metric learning;adversarial learning","Electroencephalography;Emotion recognition;Brain modeling;Training;Accuracy;Feature extraction;Data models;Deep learning;Adversarial machine learning","","1","","32","CCBYNCND","11 Sep 2024","","","IEEE","IEEE Journals"
"Adaptive EEG Emotion Recognition Model Based on SCSSA Combined with CNN-Attention-LSTM","G. Hou; Z. Lai; Z. Li; L. Wang","China Three Gorges University, Yichang, China; China Three Gorges University, Yichang, China; China Three Gorges University, Yichang, China; Zhuhai City Polytechnic, Zhuhai, China","2024 6th International Conference on Internet of Things, Automation and Artificial Intelligence (IoTAAI)","2 Oct 2024","2024","","","694","697","Due to the time-domain, frequency-domain, and brain connectivity features, EEG signals are well-suited for analysis using Convolutional Neural Networks (CNN) and Long Short-Term Memory networks (LSTM). This paper first extracts the EEG signals in segmented time domains, performs Differential Entropy (DE) feature extraction, and converts them into a four-dimensional feature matrix. This matrix is then input into an improved CNN-attention-LSTM model for feature extraction and classification. Additionally, to enhance the model's adaptability to different scenarios, the SCSSA algorithm is introduced to optimize the model's hyperparameters. Experimental results show that in the DEAP dataset, the classification accuracies for the arousal and valence dimensions are 93.47% and 92.14%, respectively, and the accuracy for the four-class arousal-valence dimension task is 90.62%.","","979-8-3503-8697-4","10.1109/IoTAAI62601.2024.10692370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10692370","EEG;Emotion Recognition;Deep Learning;Optimized Sparrow Algorithm","Adaptation models;Emotion recognition;Accuracy;Brain modeling;Feature extraction;Electroencephalography;Data models;Time-domain analysis;Matrix converters;Long short term memory","","","","10","IEEE","2 Oct 2024","","","IEEE","IEEE Conferences"
"Advanced Multimodal Emotion Recognition Using Integrative Analysis of Multilingual Text, EEG, Voice Signals, and Facial Analysis","J. S. Reddy; D. A. Surat; P. Shyamala; S. S","Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Bengaluru, India; Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Bengaluru, India; Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Bengaluru, India; Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Bengaluru, India",2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),"13 Mar 2025","2025","","","1705","1710","Emotion recognition with the help of AI is critically important for improving the processes of human computer interaction, and can be effectively implemented in healthcare, robotics, and mental health analysis. Systems that adopt inputs in a single mode only, be it text, voice, or facial expressions all have their issues such as low accuracy, easiness to fake and the inability to comprehend the context. These are overcome by a multimodal method that uses textual, voice, facial and EEG data to be more reliable or accurate Multilingual text preprocessing hence translates the inputs from other regions to English for the ML/DL algorithms to classify it as either having positive, neutral or negative feelings. Voice converter translates into feelings with pitch and rhythm with the help of DL models and face using DeepFace even in brightness variations. The Electroencephalogram or EEG signals are obtained at different times and those with higher layers of neural works are analyzed in real time. Fusion strategies also help in resolving compatibility issues and conflict between the modalities and increases the security against any attempt of adulteration. Through this approach, it is possible to detect accurate real-time emotion with aims of screening of mental health, enhance HCI and designs of empathetic robots. Future work therefore seeks to enhance integration methods, extend its use and investigate other approaches for a higher level of enhancement.","","979-8-3315-2754-9","10.1109/IDCIOT64235.2025.10915126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10915126","Emotion Recognition;Artificial Intelligence;Multimodal Data Integration;Human-Computer Interaction;EEG Signal Analysis;Deep Learning;Facial Expression Analysis;Voice Signal Processing;Multilingual Text Processing;Real-Time Emotion Detection","Human computer interaction;Emotion recognition;Accuracy;Translation;Mental health;Medical services;Electroencephalography;Real-time systems;Multilingual;Robots","","","","18","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"EEG Classification by Factoring in Sensor Spatial Configuration","L. S. Mokatren; R. Ansari; A. E. Cetin; A. D. Leow; O. A. Ajilore; H. Klumpp; F. T. Yarman Vural","Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA; Department of Psychiatry, University of Illinois at Chicago, Chicago, IL, USA; Department of Psychiatry, University of Illinois at Chicago, Chicago, IL, USA; Department of Psychiatry, University of Illinois at Chicago, Chicago, IL, USA; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey",IEEE Access,"17 Feb 2021","2021","9","","19053","19065","Electroencephalography (EEG) serves as an effective diagnostic tool for mental disorders and neurological abnormalities. Enhanced analysis and classification of EEG signals can help improve performance in classifying the disorders and abnormalities. A new approach is examined here for enhancing EEG classification performance using a novel model of data representation that leverages knowledge of spatial layout of EEG sensors. An investigation of the performance of the proposed data representation model provides evidence of consistently higher classification accuracy of the proposed model compared with a model that ignores the sensor layout. The performance is assessed for models that represent the information content of the EEG signals in two different ways: a one-dimensional concatenation of the channels of the frequency bands and a proposed image-like two-dimensional representation of the EEG channel locations. The models are used in conjunction with different machine learning techniques. Performance of these models is examined on two tasks: social anxiety disorder classification, and emotion recognition using a dataset, DEAP, for emotion analysis using physiological signals. We hypothesize that the proposed two-dimensional model will significantly outperform the one-dimensional model and this is validated in our results as this model consistently yields 5-8% higher accuracy in all machine learning algorithms investigated. Among the algorithms investigated, Convolutional Neural Networks provide the best performance, far exceeding that of Support Vector Machine and k-Nearest Neighbors algorithms.","2169-3536","","10.1109/ACCESS.2021.3054670","NIMH K23 MH093679 (HK) and the Center for Clinical and Translational Research (CCTS)(grant numbers:UL1RR029879); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336002","Machine learning;EEG;CNN;spatio-temporal features;emotion recognition;SAD","Electroencephalography;Brain modeling;Data models;Task analysis;Emotion recognition;Machine learning;Machine learning algorithms","","17","","46","CCBY","26 Jan 2021","","","IEEE","IEEE Journals"
"EEG-based Emotion Recognition Under Convolutional Neural Network with Differential Entropy Feature Maps","Y. Li; C. M. Wong; Y. Zheng; F. Wan; P. U. Mak; S. H. Pun; M. I. Vai","Department of Electrical and Computer Engineering, University of Macau, Macau S.A.R, China; Department of Electrical and Computer Engineering, University of Macau, Macau S.A.R, China; Department of Computer and Information Science, University of Macau, Macau S.A.R, China; Department of Electrical and Computer Engineering, University of Macau, Macau S.A.R, China; Department of Electrical and Computer Engineering, University of Macau, Macau S.A.R, China; State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau S.A.R, China; State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau S.A.R, China",2019 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA),"20 Apr 2020","2019","","","1","5","In recent electroencephalograph (EEG)-based emotion recognition, the differential entropy (DE) features extracted from multiple electrodes are organized as a 2D feature map for convolutional neural network (CNN) in order to utilize the information hidden in the electrodes. In this study, we attempt to investigate the influence of different feature maps on the recognition performance. Six different 2D feature maps (M1-M4: baseline feature maps without sparsity and location relationship, M5-M6: pre-defined feature maps with sparsity and location relationship) are used to organize the DE features for the traditional CNN model. Evaluation study on the DEAP dataset finds that the 2D feature map configuration exhibits statistically significant effect on the classification performance of the traditional CNN model in classifying the high/low arousal and high/low valence, respectively. However, the differences are rather limited, e.g., only 1% improvement can be resulted from selecting the optimal 2D feature map among 6 feature maps. This implies that the feature map may not be a critical issue when applying the DE features to classifying the emotion states in a CNN.","2377-9322","978-1-5386-8344-6","10.1109/CIVEMSA45640.2019.9071612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9071612","Emotion recognition;electroencephalograph;convolutional neural network;differential entropy;feature map","Electroencephalography;Emotion recognition;Two dimensional displays;Feature extraction;Electrodes;Physiology;Convolutional neural networks","","8","","20","IEEE","20 Apr 2020","","","IEEE","IEEE Conferences"
"Accurate EEG-Based Emotion Recognition using LSTM and BiLSTM Networks","M. Yaacob; T. S. Gunawan; M. I. F. Abu Bakar; S. H. Yusoff; M. Kartiwi; N. M. Yusoff","Electrical and Computer Eng. Department, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Electrical and Computer Eng. Department, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Electrical and Computer Eng. Department, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Electrical and Computer Eng. Department, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Information Systems Department, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Faculty of Artificial Intelligence, Universiti Tekologi Malaysia, Kuala Lumpur, Malaysia","2024 IEEE 10th International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA)","18 Sep 2024","2024","","","13","18","Emotion recognition is crucial for advancing human-computer interaction and mental health diagnostics. Unlike speech or image-based methods, EEG provides a direct, non-manipulable measure of neural activity, offering a more reliable insight into genuine emotional states. This study addresses the challenge of accurately recognizing emotions from EEG signals by leveraging deep learning techniques. The primary objective is to develop an efficient emotion recognition system using Long Short-Term Memory (LSTM) and Bidirectional LSTM (BiLSTM) networks. EEG data were preprocessed, and features were extracted using wavelet packet decomposition. The LSTM and BiLSTM models were then trained to classify emotions into binary and multiclass categories. Results demonstrated that the BiLSTM model achieved superior accuracy, with 91.78% for binary classification and 85.21% for multiclass classification. These findings highlight the potential of advanced deep learning models in enhancing emotion recognition accuracy, contributing significantly to fields such as affective computing and mental health monitoring.","2640-6535","979-8-3503-6528-3","10.1109/ICSIMA62563.2024.10675567","Universiti Teknologi Malaysia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10675567","EEG-based Emotion Recognition;Long Short-Term Memory (LSTM);Bidirectional LSTM (BiLSTM);Deep Learning;Affective Computing","Deep learning;Emotion recognition;Recurrent neural networks;Accuracy;Computational modeling;Mental health;Brain modeling","","","","11","IEEE","18 Sep 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition With Emotion Localization via Hierarchical Self-Attention","Y. Zhang; H. Liu; D. Zhang; X. Chen; T. Qin; Q. Zheng","School of Computer Science and Technology, Moeklinns Lab, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Moeklinns Lab, Xi’an Jiaotong University, Xi’an, China; Department of Computer Science, Fredrik Bajers Vej 7K, Aalborg University, Aalborg, Denmark; School of Computer Science and Technology, Moeklinns Lab, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Moeklinns Lab, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Moeklinns Lab, Xi’an Jiaotong University, Xi’an, China",IEEE Transactions on Affective Computing,"18 Sep 2023","2023","14","3","2458","2469","Emotion recognition based on electroencephalography (EEG) has attracted significant attention due to its wide range of applications, especially in Human-Computer Interaction(HCI). Previous research treats different segments of EEG signals uniformly, ignoring the fact that emotions are unstable and discrete during an extended period. In this paper, we propose a novel two-step spatial-temporal emotion recognition framework. First, considering that the human emotion has not only ”short-term continuity” but also ”long-term similarity”, we propose a hierarchical self-attention network to jointly model local and global temporal information, so as to localize most related segments and reduce the influence of noise at the temporal level. Second, in order to extract discriminative features at the spatial level to enhance the emotion recognition performance, we further employ the squeeze-and-excitation module (SE module) along with the channel correlation loss (CC-Loss) to select the most task-related channels. We also define a new task called emotion localization, which aims to localize fragments with stronger emotions. We evaluate the proposed method on the proposed emotion localization task and typical emotion recognition task with three publicly available datasets, i.e., SEED, DEAP, and MAHNOB-HCI. The experimental results demonstrate that the proposed approach outperforms state-of-the-art methods.","1949-3045","","10.1109/TAFFC.2022.3145623","National Key Research and Development Program of China(grant numbers:2020AAA0108800); National Natural Science Foundation of China(grant numbers:62137002,61721002,62172324,62192781); Innovation Research Team of Ministry of Education(grant numbers:IRT_17R86); Project of China Knowledge Centre for Engineering Science and Technology; Chinese academy of engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698041","EEG-based emotion recognition;emotion localization;hierarchical self-attention;channel correlation loss","Emotion recognition;Electroencephalography;Feature extraction;Brain modeling;Task analysis;Location awareness;Computational modeling","","30","","48","IEEE","31 Jan 2022","","","IEEE","IEEE Journals"
"Multimodal Emotion Recognition for Hearing-impaired Subjects by Fusing EEG Signals and Facial Expressions","Y. Chen; Z. Bai; M. Cheng; Y. Liu; X. Zhao; Y. Song","School of Electrical Engineering and Automation Tianjin University of Technology, Tianjin; School of Electrical Engineering and Automation Tianjin University of Technology, Tianjin; School of Electrical Engineering and Automation Tianjin University of Technology, Tianjin; School of Electrical Engineering and Automation Tianjin University of Technology, Tianjin; School of Electrical Engineering and Automation Tianjin University of Technology, Tianjin; School of Electrical Engineering and Automation Tianjin University of Technology, Tianjin",2023 42nd Chinese Control Conference (CCC),"18 Sep 2023","2023","","","1","6","With the development and improvement of artificial intelligence technology and multi-source information integration theory, emotional recognition has gradually transitioned from single-modality emotional recognition to multimodal. Due to the shortage of an emotional cognitive channel, the hearing-impaired subjects may have an emotional cognitive deviation in daily communication. Therefore, we classified the four kinds of emotions (happiness, fear, calmness, sadness) of the hearing-impaired subjects based on the feature-level fusion network which combined electroencephalography (EEG) signals and facial expressions. In this network, we adopted different feature extraction methods to obtain shallow features related to emotional changes in the two modalities, and then used multi -head cross-attention mechanism to the feature-level fusion layer between the two modalities. The results show that the average classification accuracy of four emotions recognition after multimodal fusion achieves 92.09%, which is higher than EEG signals (15.40%) and facial expressions (11.49%).","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10239741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10239741","Emotion recognition;EEG;Facial expressions;Hearing-impaired subjects","Emotion recognition;Feature extraction;Motion pictures;Electroencephalography;Data mining;Artificial intelligence","","1","","19","","18 Sep 2023","","","IEEE","IEEE Conferences"
"EEG-based Emotion Identification using General Factor Analysis","Garima; N. Rathee; N. Goel","Department of Electronics and Communication Engineering, Indira Gandhi Delhi Technical University for Women, Delhi, India; Department of Electronics and Communication Engineering, Maharaja Surajmal Institute of Technology, Delhi, India; Department of Electronics and Communication Engineering, Indira Gandhi Delhi Technical University for Women, Delhi, India","2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","8 Dec 2022","2022","","","1","5","Emotion identification using electroencephalography (EEG) has been quite significant for Brain-Machine Interfacing and has achieved great attention recently. Most of the conventional methods have focused on feature extraction techniques, but the combination of feature extraction and dimensionality reduction techniques which apart from reducing the dimensions and also focusing on the preservation of the most relevant information, is often ignored. This paper aims at devising a technique that comprises a feature extraction technique i.e., Discrete Wavelet Transform followed by general factor analysis (converts multi-dimensional features to features of less-dimensions while conserving the most relevant information), and later on the obtained feature vectors are classified using SVM and KNN. For the evaluation of our proposed technique, extensive experimentations have been performed on the publically available DREAMER dataset. The proposed technique has obtained an accuracy as high as 96.8% for valence label classification, which significantly surpasses the state-of-the-art methodologies.","","978-1-6654-7433-7","10.1109/ICRITO56286.2022.9964843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964843","Electroencephalography (EEG);Emotion identification;classification;Discrete Wavelet Transform (DWT);General Factor Analysis (GFA);accuracy","Support vector machines;Dimensionality reduction;Focusing;Feature extraction;Wavelet analysis;Market research;Electroencephalography","","","","29","IEEE","8 Dec 2022","","","IEEE","IEEE Conferences"
"MEMD-HHT based Emotion Detection from EEG using 3D CNN","M. Islam; T. Lee","Department of Electronic Engineering, The Chinese University of Hong Kong, Sha Tin, New territory, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Sha Tin, New territory, Hong Kong",2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"8 Sep 2022","2022","","","284","287","In this study, the Multivariate Empirical Mode Decomposition (MEMD) is applied to multichannel EEG to obtain scale-aligned intrinsic mode functions (IMFs) as input features for emotion detection. The IMFs capture local signal variation related to emotion changes. Among the extracted IMFs, the high oscillatory ones are found to be significant for the intended task. The Marginal Hilbert spectrum (MHS) is computed from the selected IMFs. A 3D convolutional neural network (CNN) is adopted to perform emotion detection with spatial-temporal-spectral feature representations that are constructed by stacking the multi-channel MHS over consecutive signal segments. The proposed approach is evaluated on the publicly available DEAP database. On binary classification of valence and arousal level (high versus low), the attained accuracies are 89.25% and 86.23% respectively, which significantly outperform previously reported systems with 2D CNN and/or conventional temporal and spectral features.","2694-0604","978-1-7281-2782-8","10.1109/EMBC48229.2022.9871012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9871012","emotion detection;EEG;MEMD;marginal Hilbert spectrum;3D CNN","Emotion recognition;Solid modeling;Three-dimensional displays;Empirical mode decomposition;Stacking;Feature extraction;Electroencephalography","Arousal;Databases, Factual;Electroencephalography;Emotions;Neural Networks, Computer","4","","25","IEEE","8 Sep 2022","","","IEEE","IEEE Conferences"
"Multimodal Emotion Recognition From EEG Signals and Facial Expressions","S. Wang; J. Qu; Y. Zhang; Y. Zhang","School of Computer and Information Technology, Liaoning Normal University, Dalian, China; School of Computer and Information Technology, Liaoning Normal University, Dalian, China; School of Information Engineering, Huzhou University, Huzhou, China; School of Computer and Information Technology, Liaoning Normal University, Dalian, China",IEEE Access,"6 Apr 2023","2023","11","","33061","33068","Emotion recognition has attracted attention in recent years. It is widely used in healthcare, teaching, human-computer interaction, and other fields. Human emotional features are often used to recognize different emotions. Currently, there is more and more research on multimodal emotion recognition based on the fusion of multiple features. This paper proposes a deep learning model for multimodal emotion recognition based on the fusion of electroencephalogram (EEG) signals and facial expressions to achieve an excellent classification effect. First, a pre-trained convolution neural network (CNN) is used to extract the facial features from the facial expressions. Next, the attention mechanism is introduced to extract more critical facial frame features. Then, we apply CNNs to extract spatial features from original EEG signals, which use a local convolution kernel and a global convolution kernel to learn the features of left and right hemispheres channels and all EEG channels. After feature-level fusion, the fusion features of the facial expression features and EEG features are fed into the classifier for emotion recognition. This paper conducted experiments on the DEAP and MAHNOB-HCI datasets to evaluate the performance of the proposed model. The accuracy of valence dimension classification is 96.63%, and arousal dimension classification is 97.15% on the DEAP dataset, while 96.69% and 96.26% on the MAHNOB-HCI dataset. The experimental results show that the proposed model can effectively recognize emotions.","2169-3536","","10.1109/ACCESS.2023.3263670","National Natural Science Foundation of China(grant numbers:61772252); Natural Science Foundation of Liaoning Province of China(grant numbers:2019-MS-216); Scientific Research Foundation of the Education Department of Liaoning Province(grant numbers:LJKZ0965); Huzhou Science and Technology Plan Project(grant numbers:2022GZ08); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089483","Multimodal emotion recognition;EEG;facial expressions;deep learning;attention mechanism","Feature extraction;Emotion recognition;Electroencephalography;Brain modeling;Convolution;Deep learning;Facial features","","34","","34","CCBYNCND","31 Mar 2023","","","IEEE","IEEE Journals"
"EEG-Based Cross-Subject Emotion Recognition Using GADF and CADAN","D. -D. Zhao; Q. Zhao","College of Engineering, Qufu Normal University, Rizhao, China; College of Engineering, Qufu Normal University, Rizhao, China",2023 China Automation Congress (CAC),"19 Mar 2024","2023","","","381","386","In recent years, EEG emotion recognition based on deep learning has received great attention. However, most of the EEG signals which input into deep learning models are unidimensional time series, resulting the inability to utilize the superiority of deep learning in images classification. To deal with this problem, the paper uses Gramian Angular Difference Field (GADF) to transform unidimensional EEG signals into two-dimensional images. Based on transfer learning, we adopt Domain Adaptation Network with Channel Attention (CADAN) consisting of channel attention mechanism and domain adaptation network for cross-subject emotion recognition task. Then, by using the leave-one-out method, the two-dimensional grayscale images obtained by GADF are divided into source and target domains. Then, these images are input into the CADAN model for cross-subject experiment. The experiment achieves the average accuracy of 0.6898 on the SEED dataset, indicating that the GADF _ CADAN method can effectively perform cross-subject task.","2688-0938","979-8-3503-0375-9","10.1109/CAC59555.2023.10450444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450444","EEG;Gramian angular difference field;Transfer learning;Cross-subject;Domain Adaptation Network","Deep learning;Adaptation models;Emotion recognition;Transforms;Brain modeling;Electroencephalography;Task analysis","","1","","26","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"Predictive Healthcare with Integrated EEG and Facial Emotion Recognition using Deep Learning and Transfer Learning","S. Mahato; S. D","Department of Electronics and Communication Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Chennai, India; School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India",2024 Asia Pacific Conference on Innovation in Technology (APCIT),"18 Sep 2024","2024","","","1","6","The main objective of this innovative project is to transform modern-day neurological healthcare, additionally pairing and synchronizing EEG signals with facial recognition information for efficient detection of neurological sicknesses. Machine learning algorithms are used for promptitude, ensuring rapid interventions during the course of the disease, and for careful and efficient balancing of the patient’s conditions. The system based on the above-discussed approach is characterized by a patient-centered nature ensuring that trust and ethical liability are the top priorities of the developed model. Such system supported by multiple machine learning algorithms also increase the accuracy of the diagnosis, its potential for early identification, and targeted therapy, ultimately having a beneficial effect on the public health. The researches findings demonstrate that there are convincing proofs that machine learning can be used for predictive healthcare: VGG19 achieved 86.28%, Random Forest Classifier 94.48%, and for epilepsy, Random Forest and Gradient Boosting for brain tumors 92.12% and for Parkinson’s disease 90.52%, respectively. Ensembled EEG analysis had the accuracy of 91.83%. Importantly, a result of 82.51% accuracy was achieved with the final decision-making process using transfer learning with the method of ensembled models and VGG19. Thus, it is possible to conclude that different machine learning algorithms are effective in predictive healthcare and can promote the progress of medical technology to benefit public health.","","979-8-3503-6153-7","10.1109/APCIT62007.2024.10673446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10673446","EEG;Facial Recognition;Machine Learning;Depression;Epilepsy;Brain Tumors;Parkinson’s disease;Random Forest Classifier;Gradient Boosting Classifier;Ensembling;VGG19;Transfer Learning","Accuracy;Machine learning algorithms;Transfer learning;Predictive models;Brain modeling;Boosting;Electroencephalography","","1","","22","IEEE","18 Sep 2024","","","IEEE","IEEE Conferences"
"Spectral Features of EEG Signals for the Automated Recognition of Negative Emotional States","F. N. Feradov; T. D. Ganchev","Department of Electronic Equipment and Microelectronics, Technical University of Varna, Varna, Bulgaria; Department of Computer Sciences and Engineering, Technical University of Varna, Varna, Bulgaria",2019 IEEE XXVIII International Scientific Conference Electronics (ET),"24 Oct 2019","2019","","","1","4","In the presented paper we investigate the properties of spectral EEG features for the detection of negative emotional states. In particular, the proposed features represent the dynamics of energy distribution in the frequency range of 20-35 Hz, based on a time-frequency analysis of multichannel EEG signal. The experimental evaluation is based on data from the DEAP database. We report results with J48- and SMO-based classifiers, in terms of average classification accuracy, 94.3% and 96.8%, respectively.","","978-1-7281-2574-9","10.1109/ET.2019.8878557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8878557","EEG;Emotion classification;Cognitive activity;Spectral Features","Electroencephalography;Feature extraction;Emotion recognition;Physiology;Psychology;Time-frequency analysis;Classification algorithms","","2","","25","IEEE","24 Oct 2019","","","IEEE","IEEE Conferences"
"Spatio-Temporal Swin Transformer-based 4-D EEG Emotion Recognition","Z. Chen; J. Jin; J. Pan","School of Software, South China Normal University, Foshan, China; School of Software, South China Normal University, Foshan, China; School of Software, South China Normal University, Foshan, China",2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Jan 2024","2023","","","1850","1855","Several studies have shown that electroencephalogram (EEG) can be interfered by eye movements and facial muscle movements. These interferences can reduce the accuracy of EEG emotion recognition. In terms of model selection, EEG-based emotion recognition studies mainly use convolutional neural networks and similar methods. These methods rely on global differences to distinguish different emotional states, but overlook the influence of local EEG changes on emotional states. In this paper, we use four-dimensional artificial EEG features as input to a Spatio-Temporal Swin Transformer. The use of artificial EEG features is beneficial for the interpretability of the model. Spatiotemporal attention modules can extract secondary features and useful information from EEG signals while eliminating redundant noise interference. Compared with other emotion recognition methods, the advantage of our study is that the input and output of the spatio-temporal attention modules are consistent. It can adapt to the original input size of the model and can be widely applied as a separable module in other models. We use the public emotion EEG datasets SEED and SEED-IV to evaluate the feasibility and effectiveness of our model. The accuracy of single-subject and cross-subject emotion three-class classification on the SEED dataset is 94.13% ± 2.11% and 89.33% ± 4.37%, respectively. The accuracy of single-subject emotion four-class classification is 82.34% ± 8.17% on the SEED-IV dataset. Our study achieves high accuracy in emotion classification and real-time processing.","2156-1133","979-8-3503-3748-8","10.1109/BIBM58861.2023.10385526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385526","emotion recognition;electroencephalogram;feature fusion;Swin Transformer","Emotion recognition;Adaptation models;Interference;Brain modeling;Feature extraction;Transformers;Electroencephalography","","","","27","IEEE","18 Jan 2024","","","IEEE","IEEE Conferences"
"A Long Short Term Memory Deep Learning Network for the Classification of Negative Emotions Using EEG Signals","D. Acharya; S. Goel; H. Bhardwaj; A. Sakalle; A. Bhardwaj","Computer Science Engineering, Bennett University, Greater Noida, India; Computer Science Engineering, Bennett University, Greater Noida, India; Computer Science Engineering, Bennett University, Greater Noida, India; Computer Science Engineering, Gautam Buddha University, Greater Noida, India; Computer Science Engineering, Bennett University, Greater Noida, India",2020 International Joint Conference on Neural Networks (IJCNN),"28 Sep 2020","2020","","","1","8","In cognitive science and human-computer interaction, automatic human emotion recognition using physiological stimuli is a key technology. This research considers classification of negative emotions using EEG signals in response to emotional clips. This paper introduces a long short term memory deep learning (LSTM) network to recognize emotions using EEG signals. The primary goal of this approach is to assess the classification performance of the LSTM model for classifying emotions. The secondary goal is to assess the human behavior of different age groups and genders. We have compared the performance of Multilayer Perceptron (MLP), K-nearest neighbors (KNN), Support Vector Machine (SVM), Deep Belief Network based SVM (DBN-SVM), and LSTM based deep learning model for classification of negative emotions using brain signals. The analysis shows that for four class of negative emotion recognition LSTM based deep learning model provides classification accuracy as 81.63%, 84.64%, 89.73%, and 92.84% for 50-50, 60-40, 70-30, and 10-fold cross-validation. Generalizability and reliability of this approach is evaluated by applying our approach to publicly available EEG dataset DEAP and SEED. In compliance with the self-reported feelings, brain signals of 26-35 years of age group provided the highest emotional identification. Among genders, females are more emotionally active as compared to males.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207280","Emotion Recognition;Deep learning;EEG;Fast Fourier Transformation;LSTM","Electroencephalography;Emotion recognition;Feature extraction;Machine learning;Brain modeling;Motion pictures","","24","","19","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"Classification of Human Emotions Based on Multivariate Spectral EEG Attributes and CNNs","M. C. Yildirim; M. A. Ozdemir; O. Guren","Dept. of Biomedical Eng., Izmir Katip Celebi University, Izmir, Türkiye; Dept. of Biomedical Eng., Izmir Katip Celebi University, Izmir, Türkiye; Dept. of Biomedical Eng., Izmir Katip Celebi University, Izmir, Türkiye",2023 Medical Technologies Congress (TIPTEKNO),"19 Dec 2023","2023","","","1","4","Human emotions are complex behavioral processes involving neural and chemical interactions in many layers of the human body. It plays a significant role in physical and mental health, in daily decisions, and in social communication. Investigating emotion is among the most popular areas of electroencephalogram (EEG) topics in the literature since physiological signals are hard to conceal and it provides harmless recording, easy accessibility, portability, and cost-effectiveness. Hereby, we present a transfer learning method to classify human emotions by EEG. First, raw EEG recordings were preprocessed and segmented. Then, the multivariate synchrosqueezing transform (MSST) was applied to these segments, and the obtained time-frequency images were fed into a CNN-based transfer learning model. In total, 45 EEG recordings from 15 subjects were trained across repetitive sessions. Inception V3 architecture was used in the training. Performance analyses were conducted for a binary classification that separates positive and negative emotions. In the leave-one-subject-out training approach, up to 100% validation accuracy was reached across 3 different window sizes. Among all window size-based training, it was observed that the 16-sec window size showed 78.05% average validation accuracy. The results proved that the proposed method demonstrated highly promising results in subject-based emotion classification. However, the subject-based average accuracy was not as high as the training of the shuffled dataset.","2687-7783","979-8-3503-2896-7","10.1109/TIPTEKNO59875.2023.10359239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10359239","EEG;emotion recognition;deep learning;multivariate synchrosqueezing transform","Training;Time-frequency analysis;Image segmentation;Transfer learning;Transforms;Electroencephalography;Physiology","","","","25","IEEE","19 Dec 2023","","","IEEE","IEEE Conferences"
"EEG‐Based Emotion Recognition and Classification","S. Sanei; J. A. Chambers","Cardiff University; Department of Electronic and Electrical Engineering, University of Bath, UK",EEG Signal Processing and Machine Learning,"","2021","","","479","523","Summary <p>Emotion is central to human daily experience, influencing cognition, perception, and everyday tasks such as learning, communication, and even rational decision‐making. However, the large number of emotion states and the overlaps between the corresponding brain regions make analysis of emotion very challenging for technologists and neuroscience researchers. A hybrid of the somatic and cognitive theories of emotion is the perceptual theory. Skin impedance can be checked using a galvanic skin response sensor. Electromyography is used to measure the muscle activities of the face, neck, and shoulder which are very likely to change due to certain emotions. Study of emotion using electroencephalography has become more attractive due to many algorithms developed by the signal processing community. Direct detection and classification of emotion signal sources are generally complex and advanced methods in signal processing are needed.</p>","","9781119386926","10.1002/9781119386957.ch14","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10953902.pdf&bkn=10950340&pdfType=chapter","","Physiology;Anxiety disorders;Skin;Limbic system;Brain;Mood;Muscles;Emotion recognition;Brain modeling;Heart beat","","","","","","8 Apr 2025","","","Wiley","Wiley AI eBook Chapters"
"Evolutionary Ensemble Learning for EEG-Based Cross-Subject Emotion Recognition","H. Zhang; T. Zuo; Z. Chen; X. Wang; P. Z. H. Sun","School of Microelectronics, Shanghai University, Shanghai, China; School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Department of Industrial and Manufacturing Systems Engineering, The University of Hong Kong, Hong Kong; CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Journal of Biomedical and Health Informatics,"2 Jul 2024","2024","28","7","3872","3881","Electroencephalogram (EEG) has been widely utilized in emotion recognition due to its high temporal resolution and reliability. However, the individual differences and non-stationary characteristics of EEG, along with the complexity and variability of emotions, pose challenges in generalizing emotion recognition models across subjects. In this paper, an end-to-end framework is proposed to improve the performance of cross-subject emotion recognition. A novel evolutionary programming (EP)-based optimization strategy with neural network (NN) as the base classifier termed NN ensemble with EP (EPNNE) is designed for cross-subject emotion recognition. The effectiveness of the proposed method is evaluated on the publicly available DEAP, FACED, SEED, and SEED-IV datasets. Numerical results demonstrate that the proposed method is superior to state-of-the-art cross-subject emotion recognition methods. The proposed end-to-end framework for cross-subject emotion recognition aids biomedical researchers in effectively assessing individual emotional states, thereby enabling efficient treatment and interventions.","2168-2208","","10.1109/JBHI.2024.3384816","Science and Technology Planning Project of Shenzhen(grant numbers:JCYJ20230807093819039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10490105","Cross-subject transfer learning;electro- encephalogram;emotion recognition;ensemble learning","Emotion recognition;Brain modeling;Electroencephalography;Feature extraction;Biological neural networks;Adaptation models;Data models","Humans;Electroencephalography;Emotions;Signal Processing, Computer-Assisted;Neural Networks, Computer;Machine Learning;Algorithms;Pattern Recognition, Automated;Databases, Factual;Adult;Female;Male","5","","51","IEEE","4 Apr 2024","","","IEEE","IEEE Journals"
"An In-depth Exploration of ResNet-50 for Complex Emotion Recognition to Unraveling Emotional States","T. N. V. S. Praveen; D. Sivathmika; G. Jahnavi; J. Bolledu","Department of Computer Science and Engineering, Lakireddy Bali Reddy College of Engineering (Autonomous), Mylavaram, India; Department of Computer Science and Engineering, Lakireddy Bali Reddy College of Engineering (Autonomous), Mylavaram, India; Department of Computer Science and Engineering, Lakireddy Bali Reddy College of Engineering (Autonomous), Mylavaram, India; Department of Computer Science and Engineering, Lakireddy Bali Reddy College of Engineering (Autonomous), Mylavaram, India",2023 International Conference on Advancement in Computation & Computer Technologies (InCACCT),"8 Jun 2023","2023","","","1","5","Emotional Recognition is an important task in computer vision and has many applications, including humanrobot interaction, virtual assistants, and mental health monitoring. In recent years, deep learning models have shown great promise in accurately recognizing emotions from images and videos. This paper proposes the ResNet-50 model for emotional recognition. ResNet-50 is a popular deep learning architecture that has been successfully applied to image classification tasks, and can be adapted for emotional recognition. Using ResNet-50 for emotional recognition offers several advantages over existing approaches. Firstly, ResNet-50 is a well-established architecture that has been extensively used in many computer vision tasks, which makes it a reliable choice. Secondly, ResNet-50 has a deep architecture with many layers that allows it to capture more complex patterns and features from images, which can lead to better emotion recognition performance. Finally, ResNet-50 is relatively efficient in terms of computational resources, which means it can be trained and deployed on a variety of hardware, including low-power devices such as smartphones and embedded systems. ResNet-50 for emotional recognition has several advantages over existing approaches, including reliability, better performance, and efficient use of computational resources. These advantages make ResNet-50 a promising candidate for emotion recognition tasks in a wide range of applications.","","979-8-3503-9648-5","10.1109/InCACCT57535.2023.10141774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10141774","Emotional Recognition;Resnet-50;Deep Learning;Computer Vision;Image Classification;Pattern Recognition;Feature Extraction;Convolutional Neural Network","Performance evaluation;Emotion recognition;Computer vision;Image recognition;Computational modeling;Virtual assistants;Computational efficiency","","5","","15","IEEE","8 Jun 2023","","","IEEE","IEEE Conferences"
"Research on EEG Emotion Recognition Based on Multi-scale Residual Convolutional Neural Network","X. Lian; Z. Shi; C. Gao; G. Ma; Y. Wu; W. Guan","School of Computer and Artificial Intelligence, Beijing Technology and Business University, Beijing, China; School of Computer and Artificial Intelligence, Beijing Technology and Business University, Beijing, China; School of Computer and Artificial Intelligence, Beijing Technology and Business University, Beijing, China; School of Computer and Artificial Intelligence, Beijing Technology and Business University, Beijing, China; School of Computer and Artificial Intelligence, Beijing Technology and Business University, Beijing, China; School of Computer and Artificial Intelligence, Beijing Technology and Business University, Beijing, China",2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),"26 Nov 2024","2024","","","204","207","Emotion recognition based on EEG signals has become an essential method in fields such as human-computer interaction and disease assessment. To further improve the recognition effect of human emotions, this paper fused the spatial and frequency information in EEG signals. It proposed a multi-scale residual convolutional neural network (ECA-Resnet-CNN) with an ECA attention mechanism to perform multi-scale information fusion on the constructed original features. The method first divides the EEG signals into six frequency bands; then, the differential entropy features are extracted from each frequency band and converted into a spatial-band three-dimensional feature matrix; then, the deep information of the three-dimensional feature matrix is extracted through a multi-scale residual convolutional neural network; finally, the weights of different channels of the feature map are adjusted by introducing the ECA attention mechanism, and the emotion classification is completed. The results show that on the Deap dataset, the arousal dimension and valence dimension binary classification accuracy of this method can reach $97.16 \%$ and $\mathbf{9 6. 3 0 \%}$ respectively, which is a significant improvement compared with the existing mainstream emotion recognition models, proving the effectiveness of this method.","","979-8-3315-0661-2","10.1109/ICBASE63199.2024.10762345","Beijing Technology and Business University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10762345","EEG signal;Emotion recognition;Differential entropy;Attention mechanism;Residual network","Emotion recognition;Attention mechanisms;Accuracy;Feature extraction;Brain modeling;Electroencephalography;Data mining;Convolutional neural networks;Matrix converters;Residual neural networks","","","","10","IEEE","26 Nov 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition Based on EEG Signals and Deep Neural Networks Architectures","G. Giannakakis; A. Karasmanoglou; M. Antonakakis; P. Vorgia; M. Zervakis","Department of Electronic Engineering, Institute of Computer Science, Foundation for Research and Technology Hellas (FORTH), Hellenic Mediterranean University, Heraklion, Greece; Department of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece; Department of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece; Department of Electrical and Computer Engineering, Technical University of Crete, Chania, Greece; Institute of Agri-food and Life Sciences, Hellenic Mediterranean University, Heraklion, Greece",2024 12th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW),"24 Apr 2025","2024","","","189","192","Emotion recognition is a significant field of study in the area of affective computing, with compelling applications for human-computer interaction and mental health. Various approaches have been proposed that are judged for their effectiveness, but there is the notion that physiological signals provide a more objective and reliable emotion estimation. In this study, we investigate the emotion recognition efficacy based on electroencephalogram (EEG) signals and machine learning techniques. We utilized four (4) publicly available datasets, the MAHNOB, SEED, SEED IV and DREAMER dataset to evaluate the genaralization performance of a Deep Neural Network (DNN) in classifying emotional states. Feature extraction techniques including time-domain, frequency-domain, and topographical maps (CSP) were utilized to derive feature representations of EEG signals. Using a simple Multi-Layer Perceptron (MLP) architecture we were able to achieve best accuracy of 94 % and best weighted F1 of 91 % for DREAMER arousal in Leave One Trial Out (LOTO) 93% and 90% for Leave One Subject Out (LOSO) respectively.","","979-8-3315-1645-1","10.1109/ACIIW63320.2024.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10970221","emotion recognition;EEG;EEG rhythms;machine learning","Emotion recognition;Affective computing;Artificial neural networks;Machine learning;Computer architecture;Feature extraction;Electroencephalography;Physiology;Reliability;Time-domain analysis","","","","14","IEEE","24 Apr 2025","","","IEEE","IEEE Conferences"
"Emotion Recognition from EEG during Self-Paced Emotional Imagery","C. A. Kothe; S. Makeig; J. A. Onton","Swartz Center for Computational Neuroscience, University of California San Diego, La Jolla, CA, USA; Swartz Center for Computational Neuroscience, University of California San Diego, La Jolla, CA, USA; Naval Health Research Center, San Diego, CA, USA",2013 Humaine Association Conference on Affective Computing and Intelligent Interaction,"12 Dec 2013","2013","","","855","858","Here we present an analysis of a 12-subject electroencephalographic (EEG) data set in which participants were asked to engage in prolonged, self-paced episodes of guided emotion imagination with eyes closed. Our goal is to correctly predict, given a short EEG segment, whether the participant was imagining a positive respectively negative-valence emotional scenario during the given segment using a predictive model learned via machine learning. The challenge lies in generalizing to novel (i.e., previously unseen) emotion episodes from a wide variety of scenarios including love, awe, frustration, anger, etc. based purely on spontaneous oscillatory EEG activity without stimulus event-locked responses. Using a variant of the Filter-Bank Common Spatial Pattern algorithm, we achieve an average accuracy of 71.3% correct classification of binary valence rating across 12 different emotional imagery scenarios under rigorous block-wise cross-validation.","2156-8111","978-0-7695-5048-0","10.1109/ACII.2013.160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681552","emotion;valence;brain-computer interface;EEG;machine learning;guided imagery","Electroencephalography;Emotion recognition;Band-pass filters;Accuracy;Scalp;IIR filters;Spatial filters","","35","2","17","IEEE","12 Dec 2013","","","IEEE","IEEE Conferences"
"Inter Subject Emotion Recognition Using Spatio-Temporal Features From EEG Signal","M. Asif; D. Srivastava; A. Gupta; U. S. Tiwary","Department of Information Technology, Indian Institute of Information Technology, Allahabad Prayagraj, India; Department of Information Technology, Indian Institute of Information Technology, Allahabad Prayagraj, India; Department of Information Technology, Indian Institute of Information Technology, Allahabad Prayagraj, India; Department of Information Technology, Indian Institute of Information Technology, Allahabad Prayagraj, India",2023 27th International Computer Science and Engineering Conference (ICSEC),"4 Dec 2023","2023","","","1","4","Inter-subject or subject-independent emotion recognition has been a challenging task in affective computing. This work is about an easy-to-implement emotion recognition model that classifies emotions from EEG signals subject independently. It is based on the famous EEGNet architecture, which is used in EEG-related BCIs. We used the ‘Dataset on Emotion using Naturalistic Stimuli’ (DENS) dataset. The dataset contains the 'Emotional Events'- the precise information of the emotion timings that participants felt. The model is a combination of regular, depthwise and separable convolution layers of CNN to classify the emotions. The model has the capacity to learn the spatial features of the EEG channels and the temporal features of the EEG signals variability with time. The model is evaluated for the valence space ratings. The model achieved an accuracy of 73.04%.","2768-0592","979-8-3503-4210-9","10.1109/ICSEC59635.2023.10329776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329776","Affective Computing;CNN;DENS Dataset;EEG;Emotion Recognition;Subject Independent Emotions","Computer science;Emotion recognition;Affective computing;Convolution;Computational modeling;Computer architecture;Brain modeling","","1","","17","IEEE","4 Dec 2023","","","IEEE","IEEE Conferences"
"Advancing Human Computer Interaction Through a Hybrid EEG-Based Emotion Recognition System","K. A.; M. M; D. P; M. A; M. S","Department of ECE, Mahendra Institute of Technology, Namakkal, Tamil Nadu, India; Department of ECE, Mahendra Institute of Technology, Namakkal, Tamil Nadu, India; Department of ECE, Mahendra Institute of Technology, Namakkal, Tamil Nadu, India; Department of ECE, Mahendra Institute of Technology, Namakkal, Tamil Nadu, India; Department of ECE, Mahendra Institute of Technology, Namakkal, Tamil Nadu, India",2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),"12 Jun 2024","2024","","","01","06","An emotional recognition system helps human-computer interaction systems by using a deep learning approach to find pattern recognition. This research advances the HCI field and lays the foundation for a new era of emotionally intelligent and responsive technology interfaces. The EEG emotion recognition system is consisting of machine learning algorithms for preprocessing, feature extraction, feature selection, and classification. The original raw signal contains noise and irrelevant signals to be removed using ICA (independent component analysis). The next step is feature extraction to extract features such as variance, standard deviation, kurtosis, and entropy from the EEG signal using the discrete wavelet transform (DWT). These features are selected based on the type of emotion using linear discriminant analysis (LDA). The hybrid classification algorithm uses CNN (convolutional neural network) and GRU (gated recurrent unit) to classify the emotion. The results of this hybrid classification model are precision, accuracy, and sensitivity for better outcomes.","","979-8-3503-1860-9","10.1109/ICDCECE60827.2024.10548956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548956","EEG signal;emotion recognition;ICA;DWT;LDA;CNN;and GRU","Human computer interaction;Emotion recognition;Sensitivity;Transforms;Feature extraction;Electroencephalography;Pattern recognition","","","","18","IEEE","12 Jun 2024","","","IEEE","IEEE Conferences"
"Research and Analysis of Facial Emotion Recognition Based on Convolutional Neural Network","H. Qu; F. Jing","Yunnan College of Business Management, Yunnan, China; Yunnan College of Business Management, Yunnan, China","2024 5th International Conference on Computer Vision, Image and Deep Learning (CVIDL)","26 Jul 2024","2024","","","434","437","Product neural network is one of the most popular deep learning models, because of its good feature extraction ability and classification prediction effect, it is widely used in the field of image recognition, face recognition and natural language processing, etc. The CNN’s local connectivity, weight sharing and downsampling are the significant features that differentiate it from other network models. User emotion recognition is the basis of human-machine emotional interaction, for in the welcoming scene, the user emotion performance in the interaction process is mostly composite emotions, so the recognition of the basic emotion category can not satisfy the robot for the characterization of any user emotion, for this kind of problem, this paper proposes an emotion cognition method based on the characterization of facial features, and under the role of convolutional neural network, the human facial emotion recognition is carried out. research and exploration. Combined with the research of human facial emotion recognition under big data technology is a new field of big data used in human research, this study is based on the research foundation of previous scholars, and carries out in-depth exploration and analysis, which plays a certain role in promoting the research of human facial emotion recognition.","","979-8-3503-7382-0","10.1109/CVIDL62147.2024.10604047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10604047","Convolutional neural;Facial emotion recognition;Fusion research and analysis;Emotion recognition","Deep learning;Emotion recognition;Image recognition;Face recognition;Predictive models;Big Data;Brain modeling","","1","","10","IEEE","26 Jul 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition for Hearing Impaired and Normal Individuals With Residual Feature Pyramids Network Based on Time–Frequency–Spatial Features","F. Hou; J. Liu; Z. Bai; Z. Yang; J. Liu; Q. Gao; Y. Song","Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, TUT Maritime College, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China",IEEE Transactions on Instrumentation and Measurement,"7 Feb 2023","2023","72","","1","11","With the development of affective computing, discriminative feature selection is critical for electroencephalography (EEG) emotion recognition. In this article, we fused four EEG feature matrices constructed by the preprocessed signal, the differential entropy (DE), the symmetric difference, and the symmetric quotient based on the International 10–20 system, which integrates time-, frequency-, and spatial-domain information of EEG signals. For the feature classification model, we used the space-to-depth (S2D) layer instead of the convolutional neural network (CNN) as the backbone to reduce the calculation of the model without affecting the classification performance. The residual feature pyramid network (RFPN) was proposed to obtain the correlation of channels, and then, the deep multiscale semantic information of EEG feature maps is captured. The emotion classification strategy was evaluated by DEAP, SEED, SEED-IV, and our hearing-impaired EEG dataset (HIED). The classification accuracies were 93.56% (four-class, DEAP), 96.84% (three-class, SEED), 91.62% (four-class, SEED-IV), and 87.74% (six-class, HIED). Furthermore, we also found that the difference in emotional response between the left and right brain regions of hearing-impaired subjects is more obvious than that of normal subjects.","1557-9662","","10.1109/TIM.2023.3240230","National Natural Science Foundation of China(grant numbers:62103299); 2021 Tianjin Postgraduate Research and Innovation Project(grant numbers:2021YJSS092); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026834","Electroencephalography (EEG);emotion recognition;hearing-impaired subjects;residual feature pyramid network (RFPN);space-to-depth (S2D) layer","Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Electrodes;Entropy;Videos","","18","","46","IEEE","27 Jan 2023","","","IEEE","IEEE Journals"
"CMAC-based Computational Model of Affects (CCMA) for profiling emotion from EEG signals","H. Yaacob; W. Abdul; I. F. Al Shaikhli; N. Kamaruddin","Kulliyyah of Information & Communication Technology International Islamic University Malaysia Kuala Lumpur, Malaysia; Kulliyyah of Information & Communication Technology International Islamic University Malaysia Kuala Lumpur, Malaysia; International Islamic University Malaysia, Kuala Lumpur, Wilayah Persekutuan, MY; Kulliyyah of Information & Communication Technology International Islamic University Malaysia Kuala Lumpur, Malaysia",The 5th International Conference on Information and Communication Technology for The Muslim World (ICT4M),"26 Jan 2015","2014","","","1","6","Several studies have been performed to profile emotions using EEG signals through affective computing approach. It includes data acquisition, signal pre-processing, feature extraction and classification. Different combinations of feature extraction and classification techniques have been proposed. However, the results are subjective. Very few studies include subject-independent classification. In this paper, a new profiling model, known as CMAC-based Computational Model of Affects (CCMA), is proposed.), CMAC is presumed to be a reasonable model for processing EEG signals with its innate capabilities to solve non-linear problems through self-organization feature mapping (SOFM). Features that are extracted using CCMA are trained using Evolving Fuzzy Neural Network (EFuNN) as the classifier. For comparison, classification of emotions using features that are derived from power spectral density (PSD) was also performed. The results shows that the performance of using CCMA for profiling emotions outperforms the performance of classifying emotions from PSD features.","","978-1-4799-6242-6","10.1109/ICT4M.2014.7020584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020584","CMAC;EEG;affective computing;EFuNN;feature extraction","Feature extraction;Electroencephalography;Brain modeling;Computational modeling;Associative memory;Vectors;Adaptation models","","","","22","IEEE","26 Jan 2015","","","IEEE","IEEE Conferences"
"Classification of EEG signal by WT-CNN model in emotion recognition system","B. Zhang; H. Jiang; L. Dong","Brain Cognitive Computing Lab., Minzu University of China, Beijing, China; Brain Cognitive Computing Lab., Minzu University of China, Beijing, China; Brain Cognitive Computing Lab., Minzu University of China, Beijing, China",2017 IEEE 16th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),"16 Nov 2017","2017","","","109","114","With the continuous development of the computer, the brain computer interface system (BCI) has become an important part of computer research. Emotion recognition is an important task for computer to understand the human status in BCI. Affective computing (AC) aim to develop the model of emotions and advance the affective intelligence of computers. In this paper, we proposed a new method to better the classification of EEG signals in emotion recognition, in which, WT-CNN was used to extracting features and recognize two emotions (positive, negative) according to the output of wavelet transform on raw signal. The simulation shows that WT-CNN could obtain better results, and the best results could be reached to 88%.","","978-1-5386-0771-8","10.1109/ICCI-CC.2017.8109738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8109738","Emotion recognition;EEG;Convolution neural network;Wavelet transform","Emotion recognition;Feature extraction;Brain modeling;Computational modeling;Electroencephalography;Wavelet transforms","","14","","9","IEEE","16 Nov 2017","","","IEEE","IEEE Conferences"
"Constructing a Personalized Cross-Day EEG-Based Emotion-Classification Model Using Transfer Learning","Y. -P. Lin","Institute of Medical Science and Technology, National Sun Yat-sen University, Kaohsiung, Taiwan",IEEE Journal of Biomedical and Health Informatics,"5 May 2020","2020","24","5","1255","1264","State-of-the-art electroencephalogram (EEG)-based emotion-classification works indicate that a personalized model may not be well exploited until sufficient labeled data are available, given a substantial EEG non-stationarity over days. However, it is impractical to impose a labor-intensive, time-consuming multiple-day data collection. This study proposes a robust principal component analysis (RPCA)-embedded transfer learning (TL) to generate a personalized cross-day model with less labeled data, while obviating intra- and inter-individual differences. Upon the add-session-in validation on two datasets MDME (five-day data of 12 subjects) and SDMN (single-day data of 26 subjects), the experimental results showed that TL enabled the classifier of an MDME individual (using his/her 1st-day session only) to improve progressively in valence and arousal classification by adding similar source sessions (SSs) via the within-dataset TL (wdTL) and cross-dataset TL (cdTL) manners. When recruiting three SSs to test on the 5th-day session, the wdTL improvement (valence: 11.19%, arousal: 5.82%) marginally outperformed the subject-dependent (SD) counterpart (valence: 9.75%, arousal: 3.77%) that was obtained using their own 2nd-4th-day sessions only. The cdTL returned a similar trend in valence (8.35%), yet it was less effective in arousal (0.81%). Most importantly, such cross-day enhancements did not occur in either SD or TL scenarios until RPCA processing. This work sheds light on how to construct a personalized model by leveraging ever-growing EEG repositories.","2168-2208","","10.1109/JBHI.2019.2934172","Ministry of Science and Technology, Taiwan(grant numbers:MOST 106-2628-E-110-002-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793187","Affective computing;EEG-based emotion classification;EEG non-stationarity;transfer learning","Electroencephalography;Brain modeling;Data models;Oscillators;Principal component analysis;Emotional responses;Sparse matrices","Electroencephalography;Emotions;Female;Humans;Machine Learning;Male;Principal Component Analysis;Signal Processing, Computer-Assisted","40","","36","IEEE","9 Aug 2019","","","IEEE","IEEE Journals"
"Emotional Classification of EEG Signal using Image Encoding and Deep Learning","A. K. A; G. M; L. R","Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore, Amrita Vishwa Vidyapeetham, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore, Amrita Vishwa Vidyapeetham, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore, Amrita Vishwa Vidyapeetham, India","2021 Seventh International conference on Bio Signals, Images, and Instrumentation (ICBSII)","4 Jun 2021","2021","","","1","5","For humans, emotions are important and play a significant role in human insight. Emotions are commonly identified by speech, facial expression and gesture. Recently Electroencephalograph (EEG) based emotion recognition have accumulated solid interest in the research community and it provides cheap, portable and reliable techniques for emotion recognition. In this work, classification of seed database having three emotions like positive, neutral and negative was performed, which was publicly available. This paper has two parts, time-series to image conversion of EEG signal and classification of emotion. First part, the data is transformed to an image that is used to analyse the EEG signal and in second part, the transformed image passes through deep learning to understand the emotions encountered during the EEG signal generation. Experimental results indicate that the scalogram of image encoding provides the best classification accuracy of 98%, compared to spectrogram and Hilbert Huang Transform (HHT) 78% and 75% classification accuracy respectively.","","978-1-6654-4126-1","10.1109/ICBSII51839.2021.9445187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9445187","EEG;Spectrogram;Scalogram;Hilbert-Huang Transform (HHT)","Deep learning;Emotion recognition;Image coding;Instruments;Transforms;Feature extraction;Solids","","13","","23","IEEE","4 Jun 2021","","","IEEE","IEEE Conferences"
"Exploring Gender Differences in EEG-Based Emotion Recognition with Haptic Vibration","X. Wang; B. Xu; J. Wang; W. Zhang; A. Song","The State Key Laboratory of Digital Medical Engineering Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China; The State Key Laboratory of Digital Medical Engineering Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China; The State Key Laboratory of Digital Medical Engineering Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; The State Key Laboratory of Digital Medical Engineering Jiangsu Key Laboratory of Remote Measurement and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China",2023 IEEE International Conference on Development and Learning (ICDL),"25 Dec 2023","2023","","","22","27","The study of gender differences in emotions has attracted interest in psychology and neuroscience for decades. However, gender differences in the emotions evoked by haptic stimuli still need to be investigated further. In this paper, we applied visual-auditory-haptic fusion stimuli to evoke four target emotions (joy, sadness, fear, and neutral), and the electroen-cephalogram (EEG) signals were recorded synchronously. The differential entropy (DE) feature was extracted, and the support vector machine (SVM) was employed for emotion classification. Experimental results indicated that females generally behaved with lower brain activation changes than males in most frequency bands and brain regions. Meanwhile, haptic stimuli improved emotion classification accuracy, especially for females. In addition, females showed greater individual differences than males, and haptic stimuli reduced the differences in the same gender, particularly in males. This research promises to facilitate the application of emotion recognition in haptic interactions.","","978-1-6654-7075-9","10.1109/ICDL55364.2023.10364503","National Key Research and Development Program of China(grant numbers:2022YFC2405602); Natural Science Foundation of Jiangsu Province(grant numbers:BK20221464); National Natural Science Foundation of China(grant numbers:92148205,62173088,62173089); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10364503","emotion recognition;gender differences;haptic vibration;electroencephalogram","Support vector machines;Vibrations;Emotion recognition;Neuroscience;Psychology;Feature extraction;Entropy","","2","","24","IEEE","25 Dec 2023","","","IEEE","IEEE Conferences"
"Natural Human Emotion Recognition Based on Various Mixed Reality(MR) Games and Electroencephalography (EEG) Signals","A. S. M. Miah; J. Shin; M. M. Islam; Abdullah; M. K. I. Molla","School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; Computer Science and Engineering, Bangladesh Army University of Science and Technology(BAUST), Saidpur, Bangladesh; Computer Science and Engineering, Bangladesh Army University of Science and Technology(BAUST), Saidpur, Bangladesh; Computer Science and Engineering, University of Rajshahi Rajshahi",2022 IEEE 5th Eurasian Conference on Educational Innovation (ECEI),"20 Jul 2022","2022","","","408","411","We estimated willing and natural emotions while playing Mixed reality (MR) games. We have shown the performance accuracy of the labeling with game type and self-assessment EEG data. This study is conducted to improve the Virtual reality (VR) and MR world to be more realistic and suitable to the needs. We have used the GAMEEMO dataset to evaluate our proposed method. First, participants took subjective electroencephalography (EEG) signals while playing the games, then took their self-assessments for labeling EEG signals based on Game rating. There were four categories of Games Boring (G1), Calm (G2), Horror(G3), and Funny(G4) in the dataset. We labeled the collected dataset in two ways: the dataset with the type of games and with the Self-assessment manikin. After that, we calculated mean and standard deviation (STD) from both types of EEG data. Based on the feature vector, we applied a machine-learning algorithm to classify the emotion of the subject for the game based on EEG data. Specifically, we have employed multinomial logistic regression (MLR), support vector machine (SVM), K-nearest neighbor (KNN) machine learning algorithm with cross-validation. With the method, we achieved 99.80% of accuracy for SVM with STD and 99.00% for the MLR with the mean method. This accuracy is 3.91‒4.92% better than that of the existing work, which shows an improved understanding of MR and human emotion and helps understand better human emotion and the area of VR and MR, estimated willing emotion. Based on the study result, a better life for the next generation can be provided. Finally, this study is the first step to prevent negative MR content from spreading out.","","978-1-6654-3318-1","10.1109/ECEI53102.2022.9829482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829482","Electroencephalography (EEG);Virtual Reality (VR);Mixed Reality (MR);Emotion;Game","Support vector machines;Technological innovation;Solid modeling;Machine learning algorithms;Games;Virtual reality;Feature extraction","","13","","10","IEEE","20 Jul 2022","","","IEEE","IEEE Conferences"
"Emotion detection from EEG using transfer learning","S. Sidharth; A. A. Samuel; R. H; J. T. Panachakel; S. Parveen K","Department of Electronics and Communication Engineering, College of Engineering, Trivandrum, India; Department of Electronics and Communication Engineering, College of Engineering, Trivandrum, India; Department of Electronics and Communication Engineering, College of Engineering, Trivandrum, India; Department of Electronics and Communication Engineering, College of Engineering, Trivandrum, India; Department of Electronics and Communication Engineering, College of Engineering, Trivandrum, India",2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"11 Dec 2023","2023","","","1","4","In this study, we employed transfer learning to overcome the challenge of limited data availability in EEG-based emotion detection. The base model used in this study was Resnet50. Additionally, we employed a novel feature combination in EEG-based emotion detection. The input to the model was in the form of an image matrix, which comprised Mean Phase Coherence (MPC) and Magnitude Squared Coherence (MSC) in the upper-triangular and lower-triangular matrices, respectively. We further improved the technique by incorporating features obtained from the Differential Entropy (DE) into the diagonal. The dataset used in this study, SEED EEG (62 channel EEG), comprises three classes (Positive, Neutral, and Negative). We calculated both subject-independent and subject-dependent accuracy. The subject-dependent accuracy was obtained using a 10-fold cross-validation method and was 93.1%, while the subject-independent classification was performed by employing the leave-one-subject-out (LOSO) strategy. The accuracy obtained in subject-independent classification was 71.6%. Both of these accuracies are at least twice better than the chance accuracy of classifying 3 classes. The study found the use of MSC and MPC in EEG-based emotion detection promising for emotion classification. The future scope of this work includes the use of data augmentation techniques, enhanced classifiers, and better features for emotion classification.","2694-0604","979-8-3503-2447-1","10.1109/EMBC40787.2023.10340389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340389","Brain-computer interface;emotion detection;transfer learning;electroencephalogram;mean phase coherence;magnitude squared coherence;SEED_EEG","Emotion recognition;Solid modeling;Technological innovation;Biological system modeling;Transfer learning;Coherence;Brain modeling","Electroencephalography;Emotions;Learning;Entropy;Machine Learning","7","","17","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Emotion Detection Using Hybrid Deep Sequential Learning Architecture from EEG Signals","A. Suting; D. Thangkhiew; A. Kumar; A. Halder","Department of Computer Application, North-Eastern Hill University, Meghalaya, India; Department of Computer Application, North-Eastern Hill University, Meghalaya, India; Department of Computer Application, North-Eastern Hill University, Meghalaya, India; Department of Computer Application, North-Eastern Hill University, Meghalaya, India",2024 IEEE 16th International Conference on Computational Intelligence and Communication Networks (CICN),"27 Jan 2025","2024","","","316","320","Emotions play a vital role in decision-making, planning, reasoning, and other mental states in daily life, which are widely recognized as crucial roles in human interactions. It recognizes through non-physiological signals such as facial expressions, speech, and body language or physiological signals such as changes in mental state, heart rate, breathing, body temperature, and muscle tension. Electroencephalography (EEG) signals have good time resolution, which can provide a direct and comprehensive means for emotion recognition with higher classification accuracy. In this study, a novel emotion detection using hybrid deep sequential learning architecture from EEG signal dataset is proposed. It utilizes the advantages of Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks as it concatenation of LSTM and GRU deep sequential architectures. The proposed method outperforms other compared deep sequential classifiers in terms of accuracy, precision, recall, and $F1$-score, with 99.41%, 0.99, 0.99, and 0.99 respectively.","2472-7555","979-8-3315-0526-4","10.1109/CICN63059.2024.10847568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10847568","Emotion Detection;EEG Signal;Deep Learning;LSTM;GRU","Emotion recognition;Accuracy;Computer architecture;Speech recognition;Muscles;Electroencephalography;Physiology;Planning;Long short term memory;Signal resolution","","","","20","IEEE","27 Jan 2025","","","IEEE","IEEE Conferences"
"Analysis of Feature Extraction Models for Emotion Recognition using EEG Signals","K. Kannadasan; M. T. I. Miraj; K. S. Bheekharry; B. S. Begum","Department of Computer Science and Engineering, National Institute of Technology Tiruchirappalli, Tiruchirappalli, Tamil Nadu; Department of Computer Science and Engineering, National Institute of Technology Tiruchirappalli, Tiruchirappalli, Tamil Nadu; Department of Computer Science and Engineering, National Institute of Technology Tiruchirappalli, Tiruchirappalli, Tamil Nadu; Department of Computer Science and Engineering, National Institute of Technology Tiruchirappalli, Tiruchirappalli, Tamil Nadu",2022 IEEE 3rd Global Conference for Advancement in Technology (GCAT),"12 Dec 2022","2022","","","1","6","Electroencephalogram (EEG) based affective brain-computer interface (a-BCI) systems are gaining interest among researchers in recent decades. a-BCI systems interpret/recognize human emotions using features extracted from the EEG signals. Hence, features play a crucial role in building EEG based emotion recognition models. As a result, analysis of feature extraction models becomes inevitable. In this work, we have proposed an analysis model for analyzing the feature extraction models for EEG based emotion recognition with the help of the DEAP dataset. Features were extracted using manual feature extraction techniques and the convolutional neural network. Several combinations of feature sets were given as input to classifiers and the results obtained were analyzed with various evaluation metrics. The proposed analysis model will help the researchers to choose the feature extraction model for emotion recognition.","","978-1-6654-6855-8","10.1109/GCAT55367.2022.9972159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9972159","Emotion Recognition;DEAP Dataset;Feature Extraction;Convolutional Neural Network;Handcrafted Features","Measurement;Analytical models;Emotion recognition;Time-frequency analysis;Manuals;Feature extraction;Brain modeling","","1","","15","IEEE","12 Dec 2022","","","IEEE","IEEE Conferences"
"EmotioNet: A 3-D Convolutional Neural Network for EEG-based Emotion Recognition","Y. Wang; Z. Huang; B. McCane; P. Neo","Department of Computer Science, University of Otago, Dunedin, New Zealand; Department of Computer Science, University of Otago, Dunedin, New Zealand; Department of Computer Science, University of Otago, Dunedin, New Zealand; Department of Psychology, University of Otago, Dunedin, New Zealand",2018 International Joint Conference on Neural Networks (IJCNN),"14 Oct 2018","2018","","","1","7","In this paper, an emotional EEG-specific three-dimensional Convolutional Neural Network, EmotioNet, is proposed and implemented to accurately recognize emotion states. For the first time, raw data in the benchmark emotional EEG database, i.e., DEAP, are used as the input to a CNN architecture. In order to investigate the spatio-temporal character of emotional features, the effectiveness of 2-D and 3-D convolution kernels, which extract spatial and temporal features separately and simultaneously, are compared in detail. Furthermore, two major problems of EEG-based emotion recognition, namely, covariance shift and the unreliability of emotional ground truth, are described, and the effectiveness of batch normalization and dense prediction, which alleviate these problems respectively, are also investigated. Experimental results show that 3-D kernels, batch normalization, and dense prediction are all essential techniques for the emotional EEG-specific CNN architecture. The proposed EmotioNet, namely, a 3-D covariance shift adaptation-based CNN with a dense prediction layer, achieves classification rates of 73.3% and 72.1% for arousal and valence, equivalent to the best performance of several previous studies. Importantly, our results are based on automatic feature extraction, which is in contrast to previous handcrafted features. Therefore, EmotioNet provides a new method for EEG-based emotion recognition.","2161-4407","978-1-5090-6014-6","10.1109/IJCNN.2018.8489715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489715","Emotion recognition;EEG;3-D CNN;spatiotemporal emotional features;covariance shift;the unreliability of emotional ground truth","Feature extraction;Electroencephalography;Emotion recognition;Brain modeling;Support vector machines;Training;Videos","","64","","24","IEEE","14 Oct 2018","","","IEEE","IEEE Conferences"
"Instance-Based Genre-Specific Music Emotion Prediction with An EEG Setup","X. Liu; A. A. P. Wai; S. Kumaran; Y. R. Saravanan; Z. Lin","Nanyang Technological University, Singapore, Singapore, SG; Nanyang Technological University, Singapore, Singapore, SG; National University Hospital, Singapore, SG; National University Hospital, Singapore, SG; Nanyang Technological University, Singapore, Singapore, SG",2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"28 Oct 2018","2018","","","2092","2095","This paper explores a novel direction in music-induced emotion (music emotion) analysis - the effects of different genres on the prediction of music emotion. We aim to compare the performance of various classifiers in the prediction of the emotion induced by music, as well as to investigate the adaptation of advanced features (such as asymmetries) in improving classification accuracy. The study is supported by real-world experiments where 10 subjects listened to 20 musical pieces from 5 genres- classical, heavy metal, electronic dance music, pop and rap, during which electroencephalogram (EEG) data were collected. A maximum 10-fold cross-validation accuracy of 98.4% for subject-independent and 99.0% for subject-dependent data were obtained for the classification of short instances of each song. The emotion of popular music was shown to have been most accurately predicted, with a classification accuracy of 99.6%. Further examination was conducted to investigate the effect of music emotion on the relaxation of subjects while listening.","1558-4615","978-1-5386-3646-6","10.1109/EMBC.2018.8512630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8512630","Music Emotion;Electroencephalography (EEG);Machine Learning;Music Genres","Music;Electroencephalography;Feature extraction;Brain modeling;Forestry;Metals;Training","Auditory Perception;Electroencephalography;Emotions;Music;Relaxation","3","","21","IEEE","28 Oct 2018","","","IEEE","IEEE Conferences"
"A Semi-automatic Feature Fusion Model for EEG-based Emotion Recognition","G. Zhang; S. Li; J. Wang; Y. Zhou; T. Xu","School of Software, Northwestern Polytechnical University, Xi’an, P.R. China; School of Education, Shaanxi Normal University, Xi’an, P.R. China; School of Software, Northwestern Polytechnical University, Xi’an, P.R. China; School of Education, Shaanxi Normal University, Xi’an, P.R. China; School of Software, Northwestern Polytechnical University, Xi’an, P.R. China",2021 27th International Conference on Mechatronics and Machine Vision in Practice (M2VIP),"7 Jan 2022","2021","","","726","731","Electroencephalogram (EEG) is usually used to study cognitive activities, which have different temporal, frequency-domain features. Scientists attempted to find crucial features to improve recognition accuracy but challenging. This paper proposed a novel confused emotion recognition method based on EEG, which combine automatic feature extraction (deep learning) and knowledge-based feature extraction. To evaluate our method, we designed an experiment to collect data, the basic idea of which is to induce the confused emotion based on the English listening test. The results show that our method performs better in experiments than Convolution Neural Networks(CNN) and Support Vector Machine (SVM).","","978-1-6654-3153-8","10.1109/M2VIP49856.2021.9665129","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665129","","Support vector machines;Emotion recognition;Mechatronics;Convolution;Machine vision;Frequency-domain analysis;Knowledge based systems","","","","11","IEEE","7 Jan 2022","","","IEEE","IEEE Conferences"
"Emotion Recognition Using Brain Signals","N. Sulthan; N. Mohan; K. A. Khan; S. S.; M. Shanir P.P","Department of Electrical and Electronics engineering, TKM college of engineering, Kollam, Kerala, India; Department of Electrical and Electronics engineering, TKM college of engineering, Kollam, Kerala, India; Department of Electrical and Electronics engineering, TKM college of engineering, Kollam, Kerala, India; NA; NA",2018 International Conference on Intelligent Circuits and Systems (ICICS),"4 Oct 2018","2018","","","315","319","James-Lange defined emotion as ""A positive or negative experience that is associated with a particular pattern of physiological activity"". Emotions produce different psycho physiological signal which is either stimulated by conscious or unconscious profundity of a situation or an object. Emotion has a key role in human communication as well as in modern BCI (brain-computer interaction) systems. Most of the contemporary BCI systems are lacking emotional intelligence and also they are not able to detect human emotional states for proper execution of action. Electroencephalogram (EEG) is one of the common methods to acquire brain signals for brain computer interface (BCI). Most of the works for emotion recognition using EEG data are complex and with relatively average performance obtained. Hence, there is still a lot of space for developing a better performing system in emotion recognition. In the present work, a wavelet based better performing feature extraction algorithm is proposed. Further this algorithm is tested for different classifiers namely k Nearest Neighbor (KNN), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Support Vector Machine (SVM). The average sensitivity obtained for the present work was 92.97% with an accuracy of 91.67%, which is better result than the previous work.","","978-1-5386-6483-4","10.1109/ICICS.2018.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8479592","Emotion recognition;EEG;BCI;Discrete wavelet transform (DWT)","Feature extraction;Electroencephalography;Emotion recognition;Support vector machines;Sensitivity;Classification algorithms;Brain modeling","","8","","28","IEEE","4 Oct 2018","","","IEEE","IEEE Conferences"
"Exploring the Effect of Age and Sex on Subject-Independent EEG-Based Emotion Recognition Methods","C. E. Valderrama; A. Sheoran; Q. Liu","Dept. of Applied Computer Science, University of Winnipeg, Winnipeg, Canada; Dept. of Applied Computer Science, University of Winnipeg, Winnipeg, Canada; Dept. of Applied Computer Science, University of Winnipeg, Winnipeg, Canada",2024 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),"12 Sep 2024","2024","","","319","323","In the field of emotion recognition, there are two approaches used to develop predictive models that recognize emotions from EEG signals: subject-dependent and subject-independent. Subject-independent models, although more practical, tend to yield a lower performance due to the high variability of EEG signals between individuals. Recent studies have indicated that incorporating prior demographic information about individuals can improve the accuracy of subject-independent approaches. Some studies have supported this claim by showing that including individuals’ sex can boost model accuracy. However, until now, no one has used interpretable models to measure to what extent demographics can enhance subject-independent approaches. In this work, we follow this direction by using a logistic regression model to correlate the output of a deep learning model with subjects’ age and sex, thereby evaluating whether these factors impact emotion prediction. Our analysis indicates that the ‘sex’ variable significantly influenced the predictions of the deep learning model in three out of five emotions, whereas ‘age’ does not have any effect. These findings suggest that sex is a factor that needs to be considered when designing EEG-based emotion recognition models, which could lead to more robust subject-independent models with potential applications in areas such as healthcare, education, and marketing.","2576-7046","979-8-3503-7162-8","10.1109/CCECE59415.2024.10667058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10667058","Emotion recognition;Electroencephalogram;Subjects’ demographics;Deep learning;Logistic regression","Deep learning;Emotion recognition;Logistic regression;Accuracy;Stacking;Education;Medical services","","","","14","IEEE","12 Sep 2024","","","IEEE","IEEE Conferences"
"Bi-Branch Vision Transformer Network for EEG Emotion Recognition","W. Lu; T. -P. Tan; H. Ma","School of Computer Sciences, Universiti Sains Malaysia (USM), Penang, Malaysia; School of Computer Sciences, Universiti Sains Malaysia (USM), Penang, Malaysia; Henan High-Speed Railway Operation and Maintenance Engineering Research Center, Zhengzhou Railway Vocational and Technical College, Zhengzhou, China",IEEE Access,"17 Apr 2023","2023","11","","36233","36243","Electroencephalogram (EEG) signals have emerged as an important tool for emotion research due to their objective reflection of real emotional states. Deep learning-based EEG emotion classification algorithms have made encouraging progress, but existing models struggle with capturing long-range dependence and integrating temporal, frequency, and spatial domain features that limit their classification ability. To address these challenges, this study proposes a Bi-branch Vision Transformer- based EEG emotion recognition model, Bi-ViTNet, that integrates spatial-temporal and spatial-frequency feature representations. Specifically, Bi-ViTNet is composed of spatial-frequency feature extraction branch and spatial-temporal feature extraction branch that fuse spatial-frequency-temporal features in a unified framework. Each branch is composed of Linear Embedding and Transformer Encoder, which is used to extract spatial-frequency features and spatial-temporal features. Finally, fusion and classification are performed by the Fusion and Classification layer. Experiments on SEED and SEED-IV datasets demonstrate that Bi-ViTNet outperforms state-of-the-art baselines.","2169-3536","","10.1109/ACCESS.2023.3266117","Henan Provincial Science and Technology Research Project(grant numbers:232102240091); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098561","Affective computing;EEG-based emotion recognition;transformer","Electroencephalography;Feature extraction;Emotion recognition;Brain modeling;Transformers;Deep learning;Physiology","","20","","69","CCBYNCND","10 Apr 2023","","","IEEE","IEEE Journals"
"Research on Emotion Recognition Based on Deep Learning Mixed Modalities","B. Mi; J. Lu; F. Zheng","Department of Computer and Simulation Technology, Naval Medical University, Shanghai, China; Department of Computer and Simulation Technology, Naval Medical University, Shanghai, China; Department of Computer and Simulation Technology, Naval Medical University, Shanghai, China",2022 4th International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM),"27 Mar 2023","2022","","","133","138","With the rapid development of artificial intelligence and machine learning in recent years, emotion recognition has gradually become an important research topic. Emotion recognition in one direction has a good research foundation after long-term development, and from multiple directions, more effective information can be extracted, thereby improving the accuracy of emotion recognition. This paper analyzes from the perspective of emotional recognition of physiological signals such as brainwave signals and facial emotion recognition, respectively, preprocessing, feature extraction, SVM feature classification, LSTM combined with convolutional neural network emotion recognition for the acquired signals. And the accuracy of mixed-modal emotion recognition is compared. Compared with single facial expression emotion recognition, mixed-modal emotion recognition extracts more feature information and has a higher accuracy.","","978-1-6654-6399-7","10.1109/AIAM57466.2022.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071422","EEG recognition system;emotion recognition;artificial intelligence;modal fusion","Support vector machines;Deep learning;Emotion recognition;Face recognition;Feature extraction;Physiology;Electroencephalography","","","","25","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"Emotion Recognition Related to Stock Trading Using Machine Learning Algorithms With Feature Selection","E. P. Torres; E. A. Torres; M. Hernández-Álvarez; S. G. Yoo","Departamento de Informática y Ciencias de la Computación, Facultad de Ingeniería de Sistemas, Escuela Politécnica Nacional, Quito, Ecuador; Pontificia Universidad Católica del Ecuador, Quito, Ecuador; Departamento de Informática y Ciencias de la Computación, Facultad de Ingeniería de Sistemas, Escuela Politécnica Nacional, Quito, Ecuador; Departamento de Informática y Ciencias de la Computación, Facultad de Ingeniería de Sistemas, Escuela Politécnica Nacional, Quito, Ecuador",IEEE Access,"10 Nov 2020","2020","8","","199719","199732","This article proposes an emotion elicitation method to develop our Stock-Emotion dataset: a collection of the participants' electroencephalogram (EEG) signals who paper-traded using real stock market data, virtual money, and outcomes that emotionally affected them. A system for emotion recognition using this dataset was tested. The system extracted from the EEG signals the following features: five frequency bands, Differential Entropy (DE), Differential Asymmetry (DASM), and Rational Asymmetry (RASM), for each band. Our system then carried out feature selection using a filter method (Mutual Information Matrix), combined with a wrapper process (Chi-Square statistics) and alternatively using the embedded algorithms in a Deep Learning classifier. Finally, this work classified emotions in four quadrants of the circumplex model using Random Forest and Deep Learning algorithms. Our findings show that 1) the proposed emotion elicitation method is useful to provoke affective states associated with trading, 2) the proposed feature selection process improved the classification performance of our emotion recognition system, and 3) classifier performance of the system can recognize trading related emotions and has results comparable with the state of the art research corresponding to a similar number of output classes.","2169-3536","","10.1109/ACCESS.2020.3035539","Escuela Politécnica Nacional, Quito, Ecuador; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9247095","BCI;EEG;DE;DASM;RASM;random forest;deep learning;emotion recognition;stock market trading;emotion elicitation","Deep learning;Emotion recognition;Feature extraction;Electroencephalography;Classification algorithms;Stock markets;Mutual information","","27","","52","CCBY","3 Nov 2020","","","IEEE","IEEE Journals"
"Cross-Session EEG-Based Emotion Recognition Via Maximizing Domain Discrepancy","X. Zhu; Y. Ye; L. Lu; Y. Li; H. Wu","College of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; College of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; College of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; College of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; College of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China",2022 3rd International Conference on Electronic Communication and Artificial Intelligence (IWECAI),"11 Apr 2022","2022","","","568","572","Cross-session pose a challenge to the application of EEG-based emotion recognition (ER), which is due to the non-stationary nature of EEG that causes the EEG to reveal distribution discrepancies over time, thereby leading to degradation of performance. The traditional way is by collecting labelled data over multiple sessions and then retraining a new model, but this is time-consuming and labor-intensive. In this paper, we propose the Maximizing Domain Discrepancy for EEG-based ER (MDD-ER) to improve cross-session performance. MDD-ER applies distinct domain adaptation strategies to alleviate feature distribution discrepancies between source session and target session at different levels: for shallow features, we use maximum mean discrepancy (MMD) to align the source and target domains based on statistical criterion, and for deep features, we adversarially train two classifiers, which effectively improves the alignment precision of the source and target domains due to the consideration of the class of features. Consequently, the proposed MDD-ER method can improve model generalization across sessions. We conduct comprehensive experiments on SEED dataset, and the experimental results demonstrate the effectiveness of the proposed MDD-ER method in cross-session ER.","","978-1-6654-7997-4","10.1109/IWECAI55315.2022.00116","National Natural Science Foundation of China(grant numbers:61976047); Science &; Technology Department of Sichuan Province of China(grant numbers:22ZDYF2694,2021YFG0331); Fundamental Research Funds for the Central Universities(grant numbers:ZYGX2021YGLH016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9750789","Emotion recognition;Electroencephalography;Deep learning;Domain Adaption","Degradation;Emotion recognition;Adaptation models;Brain modeling;Feature extraction;Electroencephalography;Data models","","","","14","IEEE","11 Apr 2022","","","IEEE","IEEE Conferences"
"Human Attention Recognition with Machine Learning from Brain-EEG Signals","R. Hassan; S. Hasan; M. J. Hasan; M. R. Jamader; D. Eisenberg; T. Pias","Department of CSE, University of Asia Pacific, Bangladesh & Virginia Tech, USA; Department of CSE, University of Asia Pacific, Bangladesh & Virginia Tech, USA; Department of CSE, University of Asia Pacific, Bangladesh & Virginia Tech, USA; Department of CSE, University of Asia Pacific, Bangladesh & Virginia Tech, USA; Department of CSE, University of Asia Pacific, Bangladesh & Virginia Tech, USA; Department of CSE, University of Asia Pacific, Bangladesh & Virginia Tech, USA","2020 IEEE 2nd Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)","22 Sep 2020","2020","","","16","19","Emotion recognition has always been a very popular field of research. Recently, EEG brain waves are used to recognize the emotional states of a person. Attention level also plays an important role in human life, but still demands more investigation. This paper proposes a noble human attention level recognition system using advanced machine learning algorithms. The system is cost-effective, single-channeled, and time-frequency scalp-EEG signals-based. In this study, the Bitalino EEG sensor board is used to record EEG signals from 30 human subjects in different attention states. Initially, the attention level was classified into three categories, which are focused, neutral and distracted. The data was taken while the subjects were watching interesting videos and boring lectures, doing simple and interesting math problems, and solving interesting and hard puzzles. At first, these EEG signals are pre-processed to remove noise such as muscle movement. Statistical coefficients (i.e. mean, standard deviation, skewness, kurtosis, and entropy) and statistical wavelet transform are used to extract meaningful features from the EEG signal. We mainly used two multi-scale wavelet packet statistics (WPS) and multi-scale wavelet packet energy statistics (WPES) to generate the feature vector. This feature vector was used to train the complex hybrid model with CNN and LSTM. This proposed method achieved almost 89% accuracy while determining the attention level of a subject.","","978-1-7281-8712-9","10.1109/ECBIOS50299.2020.9203672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203672","EEG (Electroencephalography);brainwave;wavelet decomposition;deep learning","Electroencephalography;Brain modeling;Mathematical model;Spectrogram;Machine learning;Diseases;Conferences","","16","","16","IEEE","22 Sep 2020","","","IEEE","IEEE Conferences"
"Modeling the affective space of 360 virtual reality videos based on arousal and valence for wearable EEG-based VR emotion classification","N. S. Suhaimi; C. T. B. Yuan; J. Teo; J. Mountstephens","Universiti Malaysia Sabah, Kota Kinabalu, Sabah, MY; Universiti Malaysia Sabah, Kota Kinabalu, Sabah, MY; Faculty of Medical and Health Sciences, Universiti Malaysia Sabah Kota Kinabalu, Malaysia; Faculty of Medical and Health Sciences, Universiti Malaysia Sabah Kota Kinabalu, Malaysia",2018 IEEE 14th International Colloquium on Signal Processing & Its Applications (CSPA),"31 May 2018","2018","","","167","172","This study attempts to produce a novel database for emotional analysis which uses virtual reality (VR) contents, obtained from third party sources such as YouTube, Discovery VR, Jaunt VR, NYT VR, Veer VR and Google Cardboard, as the visual stimuli in the classification of emotion using commercial-of-the-shelf (COTS) wearable electroencephalography (EEG) headsets. While there are available sources for emotional analysis such as Dataset for Emotion Analysis using EEG, Physiological and video signals (DEAP) dataset presented by Koelstra et al. and Database for Emotional Analysis in Music (DEAM) dataset by Soleymani et al, their contents are focused on using music stimuli and music video stimuli. The database which will be presented here will consist of novel affective taggings using virtual reality content, specifically on Youtube 360 videos, as evaluated by 15 participants based on the Arousal-Valence emotion model (AVS). The feedback obtained from these evaluations will serve as the underlying dataset for the next stage of machine learning implementation, which is the targeted emotion classification of virtual reality stimuli using wearable EEG headsets.","","978-1-5386-0389-5","10.1109/CSPA.2018.8368706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368706","Emotion Classification;Wearable EEG;Machine Learning;Electroencephalography;Virtual Reality;Emotion Detection Dataset","Videos;Electroencephalography;Tagging;Virtual reality;Headphones;Brain modeling;Solid modeling","","13","","10","IEEE","31 May 2018","","","IEEE","IEEE Conferences"
"Multimodal Emotion Recognition based on the Fusion of EEG Signals and Eye Movement Data","Y. Song; L. Feng; W. Zhang; X. Song; M. Cheng","School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Technical College for the Deaf, Tianjin University of Technology, Tianjin, China; Engineering Training Center, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China",2024 IEEE 25th China Conference on System Simulation Technology and its Application (CCSSTA),"1 Oct 2024","2024","","","127","132","Emotion recognition research plays a crucial role in advancing brain-computer interfaces. However, many current methods either rely on a single modal signal or use a multimodal fusion approach that may not fully consider the balance of information between modalities. In this study, we propose a novel multimodal emotion recognition method that combines electroencephalograph (EEG) signals and eye movement data using a cosine loss fusion approach. To extract deep features from the two modalities, we utilize ResNet 18 with a gated attention mechanism for EEG feature extraction, and DenseNet for eye movement data feature extraction. The cosine loss function is employed to address inter-modal differences and ensure a balanced fusion of information. Our proposed method achieves an impressive classification accuracy of $91.16 \%$ for the emotions of happiness, fear, sadness, and neutrality on the SEED-IV dataset. This outperforms both EEG signals (74.48%) and eye movement data $(68.38 \%)$, demonstrating the effectiveness of our approach in emotion recognition tasks.","","979-8-3503-6660-0","10.1109/CCSSTA62096.2024.10691734","Tianjin University of Technology; Tianjin University of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691734","emotion recognition;EEG signals;eye movement data;cosine loss","Emotion recognition;Attention mechanisms;Accuracy;Logic gates;Feature extraction;Brain modeling;Electroencephalography;Brain-computer interfaces;Systems simulation;Data mining","","","","18","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"MSFR-GCN: A Multi-Scale Feature Reconstruction Graph Convolutional Network for EEG Emotion and Cognition Recognition","D. Pan; H. Zheng; F. Xu; Y. Ouyang; Z. Jia; C. Wang; H. Zeng","School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Second School of Clinical Medicine, Zhejiang Chinese Medical University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"18 Aug 2023","2023","31","","3245","3254","Graph Convolutional Network (GCN) excels at EEG recognition by capturing brain connections, but previous studies neglect the important EEG feature itself. In this study, we propose MSFR-GCN, a multi-scale feature reconstruction GCN for recognizing emotion and cognition tasks. Specifically, MSFR-GCN includes the MSFR and feature-pool characteristically, with the MSFR consisting of two sub-modules, multi-scale Squeeze-and-Excitation (MSSE) and multi-scale sample re-weighting (MSSR). MSSE assigns weights to channels and frequency bands based on their separate statistical information, while MSSR assigns sample weights based on combined channel and frequency information. The feature-pool, which pools across the feature dimension, is applied after GCN to retain EEG channel information. The MSFR-GCN achieves excellent results in emotion recognition when first tested on two public datasets, SEED and SEED-IV. Than the MSFR-GCN is tested on our self-collected Emotion and Cognition EEG dataset (ECED) for both emotion and cognition classification tasks. The results show MSFR-GCN’s good performance in emotion and cognition classification tasks and reveal the implicit relationship between the two, which may provide aid in the rehabilitation of people with cognitive impairments from an emotional perspective.","1558-0210","","10.1109/TNSRE.2023.3304660","National Science Foundation of China(grant numbers:62076083); National Key Research and Development Program of China(grant numbers:2022YFE0199300); Hangzhou Artificial Intelligence Major Technological Innovation Project(grant numbers:2022AIZD0159); Leading Goose Research and Development Program of Zhejiang(grant numbers:2023C03026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10216317","EEG emotion recognition;EEG cognition recognition;graph convolutional network;SE attention;graph pool","Electroencephalography;Emotion recognition;Feature extraction;Cognition;Brain modeling;Convolution;Frequency-domain analysis","Humans;Cognition;Brain;Emotions;Recognition, Psychology;Electroencephalography","23","","45","CCBY","14 Aug 2023","","","IEEE","IEEE Journals"
"The Back-propagation Neural Network Classification of EEG Signal Using Time Frequency Domain Feature Extraction","D. K. Theresia; D. Ana; A. Faqih; B. Kusumoputro","Departement of Electrical Engineering, Faculty of Engineering Universitas Indonesia, Depok, Indonesia; Departement of Electrical Engineering, Faculty of Engineering Universitas Indonesia, Depok, Indonesia; Departement of Electrical Engineering, Faculty of Engineering Universitas Indonesia, Depok, Indonesia; Departement of Electrical Engineering, Faculty of Engineering Universitas Indonesia, Depok, Indonesia",2019 16th International Conference on Quality in Research (QIR): International Symposium on Electrical and Computer Engineering,"14 Nov 2019","2019","","","1","4","In the recognition of emotions based on EEG signals, the feature extraction used is as important as classifier for the results of classification. The better the feature extraction is used, the better the results of the classification. In the other hand, there is no definitive approach for feature extraction in emotion recognition based on EEG signal. In this paper, we use nine types of time frequency domains as features, Principal Component Analysis (PCA) as dimension reduction method and Back-propagation Neural Network as classifiers. This method is implemented using a database that can be accessed by the public. The results of the experiment show that time frequency domain feature extraction and back propagation can achieve 63.75 % recognition rate.","","978-1-7281-1898-7","10.1109/QIR.2019.8898288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898288","EEG;Time Variant Feature;PCA;Back-propagation Neural Network;DEAP","Principal component analysis;Feature extraction;Testing;Training;Electroencephalography;Time-frequency analysis;Data mining","","3","","9","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Research on Emotion Recognition in Hearing-impaired EEG Based on Attention Residual Network","Y. Song; Z. Niu; Z. Mao","School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Maritime College, Tianjin University of Technology, Tianjin, China; Technical College for the Deaf, Tianjin University of Technology, Tianjin, China",2024 IEEE 25th China Conference on System Simulation Technology and its Application (CCSSTA),"1 Oct 2024","2024","","","139","143","With the advancement of affective computing and computer science, there is a growing interest in emotion recognition based on electroencephalogram (EEG). In this study, we utilized facial expression images to elicit emotional responses and collected EEG data from fifteen hearing-impaired individuals and fifteen individuals with normal hearing, across five emotions (happy, neutral, sad, fear, and anger). The acquired EEG data were filtered to reduce noise interference, and then independent component analysis (ICA) was used to remove ocular artifacts as well as other noise. Power spectral density (PSD), differential entropy (DE), and wavelet entropy (WE) features were extracted from preprocessed EEG signals. For classification model, the attention residual network consisted of attention residual blocks and the structure of the residual network was adapted. Integration of attention blocks within each residual block enabled the network to selectively focus on emotional representation features extraction at different stages. Results from the experiments demonstrated that using DE as a feature for classification yielded superior performance compared to other features. The average classification accuracies were $77.6 \%$ and $82.1 \%$ for the hearing-impaired and normal subject groups respectively.","","979-8-3503-6660-0","10.1109/CCSSTA62096.2024.10691840","Tianjin University; Tianjin University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691840","emotion recognition;electroencephalogram (EEG);hearing-impaired;attention mechanism;residual network","Emotion recognition;Accuracy;Noise;Interference;Feature extraction;Brain modeling;Electroencephalography;Entropy;Systems simulation;Residual neural networks","","","","9","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"Deep Feature Extraction and Attention Fusion for Multimodal Emotion Recognition","Z. Yang; D. Li; F. Hou; Y. Song; Q. Gao","School of Electrical Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications for Complicated Systems, Maritime College, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China",IEEE Transactions on Circuits and Systems II: Express Briefs,"5 Mar 2024","2024","71","3","1526","1530","Recently, electroencephalogram (EEG)-based multimodal emotion recognition has emerged as one of the research hotspots in affective computing. However, the existing methods tend to ignore the interaction information between the EEG and other modal features. In this brief, we propose a novel model termed EEANet (EEG and eye movement Attention Network) to find the modal correlation at feature level. The DE feature and 31 eye movement features were extracted from the pre-processed EEG and eye movement signals, and then two feedforward encoders were used to capture the deep features, respectively. The interactive attention layer is applied to learn multi-modal complementary information and semantic-level context information. Finally, the multi-head self-attention mechanism allows the model to focus on the discriminative features for emotion classification. The model was verified on the SEED-IV dataset, and the results showed that the accuracy of emotion recognition was significantly improved with the EEANet, and the average accuracy of the four classifications was 92.26%.","1558-3791","","10.1109/TCSII.2023.3318814","National Natural Science Foundation of China(grant numbers:62103299); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10265209","EEG;eye movement;interactive attention;self-attention;emotion recognition","Electroencephalography;Feature extraction;Emotion recognition;Feedforward systems;Brain modeling;Correlation","","1","","17","IEEE","27 Sep 2023","","","IEEE","IEEE Journals"
"Spectral Features for the Classification of Familiarity from EEG Recordings","F. Feradov","Department of Electronics and Microelectronics, Technical University of Varna, Varna, Bulgaria","2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)","15 Nov 2021","2021","","","323","327","Familiarity with different objects or stimuli plays an essential role in forming behavioral and emotional responses. The present paper examines the applicability of spectral features in the classification of levels of familiarity from EEG signals. In particular, examining the differences of PSD of frequency bands and covariance coefficients as EEG features is carried out. The experimental evaluation of the proposed features is conducted using data from the DEAP database and kNN, C4.5 and SVM classifiers, and mean classification accuracy of up to 99.5% is reported.","2623-8764","978-953-233-101-1","10.23919/MIPRO52101.2021.9596908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9596908","EEG signal processing;affective computing;automated classification of cognition","Support vector machines;Databases;Feature extraction;Electroencephalography;Cognition;Emotional responses;Signal representation","","","","16","","15 Nov 2021","","","IEEE","IEEE Conferences"
"EEG Data Analytics to Distinguish Happy and Sad Emotions Based on Statistical Features","Y. Pamungkas; A. D. Wibawa; M. H. Purnomo","Department of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Electrical Engineering, Department of Computer Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Electrical Engineering, Department of Computer Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",2021 4th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI),"11 Feb 2022","2021","","","345","350","Affective computing is part of the important study of Human-Computer Interaction. Currently, EEG-based affective computing (emotion recognition) has become an interesting issue to be studied further. Emotions are not only closely related to aspects of HCI but also affect human health. Meanwhile, EEG is also considered a transparent tool in objectively revealing human emotions because the brain naturally produces EEG signals. This study focuses on comparing and classifying human emotions (happy and sad) based on EEG data. The channels used for recording EEG data are F7, F8, FP1, and FP2. Data preprocessing such as signal filtering, Independent Component Analysis, and Band Decomposition aims to clean the raw signal from artifacts and separate the signals according to specific frequency bands (Alpha, Beta, and Gamma). Then, statistical feature extraction is performed in the time domain to obtain the Mean values, Mean Absolute Value (MAV), and Standard Deviation values for further data analysis. The results show that emotion of happy has a higher feature value compared to emotion of sad. In the classification of happy and sad emotions using several algorithms, Random Forest signifies the highest classification accuracy (88.90%), compared to other algorithms such as SVM (86.70%), K-NN (88.87%), and Naive Bayes (86.63%).","","978-1-6654-0151-7","10.1109/ISRITI54043.2021.9702766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9702766","EEG-based affective computing;emotion recognition;random forest;SVM;KNN","Support vector machines;Seminars;Affective computing;Data analysis;Feature extraction;Electroencephalography;Classification algorithms","","4","","27","IEEE","11 Feb 2022","","","IEEE","IEEE Conferences"
"Multilayer perceptron for EEG signal classification during listening to emotional music","Yuan-Pin Lin; Chi-Hong Wang; Tien-Lin Wu; Shyh-Kang Jeng; Jyh-Horng Chen","Department of Electrical Engineering, National Taiwan University, Taiwan; Yung-Ho Branch, Cardinal Tien Hospital, Taiwan; Department of Electrical Engineering, National Taiwan University, Taiwan; Department of Electrical Engineering, National Taiwan University, Taiwan; Department of Electrical Engineering, National Taiwan University, Taiwan",TENCON 2007 - 2007 IEEE Region 10 Conference,"14 Jan 2008","2007","","","1","3","In this study an electroencephalography (EEG) signal-based emotion classification algorithm was investigated. Several excerpts of emotional music were used as stimulus for elicitation of emotion-specific EEG signal. Besides, the hemispheric asymmetry alpha power indices of brain activation were extracted as feature vector for training multilayer perceptron classifier (MLP) in order to learn four targeted emotion categories, including joy, angry, sadness, and pleasure. The results demonstrated that the average classification accuracy of MLP could be 69.69% in five subjects for four emotional categories, which is much higher than chance probability of 25%.","2159-3450","978-1-4244-1271-6","10.1109/TENCON.2007.4428831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4428831","","Multilayer perceptrons;Electroencephalography;Pattern classification;Multiple signal classification;Pollution measurement;Classification algorithms;Flowcharts;Human computer interaction;Emotion recognition;Electromyography","","30","","8","IEEE","14 Jan 2008","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition based on DEAP Dataset with Genetic Algorithm Augmented Multi-Layer Perceptron","S. Kulkarni; P. Patil","Department of MCA, KLE Technological University, Hubballi; Department of CSE, KLE Technological University, Hubballi",2023 OITS International Conference on Information Technology (OCIT),"19 Feb 2024","2023","","","687","692","Emotion Recognition plays a pivotal role within the fields of Affective Computing and Brain-Computer Interface. Emotions are the behavioral responses representing the mental state of a person. Recognizing the emotions of a person is indeed crucial for various aspects of human-computer interactions and understanding and responding to one's mental health. Here the method is developed to identify the emotions based on EEG by applying Genetic Algorithm(GA) enhanced Multi-Layer Perceptron(MLP) is a promising path of research that has the ability to contribute to the field of affective computing and improve our understanding of individual emotional responses. To quantify the emotions the Power Spectral Density features with reference to the valence-arousal scale are employed. The architecture of the MLP is optimized using a Genetic Algorithm. The proposed model achieved relatively high accuracy in classifying emotions, distinguishing between high/low valence and high/low arousal emotions, as well as categorizing emotions into four classes based on valence and arousal scales. The planned emotion recognition model has achieved high accuracy in classifying emotions based on valence and arousal dimensions. Achieving accuracy of 91.10% for Valence(low/high) classification and 91.02% for Arousal(low/high) classification is indeed commendable. Furthermore, achieving an accuracy of 83.52% for classifying emotions into four categories based on both valence and arousal (HVLA, HVHA, LVLA, HVHA). It is observed that the stated outcomes are improved compared to current literature in the field of emotion recognition.","","979-8-3503-5823-0","10.1109/OCIT59427.2023.10431149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431149","EEG;Emotions;Valence;Arousal;Power Spectral Density(PSD);Genetic Algorithm(GA);Multi-Layer Perceptron(MLP)","Emotion recognition;Machine learning algorithms;Machine learning;Brain modeling;Feature extraction;Classification algorithms;Genetic algorithms","","1","","28","IEEE","19 Feb 2024","","","IEEE","IEEE Conferences"
"Emotion recognition from EEG signals using machine learning model","A. K. R; S. Sundar; M. S. P. P","Industrial Instrumentation and Control Engineering, TKM College of Engineering, Kollam, Kerala, India; Centre for Artificial Intelligence, TKM College of Engineering, Kollam, Kerala, India; Dept. of Electrical and Electronics Engineering, TKM College of Engineering, Kollam, Kerala, India","2022 5th International Conference on Multimedia, Signal Processing and Communication Technologies (IMPACT)","1 Feb 2023","2022","","","1","6","The objective of this work is to identify emotion from EEG signals, that represent the brain activity of individuals. With the rapid advancement of machine learning algorithms and numerous real-world applications of brain-computer interface for regular people, emotion categorization from EEG data has recently gained a lot of attention. Researchers previously have little knowledge of the specific interactions between distinct EEG characteristics and various emotional states. Using EEG-based emotion recognition, the computer may be able to understand the emotional state of the user. This work is executed with DEAP dataset with 32 channels for EEG recording, it achieves a better classification accuracy with the random forest machine learning (ML) algorithm. In the subject-specific experiment an average best accuracy of 91.26% and in the subject-dependent experiment, the best accuracy of 78.5% is obtained for the random forest classifier. The proposed method is superior to other studies when compared to those using a four-class problem because those have only achieved the best accuracy of 71.43%.","","978-1-6654-7647-8","10.1109/IMPACT55510.2022.10029284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10029284","Random forest;Kfold;ML;EEG;emotion recognition","Emotion recognition;Machine learning algorithms;Databases;Feature extraction;Brain modeling;Electroencephalography;Recording","","","","40","IEEE","1 Feb 2023","","","IEEE","IEEE Conferences"
"Multimodal Physiological Signal Emotion Recognition Method Based on Attention Mechanism","M. Zhao; C. Cheng; L. Feng","Department of Computer Science and Technology, Dalian University of Technology, Dalian, China; Department of Computer Science and Technology, Dalian University of Technology, Dalian, China; School of Innovation and Entrepreneurship, Dalian University of Technology, Dalian, China","2024 International Conference on Artificial Intelligence, Deep Learning and Neural Networks (AIDLNN)","14 Feb 2025","2024","","","222","226","Prior research in emotion recognition has mainly concentrated on examining physiological signals from a single modality, such as electroencephalogram (EEG) or various peripheral physiological signals (PPS). However, given the multimodal nature of human emotional expression, these unimodal methodologies fail to reach satisfactory accuracy. To overcome this challenge, we introduce a multimodal emotion recognition approach utilizing the attention mechanism (MERA), consisting of both a unimodal feature extraction module and a multimodal feature fusion module. During the unimodal feature extraction phase, we investigate the complex interactions among various channels and employ an encoder module to precisely capture the subtle distinctions in emotional fluctuations. In the multimodal feature fusion phase, a multi-head attention mechanism is employed to emphasize the complementary nature of various modalities and the complex interplay among signals through dynamic weighting, thereby enhancing the accuracy of emotion recognition. Experiments carried out on the DEAP and MAHNOB-HCI datasets reveal superior outcomes for our method in binary and four-class emotion classification tasks across various frequency bands. Our framework outperforms contemporary leading approaches in terms of accuracy for all types of classification.","","979-8-3315-2081-6","10.1109/AIDLNN65358.2024.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10873248","Multimodal;Attention;Electroencephalogram (EEG)","Deep learning;Emotion recognition;Attention mechanisms;Accuracy;Fluctuations;Neural networks;Feature extraction;Physiology;Electroencephalography;Artificial intelligence","","","","20","IEEE","14 Feb 2025","","","IEEE","IEEE Conferences"
"Interpretable Emotion Classification Using Multidomain Feature of EEG Signals","K. Zhao; D. Xu; K. He; G. Peng","Department of Informatics, Yunnan University, Kunming, China; Department of Informatics, Yunnan University, Kunming, China; Department of Informatics, Yunnan University, Kunming, China; Information and Technology Center, Yunnan University, Kunming, China",IEEE Sensors Journal,"31 May 2023","2023","23","11","11879","11891","Research on affective computing by physiological signals has become a new and important research branch of artificial intelligence. However, most of the research focuses only on the improvement of emotion classification models while neglecting the correlation between subjective emotional labels and implicit electroencephalogram (EEG) feature information. In this study, we proposed an interpretable emotion classification framework based on food images as visual stimuli and a proposed EEG dataset acquired by a portable single-electrode EEG device. Then, we construct a multidomain feature extraction method based on sliding time windows, which can effectively extract single-channel EEG features. Finally, we chose the extreme gradient boosting (XGBoost) model as our classifier in the proposed classification framework. The optimal accuracy of our single-channel EEG classification model was 94.76%, which reached the classification performance of multichannel EEG classification. Based on XGBoost, we interpret the global and the local emotion feature selection method by calculating Shapley additive explanation (SHAP) values. Through the analysis of interpretable methods, we can not only obtain the contribution degree of features from three types of emotional labels (positive, negative, and neutral) but also know whether features have a positive or negative effect on classification results. The source codes of this work are publicly available at: https://github.com/VCMHE/MDF-EEG.","1558-1748","","10.1109/JSEN.2023.3266322","National Natural Science Foundation of China(grant numbers:62162068,62202416,61540062,62061049); Yunnan Ten Thousand Talents Program and Yunling Scholars Special Project(grant numbers:YNWR-YLXZ-2018-022); joint fund of Yunnan Provincial Science and Technology Department-Yunnan University “Double First Class” Construction(grant numbers:2019FY003012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102828","Electroencephalogram (EEG);extreme gradient boosting (XGBoost);interpretable emotion classification;Shapley additive explanation (SHAP)","Electroencephalography;Feature extraction;Brain modeling;Emotion recognition;Computational modeling;Time-frequency analysis;Visualization","","5","","43","IEEE","14 Apr 2023","","","IEEE","IEEE Journals"
"EEG Emotion Recognition Using Dynamical Graph Convolutional Neural Networks","T. Song; W. Zheng; P. Song; Z. Cui","Key Laboratory of Child Development and Learning Science, Ministry of Education, School of Information Science and Engineering, Southeast University, Nanjing, China; Laboratory of Child Development and Learning Science, Ministry of Education, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; School of Computer and Control Engineering, Yantai University, Yantai, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, P.R. China",IEEE Transactions on Affective Computing,"14 Aug 2020","2020","11","3","532","541","In this paper, a multichannel EEG emotion recognition method based on a novel dynamical graph convolutional neural networks (DGCNN) is proposed. The basic idea of the proposed EEG emotion recognition method is to use a graph to model the multichannel EEG features and then perform EEG emotion classification based on this model. Different from the traditional graph convolutional neural networks (GCNN) methods, the proposed DGCNN method can dynamically learn the intrinsic relationship between different electroencephalogram (EEG) channels, represented by an adjacency matrix, via training a neural network so as to benefit for more discriminative EEG feature extraction. Then, the learned adjacency matrix is used to learn more discriminative features for improving the EEG emotion recognition. We conduct extensive experiments on the SJTU emotion EEG dataset (SEED) and DREAMER dataset. The experimental results demonstrate that the proposed method achieves better recognition performance than the state-of-the-art methods, in which the average recognition accuracy of 90.4 percent is achieved for subject dependent experiment while 79.95 percent for subject independent cross-validation one on the SEED database, and the average accuracies of 86.23, 84.54 and 85.02 percent are respectively obtained for valence, arousal and dominance classifications on the DREAMER database.","1949-3045","","10.1109/TAFFC.2018.2817622","National Basic Research Program of China (973 Program)(grant numbers:2015CB351704); National Natural Science Foundation of China(grant numbers:61572009,61703360,61772276); Key Research and Development Program of Jiangsu Province, China(grant numbers:BE2016616); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320798","EEG emotion recognition;adjacency matrix;graph convolutional neural networks (GCNN);dynamical convolutional neural networks (DGCNN)","Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Convolutional neural networks;Convolution;Biological neural networks","","842","","61","IEEE","21 Mar 2018","","","IEEE","IEEE Journals"
"EEG based patient emotion monitoring using relative wavelet energy feature and Back Propagation Neural Network","P. D. Purnamasari; A. A. P. Ratna; B. Kusumoputro","Department of Electrical Engineering, Universitas Indonesia, Depok, Indonesia; Universitas Indonesia, Depok, Jawa Barat, ID; Department of Electrical Engineering, Universitas Indonesia, Depok, Indonesia",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"5 Nov 2015","2015","","","2820","2823","In EEG-based emotion recognition, feature extraction is as important as the classification algorithm. A good choice of features results in higher recognition rate. However, there is no standard method for feature extraction in EEG-based emotion recognition, especially for real time monitoring, where speed of computation is crucial. In this work, we assess the use of relative wavelet energy as features and Back Propagation Neural Network (BPNN) as classifier for emotion recognition. This method was implemented in simulated real time emotion recognition by using a publicly accessible database. The results showed that relative wavelet energy and BPNN achieved an average recognition rate of 92.03%. The highest average recognition rate was achieved when the time window was 30s.","1558-4615","978-1-4244-9271-8","10.1109/EMBC.2015.7318978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318978","emotion recognition;emotion monitoring;brain wave;EEG;wavelet energy","Emotion recognition;Databases;Electroencephalography;Discrete wavelet transforms;Feature extraction;Psychology;Principal component analysis","Algorithms;Electroencephalography;Emotions;Humans;Monitoring, Physiologic","12","","12","IEEE","5 Nov 2015","","","IEEE","IEEE Conferences"
"Combined analysis of GSR and EEG signals for emotion recognition","P. Tarnowski; M. Kołodziej; A. Majkowski; R. J. Rak","Measurement and Information Systems, Institute of Theory of Electrical Engineering, Warszawa; Measurement and Information Systems, Institute of Theory of Electrical Engineering, Warszawa; Measurement and Information Systems, Institute of Theory of Electrical Engineering, Warszawa; Measurement and Information Systems, Institute of Theory of Electrical Engineering, Warszawa",2018 International Interdisciplinary PhD Workshop (IIPhDW),"21 Jun 2018","2018","","","137","141","An article presents the results of research related to the detection of emotions using combined analysis of galvanic skin response (GSR) and electroencephalographic (EEG) signals. Twenty seven volunteers participated in the experiment. Emotions were evoked by presentation of a set of twenty one movies. Emotions, evoked by individual movies, were later rated by participants according to valence and arousal. GSR signal was used to indicate the most stimulating movies, then features extracted from EEG signal were used to classify emotions. To determine the features EEG signal was analyzed in the frequency domain using fast Fourier transform (FFT) algorithm. For classifying emotions, according to valence and arousal, two classifiers were implemented: support vector machine (SVM) and k-nearest neighbors (k-NN).","","978-1-5386-6143-7","10.1109/IIPHDW.2018.8388342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8388342","galvanic skin response (GSR);electroencephalography (EEG);emotion recognition;arousal;valence;classification","Motion pictures;Electroencephalography;Electrodes;Emotion recognition;Skin;Feature extraction;Thumb","","19","","21","IEEE","21 Jun 2018","","","IEEE","IEEE Conferences"
"An RFID-Based BCI System for Emotion Detection Using EEG patterns","A. Mishra; A. Singh; A. Ujlayan","Amity School of Engineering & Technology, Amity University Uttar Pradesh, Noida, India; Amity School of Engineering & Technology, Amity University Uttar Pradesh, Noida, India; School of Vocational Studies and Applied Sciences, Gautam Buddha University, Greater Noida, India",2021 IEEE International Conference on RFID Technology and Applications (RFID-TA),"30 Nov 2021","2021","","","5","8","Emotion categorization using electroencephalogram (EEG) signals can be a difficult task and requires cognitive analysis of EEG patterns for mapping them to a specific emotion. Such a system can be utilized in many ways. The concept of emotions is itself complex and fuzzy in nature. Therefore, to handle the fuzziness of individual’s emotional experiences and presenting them as a discrete emotion is all that is required by many EEG-based brain computer interface (BCI) systems. In this work, we present an RFID-based BCI system for Emotion classification from EEG patterns. The presented system uses a fuzzy inference system (FIS) that utilizes the valence and arousal modalities to model the individual’s emotion. The major steps of the system are: preprocessing recorded EEG patterns and saving them with individual’s identification using RFID tags, then preprocessed EEG patterns are decomposed into different frequency bands using wavelet decomposition, followed by feature extraction step, feature matrix so obtained is used for training a k-nearest neighbor model to generate valence and arousal values for given EEG patterns. The output of KNN is fed to the FIS which models the pattern as a discrete emotion. system gives a new dimension to the BCI-based content servers and can also be used in healthcare services which is the need of the hour.","","978-1-6654-2657-2","10.1109/RFID-TA53372.2021.9617423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9617423","Fuzzy inference system;EEG;emotion classification;BCI;RFID","Fuzzy logic;Training;Medical services;Brain modeling;Feature extraction;Electroencephalography;Brain-computer interfaces","","","","23","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Expression-EEG Based Collaborative Multimodal Emotion Recognition Using Deep AutoEncoder","H. Zhang","Department of Educational Technology, Inner Mongolia Normal University, Hohhot, China",IEEE Access,"17 Sep 2020","2020","8","","164130","164143","Emotion recognition has shown many valuable roles in people's lives under the background of artificial intelligence technology. However, most existing emotion recognition methods have poor recognition performance, which prevents their promotion in practical applications. To alleviate this problem, we proposed an expression-EEG interaction multi-modal emotion recognition method using a deep automatic encoder. Firstly, decision tree is applied as objective feature selection method. Then, based on the facial expression features recognized by sparse representation, the solution vector coefficients are analyzed to determine the facial expression category of the test samples. After that, the bimodal deep automatic encoder is adopted to fuse the EEG signals and facial expression signals. The third layer of BDAE extracts features for training of supervised learning. Finally, LIBSVM classifier is used to complete classification task. We carried out experiments on a constructed video library to verify the proposed emotion recognition method. The results show that the proposed method can effectively extract and integrate high-level emotion-related features in EEG and facial expression signals. The recognition rate of discrete emotion state type and the average emotion recognition rate have been improved relatively, in which the average emotion recognition rate is 85.71%. Overall, the emotion recognition ability has been greatly improved.","2169-3536","","10.1109/ACCESS.2020.3021994","Key Technology Project of Inner Mongolia Autonomous Region(grant numbers:2020GG0170); Natural Science Foundation Project of Inner Mongolia Autonomous Region(grant numbers:2015MS0634); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187342","EEG signals;facial expression features;multi-modal fusion;deep automatic encoder;emotion recognition;decision tree;sparse representation","Emotion recognition;Feature extraction;Electroencephalography;Face recognition;Diseases;Brain modeling;Physiology","","71","","40","CCBY","7 Sep 2020","","","IEEE","IEEE Journals"
"Multi-Modal Emotion Recognition Using EEG and Eye Tracking Features","P. Iacono; N. Khan","Electrical and Computer Engineering, Toronto Metropolitan University; Electrical and Computer Engineering, Toronto Metropolitan University",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","5","Multi-modal emotion recognition from various human physiological indicators has emerged as a large topic of interest, including the use of EEG, ECG, GSR and Eye Tracking features. This work introduced a simple CNN based multi-modal EEG and Eye Tracking emotion recognition model for the SEED V dataset. In contrast to other works on the SEED V dataset, different Differential Entropy time windows were tested for EEG feature extraction. EEG signals were arranged in a 2D image format to preserve spatial relationships between electrode placements on patients during the trials. The proposed model with a 1 second processing window for EEG features achieved state of the art results in Leave One Subject Out Validation, with a mean accuracy of 0.935 ± 0.038 on the SEED V dataset. A noticeable improvement was noted over the same multi-modal model using a 4 second processing window for EEG features, highlighting the importance of smaller time windows for EEG feature processing in emotion recognition problems.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10781843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781843","","Emotion recognition;Accuracy;Biological system modeling;Gaze tracking;Brain modeling;Feature extraction;Electroencephalography;Data models;Physiology;Entropy","Electroencephalography;Humans;Emotions;Eye-Tracking Technology;Signal Processing, Computer-Assisted;Algorithms;Neural Networks, Computer","","","12","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Cloud-Based Human Emotion Classification Model from EEG Signals","S. Jamal; M. V. Cruz; J. Kim","Department of Information Technology, Georgia Southern University, Statesboro, GA, U.S.A; Department of Information Technology, Georgia Southern University, Statesboro, GA, U.S.A; Department of Information Technology, Georgia Southern University, Statesboro, GA, U.S.A","2023 IEEE 14th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","17 Nov 2023","2023","","","0057","0064","Human emotions are complex, mutiifaceted phenomena, defined as multidimensional subjective experiences influenced by several factors like cognitive processing, social norms, genetics etc. Emotion recognition can be stated as a vital aspect of artificial intelligence which is intensively utilized in the enhancement of human-brain-computer interface in recent days. The complexity of emotional analysis can be dealt with in depth by classifying and analyzing the physiological signals over non-physiological signals. A potential non-invasive technique for measuring the electrical activities of the human brain technology is Electroencephalography (EEG). In response to different stimuli, the event-related-potentials can be measured by EEG to classify different emotional states with the help of machine learning algorithms. This study focuses on building machine learning models (Logistic Regression, Decision Forest, and Boosted Decision Tree) for classifying emotional stages (positive, negative, and neutral) on a cloud-based environment, Microsoft Azure Cloud. It concludes by comparing the classification models by gauging the evaluation metrics, i.e., precision, recall, and accuracy. The highest accuracy is achieved by the Boosted Decision Tree model which is around 99% and this has surpassed the performance of other existing state-of-art models for human emotion classification. The obtained accuracy for Decision Forest and Logistic Regression are consecutively 96% and 93%. To generate an optimal result and avoid overfitting/over-learning issues, some remarkable measures have been undertaken, i.e., tuning and iterating of hyperparameters, optimizing tree depth and nodes numbers, controlling random seeds etc. Our valuable findings will provide significant insights into psychological research to collaborate human-brain-computer interface with artificial intelligence.","","979-8-3503-0413-8","10.1109/UEMCON59035.2023.10316087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10316087","Artificial intelligence;EEG;machine learning;cloud computing;Microsoft Azure;brain computer interface","Logistic regression;Machine learning algorithms;Psychology;Forestry;Brain modeling;Mobile communication;Electroencephalography","","4","","31","IEEE","17 Nov 2023","","","IEEE","IEEE Conferences"
"Detection of Emotion from EEG Signal Using Deep Learning: Bi-LSTM and GRU","M. H. Maliha; A. R. Lopa; M. H. Chowdhury","Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, Chittagong, Bangladesh; Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, Chittagong, Bangladesh; Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, Chittagong, Bangladesh",2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT),"23 May 2024","2024","","","131","136","Emotion states are linked to a wide range of human feelings, thoughts, and actions. They affect our ability to act properly in situations like making decisions, perceiving things, and being smart. So, research on recognizing emotions through emotional signs makes Brain Computer Interface (BCI) systems a better subject for clinical uses and social interactions between people. Electroencephalogram (EEG) signal is an electrophysiological method for brain signal recording. Human emotion arises from the brain, so the detection of emotion can be done by using EEG signals to decide human psychological change during the different emotional states. The aim of this study is to classify human emotion named arousal, valence, dominance and liking. The classification has been done for both binary and multiclass classification from EEG signals. Publicly available “DEAP” dataset is engaged in this work, and EEG recording from 13 channels was used. Fast Fourier Transform (FFT) has been used to separate frequency bands from the EEG signal, and then Power Spectrum Density (PSD) features have been extracted. Two deep learning methods, Bidirectional Long Short-Term Memory (Bi-LSTM) and Gated Recurrent Unit (GRU) have been used for the classification of emotion. We found that the GRU+Bi-LSTM model gives the best result for the binary classification with 96.53% accuracy and Bi-LSTM can predict best accuracy of 92.36% for multiclass classification.","2769-5700","979-8-3503-8577-9","10.1109/ICEEICT62016.2024.10534367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534367","arousal;Bi-LSTM;dominance;GRU;liking;valence","Deep learning;Training;Emotion recognition;Fast Fourier transforms;Brain modeling;Feature extraction;Electroencephalography","","","","17","IEEE","23 May 2024","","","IEEE","IEEE Conferences"
"Enhancing EEG Analysis for Rapid Brain Activities Detection in Patients","A. Yadav; R. K. Ahmed; A. R; S. S. Mercy","Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Chennai; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Chennai; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Chennai; Department of Computer Science and Design, R.M.K. Engineering College, Chennai",2024 First International Conference for Women in Computing (InCoWoCo),"6 Feb 2025","2024","","","1","7","Electroencephalogram (EEG) analysis plays a critical role in diagnosing various neurological conditions by detecting abnormal brain activities. However, the complex nature of EEG signals poses challenges for traditional analysis methods, which often struggle to handle diverse and high-dimensional data effectively. This paper develops a new technique of EEG signal classification using machine learning and deep learning models. Specifically, attention has been given to the CatBoost, ResNet34D, EfficientNetB0, and EfficientNetB2 models. The proposed approach utilized a vast EEG dataset with 11,000 samples of spectrograms, and it conducted an in-depth comparison of these models. Data preprocessing included features developed with statistical measures and the formation of uniform EEG spectrograms. All these models were evaluated in terms of their KL divergence score, defined as a measure of the divergence between the predicted and target distributions. From all the models considered, CatBoost proved to be one with an excellent performance, having had a KL score value at 0.78 compared to that of deep learning models. This work demonstrates the potential of gradient boosting for EEG-activity classification tasks and can be of benefit as a source of insight for interested applications in real-time brain activity monitoring and diagnostics of neurological issues.","","979-8-3315-1894-3","10.1109/InCoWoCo64194.2024.10863372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10863372","EEG Signal Classification;CatBoost;Deep Learning;ResNet34D;EfficientNet;Kullback-Leibler Divergence;Brain Activity Detection;Feature Engineering","Deep learning;Accuracy;Computational modeling;Pattern classification;Brain modeling;Electroencephalography;Real-time systems;Computational efficiency;Spectrogram;Long short term memory","","","","18","IEEE","6 Feb 2025","","","IEEE","IEEE Conferences"
"Efficient Channel Attention Based Hybrid Networks for EEG Emotion Recognition","T. Wang; D. Wang","College of Electrical Engineering, Qingdao University, Qingdao, China; College of Electrical Engineering, Qingdao University, Qingdao, China",2024 IEEE 7th International Conference on Information Systems and Computer Aided Education (ICISCAE),"3 Dec 2024","2024","","","413","418","Electroencephalogram (EEG) emotion recognition which utilizes EEG signals to automatically identify and evaluate emotional states in humans, has seen applications of deep learning methods in recent times. In order to effectively utilize the channel features and time series properties of EEG, the efficient channel attention (ECA) based hybrid networks for EEG emotion recognition is introduced in this article. The hybrid networks consist of three parts: the one-dimensional convolutional network (1DCNN) is responsible for extracting features from the EEG data; the residual network embeds the ECA mechanism, which is designed to strengthen the extraction of channel features of the input data; the long short term memory network (LSTM) is employed to capture temporal relationships in the EEG sequences. The adaptive momentum estimation (Adam) optimization algorithm is used to update the parameters of the network, which can automatically adjust the learning rate of each parameter based on previous gradients. In the experiments on the DEAP dataset, ECA based hybrid networks achieves 99.50% and 99.42% accuracy for the binary and four-class tasks, respectively. Compared with previous methods, the proposed method is proved to have better multi classification performance.","2770-663X","979-8-3503-5076-0","10.1109/ICISCAE62304.2024.10761714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10761714","electroencephalogram;emotion recognition;hybrid networks;channel attention;multi-classification","Emotion recognition;Accuracy;Time series analysis;Feature extraction;Electroencephalography;Stability analysis;Data mining;Long short term memory;Optimization;Residual neural networks","","","","17","IEEE","3 Dec 2024","","","IEEE","IEEE Conferences"
"EEG-based emotion recognition based on kernel Fisher's discriminant analysis and spectral powers","Y. -H. Liu; W. -T. Cheng; Y. -T. Hsiao; C. -T. Wu; M. -D. Jeng","Department of Mechanical Engineering, Chung Yuan Christian University, Chungli, Taiwan; Department of Mechanical Engineering, Chung Yuan Christian University, Chungli, Taiwan; Department of Mechanical Engineering, Chung Yuan Christian University, Chungli, Taiwan; School of Occupational Therapy, College of Medicine National Taiwan University, Taiwan; Department of Electrical Engineering, National Taiwan Ocean University, Taiwan","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","4 Dec 2014","2014","","","2221","2225","In this paper, a feature extraction method called kernel Fisher's emotion pattern (KFEP) based on the kernel Fisher's discriminant analysis and spectral powers of multiple EEG rhythms is proposed for emotion recognition. An emotion-induction paradigm is designed for emotional EEG data collection, where a set of pictures selected from the International Affective Picture System (IAPS) are used as the emotion induction stimuli. Experimental results indicate that the KFEP feature performs better than the commonly used spectral power features. Our proposed KFEP achieves high classification accuracies of valence (78.49%) and arousal (81.93%).","1062-922X","978-1-4799-3840-7","10.1109/SMC.2014.6974254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974254","EEG;emotion recognition;kernel Fisher's discriminant analysis;brain-computer interface","Electroencephalography;Feature extraction;Kernel;Emotion recognition;Principal component analysis;Brain modeling;Vectors","","9","","38","IEEE","4 Dec 2014","","","IEEE","IEEE Conferences"
"Eeg Feature Selection Using Orthogonal Regression: Application to Emotion Recognition","X. Xu; F. Wei; Z. Zhu; J. Liu; X. Wu","The School of Artificial Intelligence, Beijing Normal University, CN; The School of Artificial Intelligence, Beijing Normal University, CN; The School of Artificial Intelligence, Beijing Normal University, CN; The School of Artificial Intelligence, Beijing Normal University, CN; The School of Artificial Intelligence, Beijing Normal University, CN","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","1239","1243","A common drawback of the EEG applications is that the volume conduction of human head leads to lots of redundant information in EEG recordings. To reduce the redundancy and choose informative EEG features, in this paper, we propose an EEG feature selection technique, termed as Feature Selection with Orthogonal Regression (FSOR). Compared with classical feature selection methods, for nonlinear and nonstationary EEG signals, FSOR can employ orthogonal regression to preserve more discriminative information in the subspace. To verify the EEG feature selection performance, we collected a multichannel EEG dataset for emotion recognition and compared FSOR with two popular feature selection methods. The experimental results demonstrate the advantage of FSOR method over others for reducing the redundant information among the EEG relevant features. Additionally, we found that the absolute power ratio of beta wave to theta wave is the most discriminative feature, and beta band is the critical band for emotion recognition.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054457","EEG feature selection;discriminative feature;redundant information;orthogonal regression;embedded approaches","Emotion recognition;Redundancy;Signal processing;Feature extraction;Electroencephalography;Magnetic heads;Speech processing","","12","","22","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"EEG Emotion Detection Using Multi-Model Classification","M. A. Abdullah; L. R. Christensen",Sudan University of Science and Technology; IT University of Copenhagen,2018 International Conference on Bioinformatics and Systems Biology (BSB),"25 Jul 2019","2018","","","178","182","In the field of Human-Machine Interaction HMI identifying the human emotion is an important input for the machines to better enhance the communication. In this paper, we identifying 10 different emotions based on Valance, Arousal and Dominance using five different models and using new filters using Riemannian geometry to enhance classification. EEG signals are collected and passed to the proposed models, the accuracy of the detection was ranging from 50% to 70%. Two sessions have been conducted per subject to collect the data for training and for testing the models. The SAM assessment technique has been used to tag the training data.","","978-1-5386-6434-6","10.1109/BSB.2018.8770695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770695","EEG;Emotion Recognition;Emotion Detection;HMI;BCI;Riemannian Geometry;Tangent Space","Brain modeling;Electroencephalography;Data models;Covariance matrices;Feature extraction;Classification algorithms;Filtering","","4","","17","IEEE","25 Jul 2019","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition in Music Listening","Y. -P. Lin; C. -H. Wang; T. -P. Jung; T. -L. Wu; S. -K. Jeng; J. -R. Duann; J. -H. Chen","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Neurology, Cardinal Tien Hospital, Taipei, Taiwan; Swartz Center of Computational Neuroscience, University of California, San Diego, CA, USA; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Institute of Neural Computation, University of California, San Diego, CA, USA; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan",IEEE Transactions on Biomedical Engineering,"14 Jun 2010","2010","57","7","1798","1806","Ongoing brain activity can be recorded as electroen-cephalograph (EEG) to discover the links between emotional states and brain activity. This study applied machine-learning algorithms to categorize EEG dynamics according to subject self-reported emotional states during music listening. A framework was proposed to optimize EEG-based emotion recognition by systematically 1) seeking emotion-specific EEG features and 2) exploring the efficacy of the classifiers. Support vector machine was employed to classify four emotional states (joy, anger, sadness, and pleasure) and obtained an averaged classification accuracy of 82.29% ± 3.06% across 26 subjects. Further, this study identified 30 subject-independent features that were most relevant to emotional processing across subjects and explored the feasibility of using fewer electrodes to characterize the EEG dynamics during music listening. The identified features were primarily derived from electrodes placed near the frontal and the parietal lobes, consistent with many of the findings in the literature. This study might lead to a practical system for noninvasive assessment of the emotional states in practical or clinical applications.","1558-2531","","10.1109/TBME.2010.2048568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5458075","EEG;emotion;machine learning;music","Emotion recognition;Electroencephalography;Brain;Electrodes;Hospitals;Humans;Electromyography;IEEE activities;Support vector machines;Support vector machine classification","Adult;Algorithms;Artificial Intelligence;Bayes Theorem;Electrodes;Electroencephalography;Emotions;Evoked Potentials, Auditory;Female;Humans;Male;Music;Pattern Recognition, Automated;Signal Processing, Computer-Assisted","668","1","38","IEEE","3 May 2010","","","IEEE","IEEE Journals"
"Automated emotion identification system utilizing EEG bands extracted via wavelet filter banks","D. Pachori; T. K. Gandhi","Department of Electronics and Communication Engineering, Indian Institute of Information Technology Nagpur, Nagpur, Maharashtra, India; Department of Electrical Engineering, Indian Institute of Technology Delhi, New Delhi, India",2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"4 Nov 2024","2024","","","1","6","In this paper, a new framework in order to identify the human emotions for three classes using the electroencephalogram (EEG) signals has been proposed. It is difficult to reliably and effectively identify different kinds of emotion from nonstationary EEG data. With the EEG recordings, appropriate signal processing and machine learning techniques can be used to design an automated system in order to identify human emotions. The proposed framework is evaluated through extensive experiments utilizing the SJTU Emotion EEG dataset (SEED). The EEG signal is decomposed into several bands by the wavelet filter banks technique. Then from each band, two features are computed which are wavelet energy and Shannon entropy, and then both features are fused to obtain a combined feature. By using support vector machine (SVM), the features are classified into various different emotion states such as positive, neutral, and negative. An accuracy of $\mathbf{8 4. 8 \%}$ is achieved for human emotion detection using EEG signals in the proposed framework. The proposed human emotion recognition algorithm finds applications in brain-computer interface, psychology, and human-machine interface fields.","2473-7674","979-8-3503-7024-9","10.1109/ICCCNT61001.2024.10725942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10725942","Emotion;EEG signals;bands;signal processing;human emotion recognition;wavelet filter banks technique","Support vector machines;Emotion recognition;Accuracy;Filter banks;Signal processing algorithms;Machine learning;Electroencephalography;Entropy;Reliability;Signal resolution","","","","37","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition Using Temporal Convolutional Network and Vision Transformer","Y. Su; Y. Zhou; X. Li; Q. Cai; Y. Liu","College of Computer Science and Engineering, Northwest Normal University, Lanzhou, China; College of Computer Science and Engineering, Northwest Normal University, Lanzhou, China; College of Computer Science and Engineering, Northwest Normal University, Lanzhou, China; College of Computer Science and Engineering, Northwest Normal University, Lanzhou, China; College of Computer Science and Engineering, Northwest Normal University, Lanzhou, China",2024 International Joint Conference on Neural Networks (IJCNN),"9 Sep 2024","2024","","","1","8","Electroencephalogram (EEG)-based emotion recognition is a crucial component of affective computing. While the Transformer has proven successful in computer vision, its application in the EEG remains a challenge. This paper introduces a novel model, the Temporal Convolutional Vision Transformer (TC-ViT), seamlessly integrating the architectures of the Temporal Convolutional Network (TCN) and Vision Transformer (ViT), which employs the TCN module to extract preliminary spectral-spatial features from EEG and the ViT module to enhance the extracted features discriminative capability. Emotion classification is executed using a multilayer perceptron. The proposed TC-ViT model demonstrates exceptional performance on the SEED (99.31%) and SEED-IV (93.72%) public datasets, outperforming state-of-the-art methods in EEG-based emotion recognition.","2161-4407","979-8-3503-5931-2","10.1109/IJCNN60899.2024.10651111","Innovation Fund; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10651111","Emotion recognition;EEG;TCN;ViT","Computer vision;Emotion recognition;Convolution;Computational modeling;Computer architecture;Feature extraction;Transformers","","","","27","IEEE","9 Sep 2024","","","IEEE","IEEE Conferences"
"Design of Low-power EEG Detection System for Emotion Classification","Z. Mo; Y. Yuan; W. Shi; J. Mai; D. Zeng; X. Liu; G. Li","College of Electronic and Information Engineering, Shenzhen University, Shenzhen, China; Geehy Microelectronics Inc., Zhuhai, China; College of Electronic and Information Engineering, Shenzhen University, Shenzhen, China; College of Electronic and Information Engineering, Shenzhen University, Shenzhen, China; College of Electronic and Information Engineering, Shenzhen University, Shenzhen, China; Geehy Microelectronics Inc., Zhuhai, China; Geehy Microelectronics Inc., Zhuhai, China",2024 7th International Conference on Electronics Technology (ICET),"18 Sep 2024","2024","","","1041","1045","This paper proposes an emotion classification system by EEG signal processing and detecting, with hardware implementation and verifications via FPGA platform. The results show that the classification accuracy of the proposed EEG emotion classification system is 78.11%, which is about 2% different from the simulation results of Matlab software. This design also uses approximate unit to achieve low power consumption. Comparing the hardware resource consumption and classification accuracy with other state-of-art EEG emotion classification systems, the system proposed in this paper has advantages in hardware resource consumption when the classification accuracy is not significantly reduced, which indicate that this proposed design can be competent module in wearable systems.","2768-6515","979-8-3503-6395-1","10.1109/ICET61945.2024.10673238","Research and Development; National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10673238","EEG signal;emotion classification;Fast Fourier transform;Support Vector Machine;FPGA","Accuracy;Simulation;Transforms;Signal processing;Electroencephalography;Hardware;Vectors","","","","11","IEEE","18 Sep 2024","","","IEEE","IEEE Conferences"
"A smart HMI for driving safety using emotion prediction of EEG signals","G. S. Thirunavukkarasu; H. Abdi; N. Mohajer","Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Australia; Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Australia; Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Australia","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","9 Feb 2017","2016","","","004148","004153","This paper provides an overview on the past pieces of literature on emotion prediction systems and the different machine learning algorithms used to classify emotions. We propose a system which incorporates the emotion prediction system with a custom Smart Human Machine Interface (SHMI) for vehicle drivers to improve drive safety. This is achieved based on EEG signals and basic vehicle information's obtained from an OBD (On-Board Diagnostics) data. EEG signals are classified into four emotional states: happy, sad, relaxed and angry. In this paper, we present an initial development of the Smart Human Machine Interface (SHMI) for emotion detection for vehicle applications. To evaluate the classification of the EEG signals we use Russell's circumflex model, Higuchi Fractal Dimension (HFD), PSD (Power Spectral Density) for feature extraction and Support Vector Machines (SVM) for classification.","","978-1-5090-1897-0","10.1109/SMC.2016.7844882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844882","Electroencephalogram (EEG) Sensors;Smart Human Machine Interface (SHMI);Support Vector Machines (SVM);PSD (Power Spectral Density);OBD (On-Board Diagnostics);Higuchi Fractal Dimension (HFD)","Support vector machines;Music;Electroencephalography;Niobium;Feature extraction;Modems;Emotion recognition","","15","","45","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Multi channel brain EEG signals based emotional arousal classification with unsupervised feature learning using autoencoders","D. Ayata; Y. Yaslan; M. Kamasak","Faculty of Computer and Informatics, Istanbul Technical University, Istanbul, Turkey; Faculty of Computer and Informatics, Istanbul Technical University, Istanbul, Turkey; Faculty of Computer and Informatics, Istanbul Technical University, Istanbul, Turkey",2017 25th Signal Processing and Communications Applications Conference (SIU),"29 Jun 2017","2017","","","1","4","The importance of learning important features in an automatic manner is growing exponentially as the volume of data and number of systems using pattern recognition techniques continue to increase. In this paper, arousal recognition from multi channels EEG signals was conducted using human crafted statistical features and learned features from 32 different EEG source channels. We have obtained 98.99% accuracy rate with unsupervised feature learning approach for Arousal classification. Unsupervised feature learning worked better compared to handcrafted feature approach.","","978-1-5090-6494-6","10.1109/SIU.2017.7960629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960629","Brain EEG Analysis;Arousal Recognition;Multi-channel sensor processing;Signal Processing;kNN;Random Forests;Autoencoders;Feature Learning;Brain Computer Interface","Electroencephalography;Feature extraction;Brain modeling;Training;Decision trees;Videos;Discrete wavelet transforms","","7","","29","IEEE","29 Jun 2017","","","IEEE","IEEE Conferences"
"Advanced Hybrid Model: Multimodal Emotion Recognition with Improved EEG Feature Fusion","S. Bharti; A. Garg; P. Sharma; A. Shandilya; P. Kanan; A. Kumar","Electronics and Communication (ECE), IGDTUW, Delhi, India; Electronics and Communication (ECE), IGDTUW, Delhi, India; Electronics and Communication (ECE), IGDTUW, Delhi, India; Electronics and Communication (ECE), IGDTUW, Delhi, India; Electronics and Communication (ECE), IGDTUW, Delhi, India; Electronics and Communication (ECE), IGDTUW, Delhi, India","2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)","20 Jan 2025","2024","","","1","5","Emotion is a common occurrence in daily life as a subjectively psychological reaction to environmental stimuli. Emotion recognition via EEG data is quickly becoming a multidisciplinary research subject due to advances in artificial intelligence and brain science. This study compares several machine learning techniques, including Support Vector Machine (SVM), K-nearest neighbour, Linear Discriminant Analysis, Logistic Regression, and Decision Trees. Existing works achieves enhancement on separate/single analysis based models. Each of these models is tested both with and without principal component analysis (PCA) for reducing the dimensionality. This work achieves sustainable computing on all models. Moreover, the epoch data from EEG sensor channels is analysed. This work analysed that in the 30th to 45th segmentation interval, PCA with SVM performed the best, yielding an F1-score of 84.73% with 98.01% recall. This paper summarises current advances and identifies important directions for future study, making it a useful resource for scholars and practitioners in the domains of machine learning and EEG analysis.","","979-8-3315-2871-3","10.1109/IDICAIEI61867.2024.10842815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10842815","brain computer interface (BCI);convolutional neural network (CNN);electroencephalography (EEG);emotion Detection;sensor networks","Support vector machines;Electrodes;Emotion recognition;Computational modeling;Machine learning;Brain modeling;Electroencephalography;Brain-computer interfaces;Convolutional neural networks;Principal component analysis","","","","37","IEEE","20 Jan 2025","","","IEEE","IEEE Conferences"
"Emotion Recognition Based On CNN","G. Cao; Y. Ma; X. Meng; Y. Gao; M. Meng","Institute of Intelligent Control and Robotics, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; Institute of Intelligent Control and Robotics, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; Institute of Intelligent Control and Robotics, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; Institute of Intelligent Control and Robotics, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; Institute of Intelligent Control and Robotics, Hangzhou Dianzi University, Hangzhou, Zhejiang, China",2019 Chinese Control Conference (CCC),"17 Oct 2019","2019","","","8627","8630","Emotion is a state that comprehensively represents human feeling, thought, behavior and it exists everywhere in daily life. Emotion recognition is an important interdisciplinary research topic in the fields of neuroscience, psychology, cognitive science, computer science and artificial intelligence. Neural network is a statistical learning model inspired by biological neural networks. This paper attempts to use the EEG signal from the DEAP data set to classify the emotion of the subjects, this data set represents the emotional classification research. Then the principal component analysis is used to reduce the dimension of the preprocessed EEG data, so the main emotional EEG features are obtained. Then the accuracy of the classification of the training samples and the test samples is tested by the CNN algorithm, and the other classification methods are compared to obtain the nerves. The network can be used as a robust classifier for brain signals even better than traditional learning techniques.","1934-1768","978-9-8815-6397-2","10.23919/ChiCC.2019.8866540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866540","Emotion recognition;DEAP;EEG;PCA;CNN;classification","Electroencephalography;Feature extraction;Brain modeling;Biological neural networks;Emotion recognition;Data models;Convolution","","27","","15","","17 Oct 2019","","","IEEE","IEEE Conferences"
"Ensemble Median Empirical Mode Decomposition for Emotion Recognition Using EEG Signal","P. Samal; M. F. Hashmi","National Institute of Technology, Warangal, India; National Institute of Technology, Warangal, India",IEEE Sensors Letters,"25 Apr 2023","2023","7","5","1","4","This letter investigates ensemble median empirical mode decomposition (MEEMD), an extension model of ensemble empirical mode decomposition, and its improved characteristics for emotion recognition. It is tough to extract the hidden patterns in the electroencephalography (EEG) signal due to the signals' nonstationary nature, which is caused by the brain's complex neuronal activity. This makes it difficult to identify emotions using EEG. This research presents a feature extraction method based on MEEMD for decoding EEG signals for emotion recognition. Analysis is done on the intrinsic mode functions (IMFs) that are retrieved by EEMD and MEEMD. When identifying emotions using multichannel EEG signals, features like power spectral density, relative powers, power ratios, entropies, mean, standard deviation, and variance are used as indications of valence and arousal scales. The results indicate that the suggested method has achieved accuracy rates of 74.3% for valence and 78% for arousal classes. DEAP EEG emotion dataset is used, and both EEMD and MEEMD models are used to evaluate the results.","2475-1472","","10.1109/LSENS.2023.3265682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097854","Sensor signal processing;electroencephalography (EEG);emotion recognition;ensemble empirical mode decomposition (EEMD);ensemble median empirical mode decomposition (MEEMD);intrinsic mode functions (IMFs)","Electroencephalography;Emotion recognition;Feature extraction;Empirical mode decomposition;Brain modeling;Support vector machines;Sensors","","21","","19","IEEE","10 Apr 2023","","","IEEE","IEEE Journals"
"EEG-Based Emotion Recognition Using Genetic Algorithm Optimized Multi-Layer Perceptron","S. Marjit; U. Talukdar; S. M. Hazarika","Dept. of Computer Sc. & Engg., IIIT Guwahati, Guwahati, India; Dept. of Computer Sc. & Engg., IIIT Guwahati, Guwahati, India; Dept. of Mechanical Engineering, Biomimetic Robotics and AI Lab, IIT Guwahati, India",2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA),"4 Nov 2021","2021","","","304","309","Emotion Recognition is an important problem within Affective Computing and Human Computer Interaction. In recent years, various machine learning models have provided significant progress in the field of emotion recognition. This paper proposes a framework for EEG-based emotion recognition using Multi Layer Perceptron (MLP). Power Spectral Density features were used for quantifying the emotions in terms of valence-arousal scale and MLP is used for classification. Genetic algorithm is used to optimize the architecture of MLP. The proposed model identifies a. two classes of emotions viz. Low/High Valence with an average accuracy of 91.10% and Low/High Arousal with an average accuracy of 91.02%, b. four classes of emotions viz. High Valence-Low Arousal (HVLA), High Valence-High Arousal (HVHA), Low Valence-Low Arousal (LVLA) and Low Valence-High Arousal (HVHA) with 83.52% accuracy. The reported results are better compared to existing results in the literature.","","978-1-6654-3323-5","10.1109/IRIA53009.2021.9588702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9588702","EEG;Emotions;Power Spectral Density;Multi-Layer Perceptron;Genetic Algorithm","Electrodes;Emotion recognition;Machine learning algorithms;Frontal lobe;Service robots;Machine learning;Computer architecture","","9","","31","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"Emotion Recognition Method Using U-Net Neural Network With Multichannel EEG Features and Differential Entropy Characteristics","H. Huang; Y. Deng; B. Hao; W. Liu; X. Tu; G. Zeng","Department of Computer Applications, Chengdu College of University of Electronic Science and Technology of China, Chengdu, China; Department of Computer Applications, Chengdu College of University of Electronic Science and Technology of China, Chengdu, China; Chengdu College of University of Electronic Science and Technology of China, Chengdu, China; Chengdu College of University of Electronic Science and Technology of China, Chengdu, China; Chengdu College of University of Electronic Science and Technology of China, Chengdu, China; Department of General Surgery, West China Hospital, Division of Vascular Surgery, Sichuan University, Chengdu, China",IEEE Access,"8 Apr 2025","2025","13","","59377","59389","To improve accuracy in EEG-based emotion recognition and address current research limitations, providing a more robust framework for scientific study and mental health support. This paper presents an emotion recognition method tailored for small sample training data, effectively addressing the issue of emotion classification based on multichannel continuous electroencephalogram (EEG) signals. The proposed method first extracts differential entropy features from EEG signals and maps them into two-dimensional matrices based on their spectral characteristics. These preprocessed original data and five different frequency band matrices are then stacked into three-dimensional multichannel data, which are fed into the 3D-EEGU-Net model for classification. Experiments conducted on the public SEED dataset demonstrate an emotion recognition accuracy of 92.34%. Furthermore, to enhance the accuracy of emotion recognition for specific populations, the size of data blocks (patches) and model hyperparameters were dynamically adjusted. In the valence-based emotion ternary classification task conducted on the DEAP public dataset, the method achieved an emotion recognition accuracy of 72.02%. Comparative experiments show that the proposed method exhibits significant superiority in emotion recognition performance on both datasets. This research not only provides a new approach to EEG emotion recognition but also holds substantial potential for applications in detecting psychological disorders and physical conditions in individuals from specialized occupational groups.","2169-3536","","10.1109/ACCESS.2024.3497160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10752561","Neurofeedback;emotion recognition;EEG U-Net;electroencephalogram;differential entropy features","Emotion recognition;Solid modeling;Accuracy;Frequency-domain analysis;Transfer learning;Brain modeling;Feature extraction;Electroencephalography;Entropy;Data models","","","","39","CCBYNCND","13 Nov 2024","","","IEEE","IEEE Journals"
"EEG-based emotion recognition using empirical wavelet transform","D. Huang; S. Zhang; Y. Zhang","School of Computer and Information Technology, Liaoning Normal University, Dalian, China; School of Computer and Information Technology, Liaoning Normal University, Dalian, China; School of Computer and Information Technology, Liaoning Normal University, Dalian, China",2017 4th International Conference on Systems and Informatics (ICSAI),"8 Jan 2018","2017","","","1444","1449","Emotion recognition has a prominent status in the applications of brain-machine interface. An approach on recognizing Electroencephalography (EEG) emotion using empirical wavelet transform (EWT) and autoregressive (AR) model is given in this paper. The proposed method chooses two channels in a certain time segment to perform feature extraction. The EWT is first used to decompose EEG-based emotion data into several empirical modes, and then AR coefficients are calculated based on the selected empirical modes. Furthermore, these features constitute a feature vector and are then input into a classifier to perform emotion recognition. This paper implemented multiple experiments to verify the performance of our proposed approach on DEAP dataset. The best recognition rate of our approach achieves 67.3% for arousal dimension and 64.3% for valence dimension. The obtained results show that our proposed approach is superior to some exist methods.","","978-1-5386-1107-4","10.1109/ICSAI.2017.8248513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8248513","Empirical wavelet transform;autoregressive model;EEG;emotion recognition;support vector classifier","Brain modeling;Electroencephalography;Mathematical model;Emotion recognition;Wavelet transforms;Feature extraction","","14","","25","IEEE","8 Jan 2018","","","IEEE","IEEE Conferences"
"LUR: An Online Learning Model for EEG Emotion Recognition","G. Cao; L. Yang; Q. Zhang; J. Xi; C. Tang; Y. Tian","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Biomedical Engineering, Guangzhou Medical University, Guangzhou, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China",2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Jan 2024","2023","","","1038","1045","Emotion recognition based on EEG (Electroen-cephalogram) has been widely used in may scenarios, such as Brain-computer interface, medical health and entertainment, etc. However, large differences exist in subjects due to individual characteristics, and EEG data of different time periods usually distribute inconsistently, which hinder the further development of EEG-based research. In this paper, we propose a LUR model to partially address these challenges. In the first stage, we extracted candidate features based on neuroscience research and learned them using SVM or Naive Bayes algorithm. If the performance during the validation phase is satisfactory, we keep the features. Otherwise we replace the candidate features and use an online learning method named LUR(Learn, Unlearn, and Relearn), which continuously prunes the irrelevant connections of the current data and retains important connections to constructing a model more suitable for the subject. Because the proposed model is based on streaming data, it can continuously relearn and solve the problem that the input data is not i.i.d. to a certain extent. Experiments were conducted on two public datasets, DEAP and DREAMER, and competitive results were obtained.","2156-1133","979-8-3503-3748-8","10.1109/BIBM58861.2023.10385595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385595","Online Learning;Hemispheric Asymmetry;Affective Computing;EEG;Real-time Emotion Classification","Support vector machines;Emotion recognition;Neuroscience;Training data;Psychology;Feature extraction;Brain modeling","","1","","54","IEEE","18 Jan 2024","","","IEEE","IEEE Conferences"
"CFBC: A Network for EEG Emotion Recognition by Selecting the Information of Crucial Frequency Bands","M. Zhu; Z. Bai; Q. Wu; J. Wang; W. Xu; Y. Song; Q. Gao","Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, TUT Maritime College, Tianjin University of Technology, Tianjin, China",IEEE Sensors Journal,"2 Oct 2024","2024","24","19","30451","30461","In this work, inspired by the frequency band division theory of electroencephalogram (EEG) signals, we propose the crucial frequency band convolution (CFBC) network method to explore the crucial frequency range of frequency domain features. CFBC includes two parts: spatial feature extractor and crucial frequency band selector. First, we extracted differential entropy (DE) and power spectral density (PSD) features from the frequency domain of each EEG channel by different frequency bands. To avoid the loss of effective spatial information in processing, we interpolate the EEG feature maps by the relative location of electrodes. The spatial feature extractor captures spatial information using channel-by-channel convolution with the 2-D EEG feature maps containing electrode position information. The crucial frequency band selector performs causal dilated convolution for the selected frequency band sequence so that the features contained in different frequency bands are stacked into a feature vector. Finally, CFBC realizes the purpose of combining multiple selected frequency bands to extract the cross-frequency band feature vector. To evaluate the proposed method, we conducted a subject-dependent EEG emotion recognition experiment in the SEED dataset. The experimental result shows that the selection of a frequency band has an impact on the emotion classification effect of frequency domain features.","1558-1748","","10.1109/JSEN.2024.3440340","National Natural Science Foundation of China(grant numbers:62103299); Tianjin University of Technology Education Foundation(grant numbers:TJ22-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10639352","Crucial frequency band selector;electroencephalography (EEG);emotion recognition;spatial feature extractor;spatial-frequency feature","Feature extraction;Electroencephalography;Emotion recognition;Frequency conversion;Frequency-domain analysis;Convolutional neural networks;Brain modeling","","","","48","IEEE","19 Aug 2024","","","IEEE","IEEE Journals"
"A research on estimation of emotion using EEG signals and brain computer interfaces","N. Yıldırım; A. Varol","Software Engineering Department, Firat University, Elazig, Turkey; Software Engineering Department, Firat University, Elazig, Turkey",2017 International Conference on Computer Science and Engineering (UBMK),"2 Nov 2017","2017","","","1132","1136","The brain produces weak electrical signals that can be measured from the skull. Electroencephalography (EEG) is a method that provides monitoring electrical activity of the brain with the electrical methods. Brain Computer Interface (BCI) is a system that converts the electrical signals produced by the brain to the signals that can be interpreted by a computer or an electronic system. Brain Computer Interface aims to produce the result on developed computer systems rather than creating a response in the body from the signals sent by the brain. Emotion is defined as a psychophysiological change in the mood of a person that emerges by interaction with biochemical and environmental effects. There are many methods on emotion recognition research. These include voice recognition, recognition with mimics, and so on. However, face expression or tone of voice does not always represent feelings. Skin conductivity, skin temperature changes, blood pressure, heartbeat are also used for emotion estimation. In addition, BCI research on emotional recognition is important in recent years and it's still a new topic in research. In this paper, studies on emotion estimation using EEG signals and the brain computer interface studies in this subject have been examined and the lack of relevant literature on this subject are discussed.","","978-1-5386-0930-9","10.1109/UBMK.2017.8093523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8093523","electroencephalography;emotion recognition;brain computer interface","Electroencephalography;Emotion recognition;Electrodes;Brain-computer interfaces;Transforms;Estimation;Feature extraction","","6","","21","IEEE","2 Nov 2017","","","IEEE","IEEE Conferences"
"Integrating Bi-LSTM with Deep Learning for Emotion Analysis from Brain Wave Patterns","J. GN; N. H R","Department of Computer Science and Engineering, Canara Engineering College, Mangaluru, India; Department of Computer Science and Engineering, Canara Engineering College, Mangaluru, India","2024 International Conference on Computing, Semiconductor, Mechatronics, Intelligent Systems and Communications (COSMIC)","12 Feb 2025","2024","","","257","263","Emotion detection and classification are becoming new frontiers in the capability of brain machine interface in the present times. Several studies have demonstrated that audio, video, as well as EEG data are helpful in the automated detection of affective states. EEG based affect detection is an important aspect of psychiatric evaluation for persons. In case sensor EEG data is gathered from various experimental sessions or participants, the signals obtained tend to be non-stationary. Building an intelligent emotion recognition system, which performs fair in terms of accuracy remains a problem due to the presence of numerous noisy, non-stationary, and nonlinear EEG signals. Some researchers have provided evidence that specific EEG brain waves reflect certain thoughts or feelings. This research proposes a completely new system responsible for automatically recognizing the emotions of users using deep learning techniques of computer network EEG signals acquired during playing computer games. EEG data from 28 participants were acquired on 14 channels Emotive Epoc + portable and wearable devices. Each participant played four distinct emotional games for five minutes each, yielding a total of 20 minutes of EEG data per volunteer. It is also shown that this framework is capable of identifying four fundamental emotion types in real time during games. The findings indicate that the suggested model-based emotion identification framework is a useful way for detecting emotions from EEG data. The network achieves 99.32% accuracy while using less computation time.","","979-8-3315-1789-2","10.1109/COSMIC63293.2024.10871513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10871513","EEG;LSTM;Bi-LSTM;Deep Learning;Emotion Detection","Deep learning;Support vector machines;Emotion recognition;Video games;Accuracy;Databases;Games;Brain modeling;Electroencephalography;Wearable devices","","","","32","IEEE","12 Feb 2025","","","IEEE","IEEE Conferences"
"Empirical Evidence Relating EEG Signal Duration to Emotion Classification Performance","E. T. Pereira; H. M. Gomes; L. R. Veloso; M. R. A. Mota","Centro de Engenharia Eletrica e Informatica, Universidade Federal de Campina Grande, Paraíba, Brazil; Centro de Engenharia Eletrica e Informatica, Universidade Federal de Campina Grande, Paraíba, Brazil; Centro de Engenharia Eletrica e Informatica, Universidade Federal de Campina Grande, Paraíba, Brazil; Centro de Engenharia Eletrica e Informatica, Universidade Federal de Campina Grande, Paraíba, Brazil",IEEE Transactions on Affective Computing,"26 Feb 2021","2021","12","1","154","164","In emotion recognition using EEG, it is not generally agreed upon how much time an EEG signal sequence must have in order to maximize precision and recall rates. To the best of our knowledge, there is not a systematic evaluation of effects on classifier performance related to EEG signal durations. The human factors related to attention decreasing and tiredness increasing have imposed difficulties to create EEG datasets containing a rich variation of signal samples. This paper proposes an experimental evaluation of three different EEG datasets (DEAP, MAHNOB, and STEED) each one mainly characterized by short, intermediate and long signal (or stimulus) durations. Statistical evaluation pointed out that for an EEG dataset to be well-suited for emotion recognition it should have two main characteristics: emotion stimulus data should be publicly available and evaluated by world-wide volunteers, and media stimulus should have duration long enough to affect the subjects. Our statistical analysis revealed that, at least for the considered datasets, signals with duration longer than 60 seconds allow better classification results. This work did not analyse the impact to humans of longer stimulus media.","1949-3045","","10.1109/TAFFC.2018.2854168","Brazilian National Council for Scientific and Technological Development(grant numbers:459763/2014-8); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8408525","Emotion recognition;EEG;signal duration;affective computing;EEG datasets","Electroencephalography;Videos;Emotion recognition;Physiology;Affective computing;Skin;Electromyography","","24","","24","IEEE","9 Jul 2018","","","IEEE","IEEE Journals"
"Inner Emotion Recognition Using Multi Bio-Signals","J. Shin; J. Maeng; D. -H. Kim","Department of Electronic Engineering, Inha University; Department of Electronic Engineering, Inha University; Department of Electronic Engineering, Inha University",2018 IEEE International Conference on Consumer Electronics - Asia (ICCE-Asia),"29 Nov 2018","2018","","","206","212","This paper proposes inner emotion recognition method using multi bio-signal which combines EEG (Electroencephalogram), EMG (Electromyography) and EOG (Electrooculogram) signals. EEG signal was used for recognizing inner emotion, EMG and EOG signals for removing artifact. The acquired data were processed and five emotions were classified using SVM(Support Vector Machine) in terms of arousal and valence, respectively. The experimental results show that the recognition rate of using multimodal bio-signals is 16.8% higher than that of using only EEG signal.","","978-1-5386-5807-9","10.1109/ICCE-ASIA.2018.8552152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552152","EEG;EMG;EOG;Inner Emotion Recognition;Emotion Classification;Support Vector Machine","Electroencephalography;Electromyography;Emotion recognition;Electrooculography;Support vector machines;Conferences;Feature extraction","","7","","3","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"Enhanced Emotion Recognition from EEG Signals with ResConv U-Net Architecture","N. A. N. Azar","Computer Information systems, Near East University, Nicosia, Turkey",2024 8th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT),"28 Nov 2024","2024","","","1","6","Despite the progress in the ability to detect emotions from EEG-based signals, there is still a growing need for human-computer interaction. As emotions are subjective and influenced by several variables, recognizing them is more challenging. In recent years, subsequences of the neural network have been widely utilized to analyze the signals gained from the brain. This study presents a novel CNN architecture that combines the strengths of the residual learning and U-Net upsampling strategies, designed for one-dimensional signal analysis. An encoder-decoder structure in the proposed model effectively captures and reconstructs important temporal features of the EEG signals. The model was applied to the DEAP dataset, and evaluation metrics such as F1-score, recall, and precision were calculated and achieved accuracy in arousal and valence showing the priority of the proposed model.","2770-7962","979-8-3503-5442-3","10.1109/ISMSIT63511.2024.10757205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10757205","EEG brain signal;emotion;DEAP dataset;deep learning;U-Net;Residual block","Measurement;Emotion recognition;Analytical models;Accuracy;Mental health;Brain modeling;Feature extraction;Electroencephalography;Signal analysis;Robots","","","","28","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition Based on Photoplethysmogram and Electroencephalogram","Z. Tong; X. Chen; Z. He; K. Tong; Z. Fang; X. Wang","Yanshan University, Qinhuangdao, China; Yanshan University, Qinhuangdao, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; University of Science and Technology Beijing, Beijing, China",2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),"22 Jun 2018","2018","02","","402","407","In modern life, emotions affect people in all aspects of their work and life. Long-term emotional problems tend to cause physical and mental problems such as depression. The photoplethysmogram(PPG) and electroencephalogram (EEG) etc are often used for emotion recognition, because the emotional state produces reactions from different biological systems of the human body. This article uses an internationally released emotional classification database called DEAP dataset that contains 32-channel standard EEG signals and 8-channel peripheral physiological signals. To achieve wearable emotion continuous recognition, this paper strives to obtain satisfactory emotion recognition through fewer EEG channels and peripheral physiological signals. Machine learning methods are used to classify arousal and valence that are often used in emotional recognition. The results show that the arousal classification accuracy can achieve 68% and the valence classification accuracy can achieve 66% with 5 channel EEG signal and one channel PPG signal, which can be monitored through a wearable headband.","0730-3157","978-1-5386-2667-2","10.1109/COMPSAC.2018.10266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377894","EEG, PPG, emotion classification, machine learning, DEAP","Electroencephalography;Feature extraction;Frequency-domain analysis;Classification algorithms;Emotion recognition;Biomedical monitoring;Physiology","","17","","16","IEEE","22 Jun 2018","","","IEEE","IEEE Conferences"
"An Edge AI System-on-Chip Design with Customized Convolutional-Neural-Network Architecture for Real-time EEG-Based Affective Computing System","Y. -D. Huang; K. -Y. Wang; Y. -L. Ho; C. -Y. He; W. -C. Fang","Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan R.O.C; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan R.O.C; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan R.O.C; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan R.O.C; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan R.O.C",2019 IEEE Biomedical Circuits and Systems Conference (BioCAS),"5 Dec 2019","2019","","","1","4","In this work, we proposed an edge AI CNN chip design for EEG-based affective Computing system by using TSMC 28nm technology. To improve the performance, Artifact Subspace Reconstruction (ASR) and Short-Time Fourier Transform (STFT) were used for our signal pre-processing and features extraction. The time-frequency EEG feature map was obtained with a multi-channel Differential Asymmetry (DASM) method on 6 EEG channels: FP1, FP2, F3, F4, T7, and T8 according to 10-20 system. The total power consumption of the proposed CNN chip was 71.6mW in training mode and 29.5mW in testing mode. We used 32 subjects data from the DEAP database to validate the proposed design, achieving mean accuracies of 83.7%, 84.5%, and 70.51% for Valence-Arousal binary classification and quaternary classification respectively, showing significant performance improvement over the current related works.","2163-4025","978-1-5090-0617-5","10.1109/BIOCAS.2019.8919038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919038","Emotion Recognition;Convolutional Neural Network;Deep Learning Chip;On-chip Learning;Real-time EEG System;Human-Computer Interaction","Electroencephalography;Emotion recognition;Training;Brain modeling;Feature extraction;Convolution;Real-time systems","","8","","8","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Multiple Feature Fusion for Automatic Emotion Recognition Using EEG Signals","N. Liu; Y. Fang; L. Li; L. Hou; F. Yang; Y. Guo","School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent; School of Computer Engineering and Science, University of Kent","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","896","900","Automatic emotion recognition based on electroencephalo-graphic (EEG) signals has received increasing attention in recent years. The Deep Residual Networks (ResNets) can solve vanishing gradient problem and exploding gradient problem well in computer vision and can learn more profound semantic information. And for traditional methods, frequency features often play important role in signal processing area. Thus, in this paper, we use the pre-trained ResNets to extract deep semantic information and the linear-frequency cepstral coefficients (LFCC) as features from raw EEG signals. Then the two features are fused to improve the emotion classification performance of our approach. Moreover, several classifiers are used for our fused features to evaluate the performance and it shows that the proposed approach is effective for emotion classification. We find that the best performance is achieved when use k-nearst neighbor (KNN) as classifier, and we provide a detailed discussion for the reason.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8462518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462518","emotion recognition;EEG;Residual Networks;cepstral coefficients","Electroencephalography;Task analysis;Emotion recognition;Brain modeling;Feature extraction;Databases;Speech recognition","","34","2","21","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"FLEER: A Federated Learning Framework for EEG Emotion Recognition","M. Li; X. Tang","School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Zhejiang Electronic Information Products Inspection and Research Institute (Key Laboratory of Information Security of Zhejiang Province), Hangzhou, China","2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","11 Jul 2024","2024","","","285","288","The success of deep learning (DL) methods in the Brain-Computer Interface (BCI) field for emotion recognition of electroencephalographic (EEG) recordings has been limited due to the scarcity of large datasets. Individual difference and privacy concern restrict the feasibility of creating a large EEG-BCI dataset through merging multiple small datasets for the collaborative training of machine learning models. To solve this problem, we propose a personalized federated learning framework based on the common feature for EEG emotion recognition classification network (FLEER). FLEER learns user's common feature representation through deep learning methods and generates the personalized model for each user. We assess the effectiveness of the proposed architecture on the DEAP dataset for 4-class emotion recognition classification. The experimental results show FLEER provides an effective method for EEG emotion recognition classification while avoiding the actual data sharing and its results are comparable to the baseline in intra-subject. Additionally, our model provides 2 % better accuracy compared to other DL architectures in inter-subject.","","979-8-3503-8555-7","10.1109/AINIT61980.2024.10581649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581649","federated learning;emotion recognition;privacy protection;EEG","Deep learning;Training;Seminars;Emotion recognition;Data privacy;Federated learning;Brain modeling","","","","13","IEEE","11 Jul 2024","","","IEEE","IEEE Conferences"
"Optimized Deep Forest Emotional Awareness Recognition Based on EEG Rhythm Characteristics","J. Yan; J. Deng; D. Li; Z. Long; W. Sun; W. Xue; Q. Zhou; G. Liu","North China University of Technology, Beijing, China; North China University of Technology, Beijing, China; North China University of Technology, Beijing, China; North China University of Technology, Beijing, China; North China University of Technology, Beijing, China; North China University of Technology, Beijing, China; North China University of Technology, Beijing, China; North China University of Technology, Beijing, China",2022 4th International Conference on Intelligent Information Processing (IIP),"15 Jun 2023","2022","","","37","41","Emotion is the psychological and physiological response of human to external things, and occupies an important place in the study of human-computer interaction. Electroencephalogram (EEG) is a physiological signal that is widely used in the field of emotion awareness recognition. EEG signals can be divided into 5 basic rhythms according to frequency, and most of the existing research on emotion awareness recognition based on EEG signals directly processes the rhythms and extracts the corresponding features. This method is easy to lose the rhythm information and cannot give full play to its function. Therefore, in this study, we propose an optimized deep forest emotion awareness recognition method based on EEG rhythm characteristics to investigate the effect of rhythms on emotion awareness recognition performance. Firstly, we classify EEG rhythms into 3 types of rhythm combinations: single rhythm, compound rhythm and full rhythm according to the principle of adjacent combination, and fully consider various combination forms of rhythms; Secondly, we construct a two-dimensional input model to retain the spatial information of multi-channel EEG signals; Finally, we use the gcForest classification model for emotion recognition, which does not require feature extraction and maximizes the retention of rhythm information. We conducted extensive experiments on the DEAP dataset, and the experimental results show that the frequency band and number of rhythms affect the performance of emotion recognition, and the high frequency band rhythms have better emotion classification performance compared with the low frequency band rhythms, among which the $\beta$ rhythms are higher than the Y rhythms in validity and arousal dimension, and their classification accuracy is 96.711% and 96.633%; the classification accuracy of compound rhythms was higher than that of single rhythms, but a higher number of compound rhythms does not necessarily lead to better emotion classification performance, where in the arousal dimension, compound rhythms $((\mathrm{x}+\beta+\gamma)$ have better awareness emotion recognition performance compared to full rhythms $((+\mathrm{t}\mathrm{K}+\beta+\gamma)$, with 97% classification accuracy.","","978-1-6654-9222-5","10.1109/IIP57348.2022.00014","Beijing Municipal Education Commission; National Defense Basic Scientific Research Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148339","electroencephalogram;emotion awareness recognition;optimized deep forest;rhythm;DEAP dataset","Emotion recognition;Feature extraction;Brain modeling;Electroencephalography;Physiology;Regression analysis;Compounds","","1","","14","IEEE","15 Jun 2023","","","IEEE","IEEE Conferences"
"A 3D Convolutional Neural Network for Emotion Recognition based on EEG Signals","Y. Zhao; J. Yang; J. Lin; D. Yu; X. Cao","School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China",2020 International Joint Conference on Neural Networks (IJCNN),"28 Sep 2020","2020","","","1","6","As an important field of research in Human-Machine Interactions, emotion recognition based on the electroencephalography (EEG) signals has become common research. The traditional machine learning approaches use well-designed classifiers with hand-crafted features which may be limited to domain knowledge. Motivated by the outstanding performance of deep learning approaches in recognition tasks, we proposed a 3D convolutional neural network model to extract the spatial-temporal features automatically in the EEG signals. By the pre-processing method with baseline signals and the electrode topological structure relocated, the proposed model achieves a high accuracy rate of 96.61%, 96.43% in the Two class classification task (low/high arousal, low/high valence) and 93.53% in the Four class classification task (low arousal and low valence/high arousal and low valence/low arousal and high valence/high arousal and high valence) in the DEAP dataset, and 97.52%, 96.96% in the Two class classification task and 95.86% in the Four class classification task in the AMIGOS dataset.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207420","Emotion Recognition;Electroencephalography (EEG);3D Convolutional Neural Network (3D CNN);Spatio-temporal Features;Deep Learning","Brain modeling;Electroencephalography;Task analysis;Feature extraction;Electrodes;Solid modeling;Machine learning","","23","","27","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Classification Using Joint Adaptation Networks","H. Liu; H. Guo; W. Hu","College of Computer Science, Wuhan University of Science and Technology, Wuhan, China; College of Computer Science, Wuhan University of Science and Technology, Wuhan, China; College of Computer Science, Wuhan University of Science and Technology, Wuhan, China",2021 IEEE International Symposium on Circuits and Systems (ISCAS),"27 Apr 2021","2021","","","1","5","Emotion classification based on EEG Signals are being increasing studied because of its applicability in human- machine interaction. However, in previous research, it is commonly assumed that the training and testing data share the same distribution. Unfortunately, this assumption is not always reasonable, for the variation of EEG can cause a substantial mismatch between datasets easily. The problem mentioned above results in degeneration of traditional emotion classification methods. In this paper, we construct a novel joint adaptation networks (JAN) to address this problem for emotion classification based on EEG. Experimental results on two representative EEG datasets demonstrate its validity. Moreover, further comparisons with the state-of-the-arts methods are also made to confirm its superiority.","2158-1525","978-1-7281-9201-7","10.1109/ISCAS51556.2021.9401737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401737","emotion classification;Electroencephalogram;transfer learning;affective computing;joint adaptation networks","Training;Circuits and systems;Electroencephalography;Man-machine systems;Testing","","7","","17","IEEE","27 Apr 2021","","","IEEE","IEEE Conferences"
"Beyond Playlists: AI-Driven Emotion-Based Music Recommendation Systems","H. Shiralaskar; S. Mendhe","Department of Computer Science & Engineering Faculty of Engineering & Technology, Datta Meghe Institute of Higher Education md Research, Wardha, Maharashtra, India; Department of Computer Science & Design Faculty of Engineering & Technology, Datta Meghe Institute of Higher Education and Research, Maharashtra, India",2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS),"25 Apr 2025","2025","","","1248","1253","Music recommendation systems have evolved from simple playlist curation techniques to sophisticated AI-driven models capable of analyzing human emotions for personalized song suggestions. Emotion-based music recommendation systems integrate advanced machine learning, deep learning, and artificial intelligence techniques to detect users' emotional states through various input modes, including facial expressions, speech tone, text analysis, and physiological signals. These systems aim to enhance user experience by dynamically selecting music that aligns with real-time emotions, thereby improving mood regulation and mental well-being. This review explores the methodologies employed in emotion-based music recommendation systems, including deep learning architectures, facial recognition techniques, and sentiment analysis models. It also discusses the challenges associated with real-time emotion detection, data privacy concerns, cross-cultural differences in music perception, and system adaptability. Furthermore, the paper highlights emerging trends and future research directions, such as multimodal emotion detection, hybrid AI models, reinforcement learning, and ethical AI frameworks. Addressing these challenges and advancements will be crucial in developing more robust, accurate, and user-centric emotion-based music recommendation systems.","","979-8-3315-0574-5","10.1109/ICMLAS64557.2025.10968205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10968205","Emotion-based music recommendation;AI in music;Deep-Learning;Facial Emotion Recognition;sentiment analysis;music personalization;NLP in music;EEG-based emotion detection","Deep learning;Emotion recognition;Sentiment analysis;Analytical models;Adaptation models;Reinforcement learning;Brain modeling;Real-time systems;Physiology;Recommender systems","","","","11","IEEE","25 Apr 2025","","","IEEE","IEEE Conferences"
"Emotion estimation from EEG signals during listening to Quran using PSD features","M. Alsolamy; A. Fattouh","Department of Computer Science, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, King Abdulaziz University, Jeddah, Saudi Arabia",2016 7th International Conference on Computer Science and Information Technology (CSIT),"25 Aug 2016","2016","","","1","5","Emotions play an important role in our thinking and behavior and hence contribute in shaping up of our personality. Many theoretical and experimental researches have been conducted to recognize the emotions from verbal or non-verbal behaviors. It is well known that the electroencephalogram (EEG) signals contain rich information about the activities of the brain and they can reliably enable us to estimate the emotions if they are properly interpreted. In this paper, we propose a model to discriminate the emotional state of a person by analyzing his brain signals recorded during listening to the Quran and using a machine learning approach. It is assumed that listening to the Quran brings reverence, and hence two types of emotions emerge which are distinguished as happy and unhappy. In our analysis, we used the Power Spectral Density (PSD) of different bands as features and the Support Vector Machine (SVM) as a classifier. Experiments were conducted by 14 participants and they gave a classification accuracy rate 85.86%.","","978-1-4673-8914-3","10.1109/CSIT.2016.7549457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7549457","EEG;emotion recognition;machine learning;power spectral density (PSD);support vector machine (SVM)","Electroencephalography;Brain modeling;Emotion recognition;Support vector machines;Mathematical model;Speech;Speech recognition","","36","","23","IEEE","25 Aug 2016","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Trainable Adjacency Relation Driven Graph Convolutional Network","W. Li; M. Wang; J. Zhu; A. Song","School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; College of Arts and Sciences, Boston University, Boston, MA, USA; School of Instrument Science and Engineering, Southeast University, Nanjing, China",IEEE Transactions on Cognitive and Developmental Systems,"14 Dec 2023","2023","15","4","1656","1672","In recent years, there has been a growing research interest in using deep learning to resolve the issue of electroencephalogram (EEG)-based emotion recognition. Current research emphasizes exploiting the useful information from each single EEG channel or each individual set of multichannel EEG, but overlooks the correlation information among different multichannel EEG sets. To explore such discriminative correlation information, we propose a novel and effective method, “trainable adjacency relation driven graph convolutional network (TARDGCN),” which contains two complementary modules: 1) trainable adjacency relation (TAR) and 2) graph convolutional network (GCN). TAR optimizes the local pairwise positions of multichannel EEG sets, which helps form an improved graphic representation for GCN to learn the global correlation among these sets for classification. The proposed method is capable of dealing with the problem of small sample size but large data variation in this issue. Our experimental results conducted on the databases DREAMER and DEAP in the subject-dependent and subject-independent modes show that TARDGCN outperforms the state-of-the-art approaches in classifying all of valence, arousal, and dominance.","2379-8939","","10.1109/TCDS.2023.3270170","Aeronautical Science Foundation of China(grant numbers:20200058069001); Basic Research Project of Leading Technology of Jiangsu Province(grant numbers:BK20192004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10108053","Correlation information;electroencephalogram (EEG);emotion recognition;graph convolutional network (GCN);trainable adjacency relation (TAR)","Electroencephalography;Feature extraction;Emotion recognition;Correlation;Convolutional neural networks;Three-dimensional displays;Graphics","","13","","54","IEEE","25 Apr 2023","","","IEEE","IEEE Journals"
"EEG-Based Emotion Recognition Using Morlet Dual-Level Wavelet Contextual Neural Network with Snow Geese Algorithm","M. Sivaramkrishnan; M. Siva Ramkumar; M. Kannaiyan; M. Kanan; R. U. Baig; J. Giri","Dept of EEE, Karpagam College of Engnieering, India; Dept of ECE, SNS College of Technology; Department of Mechanical Engineer, University college of Kancheepuram, Anna University, Kanchipuram; Department of Industrial Engineering, College of Engineering, University of Business and Technology, Jeddah, Saudi Arabia; Department of Artificial Intelligence and Machine Learning, Cambridge Institute of Technology, K R Puram, Bangalore; Department of Mechanical Engineering, Yeshwantrao Chavan College of Engineering, Nagpur, India",2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL),"27 Mar 2025","2025","","","1578","1584","EEG-based Emotion Recognition (ER) works by analyzing brainwave patterns from EEG signals to understand and identify a person's emotional state. The problem of emotion identification from EEG signals remains challenging because brain activity presents both non-linear dynamic properties and individual-specific EEG patterns along with biological noise. Traditional approaches to emotion detection become inefficient due to EEG signal complexity thereby complicating accuracy and reliability measures. To tackle the challenges in EEG-based emotion recognition, this study presents a new approach called the Morlet Dual-level Wavelet Contextual Neural Network with Snow Geese Algorithm (MorDWCNNet+SGA). The method uses the SEED dataset, starting with preprocessing through the Observability-Constrained Resampling-Free Cubature Kalman Filter (OCRCKF) to enhance important EEG patterns. For feature extraction, the Multi-Discrete Wavelet Transform (MDWT) is applied to capture key EEG characteristics. Emotion classification is then performed using the Morlet Dual-level Wavelet Contextual Neural Network (MorDWCNNet), which is further optimized by the Snow Geese Algorithm (SGA) to improve accuracy. Implemented in Python and tested on the SEED dataset, the MorDWCNNet+SGA framework achieves remarkable results, with 99.9% accuracy and 99.3% sensitivity, significantly outperforming existing methods. The outcomes of the proposed method achieved high accuracy in effectively distinguishing emotional states through brainwave patterns. This method demonstrates how advanced techniques can combine synergistically to enhance ER performance resulting in improved real-world application accuracy and speed.","","979-8-3315-2392-3","10.1109/ICSADL65848.2025.10933301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10933301","Emotion Recognition;Morlet Dual-level Wavelet Contextual Neural Network;Multi-Discrete Wavelet Transform;Observability-Constrained Resampling-Free Cubature Kalman Filter;Snow Geese Algorithm","Wavelet transforms;Emotion recognition;Accuracy;Snow;Wavelet analysis;Feature extraction;Brain modeling;Electroencephalography;Classification algorithms;Biological neural networks","","","","16","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"Feature Transfer Learning in EEG-based Emotion Recognition","B. Xue; Z. Lv; J. Xue","Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China",2020 Chinese Automation Congress (CAC),"29 Jan 2021","2020","","","3608","3611","This is a challenge to enhance the distribution between the source and the target domains between different subjects, which is difficult but important for practical applications. In this paper, we proposed a transfer framework based on feature analysis for EEG-based emotion recognition. We firstly extract differential entropy features by computing logarithms of power spectral density. On this basis, transfer component analysis (TCA) was used to project the source and the target domains into a kernel Hilbert space, in order to reduce the distance between the two domains. Finally, we compare the performance of emotion recognition among different dimensions. Experiments results show that the best mean accuracy is 58.49% by using TCA-based method, which is better than the previous study of 52.06%. Meanwhile, experimental results validate the feasibility and efficiency for subject transfer emotion recognition.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327161","National Laboratory of Pattern Recognition; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327161","Differential Entropy(DE);EEG;Transfer Learning;Emotion Recognition","Feature extraction;Electroencephalography;Emotion recognition;Physiology;Kernel;Brain modeling;Data mining","","3","","15","IEEE","29 Jan 2021","","","IEEE","IEEE Conferences"
"Classification of Emotions using EEG Signals","P. Gourabathuni; R. S. Pothineni; K. C. Yelavarti","Information Technology, Velagapudi Ramakrishna Siddhartha Engineering College; Information Technology, Velagapudi Ramakrishna Siddhartha Engineering College; Information Technology, Velagapudi Ramakrishna Siddhartha Engineering College",2023 Third International Conference on Artificial Intelligence and Smart Energy (ICAIS),"27 Mar 2023","2023","","","272","276","Emotion classification remains a challenging problem in affective computing. One of the most crucial areas of study in the field of brain wave research is the classification of emotions. Classifying the types of emotions accurately is one of the major issues with the analysis of brainwave emotion. EEG signals used for real-time emotion identification are crucial for affective computing and human-computer interaction. These signals can be produced by the user while engaging in a variety of cognitive, affective, and physical tasks, representing the functionality of the brain. The resulting emotional state produced gives valuable insights on the attitudes and actions of participants in specific situations. The main objective of this research work is to classify the emotions using EEG signals. The process is divided into two steps. The first step is feature extraction and the next step is classification. The feature extraction is performed by using DWT and the selection is done by using L1 norm. The algorithms used to perform signal classification are LSTM, GRU and DNN.","","978-1-6654-6216-7","10.1109/ICAIS56108.2023.10073677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10073677","Emotions;EEG-Electroencephalography Signals;Classification;LSTM-Long Short-Term Memory;GRU-Gated Neural Network;DNN-Deep Neural Network","Affective computing;Pattern classification;Feature extraction;Electroencephalography;Real-time systems;Data models;Classification algorithms","","3","","12","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"EEG Emotion Recognition Supervised by Temporal Features of Video Stimuli","S. Ran; W. Zhong; D. Duan; F. Hu; L. Ye; Q. Zhang","Key Laboratory of Media Audio & Video (Communication University of China), Ministry of Education, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; Key Laboratory of Media Audio & Video (Communication University of China), Ministry of Education, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","7","Different types of video stimuli can activate different reactions in the human brain and these signals can be captured and analyzed for emotion recognition applications. However, accurate recognition across subjects is still challenging due to non-stationary and low signal-to-noise ratio of EEG signals. Lying at the intersection of video content analysis, we make an attempt to supervise EEG features with the external evoked video features. An end-to-end framework is proposed by extracting useful emotional representations in EEG signals with the complementarity of video stimuli. For the feature obtaining, an EEG feature extractor and a video feature extractor are combined, along with a cross-modal transformer to align the distributions of the two type features, and then a self-attention mechanism is designed for fusion. The experiments on the subset of DEAP and self-collected datasets validate that the enhancement of EEG features supervised by stimulus information is a reliable solution for subject-independent emotion recognition.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10781573","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781573","emotion recognition;Electroencephalography;video content analysis;cross-modal feature alignment","Emotion recognition;Content management;Feature extraction;Transformers;Brain modeling;Electroencephalography;Physiology;Reliability;Engineering in medicine and biology;Signal to noise ratio","Humans;Electroencephalography;Emotions;Video Recording;Signal Processing, Computer-Assisted;Photic Stimulation;Algorithms","","","29","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"AMDET: Attention Based Multiple Dimensions EEG Transformer for Emotion Recognition","Y. Xu; Y. Du; L. Li; H. Lai; J. Zou; T. Zhou; L. Xiao; L. Liu; P. Ma","Big Data Center, Nanfang Hospital, Southern Medical University, Guangzhou, China; Big Data Center, Nanfang Hospital, Southern Medical University, Guangzhou, China; Department of Pathology, People's Hospital of Guangxi Zhuang Autonomous Region, Guangxi Academy of Medical Sciences, Nanning, China; Big Data Center, Nanfang Hospital, Southern Medical University, Guangzhou, China; Department of Psychology and Economics, University of California, Los Angeles, CA, USA; Department of Electrical Engineering and Computer Sciences, UC Berkeley, Berkeley, CA, USA; Department of Infectious Diseases, Nanfang Hospital, Southern Medical University, Guangzhou, China; Big Data Center, Nanfang Hospital, Southern Medical University, Guangzhou, China; Big Data Center, Nanfang Hospital, Southern Medical University, Guangzhou, China",IEEE Transactions on Affective Computing,"5 Sep 2024","2024","15","3","1067","1077","Affective computing is an important subfield of artificial intelligence, and with the rapid development of brain-computer interface technology, emotion recognition based on EEG signals has received broad attention. It is still a great challenge to effectively explore the multi-dimensional information in the EEG data in spite of a large number of deep learning methods. In this article, we propose a deep learning model called Attention-based Multiple Dimensions EEG Transformer (AMDET), which can leverage the complementarity among the spectral-spatial-temporal features of EEG data by employing the multi-dimensional global attention mechanism. We first transform the original EEG data into 3D temporal-spectral-spatial representations and then the AMDET would use spectral-spatial transformer blocks to extract effective features in the EEG signal and focus on the critical time frame with the temporal attention block. We conduct extensive experiments on the DEAP, SEED, and SEED-IV datasets to evaluate the performance of AMDET and the results outperform the state-of-the-art baseline on three datasets. Accuracy rates of 97.48%, 96.85%, 97.17%, 87.32% were achieved in the DEAP-Arousal, DEAP-Valence, SEED, and SEED-IV datasets, respectively. Based on AMDET, we can achieve over 90% accuracy with only eight channels, significantly improving the possibility of practical applications.","1949-3045","","10.1109/TAFFC.2023.3318321","National Key R&D Program of China(grant numbers:2020YFC2006400); Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101420005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261214","Attention;electroencephalogram (EEG);emotion recognition;multi-dimensional information","Electroencephalography;Feature extraction;Emotion recognition;Time-frequency analysis;Data mining;Brain modeling;Transformers","","10","","47","IEEE","22 Sep 2023","","","IEEE","IEEE Journals"
"EEG Based Participant Independent Emotion Classification using Gradient Boosting Machines","S. Aggarwal; L. Aggarwal; M. S. Rihal; S. Aggarwal","Department Of Computer Engineering, Netaji Subhas Institute of Technology, Dwarka, Delhi, India; Department Of Computer Engineering, Netaji Subhas Institute of Technology, Dwarka, Delhi, India; Department Of Computer Engineering, Netaji Subhas Institute of Technology, Dwarka, Delhi, India; Department Of Computer Engineering, Netaji Subhas Institute of Technology, Dwarka, Delhi, India",2018 IEEE 8th International Advance Computing Conference (IACC),"18 Apr 2019","2018","","","266","271","Analysis of EEG (Electroencephalography) signals provides an alternative ingenious approach towards Emotion recognition. Nowadays, Gradient Boosting Machines (GBMs) have emerged as state-of-the-art supervised classification techniques used for robust modeling of various standard machine learning problems. In this paper, two GBM's (XGBoost and LightGBM) were used for emotion classification on DEAP Dataset. Furthermore, a participant independent model was fabricated by excluding participant number from features. The proposed approach performed well with high accuracies and faster training speed.","2473-3571","978-1-5386-6678-4","10.1109/IADCC.2018.8692106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8692106","Electroencephalography;EEG;Emotion Classification;Gradient Boosting Machines;XGBoost;LightGBM.","","","13","","34","IEEE","18 Apr 2019","","","IEEE","IEEE Conferences"
"Exploiting EEG Signals and Audiovisual Feature Fusion for Video Emotion Recognition","B. Xing; H. Zhang; K. Zhang; L. Zhang; X. Wu; X. Shi; S. Yu; S. Zhang","School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China",IEEE Access,"17 May 2019","2019","7","","59844","59861","External stimulation, mood swing, and physiological arousal are closely related and induced by each other. The exploration of internal relations between these three aspects is interesting and significant. Currently, video is the most popular multimedia stimuli that can express rich emotional semantics by its visual and auditory features. Apart from the video features, human electroencephalography (EEG) features can provide useful information for video emotion recognition, as they are the direct and instant authentic feedback on human perception with individuality. In this paper, we collected a total of 39 participants' EEG data induced by watching emotional video clips and built a fusion dataset of EEG and video features. Subsequently, the machine-learning algorithms, including Liblinear, REPTree, XGBoost, MultilayerPerceptron, RandomTree, and RBFNetwork were applied to obtain the optimal model for video emotion recognition based on a multi-modal dataset. We discovered that using the data fusion of all-band EEG power spectrum density features and video audio-visual features can achieve the best recognition results. The video emotion classification accuracy achieves 96.79% for valence (Positive/Negative) and 97.79% for arousal (High/Low). The study shows that this method can be a potential method of video emotion indexing for video information retrieval.","2169-3536","","10.1109/ACCESS.2019.2914872","Natural Science Foundation of Zhejiang Province(grant numbers:LY19F020047,LZ19F020002); National Key Research and Development Program of China(grant numbers:2017YFB1002600); National Natural Science Foundation of China(grant numbers:61402141); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8705310","Affective computing;video;EEG;multimodal;signal processing","Feature extraction;Electroencephalography;Emotion recognition;Visualization;Brain modeling;Hidden Markov models;Physiology","","42","","74","OAPA","3 May 2019","","","IEEE","IEEE Journals"
"Wavelet Analysis Based Classification of Emotion from EEG Signal","M. R. Islam; M. Ahmad","Electrical and Electronic Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Electrical and Electronic Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh","2019 International Conference on Electrical, Computer and Communication Engineering (ECCE)","4 Apr 2019","2019","","","1","6","Emotions are the most fundamental feature for non-verbal communication between human and machine. To extract the original expectation of mind, emotion recognition and classification is essential. But due to some complexities, the proper recognition of human emotion from Electroencephalogram (EEG) has become too much challenging. In this paper, we propose a system of emotion recognition from EEG signal based on Discrete Wavelet Transform. The most significant features (i) Wavelet Energy and (ii) Wavelet Entropy are calculated for detecting four different emotions namely happy, angry, sad and relaxed. Firstly we rearranged the prepossessed data properly by selecting the proper channel and sub-band. The extracted features are then trained in the K-Nearest Neighbor (KNN) algorithm to classify emotion separately. Our proposed method showed 78.7±2.6% sensitivity, 82.8±6.3% specificity and 62.3±1.1% accuracy on the internationally authorized `DEAP' database.","","978-1-5386-9111-3","10.1109/ECACE.2019.8679156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8679156","EEG;emotion;Discrete Wavelet Transform;energy;entropy;KNN","Electroencephalography;Feature extraction;Videos;Emotion recognition;Discrete wavelet transforms;Databases;Entropy","","50","","14","IEEE","4 Apr 2019","","","IEEE","IEEE Conferences"
"Multi-Scale Hyperbolic Contrastive Learning for Cross-Subject EEG Emotion Recognition","J. Chang; Z. Zhang; Y. Qian; P. Lin","Institute of Big Data Science and Industry, the School of Computer and Information Technology, Shanxi University, Taiyuan, China; Institute of Big Data Science and Industry, the School of Computer and Information Technology, Shanxi University, Taiyuan, China; Institute of Big Data Science and Industry, the School of Computer and Information Technology, Shanxi University, Taiyuan, China; Center for Mind & Brain Sciences and institute of Interdisciplinary Studies, Hunan Normal University, Hunan, Changsha, China",IEEE Transactions on Affective Computing,"","2025","PP","99","1","16","Electroencephalography (EEG) serves as a reliable and objective signal for affective computing applications. However, individual differences in EEG signals pose a significant challenge for emotion recognition tasks across subjects. To address this, we proposed a novel method called Multi-Scale Hyperbolic Contrastive Learning (MSHCL), which leverages event-relatedness to learn subject-invariant representations. MSHCL employs contrastive losses at two different scales-emotion and stimulus-to effectively capture complex EEG patterns within a hyperbolic space hierarchy. Our method is evaluated on three datasets: SEED, MPED, and FACED. It achieves 89.3% accuracy on the three-class task for SEED, 38.8% on the seven-class task for MPED, and 77.0% and 45.7% on the binary and nine-class tasks for FACED in cross-subject emotion recognition. These results demonstrate that the proposed MSHCL method superior performance over other baselines and its effectiveness in learning subject-invariant representations. The source code is available at https://github.com/JiangChang-BRAIN/MSHCL.","1949-3045","","10.1109/TAFFC.2025.3535542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10856324","EEG;emotion recognition;hyperbolic embedding;contrastive learning;cross-subject","Electroencephalography;Emotion recognition;Contrastive learning;Brain modeling;Feature extraction;Data models;Computational modeling;Affective computing;Accuracy;Vectors","","","","","IEEE","28 Jan 2025","","","IEEE","IEEE Early Access Articles"
"EEG-Based Emotion Recognition with Prototype-Based Data Representation","Y. Wang; S. Qiu; C. Zhao; W. Yang; J. Li; X. Ma; H. He","University of Chinese Academy of Sciences, Beijing, China; Research Center for Brain-inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Science; School of Computer and Information Engineering, Beijing Technology and Business University, Beijing, China; School of Computer and Information Engineering, Beijing Technology and Business University, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Science, Beijing, China",2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"7 Oct 2019","2019","","","684","689","Emotions play an important role in human communication, and EEG signals are widely used for emotion recognition. Despite the extensive research of EEG in recent year, it is still challenging to interpret EEG signals effectively due to the massive noises in EEG signals. In this paper, we propose an effective emotion recognition framework, which contains two main parts: the representation network and the prototype selection algorithm. Through our proposed representation network, samples from the same kind of emotion state are more close to each other in high-level representation, and then, we selected the prototypes from the clustering set in feature space match the following testing samples. This method takes advantage of the powerful representation ability of deep learning and learns a better describable feature space rather than learn the classifier explicitly. The experiments on SEED dataset achieves a high accuracy of 93.29% and outperforms a set of baseline methods and the recent deep learning emotion classification approaches. These experimental results demonstrate the effectiveness of our proposed emotion recognition framework.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8857340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8857340","","Prototypes;Electroencephalography;Training;Emotion recognition;Testing;Feature extraction;Deep learning","Algorithms;Electroencephalography;Emotions;Humans","7","","25","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"A Multimodal Myanmar Emotion Dataset for Emotion Recognition","K. P. Pa Aung; H. -L. Yin; T. -F. Ma; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","Effective emotion recognition is vital for human interaction and has an impact on several fields such as psychology, social sciences, human-computer interaction, and emotional artificial intelligence. This study centers on the innovative contribution of a novel Myanmar emotion dataset to enhance emotion recognition technology in diverse cultural contexts. Our unique dataset is derived from a carefully designed emotion elicitation paradigm, using 15 video clips per session for three emotions (positive, neutral, and negative), with five clips per emotion. We collected electroencephalogram (EEG) signals and eye-tracking data from 20 subjects, and each subject took three sessions spaced over several days. Notably, all video clips used in experiments have been well rated by Myanmar citizens through the Self-Assessment Manikin scale. We validated the proposed dataset’s uniqueness using three baseline unimodal classification methods, alongside two traditional multimodal approaches and a deep multimodal approach (DCCA-AM) under subject-dependent and subject-independent settings. Unimodal classification achieved accuracies ranging from 62.57% to 77.05%, while multimodal fusion techniques achieved accuracies ranging from 75.43% to 87.91%. These results underscore the effectiveness of the models, and highlighting the value of our unique dataset for cross-cultural applications.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782660","National Natural Science Foundation of China; Shanghai Jiao Tong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782660","Emotion recognition;electroencephalogram;eye-tracking;Myanmar emotion dataset;cross-cultural emotion recognition","Emotion recognition;Accuracy;Biological system modeling;Psychology;Gaze tracking;Brain modeling;Distance measurement;Electroencephalography;Cultural differences;Engineering in medicine and biology","Humans;Myanmar;Emotions;Electroencephalography;Female;Male;Adult;Algorithms;Databases, Factual;Eye-Tracking Technology","","","11","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Feature Extraction Using Power Spectral Density","M. A. Tran; L. H. Nguyen; A. Turnip; C. T. K. Le; N. Q. Luong; V. B. Vo","The University of Danang – University of Science and Technology, Danang, Vietnam; The University of Danang – University of Science and Technology, Danang, Vietnam; Department of Electrical Engineering, Univesitas Padjadjaran, Jatinangor, Indonesia; The University of Danang – University of Science and Technology, Danang, Vietnam; The University of Danang – University of Science and Technology, Danang, Vietnam; The University of Danang – University of Science and Technology, Danang, Vietnam",2023 15th International Conference on Knowledge and Systems Engineering (KSE),"6 Nov 2023","2023","","","1","4","Emotion recognition is an interesting area of study that has attracted much attention over the past decades. In this study, we address the problem of emotion feature extraction from electroencephalogram (EEG) signals. First, the public DREAMER dataset of EEG (i.e., experiment information, pre-processing, raw data, etc.) will be presented. Second, the detail procedure of EEG signal processing, including noise filtering, frequency band decomposition, etc., is described and finally, the power spectral density (PSD)-based feature extraction is proposed to investigate the correlation between PSD values and emotion states, and the obtained results are presented and discussed.","2694-4804","979-8-3503-2974-2","10.1109/KSE59128.2023.10299508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10299508","emotion state;electroencephalogram;feature extraction;power spectral density","Knowledge engineering;Emotion recognition;Correlation;Filtering;Signal processing;Feature extraction;Systems engineering and theory","","2","","13","IEEE","6 Nov 2023","","","IEEE","IEEE Conferences"
"An Affective Computing Electroencephalogram-based System with Machine Learning Algorithms","I. L. Salim; O. A. Awad; A. S. Abdulhadi","Department of Information and Communication Engineering College of IE, Al-Nahrain University, Baghdad, Iraq; Department of Medical Instrumentation, Techniques Engineering Al-Mustaqbal University, Babylon, Iraq; Department of Information and Communication Engineering College of IE, Al-Nahrain University, Baghdad, Iraq","2024 International Conference on Computer Engineering, Network, and Intelligent Multimedia (CENIM)","19 Feb 2025","2024","","","1","8","Affective Computing plays a crucial role in the Human-Computer Interaction (HCI) field by enabling computers to recognize, interpret, process, and simulate emotions evoked by positive/negative events, objects, or situations. It presents new avenues for applications ranging from personalized user interfaces to mental health monitoring. In this study, an affective computing system for human emotion recognition is developed by measuring the electrical activity of brain, Electroencephalogram (EEG) signals for 15 participants were recorded. Using Machine Learning (ML) algorithms to classify emotional states into three emotion dimensions: valence, arousal, and dominance. Two ML algorithms, Naïve Bayes (NB) and Artificial Neural Network (ANN) are exploited. The system also validated with EEG data from the public DREAMER dataset and compared with recorded data. Results demonstrate the feasibility and effectiveness of the proposed EEG-based affective computing system in accurately identifying and categorizing emotions, leveraging the designed ANN classifier; a maximum accuracy of 95.61% is achieved.","","979-8-3503-6880-2","10.1109/CENIM64038.2024.10882764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10882764","Affective Computing;EEG;HCI;Machine Learning","Computers;Affective computing;Emotion recognition;Machine learning algorithms;Accuracy;Anxiety disorders;Artificial neural networks;Electroencephalography;Classification algorithms;Reliability","","","","35","IEEE","19 Feb 2025","","","IEEE","IEEE Conferences"
"Emotion recognition based on correlation between left and right frontal EEG assymetry","M. A. Ahmed; C. K. Loo","Faculty of Computer Science & Information Technology University of Malaya, Kuala, Lumpur, Malaysia; Faculty of Computer Science & Information Technology University of Malaya, Kuala, Lumpur, Malaysia",2014 10th France-Japan/ 8th Europe-Asia Congress on Mecatronics (MECATRONICS2014- Tokyo),"26 Jan 2015","2014","","","99","103","The significant role of emotion recognition research has increased in last few years of human daily life. In this paper, we base on electroencephalogram (EEG) of the brain to recognize the internal emotion of participants. We use arousal and valence emotion elicitation process to calculate multidimensional direct information (MDI) between right and left hemisphere. The main contribution is recognizing internal emotion of people based on reading the signals from frontal asymmetry of brain so that four channels are used to represent frontal asymmetry of brain F3, F7, F4 and F8. The emotion elicitation process is focused on frontal EEG asymmetry base on using standard dataset. We test four basic emotions happy, sad, anger and relax. These emotions represent high arousal high valence, low arousal low valance, High arousal low valence and low arousal high valence in arousal and valance model. Statistical-based features from EEG signals of data inputs are extracted and used to calculate multidimensional direct information in order to find the correlation between left and right hemisphere to each emotion.","","978-1-4799-5717-0","10.1109/MECATRONICS.2014.7018585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018585","Emotion elicitation;Multidimensional Direct Information;Arousal/Valence;Electroencephalogram","Electroencephalography;Correlation;Emotion recognition;Brain modeling;Standards;Feature extraction;Fractals","","4","","15","IEEE","26 Jan 2015","","","IEEE","IEEE Conferences"
"Ensemble Machine Learning-Based Affective Computing for Emotion Recognition Using Dual-Decomposed EEG Signals","K. S. Kamble; J. Sengupta","Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology Nagpur, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology Nagpur, Nagpur, India",IEEE Sensors Journal,"31 Jan 2022","2022","22","3","2496","2507","Machine learning (ML)-based algorithms have shown promising results in electroencephalogram (EEG)-based emotion recognition. This study compares five ensemble learning-based ML (EML) algorithms with five conventional ML (CML) algorithms for recognizing multiple human emotions from EEG signals. A publicly available DREAMER database having nine emotions is used to design ML-based system, which is validated on SEED, INTERFACES, and MUSEC databases. In this study, initially, EEG signals are separated into theta, alpha, beta, and gamma bands by applying discrete wavelet transform and then empirical mode decomposition is applied for further decomposition of band-separated EEG signals into intrinsic mode functions (IMFs). Then, 31 statistical features are extracted from IMFs to design ML-based system using five multiclass EML algorithms such as bagging, random forest, rotation random forest, extreme gradient boost, and adaptive boosting. Finally, the performance of these five EML algorithms is evaluated using 10-fold cross-validation and compared against five CML algorithms using performance evaluation metrics such as accuracy, F1-score, kappa-score, and area-under-the-curve (AUC). The mean accuracy of multiclass emotion recognition over five EML algorithms is ~5.87% and ~6.08% higher than the mean accuracy of five CML algorithms, for both arousal (88.95% vs. 83.08%) and valence (88.90% vs. 82.81%) dimensions, respectively. The EML-based bagging algorithm reported the highest accuracy, F1-score, kappa-score, and AUC of 95.81%, 0.81, 0.79, and 0.98, respectively for arousal and 95.53%, 0.80, 0.77, and 0.98, respectively for valence. A similar trend is also observed on the three validation datasets. The EML algorithms provide better multiclass emotion recognition compared to CML algorithms.","1558-1748","","10.1109/JSEN.2021.3135953","Ministry of Education, Government of India, to conduct this research work as a part of a Ph.D. dissertation at the Visvesvaraya National Institute of Technology, Nagpur, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9652403","EEG;emotion recognition;multiclass classification;ensemble learning;machine learning;DREAMER","Electroencephalography;Emotion recognition;Discrete wavelet transforms;Feature extraction;Classification algorithms;Databases;Prediction algorithms","","63","","60","IEEE","15 Dec 2021","","","IEEE","IEEE Journals"
"Functional Connectivity Analysis in Multi-channel EEG for Emotion Detection with Phase Locking Value and 3D CNN","M. Islam; T. Lee","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong",2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"11 Dec 2023","2023","","","1","4","The noise-assisted multivariate Empirical mode decomposition (NA-MEMD) is applied to multi-channel EEG signals to obtain narrow-band scale-aligned intrinsic mode functions (IMFs) upon which functional connectivity analysis is performed. The connectivity pattern in relation to inherent functional activity of brain is estimated with the phase locking value (PLV). Instantaneous phase difference among different EEG channels gives PLV that is used to build the functional connectivity map. The connectivity map yields spatial-temporal feature representation which is taken as input of the proposed emotion detection system. The spatial-temporal features can be learned with a 3D convolutional neural network for classifying emotion states. The proposed system is evaluated on two publicly available DEAP and SEED dataset for binary and multi-class emotion classification. On detecting low versus high level in the valence and arousal dimensions, the attained accuracy values are 97.37% and 96.26% respectively. Meanwhile, this system yields 94.78% and 99.54% accuracy on multi-class task on DEAP and SEED, which outperform previously reported systems with other deep learning models and conventional EEG features.","2694-0604","979-8-3503-2447-1","10.1109/EMBC40787.2023.10340922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340922","emotion;functional connectivity;IMF;PLV;spatial-temporal feature;3D CNN","Emotion recognition;Solid modeling;Three-dimensional displays;Phase measurement;Feature extraction;Brain modeling;Electroencephalography","Electroencephalography;Emotions;Brain;Neural Networks, Computer;Arousal","","","22","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Human emotion recognition based on multi-channel EEG signals using LSTM neural network","P. Lu","University of Electronic Science and Technology of China, Chengdu, China",2022 Prognostics and Health Management Conference (PHM-2022 London),"1 Jul 2022","2022","","","303","308","Electroencephalogram (EEG) signal is often used in emotion recognition tasks to classify human emotions. In this paper, we propose a new approach to learn the temporal features of EEG using long and short-term memory (LSTM), which is a type of Recurrent Neural network (RNN), especially suitable for solving the problem of long-term dependencies such as gradients vanishing and exploding. In addition, to enhance the interaction between EEG signals and to learn the non-linear characteristics between EEG electrodes, we use 1D-Convolution kernel to pre-process the input EEG data. To justify the capability of this method, we set the subject-independent experiments via adopting the leave-one-out experimental strategy on SEED dataset. The result of our experiments shows that this method can effectively capture the timing relationships in EEG signals with high classification accuracy around 93%.","2166-5656","978-1-6654-7954-7","10.1109/PHM2022-London52454.2022.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9808758","Human emotion recognition;LSTM neural network;Multi-channel EEG signals","Emotion recognition;Recurrent neural networks;Brain modeling;Feature extraction;Electroencephalography;Timing;Convolutional neural networks","","3","","34","IEEE","1 Jul 2022","","","IEEE","IEEE Conferences"
"EEG-Based Hardware-Oriented Lightweight 1D-CNN Emotion Classifier","S. Cao; H. Liu; Z. Hou; X. Li; Z. Wu","School of microelectronics, Southeast University, Nanjing, China; School of Electronic Science and Engineering, Southeast University, Nanjing, China; School of microelectronics, Southeast University, Nanjing, China; School of microelectronics, Southeast University, Nanjing, China; School of microelectronics, Southeast University, Nanjing, China",2023 15th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC),"29 Sep 2023","2023","","","210","213","Emotion recognition is crucial in human-computer interaction systems. Compared to human actions and words, EEG-based emotion recognition is more impartial and accurate. With the development of deep learning technology, neural network models have been successfully applied to EEG emotion recognition and achieved good results. However, the high-performance neural network model is difficult to deploy on embedded hardware due to its complex structure and storage resource requirements. Here, a light-weight convolutional neural network is created to address this problem. To begin, we used the raw EEG data to prevent losing hidden features caused by artificial feature screening. Then, we created a one-dimensional convolutional neural network model for emotion recognition using the Conv-BN layer fusion quantization technique. Finally, the method was tested for classification tasks including Valence, Arousal and Arousal-Valence on the DEAP dataset. The accuracy of the suggested model was 96.62%, 98.18%, and 93.16%. The quantified model volume was compressed to 0.317MB, and the model calculation reached 2.1MFLOPs. According to the findings, the proposed model is more hardware-friendly and has a lighter structure than the majority of related studies.","2157-8982","979-8-3503-2617-8","10.1109/IHMSC58761.2023.00056","fundamental research funds for the central universities(grant numbers:3206002204C3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261623","EEG;emotion recognition;raw data;quantization;one-dimensional convolution;hardware friendly","Emotion recognition;Solid modeling;Quantization (signal);Computational modeling;Neural networks;Brain modeling;Electroencephalography","","2","","21","IEEE","29 Sep 2023","","","IEEE","IEEE Conferences"
"Emotion classification based on gamma-band EEG","M. Li; B. -L. Lu","Department of Computer Science and Engineering, MOE-Microsoft Key Laboratory of Intelligent Computing and Intelligent Systems, Shanghai JiaoTong University, Shanghai, China; Department of Computer Science and Engineering, MOE-Microsoft Key Laboratory of Intelligent Computing and Intelligent Systems, Shanghai JiaoTong University, Shanghai, China",2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"13 Nov 2009","2009","","","1223","1226","In this paper, we use EEG signals to classify two emotions-happiness and sadness. These emotions are evoked by showing subjects pictures of smile and cry facial expressions. We propose a frequency band searching method to choose an optimal band into which the recorded EEG signal is filtered. We use common spatial patterns (CSP) and linear-SVM to classify these two emotions. To investigate the time resolution of classification, we explore two kinds of trials with lengths of 3s and 1s. Classification accuracies of 93.5% plusmn 6.7% and 93.0%plusmn6.2% are achieved on 10 subjects for 3s-trials and 1s-trials, respectively. Our experimental results indicate that the gamma band (roughly 30-100 Hz) is suitable for EEG-based emotion classification.","1558-4615","978-1-4244-3296-7","10.1109/IEMBS.2009.5334139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5334139","","Electroencephalography;Frequency;Neuroscience;Psychology;Humans;Computer displays;Emotion recognition;USA Councils;Spatial resolution;Signal resolution","Adult;Artificial Intelligence;Biomedical Engineering;Electroencephalography;Emotions;Female;Humans;Linear Models;Male;Pattern Recognition, Automated;Photic Stimulation;Signal Processing, Computer-Assisted","98","","17","IEEE","13 Nov 2009","","","IEEE","IEEE Conferences"
"NeuroAid: Emotion-Based EEG Analysis for Parkinson's Disease Identification","E. Kumari; M. K. Shukla; O. J. Pandey; S. Yadav","Department of Information Technology, ABV-Indian Institute of Information Technology and Management, Gwalior, India; Department of Information Technology, ABV-Indian Institute of Information Technology and Management, Gwalior, India; Department of Electronics Engineering, Indian Institute of Technology (BHU) Varanasi, Varanasi, India; Department of ECE, Indian Institute of Information Technology Allahabad, Prayagraj, India",IEEE Sensors Letters,"5 Dec 2023","2023","7","12","1","4","Parkinson's disease (PD) is a neurodegenerative condition characterized by intricate behavior and neuronal function changes. The intricacy of these changes makes it difficult to identify PD in its early stages. Experts frequently use manual evaluations of patients' movements, including drawing, writing, walking, tremors, facial expressions, and speech, although this process is laborious and prone to mistakes. A more promising avenue is the utilization of electroencephalogram (EEG) readings, which provide insights into changes in brain activity. Nevertheless, it is important to note that analyzing EEG signals is challenging due to their complexity, nonstationarity, and nonlinearity. In order to overcome these challenges and gain deeper insights into PD and its associated emotions, this letter aims to leverage deep neural networks (DNNs) to extract emotional data from these intricate EEG signals. With this motivation, this letter designs a novel DNN model for PD detection. Moreover, we have conducted experiments and compared the accuracy with several state-of-the-art machine learning and deep learning methods. The performance validation of the DNN model on the benchmark EEG brainwave feeling emotions dataset pointed out the effectiveness of the proposed DNN model with a maximum accuracy of 98.43%.","2475-1472","","10.1109/LSENS.2023.3335226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335756","Sensor applications;deep neural networks (DNNs);electroencephalogram (EEG);machine learning;Parkinson's disease (PD)","Brain modeling;Electroencephalography;Training;Data models;Machine learning;Feature extraction;Adaptation models","","5","","15","IEEE","30 Nov 2023","","","IEEE","IEEE Journals"
"Self-Supervised EEG Emotion Recognition Models Based on CNN","X. Wang; Y. Ma; J. Cammon; F. Fang; Y. Gao; Y. Zhang","School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; Department of Biomedical Engineering, University of Houston, Houston, TX, USA; Department of Biomedical Engineering, University of Houston, Houston, TX, USA; School of Automation, Hangzhou Dianzi University, Hangzhou, China; Department of Biomedical Engineering, University of Houston, Houston, TX, USA",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"11 Apr 2023","2023","31","","1952","1962","Emotion plays crucial roles in human life. Recently, emotion classification from electroencephalogram (EEG) signal has attracted attention by researchers due to the rapid development of brain computer interface (BCI) techniques and machine learning algorithms. However, recent studies on emotion classification show resource utilization because they use the fully-supervised learning methods. Therefore, in this study, we applied the self-supervised learning methods to improve the efficiency of resources usage. We employed a self-supervised approach to train deep multi-task convolutional neural network (CNN) for EEG-based emotion classification. First, six signal transformations were performed on unlabeled EEG data to construct the pretext task. Second, a multi-task CNN was used to perform signal transformation recognition on the transformed signals together with the original signals. After the signal transformation recognition network was trained, the convolutional layer network was frozen and the fully connected layer was reconstructed as emotion recognition network. Finally, the EEG data with affective labels were used to train the emotion recognition network to clarify the emotion. In this paper, we conduct extensive experiments from the data scaling perspective using the SEED, DEAP affective dataset. Results showed that the self-supervised learning methods can learn the internal representation of data and save computation time compared to the fully-supervised learning methods. In conclusion, our study suggests that the self-supervised machine learning model can improve the performance of emotion classification compared to the conventional fully supervised model.","1558-0210","","10.1109/TNSRE.2023.3263570","National Natural Science Foundation of China(grant numbers:62071161,61971168,61372023); Graduate Research Innovation Fund of Hangzhou Dianzi University(grant numbers:CXJJ2022146); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089876","EEG;self-supervised;emotion classification;multi-task learning","Task analysis;Emotion recognition;Electroencephalography;Brain modeling;Feature extraction;Convolutional neural networks;Convolution","Humans;Neural Networks, Computer;Emotions;Algorithms;Machine Learning;Electroencephalography","25","","35","CCBYNCND","31 Mar 2023","","","IEEE","IEEE Journals"
"Emotional Interaction Activities for Home Robots based on EEG Emotion Recognition","T. H. Zhou; C. Yang; L. Wang; D. Li","School of Computer Science, Northeast Electric Power University, Jilin, China; School of Computer Science, Northeast Electric Power University, Jilin, China; School of Computer Science, Northeast Electric Power University, Jilin, China; School of Computer Science, Northeast Electric Power University, Jilin, China",2024 IEEE International Conference on Medical Artificial Intelligence (MedAI),"25 Dec 2024","2024","","","407","417","This study presents an emotion detection system utilizing Electroencephalogram (EEG) data, integrated into the home robot's human-robot interaction model. With advancements in affective computing technology, the demand for home robots has been on the rise, offering emotional support and companionship through emotion recognition and responses, significantly enhancing the user experience. Leveraging the DEAP dataset, this study achieved real-time recognition of users' emotional states through preprocessing, feature extraction, and classification model training. We downsampled, removed artifacts from, and filtered the raw EEG signals, extracting features from the frequency, time, spatial domains, and brain networks. LASSO regression was employed for feature selection, followed by the application of various machine learning algorithms for emotion classification. The experimental results showed that the KNN classifier performed best in emotion recognition, achieving an average accuracy of 90.3%. Further human-robot interaction tests validated the system's practical applicability, with participants rating highly the precision of emotion detection, the timeliness of interactive responses, and the overall experience. The proposed EEG-based emotion recognition system shows significant potential in enhancing the human-robot interaction experience for home robots, while also suggesting directions for further improvements in real-time performance and personalized models.","","979-8-3503-7761-3","10.1109/MedAI62885.2024.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10803397","Emotion Recognition;Home Robots;Data Mining;Human-Robot Interaction;EEG Signal Processing","Training;Emotion recognition;Computational modeling;Human-robot interaction;Feature extraction;Brain modeling;Electroencephalography;Real-time systems;User experience;Robots","","","","31","IEEE","25 Dec 2024","","","IEEE","IEEE Conferences"
"Galvanic Skin Conductance Response and Bio Inspired Algorithms for Human Emotion Classification: A Study","S. Joshi; L. N. B. Srinivas","SRM Institute of Science and Technology, Chennai, India; SRM Institute of Science and Technology, Chennai, India",2023 International Conference on Computer Communication and Informatics (ICCCI),"24 May 2023","2023","","","1","6","Exploring and implying supervised machine learning algorithms inspired with the nature acts as the significant source of motivation. Exploring various natured inspired algorithms to optimize the results of the other algorithm as well is also one of the motivations. Due to their exploration and exploitation capabilities, bio-inspired algorithms are able to bypass the local optimum and locate the global optimum. The algorithms used to create emotion detection models at the present scenario is based on support vector machines, decision trees, random forests and neural networks. It is challenging task to achieve a breakthrough in classification accuracy as traditional emotion recognition does not fully reflect the hidden relationship between EEG signals and emotions states. The more abstract, deeper and discriminating relationship between the EEG states cannot be obtained only by the isomorphic data analysis. A psychological emotion recognition based on skin electric signal features and data was developed to address the aforementioned issues. Human skin is considered for discussion as it can be easily examined without expensive devices.","2473-7577","979-8-3503-4821-7","10.1109/ICCCI56745.2023.10128247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128247","Emotion classification;Galvanic Skin Response;Swarm Intelligence algorithms","Support vector machines;Emotion recognition;Machine learning algorithms;Psychology;Prediction algorithms;Skin;Electroencephalography","","1","","26","IEEE","24 May 2023","","","IEEE","IEEE Conferences"
"Emotion recognition based on low-cost in-ear EEG","G. Li; Z. Zhang; G. Wang","Department of Micro/Nano Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Micro/Nano Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Micro/Nano Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China",2017 IEEE Biomedical Circuits and Systems Conference (BioCAS),"29 Mar 2018","2017","","","1","4","In this paper, we propose a low-cost in-ear EEG device which is implemented by refitting a commercial scalp EEG device, in order to recognize emotion in a manner that is simple, inexpensive, and popular in style. EEG signals of twelve subjects were recorded under three emotion conditions that were induced by music and video materials. By using wavelet packet transformation (WPT), two frequency features and a nonlinear feature are extracted to create a three-dimensional feature vector for each labeled EEG segment. These feature vectors are input into a support vector machine (SVM) classifier for automatic emotion recognition. The SVM classifier achieved a best 94.1% cross-validation accuracy for positive (high valence, HV) and negative (low valence, LV) two-class emotion recognition. However, the accuracy for excited (high valence and high arousal, HVHA), relaxed (high valence and low arousal, HVLA) and negative (LV) multi-class emotion classification was 58.8%. The experimental results show that the proposed low-cost in-ear EEG has outstanding accuracy for valence recognition, but poor accuracy for arousal recognition.","","978-1-5090-5803-7","10.1109/BIOCAS.2017.8325198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8325198","In-Ear EEG;Emotion Recognition;WPT;SVM","","","7","","19","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Emotional behavior analysis based on EEG signal processing using Machine Learning: A case study","S. Klibi; M. Mestiri; I. R. Farah","RIADI Laboratory, University of Manouba National School of Computer Sciences, Tunis, Tunisia; RIADI Laboratory, University of Manouba National School of Computer Sciences, Tunis, Tunisia; RIADI Laboratory, University of Manouba National School of Computer Sciences, Tunis, Tunisia",2021 International Congress of Advanced Technology and Engineering (ICOTEN),"27 Jul 2021","2021","","","1","7","Based on a well-known benchmark, a comparison between the present study and the literature was carried out. This paper investigates a variety of Machine Learning (ML) and Deep Learning (DL) algorithms for classifying emotional events using EEG brainwave data. The contribution of this paper occurs in the data processing phase more precisely at the classification level to predict human emotions either positive, neutral, or negative from EEG signals after applying several algorithms and techniques. According to Bird’s findings, RF augmenting with InfoGain information outperforms Adaptative Boosted LSTM, Adaboosted MLP, and nonboosted DEvo MLP. During the classification phase, we used different classifiers such Random Forest (RF), XgBOOST, NaiveBayes (NB), Decision Tree (DT), Linear RegressionCV (LRCV), Support Vector Machine (SVM), Linear Regression (LR), and Convolutional Neural Networks (CNN) to improve classification performance. They attained an overall accuracy of around 96,88%, 96,41%, 95,47%, 94,06%, 90,00%, 89,06%, 88,91%, and 52,66% respectively. As a result, we find that InfoGain consistently improves RF’s performance in dealing with data and outperforms other classifiers. On the other hand, the inefficiency of CNN can be explained by the lack of a big amount of data.","","978-1-6654-1224-7","10.1109/ICOTEN52080.2021.9493537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493537","Emotional behavior state recognition and classification;Artificial intelligence;Cognitive neuroscience;Brain-Computer Interface BCI;EEG signal processing","Support vector machines;Radio frequency;Emotion recognition;Machine learning algorithms;Signal processing algorithms;Virtual reality;Signal processing","","6","","26","IEEE","27 Jul 2021","","","IEEE","IEEE Conferences"
"Emotion Recognition and Stress Reduction Based on Electroencephalograph (EEG) Signals validated by Machine Learning Algorithms","S. Baliga; N. Ali; A. LS; V. P","Electronics and Communication Engineering, RV College of Engineering, Bengaluru, India; Adavanced Data Science Associate, ZS Associates, Bengaluru, India; Electronics and Communication Engineering, RV College of Engineering, Bengaluru, India; Electronics and Communication Engineering, RV College of Engineering, Bengaluru, India",2023 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES),"7 Aug 2023","2023","","","1","6","Research shows that stress is a typical reaction to external or internal irritants, but prolonged stress may impair health and brain function. Stress causes brain damage and sadness. Emotional awareness may reduce stress. Emotions complicate life. Assess emotions. Suicide note sentiment analysis has not used NLP and ML. Three parts follow. SVM algorithms categorise individuals as “under stress” or “not under stress” using EEG-based psychological states and signals. Merged LSTM reveals yoga and calming music reduce stress. Finally, emotions explain stress. DEAP synchronised brainwave dataset comprises patient data and EEG signal levels. Valence/arousal and EEG detect emotions. The discrete wavelet method (DWT) recovers alpha, gamma, theta, and beta EEG data spectral characteristics. Same-dimensional transforms isolate ICA features. ANN sorts emotions. SVM's 92.86 percent accuracy and LSTM's 75.23 percent show yoga and relaxing music soothe 75% of stressed people. A cross-validated RBF kernel-based SVM classifies beta emotions with 90.3% arousal and 91.11% valence. This research analyses mental stress EEG signals. It underlines research inconsistencies and suggests data analysis may cause them. Standardised approach, brain region of interest, stressor type, experiment duration, EEG processing, feature extraction, and classifier may alter results. Use appropriate traits to detect stress-related emotions. Yoga and music are also used to identify and treat stress in the study.","","979-8-3503-4729-6","10.1109/ICSSES58299.2023.10199291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10199291","EEG signals;EEGLAB;MATLAB;Yoga;Calm Music;Support Vector Machine;Long Short Time Memory;Emotion;Feature extraction;Classification;K-Nearest Neighbors;Artificial Neural Network;Stress;Independent Component Analysis","Support vector machines;Sentiment analysis;Atmospheric modeling;Human factors;Transforms;Mental health;Electroencephalography","","1","","20","IEEE","7 Aug 2023","","","IEEE","IEEE Conferences"
"Deep Learning Model With Adaptive Regularization for EEG-Based Emotion Recognition Using Temporal and Frequency Features","A. Samavat; E. Khalili; B. Ayati; M. Ayati","Department of Biomedical Engineering, Islamic Azad University of Tehran-Central Branch, Tehran, Iran; Department of Biomedical Engineering, University of Tarbiat Modares, Tehran, Iran; Department of Biomedical Engineering, Islamic Azad University of Tehran-Central Branch, Tehran, Iran; Department of Computer Science, University of Texas Rio Grande Valley, Edinburg, TX, USA",IEEE Access,"9 Mar 2022","2022","10","","24520","24527","Since EEG signal acquisition is non-invasive and portable, it is convenient to be used for different applications. Recognizing emotions based on Brain-Computer Interface (BCI) is an important active BCI paradigm for recognizing the inner state of persons. There are extensive studies about emotion recognition, most of which heavily rely on staged complex handcrafted EEG feature extraction and classifier design. In this paper, we propose a hybrid multi-input deep model with convolution neural networks (CNNs) and bidirectional Long Short-term Memory (Bi-LSTM). CNNs extract time-invariant features from raw EEG data, and Bi-LSTM allows long-range lateral interactions between features. First, we propose a novel hybrid multi-input deep learning approach for emotion recognition from raw EEG signals. Second, in the first layers, we use two CNNs with small and large filter sizes to extract temporal and frequency features from each raw EEG epoch of 62-channel 2-s and merge with differential entropy of EEG band. Third, we apply the adaptive regularization method over each parallel CNN’s layer to consider the spatial information of EEG acquisition electrodes. The proposed method is evaluated on two public datasets, SEED and DEAP. Our results show that our technique can significantly improve the accuracy in comparison with the baseline where no adaptive regularization techniques are used.","2169-3536","","10.1109/ACCESS.2022.3155647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723066","EEG;emotion recognition;deep learning","Electroencephalography;Feature extraction;Entropy;Brain modeling;Emotion recognition;Logic gates;Deep learning","","26","","31","CCBY","28 Feb 2022","","","IEEE","IEEE Journals"
"EEG Emotion Recognition Based on 3D-CTransNet","H. Luo; X. Zhao; T. Zhou; Z. Wang; T. Xu; H. Hu","School of Microelectronics, Shanghai University, Shanghai, China; School of Microelectronics, Shanghai University, Shanghai, China; School of Microelectronics, Shanghai University, Shanghai, China; Shanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai, China; Shanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai, China; Shanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai, China",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","Emotion recognition is of great significance for brain-computer interface and emotion computing, and EEG plays a key role in this field. However, the current design of brain computer interface deep learning model is faced with algorithmic or structural constraints, and it is difficult to recognize the complex features in EEG signals with long-term dynamic changes. To solve this issue, a hybrid CNN-Transformer structure using 3D data input is proposed and named 3D-CTransNet in this paper, which solves the problem of performance degradation of the traditional CNN-LSTM hybrid structure in the recognition of long sequence signals. At the same time, the self attention mechanism and parallel mode introduced by Transformer improve the recognition accuracy and processing speed. In addition, the 3D data feature map based on electrode position mapping effectively retains the spatial characteristics of EEG signals, which makes CNN better combine the time domain and spatial domain. Finally, the Valence-Arousal classification training of emotion is carried out on the public dataset DEAP, and the classification accuracy is 97.04%, which is about 5% higher than that of the hybrid CNN-LSTM model.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782401","","Electrodes;Emotion recognition;Three-dimensional displays;Attention mechanisms;Accuracy;Heuristic algorithms;Brain modeling;Transformers;Electroencephalography;Brain-computer interfaces","Electroencephalography;Humans;Emotions;Algorithms;Signal Processing, Computer-Assisted;Neural Networks, Computer;Brain-Computer Interfaces;Deep Learning","","","11","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"EEG-GCN: Spatio-Temporal and Self-Adaptive Graph Convolutional Networks for Single and Multi-View EEG-Based Emotion Recognition","Y. Gao; X. Fu; T. Ouyang; Y. Wang","School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China",IEEE Signal Processing Letters,"22 Jul 2022","2022","29","","1574","1578","Graph networks are naturally suitable for modeling multi-channel features of EEG signals. However, the existing study that attempts to utilize graph-based neural networks for EEG-based emotion recognition doesn’t take the spatio-temporal redundancy of EEG features and differences in brain topology into account. In this paper, we propose EEG-GCN, a paradigm that adopts spatio-temporal and self-adaptive graph convolutional networks for single and multi-view EEG-based emotion recognition. With spatio-temporal attention mechanism employed, EEG-GCN can adaptively capture significant sequential segments and spatial location information in EEG signals. Meanwhile, a self-adaptive brain network adjacency matrix is designed to quantify the connection strength between the channels, in which way to represent the diverse activation patterns under different emotion scenarios. Additionally, we propose a multi-view EEG-based emotion recognition method, which effectively integrates the diverse features of EEG signals. Extensive experiments conducted on two benchmark datasets SEED and DEAP demonstrate that our proposed method outperforms other representative methods from both single and multiple views.","1558-2361","","10.1109/LSP.2022.3179946","Beijing Natural Science Foundation(grant numbers:M22012,L192026); National Natural Science Foundation of China(grant numbers:82071171); BUPT Excellent Ph.D. Students Foundation(grant numbers:CX2022133); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9788054","EEG;emotion recognition;graph convolutional neural network;spatio-temporal attention mechanism","Electroencephalography;Feature extraction;Emotion recognition;Convolution;Frequency-domain analysis;Brain modeling;Network topology","","48","","32","IEEE","3 Jun 2022","","","IEEE","IEEE Journals"
"Emotion Recognition Based on DEAP Database Physiological Signals","T. Stajić; J. Jovanović; N. Jovanović; M. M. Janković","School of Electrical Engineering, University of Belgrade, Belgrade, Serbia; School of Electrical Engineering, University of Belgrade, Belgrade, Serbia; School of Electrical Engineering, University of Belgrade, Belgrade, Serbia; School of Electrical Engineering, University of Belgrade, Belgrade, Serbia",2021 29th Telecommunications Forum (TELFOR),"29 Dec 2021","2021","","","1","4","Recognizing and accurately classifying human emotion is a complex and challenging task. Recently, great attention is paid to the emotion recognition methods using three different approaches: based on non-physiological signals (like speech and facial expression), based on physiological signals or based on hybrid approaches. Non-physiological signals are easily controlled by the individual, so these approaches have downsides in real world applications. In this paper, an approach based on physiological signals which cannot be willingly influenced (electroencephalogram, heartrate, respiration, galvanic skin response, electromyography, body temperature) is presented. Publicly available DEAP database was used for the binary classification (high vs. low) considering four frequently used emotional parameters (arousal, valence, liking and dominance). We have extracted 1490 features from the dataset, reduced to less than 15% (200 most significant features) and applied three different classification approaches – Support Vector Machine, Boosting algorithms and Artificial Neural Networks.","","978-1-6654-2585-8","10.1109/TELFOR52709.2021.9653286","Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653286","emotion recognition;machine learning;physiological signals;DEAP database","Support vector machines;Emotion recognition;Databases;Face recognition;Artificial neural networks;Feature extraction;Physiology","","8","","26","IEEE","29 Dec 2021","","","IEEE","IEEE Conferences"
"An Efficient Deep Learning Technique for Emotion Detection Using EEG Signal","A. Vishwakarma; V. Sakalle","CSE Department, LNCT University, Bhopal, India; CSE Department, LNCT University, Bhopal, India",2024 4th International Conference on Technological Advancements in Computational Sciences (ICTACS),"21 Jan 2025","2024","","","1275","1280","EEG based emotion detection has been widely focused and it has applications in health care, HCI, and affective computing. In this paper, a new approach for detecting the emotion from the EEG signal is proposed by using Long Short-Term Memory (LSTM) networks. The proposed method is suitable for EEG data given temporal differences since it plays an important role when measuring emotional states. The first step is noise and artifacts reduction, the second step is extraction of features from time and frequency domain. All these constitute as a data input into an LSTM network, used for learning and outlining temporal characteristics inherent in the EEG sequences. and the performance of our approach is assessed on a standard EEG dataset with emotional annotations. The proposed LSTM-based model outperforms conventional ML methods and other deep learning models in terms of accuracy and their resistance to distortions.","","979-8-3503-8749-0","10.1109/ICTACS62700.2024.10841173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10841173","Recognition;Machine;Deep;LSTM;Emotion Expression;Artificial Intelligence","Deep learning;Emotion recognition;Accuracy;Simulation;Feature extraction;Brain modeling;Electroencephalography;Software;Long short term memory;Standards","","","","15","IEEE","21 Jan 2025","","","IEEE","IEEE Conferences"
"A Machine Learning Model to Recognise Human Emotions using Electroencephalogram","N. Roy; S. Aktar; M. M. Ahamad; M. A. Moni","Department of Computer Science & Engineering, Bangabandhu Sheikh Mujibur Rahman Science and Technology University, Gopalganj, Bangladesh; Department of Computer Science & Engineering, Bangabandhu Sheikh Mujibur Rahman Science and Technology University, Gopalganj, Bangladesh; Department of Computer Science & Engineering, Bangabandhu Sheikh Mujibur Rahman Science and Technology University, Gopalganj, Bangladesh; School of Health and Rehabilitation Sciences Faculty of Health and Behavioural Sciences, The University of Queensland, Australia",2021 5th International Conference on Electrical Information and Communication Technology (EICT),"16 Mar 2022","2021","","","1","6","Electroencephalogram (EEG) is one kind of signal taken from the brain, and it can detect human activities, behavior, and effects of physical and mental illness. To use artificial intelligence for analyzing electroencephalogram signals, we can communicate with the human brain without any human behaviors or gestures. The best possible recognition of emotion from an electroencephalogram has gotten a challenging task be-cause of certain complexities. Ordinarily, feature-based emotion recognition requires a definite attempt to design a proper model to work with related features. This study has chosen a publicly available feature based electroencephalogram dataset and applied a set of machine learning algorithms, and detected three types of different human emotions, i.e., relaxed, concentrating, and neutral. We use recursive feature elimination techniques and Principal Component Analysis to select features and train our models and applied sex different classifiers and assessed their performance. On recursive feature elimination techniques, the algorithm light gradient boosting machine achieves the highest average 97.7% and for Principal Component Analysis feature selection techniques, the algorithm light gradient boosting ma-chine achieves the highest average 92.17% accuracy score in 10 fold cross-validation. We also measured their performance through precision, recall, and F1 scores by using machine learning methods. By using machine learning methods we can also recognize Human Emotions.","","978-1-6654-0906-3","10.1109/EICT54103.2021.9733675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733675","Emotion Classification;EEG;Brain-Machine Interface;Machine Learning;PCA;RFE","Training;Emotion recognition;Analytical models;Machine learning algorithms;Brain modeling;Feature extraction;Boosting","","3","","14","IEEE","16 Mar 2022","","","IEEE","IEEE Conferences"
"Spatiotemporal Emotion Recognition Method Based on EEG Signals During Music Listening Using 1D-CNN & Stacked-LSTM","S. Liao; Y. Zhang; H. Yang; X. Liao","School of Computer Science, Shaanxi Normal University, Xi'an, China; School of Computer Science, Shaanxi Normal University, Xi'an, China; School of Computer Science, Shaanxi Normal University, Xi'an, China; School of Computer Science, Shaanxi Normal University, Xi'an, China",2022 International Conference on Networking and Network Applications (NaNA),"30 Dec 2022","2022","","","7","12","Recognizing people's emotions accurately can help to improve people's feeling of happiness by adjusting their emotion immediately, which makes emotion recognition an active research topic recently. Electroencephalography (EEG) signals, which are electrical response of the human brain scalp, reflecting people's emotions and psychological activities, can be applied as an important tool for the emotion recognition. This paper focuses on the emotion recognition based on EEG signals during music listening. To this end, we first propose an emotion recognition scheme by combining the one-dimensional convolutional neural network (1D-CNN) and the stacked long short term memory (Stacked-LSTM), where the 1D-CNN is exploited to extract spatial features from EEG signals automatically and the Stacked-LSTM is applied for further temporal features extraction. We then conducted lots of experiments to validate the efficiency of our proposed scheme regarding the accuracy of emotion recognition. Finally, a comparison between our proposed scheme and other commonly methods used for emotion recognition based EEG signals (e.g., EEGNet, 1D-CNN, LSTM and SVM). The experimental results showed that our proposed scheme is feasible and outperform other commonly used methods in terms of classification accuracy.","","978-1-6654-6131-3","10.1109/NaNA56854.2022.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985048","Emotion recognition;EEG;deep learning;1D-CNN;Stacked-LSTM;music listening","Support vector machines;Emotion recognition;Music;Feature extraction;Electroencephalography;Multiple signal classification;Spatiotemporal phenomena","","","","24","IEEE","30 Dec 2022","","","IEEE","IEEE Conferences"
"Analysis of bimodal emotion recognition method based on EEG signals","G. Huang; Z. Song","Xiamen University, Xiamen, Fujian Province, China; Xiamen University, Xiamen, Fujian Province, China","2021 2nd International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","8 Mar 2022","2021","","","217","220","Emotions are a natural component of life, and the human brain's central nervous system regulates their generations and expressions. The capacity of computers to identify and replicate human emotions can be improved by studying the brain mechanisms underlying human emotion display. Audio, which is currently one of the most popular types of multimedia stimulation, may communicate a wide range of emotional meanings due to its aural characteristics. In addition to audio features, EEG features can provide useful novelty for emotion recognition, as these features are the most realistic feedback on human emotion perception. This paper constructed a fused dataset of EEG and audio features based on the SEED EEG dataset. A deep learning model based on Long-Short Term Memory Network (LSTM) is utilized to find the best model for audio emotion identification using the bimodal dataset. We discovered that combining the full-band EEG power spectral density and audio fusion characteristics resulted in the best recognition. Simultaneously, We extracted features from each original stimulus audio and added audio characteristics from each period to examine the influence of emotion recognition and investigate the inherent link between the creation of EEG signals and the original stimulus audio in-depth. The study shows that the method can be used as a potential video emotion indexing method for video information retrieval.","","978-1-6654-1296-4","10.1109/AINIT54228.2021.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9725111","Bimodal;EEG;Signal processing","Seminars;Emotion recognition;Analytical models;Streaming media;Brain modeling;Feature extraction;Electroencephalography","","1","","13","IEEE","8 Mar 2022","","","IEEE","IEEE Conferences"
"Intelligent Feature Selection for EEG Emotion Classification","L. Yang; Q. Chen; Q. Zhang; S. Chao","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China",2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"14 Jan 2022","2021","","","3681","3688","Emotion classification plays a critical role in the development of human-computer interaction. EEG (Electroencephalogram) signal is an important information source, from which different features have been extracted for the study of emotion states. However, there is a large amount of redundant and irrelevant information in EEG signals, which directly interferes with emotion classification. Aiming at selecting EEG emotional features precisely and efficiently, this paper proposed a feature selection framework, termed MIGA (Mutual Information and Genetic Algorithm). It combined mutual information and genetic algorithm to improve the quality of feature subset based on three perspectives, that is correlation, contribution and synergistic effect between features. A new emotional feature IQI (Intensity Quantity of Information) was also designed in this paper. IQI is able to mine the intensity information of EEG signals, and enlarge the samples’ distance as a result. Experiments were carried on DEAP (A Database for Emotion Analysis using Physiological Signals). Results show that the classification accuracy of IQI is 5% higher than that of the traditional frequency domain feature, and MIGA reduces feature amount by 2/3 while ensuring classification accuracy. For DEAP, classification accuracy of MIGA in valence and arousal reached 88.55% and 88.14% respectively. It is indicated that, compared with existing methods, MIGA improves emotion recognition with much fewer features from EEG.","","978-1-6654-0126-5","10.1109/BIBM52615.2021.9669646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669646","EEG signals;Emotion classification;Feature selection;Machine learning","Human computer interaction;Emotion recognition;Time-frequency analysis;Uncertainty;Feature extraction;Electroencephalography;Real-time systems","","2","","29","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"Emotion recognition based on EEG directed functional connectivity using deep learning algorithm","M. T. Chai; C. M. Goh; S. A. Z. S. Aluwee","Department of Computer Science, Universiti Tunku Abdul Rahman Kampar, Perak, Malaysia; Department of Computer Science, Universiti Tunku Abdul Rahman Kampar, Perak, Malaysia; Department of Computer Science, Universiti Tunku Abdul Rahman Kampar, Perak, Malaysia",2022 3rd International Conference on Artificial Intelligence and Data Sciences (AiDAS),"26 Oct 2022","2022","","","192","197","Numerous features have been extracted from multichannel electroencephalography (EEG) signals, however, the directed functional connectivity derived from EEG signals for emotion recognition using deep learning algorithm remains unexplored. The present work aimed to exploit emotion recognition based on directed functional connectivity with a deep learning (DL) algorithm. First, this paper reviews the current trends of DL implementation for emotion recognition using EEG and identify their limitations and future directions. Second, the directed functional connectivity was extracted from EEG and was used as input to long short-term memory-recurrent neural network (LSTM-RNN). The performance of the proposed framework was validated using DEAP dataset. Results suggest that the LSTM-RNN is suitable for classifying emotional valence (low vs. high) and arousal (low vs. high) with accuracy of 95.28% and 96.17%, respectively. This study provide preliminary evidence of EEG-based directed functional connectivity and DL method for recognizing emotion.","","978-1-6654-9164-8","10.1109/AiDAS56890.2022.9918689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918689","Deep learning;electroencephalography;emotion recognition","Deep learning;Emotion recognition;Image recognition;Neural networks;Functional magnetic resonance imaging;Feature extraction;Brain modeling","","1","","35","IEEE","26 Oct 2022","","","IEEE","IEEE Conferences"
"An On-Chip Processor for Chronic Neurological Disorders Assistance Using Negative Affectivity Classification","A. R. Aslam; M. A. B. Altaf","Electrical Engineering Department, Lahore University of Management Sciences, Lahore, Pakistan; Electrical Engineering Department, Lahore University of Management Sciences, Lahore, Pakistan",IEEE Transactions on Biomedical Circuits and Systems,"21 Aug 2020","2020","14","4","838","851","Chronic neurological disorders (CND's) are lifelong diseases and cannot be eradicated, but their severe effects can be alleviated by early preemptive measures. CND's, such as Alzheimer's, Autism Spectrum Disorder (ASD), and Amyotrophic Lateral Sclerosis (ALS), are the chronic ailment of the central nervous system that causes the degradation of emotional and cognitive abilities. Long term continuous monitoring with neuro-feedback of human emotions for patients with CND's is crucial in mitigating its harmful effect. This paper presents hardware efficient and dedicated human emotion classification processor for CND's. Scalp EEG is used for the emotion's classification using the valence and arousal scales. A linear support vector machine classifier is used with power spectral density, logarithmic interhemispheric power spectral ratio, and the interhemispheric power spectral difference of eight EEG channel locations suitable for a wearable non-invasive classification system. A look-up-table based logarithmic division unit (LDU) is to represent the division features in machine learning (ML) applications. The implemented LDU minimizes the cost of integer division by 34% for ML applications. The implemented emotion's classification processor achieved an accuracy of 72.96% and 73.14%, respectively, for the valence and arousal classification on multiple publicly available datasets. The 2 x 3mm2 processor is fabricated using a 0.18 μm 1P6M CMOS process with power and energy utilization of 2.04 mW and 16 μJ/classification, respectively, for 8-channel operation.","1940-9990","","10.1109/TBCAS.2020.3008766","Higher Education Commission, Pakistan(grant numbers:7978/Punjab/NRPU/R&D/HEC/2017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139281","Continuous health monitoring;classification processor;electroencephalogram (EEG);emotion detection;machine learning;neurological disorder;support vector machine","Electroencephalography;System-on-chip;Hardware;Software;Neurological diseases;Variable speed drives;Real-time systems","Arousal;Autism Spectrum Disorder;Chronic Disease;Electroencephalography;Emotions;Equipment Design;Humans;Lab-On-A-Chip Devices;Machine Learning;Male;Monitoring, Physiologic;Nervous System Diseases;Signal Processing, Computer-Assisted;Support Vector Machine","37","","62","IEEE","13 Jul 2020","","","IEEE","IEEE Journals"
"Automatic Emotion Recognition From Multi-Band EEG Data Based on a Deep Learning Scheme With Effective Channel Attention","O. Saha; M. S. Mahmud; S. A. Fattah; M. Saquib","Department of Electrical and Electronic Engineering (EEE), Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical Engineering, The University of Texas at Dallas, Richardson, TX, USA",IEEE Access,"10 Jan 2023","2023","11","","2342","2350","Automatic emotion recognition using electroencephalogram (EEG) has obtained a wide range of attention in the domain of human-computer interaction (HCI) owing to the notable differences in brain activities in the event of different types of emotions. In this paper, a novel emotion recognition approach is proposed based on a deep learning scheme utilizing the temporal, spatial, and frequency effects of the EEG signal. As neural firing provides a pathway to elicit emotions, temporal, spatial, and frequency sub-band analysis of EEG signals uncovers salient information to categorize different classes of emotions. In this regard, temporal data from each channel are divided into major spectral bands and 2D signal matrices are constructed by combining the temporal information of different frequency band signals. After concatenating all signal matrices obtained from the available channels, a 3D feature space is obtained, which can better characterize emotion variations and, thus, better classification performance is obtained. The feature space is applied to a 2D deep neural network where the band information is passed to the depth dimension of the neural network. In order to highlight the important channels, a channel attention mechanism is proposed with the neural network to distribute the weights among the channels according to the contribution. Hence, the modified feature space effectively captures distinctive information about specific channels in the context of emotion recognition. In this study, detailed and extensive experimentations are carried out on a publicly available DEAP dataset and a very satisfactory performance is obtained for the valence and the arousal domain in 2-class scenario for the subject-dependent case. The average accuracies obtained for valence and arousal domain in binary class problem are 97.06% and 96.93%, respectively.","2169-3536","","10.1109/ACCESS.2022.3224725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9963974","Electroencephalogram (EEG);brain-computer interface (BCI);frequency bands;channel attention;deep learning","Electroencephalography;Feature extraction;Emotion recognition;Deep learning;Brain modeling;Time-frequency analysis;Three-dimensional displays","","8","","31","CCBYNCND","24 Nov 2022","","","IEEE","IEEE Journals"
"Subject-Invariant Eeg Representation Learning For Emotion Recognition","S. Rayatdoost; Y. Yin; D. Rudrauf; M. Soleymani","Swiss Center for Affective Sciences (CISA) and Computer Science Department, University of Geneva; USC Institute for Creative Technologies, University of Southern California; Department of Psychology, CISA and Centre Universitaire d’Informatique, University of Geneva; USC Institute for Creative Technologies, University of Southern California","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","3955","3959","The discrepancies between the distributions of the train and test data, a.k.a., domain shift, result in lower generalization for emotion recognition methods. One of the main factors contributing to these discrepancies is human variability. Domain adaptation methods are developed to alleviate the problem of domain shift, however, these techniques while reducing between database variations fail to reduce between-subject variability. In this paper, we propose an adversarial deep domain adaptation approach for emotion recognition from electroencephalogram (EEG) signals. The method jointly learns a new representation that minimizes emotion recognition loss and maximizes subject confusion loss. We demonstrate that the proposed representation can improve emotion recognition performance within and across databases.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414496","Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414496","EEG signals;emotion recognition;domain adaptation;deep learning;representation learning","Emotion recognition;Adaptation models;Databases;Conferences;Speech recognition;Signal processing;Brain modeling","","7","","21","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Classification of EEG correlates on emotion using features from Gaussian mixtures of EEG spectrogram","R. Khosrowabadi; A. W. b. A. Rahman","Center of Computational Intelligence, School of Computer Engineering, Nanyang Technological University, Singapore; Division of Computer Science, School of Iriformation and Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia",Proceeding of the 3rd International Conference on Information and Communication Technology for the Moslem World (ICT4M) 2010,"4 Aug 2011","2010","","","E102","E107","This paper presents the classification of EEG correlates on emotion using features extracted by Gaussian mixtures of EEG spectrogram. This method is compared with three feature extraction methods based on fractal dimension of EEG signal including Higuchi, Minkowski Bouligand, and Fractional Brownian motion. The K nearest neighbor and Support Vector Machine are applied to classify extracted features. The 4 emotional states investigated in this paper are defined using the valence-arousal plane: two valence states (positive and negative) and two arousal states (calm, excited). The accuracy of system to classify 4 emotional states is investigated on EEG collected from 26 subjects (20 to 32 years old) while exposed to emotionally-related visual and audio stimuli. The results showed that the proposed feature extraction using Gaussian mixtures of EEG spectrogram yielded better classification results using the KNN classifier.","","978-1-4244-7922-1","10.1109/ICT4M.2010.5971942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5971942","Electroencephalography (EEG) Emotion recognition;Fractal dimension;Gaussian mixture model;spectrogram","Spectrogram;Electroencephalography;Brain modeling;Feature extraction;Mathematical model","","26","","31","IEEE","4 Aug 2011","","","IEEE","IEEE Conferences"
"A real-time classification algorithm for emotion detection using portable EEG","S. Cheemalapati; M. Gubanov; M. Del Vale; A. Pyayt","University of South Florida, Tampa, FL, USA; Massachussetts Institute of Technology, Cambridge, MA, USA; University of South Florida, Tampa, FL, USA; University of South Florida, Tampa, FL, USA",2013 IEEE 14th International Conference on Information Reuse & Integration (IRI),"24 Oct 2013","2013","","","720","723","Military personnel, airplane pilots, and bus drivers often operate in stressful conditions when something unexpected can happen and cause dangerous consequences if they do not respond properly. Additionally, stress adversely affects human decision making abilities, therefore prompt, preferably real-time detection of fear is very important. Based on previous studies for non-portable multi-electrode electroencephalography (EEG) systems the ratio of the power of the slow waves to that of the fast waves increases when a person is relaxed and decreases when s/he is scared. In this study we test small portable EEG and develop algorithms for real time detection of the stressful condition — fear. During the experiment we compare EEG signals of subjects in relaxed state with those in stressed state while they are watching a scene from a scary movie. The ratio of the slow/fast wave powers was measured and the observed pattern was similar to one obtained using a multi-electrode system. We integrate stream-processing algorithms in the system to ensure real-time detection of any changes in mental condition and timely generate the alarm event.","","978-1-4799-1050-2","10.1109/IRI.2013.6642541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6642541","Emotion classification;Portable EEG;Brain-computer Interface","Electroencephalography;Real-time systems;Electrodes;Motion pictures;Engines;Physiology;Emotion recognition","","12","2","20","IEEE","24 Oct 2013","","","IEEE","IEEE Conferences"
"Emotion Personalization with Machine Learning using EEG Signals and Dry Electrodes","H. Amrani; D. Micucci; M. Nalin; P. Napoletano","Department of Informatics, System and Communication, University of Milano-Bicocca, Milano, Italy; Department of Informatics, System and Communication, University of Milano-Bicocca, Milano, Italy; ab medica s.p.a., Cerro Maggiore, Milano, Italy; Department of Informatics, System and Communication, University of Milano-Bicocca, Milano, Italy","2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)","1 Feb 2024","2023","","","132","137","Personalization is essential in enhancing the performance of machine learning models in brain-computer interfaces (BCIs) for emotion recognition, specifically in valence and arousal classification. In this work, we address the challenge of personalizing BCI models utilizing a wireless consumer non-invasive electroencephalogram (EEG) device with dry electrodes. Our research investigates the effectiveness of three machine learning algorithms in classifying valence and arousal: k-Nearest Neighbors (k-NNs), Support Vector Machines (SVMs), and Artificial Neural Networks (ANNs). To achieve personalization, we adopt an incremental learning approach by progressively incorporating high-quality subject data during model training that are taken from the ground-truth. We compare the performance of the models before and after personalization. The results demonstrate significant improvements in valence and arousal classification accuracy through personalization, with the personalized models outperforming the non-personalized models by up to 27.82% and 28.80%, respectively.","","979-8-3503-0080-2","10.1109/MetroXRAINE58569.2023.10405681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10405681","Brain-computer Interface;Electroencephalography;Dry electrodes;Emotion;Machine Learning;Personalization;Incremental Learning","Performance evaluation;Electrodes;Adaptation models;Emotion recognition;Machine learning;Brain modeling;Electroencephalography","","","","27","IEEE","1 Feb 2024","","","IEEE","IEEE Conferences"
"An evaluation of feature extraction in EEG-based emotion prediction with support vector machines","I. Wichakam; P. Vateekul","Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand",2014 11th International Joint Conference on Computer Science and Software Engineering (JCSSE),"26 Jun 2014","2014","","","106","110","Electroencephalograph (EEG) data is a recording of brain electrical activities, which is commonly used in emotion prediction. To obtain promising accuracy, it is important to perform a suitable data preprocessing; however, different works employed different procedures and features. In this paper, we aim to investigate various feature extraction techniques for EEG signals. To obtain the best choice, there are four factors investigated in the experiment: (i) the number of channels, (ii) signal transformation methods, (iii) feature representations, and (iv) feature transformation techniques. Support Vector Machine (SVM) is chosen to be our baseline classifier due to its promising performance. The experiments were conducted on the DEAP benchmark dataset. The results showed that the prediction on EEG signals from 10 channels represented by the band power one-minute features gave the best accuracy and F1.","","978-1-4799-5822-1","10.1109/JCSSE.2014.6841851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841851","EEG;emotion;feature extraction;classification;prediction","","","37","","9","IEEE","26 Jun 2014","","","IEEE","IEEE Conferences"
"Three Dimensional Emotion State Classification based on EEG via Empirical Mode Decomposition","N. Gahlan; D. Sethia","Dept. of Software Engineering, Delhi Technological University, New Delhi, India; Dept. of Software Engineering, Delhi Technological University, New Delhi, India",2023 International Conference on Artificial Intelligence and Applications (ICAIA) Alliance Technology Conference (ATCON-1),"6 Jul 2023","2023","","","1","6","Electroencephalography (EEG) is useful for mapping emotions directly from the brain, but its heterogeneous signals make it challenging to extract features accurately. Prior works for emotion classification uses EEG data without removing data heterogeneity leading to misclassification or inaccurate classification. This paper proposes an EMD-based methodology for EEG data that segments signals into multiple IMFs to remove heterogeneity and extract significant features. The proposed approach uses a Feed-Forward Neural Network (FFNN) to classify emotions via the VAD model and shows a 5-6% increment in accuracy, precision, and recall scores for emotion classification. Experimental results demonstrate good evaluation performance scores for classifying emotional states on two publicly accessible emotional datasets, AMIGOS and DREAMER.","","978-1-6654-5627-2","10.1109/ICAIA57370.2023.10169633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10169633","Emotion Classification;Electroencephalogram (EEG);Empirical Mode Decomposition (EMD);Temporal and Spectral features","Empirical mode decomposition;Feature extraction;Brain modeling;Electroencephalography;Feedforward neural networks;Data mining;Artificial intelligence","","5","","17","IEEE","6 Jul 2023","","","IEEE","IEEE Conferences"
"An EEG-based Brain-Computer Interface for Stress Analysis","S. N; J. J","Jayachitra J Dept of Information Technology, IFET College of Engineering, Villupuram, India; Jayachitra J Dept of Information Technology, IFET College of Engineering, Villupuram, India",2023 Second International Conference on Electronics and Renewable Systems (ICEARS),"5 Apr 2023","2023","","","355","360","This research study aims to investigate the feasibility of using EEG data and machine learning algorithms to accurately identify the levels of stress experienced by people with autism spectrum disorder (ASD) as well as those who are developing ordinarily. You mentioned that the study utilized both conventional brain-computer interface (BCI) methods and deep learning methodologies to train subject-dependent models using mental arithmetic stress induction and EEG data. The results of the study showed that a multiclass two-layer LSTM RNN deep learning classifier could accurately identify mental stress based on continuous EEG, which could potentially lead to the development of an EEG-based BCI that could monitor and relieve mental stress in real-time in teenagers who are autistic as well as those who are neurotypical. This article also suggested that this approach could potentially pave the way for an effective therapy that does not rely on the use of pharmaceuticals for those who suffer from anxiety disorders. However, it is important to note that further research is needed to validate the results and determine the feasibility of implementing such a therapy. As for the citation, I would need more information on the specific study you are referring to in order to provide a proper citation.","","979-8-3503-4664-0","10.1109/ICEARS56392.2023.10085458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10085458","Recognition of Emotion;Long Short-Term Memory Networks;Emotion Images;Feature Collection;Feature Extraction;Recurrent Neural Networks;Classification","Deep learning;Anxiety disorders;Human factors;Feature extraction;Brain modeling;Electroencephalography;Brain-computer interfaces","","1","","20","IEEE","5 Apr 2023","","","IEEE","IEEE Conferences"
"Improving Emotion Recognition Systems by Exploiting the Spatial Information of EEG Sensors","G. Gagliardi; A. L. Alfeo; V. Catrambone; D. Candia-Rivera; M. G. C. A. Cimino; G. Valenza","Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy",IEEE Access,"1 May 2023","2023","11","","39544","39554","Electroencephalography (EEG)-based emotion recognition is gaining increasing importance due to its potential applications in various scientific fields, ranging from psychophysiology to neuromarketing. A number of approaches have been proposed that use machine learning (ML) technology to achieve high recognition performance, which relies on engineering features from brain activity dynamics. Since ML performance can be improved by utilizing 2D feature representation that exploits the spatial relationships among the features, here we propose a novel input representation that involves re-arranging EEG features as an image that reflects the top view of the subject’s scalp. This approach enables emotion recognition through image-based ML methods such as pre-trained deep neural networks or “trained-from-scratch” convolutional neural networks. We have employed both of these techniques in our study to demonstrate the effectiveness of our proposed input representation. We also compare the recognition performance of these methods against state-of-the-art tabular data analysis approaches, which do not utilize the spatial relationships between the sensors. We test our proposed approach using two publicly available benchmark datasets for EEG-based emotion recognition tasks, namely DEAP and MAHNOB-HCI. Our results show that the “trained-from-scratch” convolutional neural network outperforms the best approaches in the literature, achieving 97.8% and 98.3% accuracy in valence and arousal classification on MAHNOB-HCI, and 91% and 90.4% on DEAP, respectively.","2169-3536","","10.1109/ACCESS.2023.3268233","Italian Ministry of Education and Research (MIUR) in the framework of the FoReLab Project (Departments of Excellence) research; Piano Nazionale di Ripresa e Resilienza (PNRR)(grant numbers:M4C2); Partenariato Esteso(grant numbers:PE00000013); "FAIR-Future Artificial Intelligence Research"-Spoke 1 "Human-Centered AI," funded by the European Commission; European Commission H2020 Framework Program of the Project "EXPERIENCE"(grant numbers:101017727); European Research Council through the European Community's 7th Framework Programme(grant numbers:FP7/2007-2013); European Research Council (ERC) Starting(grant numbers:203143); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10105240","Convolutional neural networks;electroencephalography;emotion recognition;spatial information representation","Electroencephalography;Emotion recognition;Feature extraction;Videos;Task analysis;Sensors;Convolutional neural networks","","4","","44","CCBYNCND","19 Apr 2023","","","IEEE"
"Statistical Approach for a Complex Emotion Recognition Based on EEG Features","D. Handayani; H. Yaacob; A. Wahab; I. F. T. Alshaikli","Department of Computer Science, International Islamic University Malaysia, Malaysia; Department of Computer Science, International Islamic University Malaysia, Malaysia; Department of Computer Science, International Islamic University Malaysia, Malaysia; International Islamic University Malaysia, Kuala Lumpur, Wilayah Persekutuan, MY",2015 4th International Conference on Advanced Computer Science Applications and Technologies (ACSAT),"26 May 2016","2015","","","202","207","This paper presents electroencephalogram (EEG) signals and normal distribution technique to recognize the complex emotion. In the recent years, there has been a trend towards recognizing human emotions, however not many researcher aware that human can recognize more than one emotion at one time. Thus, in this study, normal distribution is utilized to recognize the expected emotion. The feature extraction and classification were obtained using a Mel-frequency cepstral coefficients (MFCC) and multilayer perceptron (MLP). The correlation between human emotion and mood is also the essential point, since the mood can affected to the human emotion. The results show that the human emotions is strongly influenced by his initial mood.","","978-1-5090-0424-9","10.1109/ACSAT.2015.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478744","emotion recognition;mood recognition;normal distribution;Mel-frequency cepstral coefficients;multilayer perceptron","Emotion recognition;Mood;Electroencephalography;Feature extraction;Gaussian distribution;Brain modeling;Speech","","5","","13","IEEE","26 May 2016","","","IEEE","IEEE Conferences"
"Emotion Recognition Based on Microstates: A Comparison between Scalp and Source Analysis","J. Ruan; D. Xiao","School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China",2023 3rd International Conference on Electronic Information Engineering and Computer Science (EIECS),"21 Feb 2024","2023","","","829","833","The microstate of the electroencephalogram (EEG) captures spatiotemporal information from all channels, encompassing extensive electrophysiological data. Its significance to emotion recognition is substantial. However, current research into emotion recognition based on microstates remains confined to the scalp level, and due to the effects of volume conduction, the accuracy of emotion recognition might not be optimal. In this study, we employed the sLORETA method to map scalp data onto the cortex. Subsequently, we applied spatiotemporal analysis using microstate analysis techniques and extracted various features, including duration, occurrence frequency, coverage, and transition probability. We performed classification of discrete emotional labels separately for scalp and source data within the SEED and SEED-IV datasets. For the SEED dataset, the use of K-Nearest Neighbor (KNN) and Support Vector Machine (SVM) classifiers on source data resulted in an average classification accuracy increase of 6.07% and 5.93%, respectively, compared to the scalp. Similarly, for the SEED-IV dataset, corresponding increments of 6.85% and 7.5% were observed. These findings emphasize the efficacy of source microstate analysis in enhancing emotion recognition accuracy.","","979-8-3503-1667-4","10.1109/EIECS59936.2023.10435534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10435534","EEG;emotion recognition;microstates","Support vector machines;Emotion recognition;Scalp;Feature extraction;Rendering (computer graphics);Electroencephalography;Spatiotemporal phenomena","","","","29","IEEE","21 Feb 2024","","","IEEE","IEEE Conferences"
"Neural patterns between Chinese and Germans for EEG-based emotion recognition","S. -Y. Wu; M. Schaefer; W. -L. Zheng; B. -L. Lu; H. Yokoi","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Brain Science Inspired Life Support Research Center, University of Electro-Communications, Tokyo, Japan",2017 8th International IEEE/EMBS Conference on Neural Engineering (NER),"14 Aug 2017","2017","","","94","97","This paper aims to explore the neural patterns between Chinese and Germans for electroencephalogram (EEG)-based emotion recognition. Both Chinese and German subjects, wearing electrode caps, watched video stimuli that triggered positive, neutral, and negative emotions. Two emotion classifiers are trained on Chinese EEG data and German EEG data, respectively. The experiment results indicate that: a) German neural patterns are basically in accordance with Chinese ones; b) the main difference lies in the upper temporal region in Delta band which activates more when a German is in positive mood; and c) the Chinese positive emotion achieves the best accuracy while German emotions share the approximate accuracy. Moreover, Gamma band serves as the critical band for both German and Chinese emotion recognition.","1948-3554","978-1-5090-4603-4","10.1109/NER.2017.8008300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008300","","Electroencephalography;Emotion recognition;Support vector machines;Feature extraction;Electrodes;Motion pictures;Entropy","","3","","14","IEEE","14 Aug 2017","","","IEEE","IEEE Conferences"
"Deeply Coupling EEG Signals and Eye Movements for Multi-Modal and Region-Aware Emotion Recognition","Y. Chen; Y. Gao; X. Fu; H. He; T. Ouyang; S. Chen; X. Fu","School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; The Fourth Medical Center of PLA General Hospital; Department of Neurosurgery, Third Affiliated Hospital, Naval Medical University; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Automatic emotion recognition based on electroencephalogram (EEG) signals has been a significant clinical approach to detect emotional states. Given the intuitive complementation between physiological signals and behavioral signals, combining EEG signals with facial expressions, e.g., eye movements, should be a promising direction to further robustness and performance of emotion recognition methods. However, existing deep learning-based multi-modal methods tend to fuse the multi-modal features only in the last layer of the networks, which is too explict and shallow to deeply mine the implicit knowledge complementarity between the modalities. Additionally, existing methods fail to fully consider the distribution diversity of brain topology for different emotion states, which deserves further adaptive modeling. In this paper, we propose DCEE, a novel multi-modal and region-aware emotion recognition method which Deeply Couples EEG signals and Eye movements with a self-attention mechanism-based inter and intra-modal feature fusion module, and realizes adaptive modeling of brain activation patterns based on our designed emotion-region correlation matrix. Extensive experiments conducted on datasets SEED-IV and DEAP demonstrate the effectiveness of our proposed method and the complementation between EEG signals and eye movements for emotion recognition.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888584","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888584","EEG;emotion recognition;eye movement;multi-modal;feature fusion","Emotion recognition;Adaptation models;Speech recognition;Signal processing;Brain modeling;Electroencephalography;Physiology;Robustness;Topology;Speech processing","","","","18","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Emotion Recognition Method Based on EEG Signals: 4D-CapsBLnet Model Integrating Spatio-Temporal Features","C. Zhuang; L. Dai; Q. Fu","College of Computer Science, University of South China, Hengyang, China; Human Factor Institute, University of South China, Hengyang, China; Department of Management Science and Engineering, University of South China, Hengyang, China","2024 IEEE 2nd International Conference on Control, Electronics and Computer Technology (ICCECT)","7 Jun 2024","2024","","","82","87","Emotion recognition technology based on electroencephalogram (EEG) signals has become a focal point for experts and scholars globally. To improve the accuracy of machine understanding of human emotions, this paper characterizes the multidimensional information features in EEG signals, and solves the problem of 4-dimensional fusion of temporal, frequency and 2D spatial features of EEG signals, and we present a novel method named 4D-CapsBLnet. First, we transformed the multidimensional features from various channels into 4D structures for training the deep learning model. Second, we utilized Capsule Networks (CapsNet) to capture spatial and frequency information from each temporal slice of 4D inputs, while Bidirectional Long Short- Term Memory Networks (Bi-LSTM) was employed to capture temporal dependencies. To comprehensively assess the model's performance, we conducted a five-fold cross-validation with a maximum epoch limit and early stopping strategy. The average accuracy and standard deviation on the SEED, SEED-IV, and SEED-V datasets are 95.06% ± 1.65%, 88.47% ± 1.74%, and 88.23% ± 1.12%, respectively. Compared to similar research methods, the proposed model in this paper demonstrates robustness and superior performance in emotion recognition tasks.","","979-8-3503-8095-8","10.1109/ICCECT60629.2024.10545975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545975","Emotion Recognition;EEG Signals;Deep Learning;4D Feature Structure","Training;Deep learning;Emotion recognition;Computational modeling;Frequency-domain analysis;Brain modeling;Electroencephalography","","","","13","IEEE","7 Jun 2024","","","IEEE","IEEE Conferences"
"A hybrid ICA-wavelet transform for automated artefact removal in EEG-based emotion recognition","A. D. Bigirimana; N. Siddique; D. Coyle","Intelligent Systems Research Centre, Ulster University, Derry, UK; Intelligent Systems Research Centre, Ulster University, Derry, UK; Intelligent Systems Research Centre, Ulster University, Derry, UK","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","9 Feb 2017","2016","","","004429","004434","Removing artefacts from electroencephalographic (EEG) recordings normally increases their low signal-to-noise ratio and enables more reliable interpretation of brain activity. In this paper we present an evaluation of an automatic independent component analysis (ICA) procedure, a hybrid ICA - wavelet transform technique (ICA-W), for artefact removal from EEG correlated to emotional-state. Spectral and statistical features were classified with support vector machines (SVM) to assess the performance of ICA-W against the regular ICA, in terms of the accuracy of classifying emotional states from EEG. Accuracies on data from 14 subjects are reported and the results indicate that ICA-W performs better than traditional ICA in statistical and wavelet based features whilst the best overall performance is achieved when combining ICA-W with statistical features with an average accuracy across subjects of 74.11% for classifying four categories of emotion. ICA-W is therefore demonstrated to enhance EEG-based emotion recognition applications in terms of performance and ease of application.","","978-1-5090-1897-0","10.1109/SMC.2016.7844928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844928","Independent component analysis;EEG;wavelet;emotion","Electroencephalography;Feature extraction;Wavelet transforms;Training;Testing;Data mining","","14","","20","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Adaptive Flexible Analytic Wavelet Transform for EEG-Based Emotion Recognition","A. Kumar Dwivedi; O. Prakash Verma; S. Taran","Department of Electronics and Communication (ECE), Delhi Technological University (DTU), Delhi, India; Department of Electronics and Communication (ECE), Delhi Technological University (DTU), Delhi, India; Department of Electronics and Communication (ECE), Delhi Technological University (DTU), Delhi, India",IEEE Sensors Journal,"16 Sep 2024","2024","24","18","28941","28951","Video game development heavily relies on gaming emotions of players. Video games can trigger emotions that lead to hatred, aggressiveness, sadness, addiction, suicidal thoughts, etc. The effect of emotions can be reduced by studying the player’s emotional state. The emotional state influences the psychological state. The electroencephalogram (EEG) signals generated due to neurological changes in the brain give accurate information about psychological states. This work introduces the adaptive flexible analytic wavelet transform (AFAWT) for detecting emotions using EEG signals. In AFAWT, parametric optimization finds the best basis function for representing EEG signals. Particle swarm optimization (PSO) is used to solve an inequality constraint problem, enabling the selection of appropriate AFAWT parameters. The AFAWT decomposes the EEG signal into subbands (SBs). The SBs’ time-domain measures serve as features for classifying emotions in EEG signals. A post hoc multiple comparison analysis using the analysis of variance (ANOVA) test ensures the significance of the extracted features. Different classification algorithms test the obtained features for each SB. The hyperparameters of the classifiers neural network (NN), support vector machine (SVM), ensemble (EN), and k-nearest neighbors (k-NN) are optimized using ten-fold cross-validation and Bayesian optimization. Among the optimized classifiers, optimizable k-NN shows the best classification accuracy of 90.3% for four classes of emotions. Compared with other existing methods, our proposed method performs better on the same dataset.","1558-1748","","10.1109/JSEN.2024.3429523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10608067","Electroencephalogram (EEG) signals;emotion recognition;flexible analytic wavelet transform (FAWT);machine learning;particle swarm optimization (PSO)","Electroencephalography;Transforms;Wavelet transforms;Feature extraction;Wavelet analysis;Emotion recognition;Discrete wavelet transforms","","2","","34","IEEE","23 Jul 2024","","","IEEE","IEEE Journals"
"NeuroSense: A Novel EEG Dataset Utilizing Low-Cost, Sparse Electrode Devices for Emotion Exploration","T. Colafiglio; A. Lombardi; P. Sorino; E. Brattico; D. Lofù; D. Danese; E. Di Sciascio; T. Di Noia; F. Narducci","Department of Computer, Control, and Management Engineering (DIAG), Sapienza University of Rome, Rome, Italy; Department of Electrical and Information Engineering, Politecnico di Bari, Bari, Italy; Department of Electrical and Information Engineering, Politecnico di Bari, Bari, Italy; Department of Clinical Medicine, Center for Music in the Brain, Aarhus University, Aarhus C, Aarhus, Denmark; Department of Electrical and Information Engineering, Politecnico di Bari, Bari, Italy; Department of Electrical and Information Engineering, Politecnico di Bari, Bari, Italy; Department of Electrical and Information Engineering, Politecnico di Bari, Bari, Italy; Department of Electrical and Information Engineering, Politecnico di Bari, Bari, Italy; Department of Electrical and Information Engineering, Politecnico di Bari, Bari, Italy",IEEE Access,"5 Nov 2024","2024","12","","159296","159315","Emotion recognition is crucial in affective computing, aiming to bridge the gap between human emotional states and computer understanding. This study presents NeuroSense, a novel electroencephalography (EEG) dataset utilizing low-cost, sparse electrode devices for emotion exploration. Our dataset comprises EEG signals collected with the portable 4-electrodes device Muse 2 from 30 participants who, thanks to a neurofeedback setting, watch 40 music videos and assess their emotional responses. These assessments use standardized scales gauging arousal, valence, and dominance. Additionally, participants rate their liking for and familiarity with the videos. We develop a comprehensive preprocessing pipeline and employ machine learning algorithms to translate EEG data into meaningful insights about emotional states. We verify the performance of machine learning (ML) models using the NeuroSense dataset. Despite utilizing just 4 electrodes, our models achieve an average accuracy ranging from 75% to 80% across the four quadrants of the dimensional model of emotions. We perform statistical analyses to assess the reliability of the self-reported labels and the classification performance for each participant, identifying potential discrepancies and their implications. We also compare our results with those obtained using other public EEG datasets, highlighting the advantages and limitations of sparse electrode setups in emotion recognition. Our results demonstrate the potential of low-cost EEG devices in emotion recognition, highlighting the effectiveness of ML models in capturing the dynamic nature of emotions. The NeuroSense dataset is publicly available, inviting further research and application in human-computer interaction, mental health monitoring, and beyond.","2169-3536","","10.1109/ACCESS.2024.3487932","“SECURE SAFE APULIA,” itaLian system wIde Frailty nEtwork, Casa delle Tecnologie Emergenti di Matera, rete Integrata meDiterranea per l’osservazione ed Elaborazione di percorsi di Nutrizione, and OncologIA(grant numbers:PNRR-MAD-2022-12376656); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10737340","Emotion recognition;EEG dataset;low-cost EEG devices;machine learning;human-computer interaction;Russell’s model","Videos;Electroencephalography;Feature extraction;Electrodes;Color;Physiology;Brain modeling;Emotion recognition;Protocols;Data mining","","1","","54","CCBYNCND","29 Oct 2024","","","IEEE","IEEE Journals"
"Tagging Continuous Labels for EEG-based Emotion Classification","R. -F. Gu; L. -M. Zhao; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, and Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, People’s Republic of China; Department of Computer Science and Engineering, Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, and Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, People’s Republic of China; Department of Computer Science and Engineering, Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, and Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, People’s Republic of China; RuiJin-Mihoyo Laboratory, Clinical Neuroscience Center, RuiJin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, People’s Republic of China",2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"11 Dec 2023","2023","","","1","4","EEG-based emotion classification has long been a critical task in the field of affective brain-computer interface (aBCI). The majority of leading researches construct supervised learning models based on labeled datasets. Several datasets have been released, including different kinds of emotions while utilizing various forms of stimulus materials. However, they adopt discrete labeling methods, in which the EEG data collected during the same stimulus material are given a same label. These methods neglect the fact that emotion changes continuously, and mislabeled data possibly exist. The imprecision of discrete labels may hinder the progress of emotion classification in concerned works. Therefore, we develop an efficient system in this paper to support continuous labeling by giving each sample a unique label, and construct a continuously labeled EEG emotion dataset. Using our dataset with continuous labels, we demonstrate the superiority of continuous labeling in emotion classification through experiments on several classification models. We further utilize the continuous labels to identify the EEG features under induced and non-induced emotions in both our dataset and a public dataset. Our experimental results reveal the learnability and generality of the relation between the EEG features and their continuous labels.","2694-0604","979-8-3503-2447-1","10.1109/EMBC40787.2023.10341022","National Natural Science Foundation of China; School of Medicine; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341022","","Supervised learning;Tagging;Brain modeling;Electroencephalography;Brain-computer interfaces;Biology;Labeling","Electroencephalography;Emotions;Brain-Computer Interfaces","1","","15","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Emotion Recognition From Multi-Channel EEG via Deep Forest","J. Cheng; M. Chen; C. Li; Y. Liu; R. Song; A. Liu; X. Chen","Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Electronic Engineering & Information Science, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering & Information Science, University of Science and Technology of China, Hefei, China",IEEE Journal of Biomedical and Health Informatics,"5 Feb 2021","2021","25","2","453","464","Recently, deep neural networks (DNNs) have been applied to emotion recognition tasks based on electroencephalography (EEG), and have achieved better performance than traditional algorithms. However, DNNs still have the disadvantages of too many hyperparameters and lots of training data. To overcome these shortcomings, in this article, we propose a method for multi-channel EEG-based emotion recognition using deep forest. First, we consider the effect of baseline signal to preprocess the raw artifact-eliminated EEG signal with baseline removal. Secondly, we construct 2$D$ frame sequences by taking the spatial position relationship across channels into account. Finally, 2$D$ frame sequences are input into the classification model constructed by deep forest that can mine the spatial and temporal information of EEG signals to classify EEG emotions. The proposed method can eliminate the need for feature extraction in traditional methods and the classification model is insensitive to hyperparameter settings, which greatly reduce the complexity of emotion recognition. To verify the feasibility of the proposed model, experiments were conducted on two public DEAP and DREAMER databases. On the DEAP database, the average accuracies reach to 97.69% and 97.53% for valence and arousal, respectively; on the DREAMER database, the average accuracies reach to 89.03%, 90.41%, and 89.89% for valence, arousal and dominance, respectively. These results show that the proposed method exhibits higher accuracy than the state-of-art methods.","2168-2208","","10.1109/JBHI.2020.2995767","National Key R&D Program of China(grant numbers:2017YFB1002802); National Natural Science Foundation of China(grant numbers:61922075,41901350,61701160,61701158); Fundamental Research Funds for the Central Universities(grant numbers:JZ2019HGBZ0151,JZ2020HGPA0111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096541","Deep neural networks (DNNs);emotion recognition;multi-channel EEG;deep forest;spatio-temporal information","Electroencephalography;Brain modeling;Emotion recognition;Feature extraction;Databases;Forestry;Two dimensional displays","Algorithms;Arousal;Electroencephalography;Emotions;Forests;Humans;Neural Networks, Computer","165","","48","IEEE","19 May 2020","","","IEEE","IEEE Journals"
"OGSSL: A Semi-Supervised Classification Model Coupled With Optimal Graph Learning for EEG Emotion Recognition","Y. Peng; F. Jin; W. Kong; F. Nie; B. -L. Lu; A. Cichocki","School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Artificial Intelligence, OPtics and ElectroNics (iOPEN), Northwestern Polytechnical University, Xi’an, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Center for Computational and Data-Intensive Science and Engineering, Skolkovo Institute of Science and Technology, Moscow, Russia",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"20 May 2022","2022","30","","1288","1297","Electroencephalogram(EEG) signals are generated from central nervous system which are difficult to disguise, leading to its popularity in emotion recognition. Recently,semi-supervisedlearning exhibits promisingemotion recognition performance by involving unlabeled EEG data into model training. However, if we first build a graph to characterize the sample similarities and then perform label propagation on this graph, these two steps cannotwell collaborate with each other. In this paper, we propose an OptimalGraph coupledSemi-Supervised Learning (OGSSL) model for EEG emotion recognition by unifying the adaptive graph learning and emotion recognition into a single objective. Besides, we improve the label indicator matrix of unlabeledsamples in order to directly obtain theiremotional states. Moreover, the key EEG frequency bands and brain regions in emotion expression are automatically recognized by the projectionmatrix of OGSSL. Experimental results on the SEED-IV data set demonstrate that 1) OGSSL achieves excellent average accuracies of 76.51%, 77.08% and 81.29% in three cross-sessionemotion recognition tasks, 2) OGSSL is competent for discriminative EEG feature selection in emotion recognition, and 3) the Gamma frequency band, the left/righttemporal, prefrontal,and (central) parietal lobes are identified to be more correlated with the occurrence of emotions.","1558-0210","","10.1109/TNSRE.2022.3175464","National Key Research and Development Program of China(grant numbers:2017YFE0116800); National Natural Science Foundation of China(grant numbers:61971173,U20B2074); Natural Science Foundation of Zhejiang Province(grant numbers:LY21F030005); Fundamental Research Fund for the Provincial Universities of Zhejiang(grant numbers:GK209907299001-008); China Postdoctoral Science Foundation(grant numbers:2017M620470); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775684","Electroencephalogram (EEG);emotion recognition;feature selection;graph learning;semi-supervised learning","Electroencephalography;Emotion recognition;Brain modeling;Semisupervised learning;Adaptation models;Task analysis;Data models","Brain;Electroencephalography;Emotions;Humans;Parietal Lobe;Recognition, Psychology","29","","38","CCBYNCND","16 May 2022","","","IEEE","IEEE Journals"
"Data Generation for Enhancing EEG-Based Emotion Recognition: Extracting Time-Invariant and Subject-Invariant Components With Contrastive Learning","Z. Wan; Q. Yu; W. Dai; S. Li; J. Hong","School of Information Engineering, Nanchang University, Jiangxi, China; School of Information Engineering, Nanchang University, Jiangxi, China; School of Information Engineering, Nanchang University, Jiangxi, China; Department of Radiological Sciences, University of California Los Angeles, Los Angeles, CA, United States; School of Information Engineering, Nanchang University, Jiangxi, China",IEEE Transactions on Consumer Electronics,"","2024","PP","99","1","1","The utilization of Artificial Intelligence for Generative Content (AIGC) has emerged as an effective and sophisticated approach for generating synthetic Electroencephalography (EEG) signals. This approach proves beneficial in augmenting EEG data and enhancing the performance of deep learning (DL) methods for emotion recognition. However, the temporal non-stationary nature and inter-subject variability of EEG signals still pose a great challenge for the practical applications of EEG-based emotion recognition. To address the challenges, we propose a novel data generation workflow that combines multi-task learning. This workflow incorporates a generative pre-trained Transformer (EEGPT) to generate time-invariant components from raw EEG data. Additionally, we introduce a model training strategy called Contrastive Learning method for Time-invariant and Subject-invariant (CLTISI) EEG data generation. This strategy aligns inter-subject data into a shared high-dimensional space, imparting subject-invariant characteristics to the generated data. Experimental results demonstrate that the generated data not only improves the generalization of DL models but also adapts effectively to novel emotional stimuli. Furthermore, the CLTISI strategy enables DL models to maintain stable performance across diverse datasets. The regions identified as crucial for emotion recognition by the EEGPT model offer valuable insights into the neural mechanisms of human emotion processing.","1558-4127","","10.1109/TCE.2024.3414154","Youth Fund Project of Natural Science Foundation of Jiangxi Province(grant numbers:20232BAB212029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556575","Data generation;AIGC;EEG;contrastive learning;emotion recognition","Electroencephalography;Brain modeling;Data models;Emotion recognition;Adaptation models;Training;Multitasking","","4","","","IEEE","13 Jun 2024","","","IEEE","IEEE Early Access Articles"
"EEG based Emotional State Estimation using 2-D Deep Learning Technique","M. A. Ozdemir; M. Degirmenci; O. Guren; A. Akan","Department of Biomedical Engineering, Electrophysiological Signals Laboratory Izmir Katip Celebi University, Izmir, Turkey; Department of Biomedical Engineering, Electrophysiological Signals Laboratory Izmir Katip Celebi University, Izmir, Turkey; Department of Biomedical Engineering, Medical Electronic Laboratory Izmir Katip Celebi University, Izmir, Turkey; Department of Biomedical Engineering, Electrophysiological Signals Laboratory Izmir Katip Celebi University, Izmir, Turkey",2019 Medical Technologies Congress (TIPTEKNO),"11 Nov 2019","2019","","","1","4","Emotion detection is very crucial role on diagnosis of brain disorders and psychological disorders. Electroencephalogram (EEG) is useful tool that obtain complex physiological brain signals from human. In this paper, we proposed a novel approach for emotional state estimation using convolutional neural network (CNN) based deep learning technique from EEG signals. Firstly, we convert 32 lead EEG signals to 2D EEG images with Azimuthal Equidistant Projection (AEP) technique. Then, 2D images that represented measurements of EEG signals sent to CNN based deep neural network for classification. In this study, we have achieved accuracy of 95.96% two classes as negative and positive valence, 96.09% two classes as high and low arousal and 95.90% two classes as high and low arousal dominance.","2687-7783","978-1-7281-2420-9","10.1109/TIPTEKNO.2019.8895158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8895158","Convolutional Neural Network;EEG Images;Electroencephalogram;Emotion Detection Topographic EEG Maps","","","24","","23","IEEE","11 Nov 2019","","","IEEE","IEEE Conferences"
"Stability of Features in Real-Time EEG-based Emotion Recognition Algorithm","Z. Lan; O. Sourina; L. Wang; Y. Liu","Fraunhofer IDM @ NTU; Fraunhofer IDM @ NTU; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Fraunhofer IDM @ NTU",2014 International Conference on Cyberworlds,"15 Dec 2014","2014","","","137","144","Stability of algorithms is very important for electroencephalogram (EEG) based applications. Stable features should exhibit consistency among repeated measurements of the same subject. Previously, power features were reported to be one of the most stable EEG features in medical application. In this paper, stability of features in emotion recognition algorithms is studied. Our hypothesis is that the most stable features give the best intra-subject accuracy across different days in real-time emotion recognition algorithm. An experiment to induce 4 emotions such as pleasant, happy, frightened, and angry is designed and carried out in 8 consecutive days (two sessions per day) for 4 subjects to record EEG data. A novel real-time subject dependent algorithm with the most stable features is proposed and implemented. The algorithm needs just one training for each subject. The training results can be used in real-time emotion recognition applications without re-training with the adequate accuracy. The proposed algorithm is integrated with a real-time application ""Emotional Avatar"".","","978-1-4799-4677-8","10.1109/CW.2014.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980754","EEG;Emotion recognition;Fractal dimension (FD);Stability;Intra-class Correlation Coefficient (ICC)","Electroencephalography;Emotion recognition;Accuracy;Feature extraction;Training;Support vector machines;Real-time systems","","24","","34","IEEE","15 Dec 2014","","","IEEE","IEEE Conferences"
"Human emotional classification using EEG based on discrete wavelet transformation technique","V. G. Rajendran; S. Jayalalitha; K. Adalarasu; S. J. Mohammad","ECE Department, SRC, SASTRA University, Kumbakonam, Tamil Nadu, India; School of EEE, SASTRA University, Thanjavur, Tamil Nadu, India; School of EEE, SASTRA University, Thanjavur, Tamil Nadu, India; ECE Department, SRC, SASTRA University, Kumbakonam, Tamil Nadu, India","2023 First International Conference on Cyber Physical Systems, Power Electronics and Electric Vehicles (ICPEEV)","19 Jan 2024","2023","","","1","6","This paper proposes a method to classify a person’s emotional status while watching video clippings. A total of 15 students whose Electroencephalogram (EEG) signals were acquired during watching different categories of movie clippings. The human emotions such as Funny, Sad, Horror, Relax, and Romantic were considered, and short-duration movie clippings were played for 3 minutes duration for each emotion with an EEG signal acquired by using a wearable 14-channel Emotiv Epoc X device. The saline-based felt sensor type of electrodes was used in this study with a sampling rate of 256 Samples per Second. The raw EEG signal was pre-processed by enabling the built-in digital 5th-order Sinc filter and notch filter with a bandwidth of 0.2-45Hz to remove artifacts, power line noise, and signal drift. EEG sub-band energies such as alpha, theta, and beta features were extracted by decomposing the EEG signal into four levels with Daubechies (db4) as the mother wavelet. For every 5s of raw EEG data, the time-frequency features were extracted using discrete wavelet transformation using MATLAB 2022. The relative sub-band energies and band ratios such as heart rate index (HRI), neural activity, vigilance index, arousal index, performance Enhancement index, cognitive performance attentional resource index (CPARI), CNS arousal, and desynchronization were computed. The above 11 features were fed to the machine learning classifier for classification, and their classification accuracy was compared under different emotional data combinations. The result shows that the ensemble algorithm provides the highest classification accuracy of 62.8% for five emotions: Funny, Sad, Horror, Relax, and Romantic. Similarly, a detailed study was presented on comparing under 4, or 3, or 2 combinations of emotional data for classification with different classifier algorithms: decision tree, ensemble, SVM, neural network, and kernel. The highest classification accuracy of 88.80% was obtained for the ensemble classifier under two-level romantic and relax emotional data.","","979-8-3503-2356-6","10.1109/ICPEEV58650.2023.10391939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10391939","Electroencephalogram (EEG);Discrete Wavelet Transform (DWT);Relative sub-band energies;Heart rate index;CPARI;SVM","Support vector machines;Time-frequency analysis;Feature extraction;Motion pictures;Electroencephalography;Discrete wavelet transforms;Classification algorithms","","1","","16","IEEE","19 Jan 2024","","","IEEE","IEEE Conferences"
"Spatial Spectral based 3D Feature Map for EEG Emotion Recognition","M. U S; A. J","Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India",2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC),"19 Sep 2022","2022","","","247","252","One of the most commonly used non-invasive techniques for emotion recognition is the electroencephalogram (EEG). EEG can be used to record electrical activity in the brain and cannot be voluntarily fabricated. The electroencephalogram (EEG) is a physiological indicator that shows how electrical activities of brain cells cluster across the human cerebral cortex. Research works that demonstrate how the most complete characteristics of EEG, such as Power Spectral Density (PSD) can be used to classify basic emotions. This paper proposes an efficient method for predicting human emotions using spatial-spectral aspects of EEG and Convolutional Neural Network (CNN). To create a 3D map, spectral features such as PSD and Differential Entropy (DE) are extracted. The 3D brain map is used as an input to a CNN model for classifying emotions into valence and arousal by producing accuracy of 89.38% and 90.12% respectively.","","978-1-6654-7971-4","10.1109/ICESC54411.2022.9885393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9885393","Electroencephalogram;Spectral Features;Emotion Recognition;Power Spectral Density;Differential Entropy;Convolutional Neural Network","Emotion recognition;Solid modeling;Time-frequency analysis;Three-dimensional displays;Feature extraction;Brain modeling;Electroencephalography","","1","","29","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"EEG Feature Selection via Global Redundancy Minimization for Emotion Recognition","X. Xu; T. Jia; Q. Li; F. Wei; L. Ye; X. Wu","School of Artificial Intelligence, Engineering Research Center of Intelligent Technology and Educational Application, Ministry of Education, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Engineering Research Center of Intelligent Technology and Educational Application, Ministry of Education, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Engineering Research Center of Intelligent Technology and Educational Application, Ministry of Education, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Engineering Research Center of Intelligent Technology and Educational Application, Ministry of Education, Beijing Normal University, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; School of Artificial Intelligence, Engineering Research Center of Intelligent Technology and Educational Application, Ministry of Education, Beijing Normal University, Beijing, China",IEEE Transactions on Affective Computing,"28 Feb 2023","2023","14","1","421","435","A common drawback of EEG-based emotion recognition is that volume conduction effects of the human head introduce interchannel dependence and result in highly correlated information among most EEG features. These highly correlated EEG features cannot provide extra useful information, and they actually reduce the performance of emotion recognition. However, the existing feature selection methods, commonly used to remove redundant EEG features for emotion recognition, ignore the correlation between the EEG features or utilize a greedy strategy to evaluate the interdependence, which leads to the algorithms retaining the correlated and redundant features with similar feature scores in the EEG feature subset. To solve this problem, we propose a novel EEG feature selection method for emotion recognition, termed global redundancy minimization in orthogonal regression (GRMOR). GRMOR can effectively evaluate the dependence among all EEG features from a global view and then select a discriminative and nonredundant EEG feature subset for emotion recognition. To verify the performance of GRMOR, we utilized three EEG emotional data sets (DEAP, SEED, and HDED) with different numbers of channels (32, 62, and 128). The experimental results demonstrate that GRMOR is a promising tool for redundant feature removal and informative feature selection from highly correlated EEG features.","1949-3045","","10.1109/TAFFC.2021.3068496","Open Research Project of the State Key Laboratory of Media Convergence and Communication; Communication University of China(grant numbers:SKLMCC2020KF001); General Program of Beijing Municipal Natural Science Foundation(grant numbers:4212037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385899","EEG;feature selection;emotion recognition;global redundancy minimization;orthogonal regression","Electroencephalography;Feature extraction;Emotion recognition;Brain modeling;Redundancy;Electrodes;Task analysis","","16","","89","IEEE","24 Mar 2021","","","IEEE","IEEE Journals"
"EEG emotion recognition based on knowledge distillation optimized residual networks","P. Wang; C. Guo; S. Xie; X. Qiao; L. Mao; X. Fu","College of Electrical and Control Engineering, Xi'an University of Science and Technology, Xi'an, China; College of Electrical and Control Engineering, Xi'an University of Science and Technology, Xi'an, China; College of Electrical and Control Engineering, Xi'an University of Science and Technology, Xi'an, China; College of Electrical and Control Engineering, Xi'an University of Science and Technology, Xi'an, China; College of Electrical and Control Engineering, Xi'an University of Science and Technology, Xi'an, China; College of Electrical and Control Engineering, Xi'an University of Science and Technology, Xi'an, China","2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )","9 Nov 2022","2022","","","574","581","With the application of brain-computer interaction technology in various fields, emotion recognition based on EEG signals has been widely studied. However, the existing manually designed methods for extracting emotion features do not have generality, and the deep models have high parameter redundancy and resource overhead, resulting in the inability to perform end-to-end emotion recognition on embedded devices. To address this problem, we propose a knowledge distillation-based residual network emotion recognition method, which first optimizes the residual network applied to image processing to adapt to the EEG emotion classification task, then designs a deep ResNet34 residual network for adaptive extraction of features for EEG emotion classification, and then distills the emotion recognition performance of the deep residual network to the lightweight network ResNet8, which is finally used in SEED-IV and DEAP achieved 83.9% and 81.5% accuracy, respectively, which is 1.1% and 17.7% improvement in accuracy over the model without distillation learning, and verified that the proposed method can achieve model compression and recognition speedup on the basis of close to the classification performance of the deep model for better deployment on embedded devices.","2689-6621","978-1-6654-5864-1","10.1109/IAEAC54830.2022.9929699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9929699","EEG emotions;optimizing residual networks;knowledge distillation;brain-computer interaction","Performance evaluation;Knowledge engineering;Emotion recognition;Adaptation models;Temperature distribution;Brain modeling;Feature extraction","","4","","16","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"Discrete Wavelet Transform coefficients for emotion recognition from EEG signals","R. E. J. Yohanes; W. Ser; G. -b. Huang","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"10 Nov 2012","2012","","","2251","2254","In this paper, we propose to use DWT coefficients as features for emotion recognition from EEG signals. Previous feature extraction methods used power spectra density values dervied from Fourier Transform or sub-band energy and entropy derived from Wavelet Transform. These feature extracion methods eliminate temporal information which are essential for analyzing EEG signals. The DWT coefficients represent the degree of correlation between the analyzed signal and the wavelet function at different instances of time; therefore, DWT coefficients contain temporal information of the analyzed signal. The proposed feature extraction method fully utilizes the simultaneous time-frequency analysis of DWT by preserving the temporal information in the DWT coefficients. In this paper, we also study the effects of using different wavelet functions (Coiflets, Daubechies and Symlets) on the performance of the emotion recognition system. The input EEG signals were obtained from two electrodes according to 10-20 system: Fp1 and Fp2. Visual stimuli from International Affective Picture System (IAPS) were used to induce two emotions: happy and sad. Two classifiers were used: Extreme Learning Machine (ELM) and Support Vector Machine (SVM). Experimental results confirmed that the proposed DWT coefficients method showed improvement of performance compared to previous methods.","1558-4615","978-1-4577-1787-1","10.1109/EMBC.2012.6346410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6346410","","Electroencephalography;Discrete wavelet transforms;Emotion recognition;Feature extraction;Accuracy;Principal component analysis;Entropy","Algorithms;Brain;Electroencephalography;Emotions;Humans;Male;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Visual Perception;Wavelet Analysis;Young Adult","45","","15","IEEE","10 Nov 2012","","","IEEE","IEEE Conferences"
"A Domain Generative Graph Network for EEG-Based Emotion Recognition","Y. Gu; X. Zhong; C. Qu; C. Liu; B. Chen","Institute of Chongqing Key Laboratory of Non-linear Circuit and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China; Institute of Chongqing Key Laboratory of Non-linear Circuit and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China; Institute of Chongqing Key Laboratory of Non-linear Circuit and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China; Department of Electronics, Graduate School of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan; Institute of Chongqing Key Laboratory of Non-linear Circuit and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China",IEEE Journal of Biomedical and Health Informatics,"4 May 2023","2023","27","5","2377","2386","Emotion is a human attitude experience and corresponding behavioral response to objective things. Effective emotion recognition is important for the intelligence and humanization of brain-computer interface (BCI). Although deep learning has been widely used in emotion recognition in recent years, emotion recognition based on electroencephalography (EEG) is still a challenging task in practical applications. Herein, we proposed a novel hybrid model that employs generative adversarial networks to generate potential representations of EEG signals while combining graph convolutional neural networks and long short-term memory networks to recognize emotions from EEG signals. Experimental results on DEAP and SEED datasets show that the proposed model achieved the promising emotion classification performance compared with the state-of-the-art methods.","2168-2208","","10.1109/JBHI.2023.3242090","National Natural Science Foundation of China(grant numbers:61801400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035965","EEG emotion recognition;generative adversarial networks (GAN);graph convolutional neural networks (GCNN);latent representation;long short-term memory (LSTM)","Electroencephalography;Brain modeling;Feature extraction;Emotion recognition;Generative adversarial networks;Task analysis;Time-domain analysis","Humans;Emotions;Neural Networks, Computer;Electroencephalography;Brain-Computer Interfaces","27","","53","IEEE","3 Feb 2023","","","IEEE","IEEE Journals"
"Electroencephalogram Emotion Recognition Based on A Stacking Classification Model","O. Xie; Z. -T. Liu; X. -W. Ding","Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China",2018 37th Chinese Control Conference (CCC),"7 Oct 2018","2018","","","5544","5548","To improve the accuracy of Electroencephalogram (EEG) emotion recognition, a stacking emotion classification model is proposed, in which different classification models such as XGBoost, LightGBM and Random Forest are integrated to learn the features. In addition, the Renyi entropy of 32 channels' EEG signals are extracted as the feature and Linear discriminant analysis (LDA) is employed to reduce the dimension of the feature set. The proposal is tested on the DEAP dataset, and the EEG emotional states are accessed in Arousal-Valence emotion space, in which HA/LA and HV/LV are classified, respectively. The result shows that the average recognition accuracies of 77.19% for HA/LA and 79.06% for HV/LV are obtained, which demonstrates that the proposal is feasible in EEG emotion recognition.","1934-1768","978-988-15639-5-8","10.23919/ChiCC.2018.8483496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8483496","Emotion recognition;Electroencephalogram;Stacking model;Renyi entropy","Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Stacking;Entropy;Predictive models","","11","","19","","7 Oct 2018","","","IEEE","IEEE Conferences"
"MMPosE: Movie-Induced Multi-Label Positive Emotion Classification Through EEG Signals","X. Du; X. Deng; H. Qin; Y. Shu; F. Liu; G. Zhao; Y. -K. Lai; C. Ma; Y. -J. Liu; H. Wang","Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; School of Computer Science and Informatics, Cardiff University, Cardiff, Wales, U.K.; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Affective Computing,"28 Nov 2023","2023","14","4","2925","2938","Emotional information plays an important role in various multimedia applications. Movies, as a widely available form of multimedia content, can induce multiple positive emotions and stimulate people's pursuit of a better life. Different from negative emotions, positive emotions are highly correlated and difficult to distinguish in the emotional space. Since different positive emotions are often induced simultaneously by movies, traditional single-target or multi-class methods are not suitable for the classification of movie-induced positive emotions. In this paper, we propose TransEEG, a model for multi-label positive emotion classification from a viewer's brain activities when watching emotional movies. The key features of TransEEG include (1) explicitly modeling the spatial correlation and temporal dependencies of multi-channel EEG signals using the Transformer structure based model, which effectively addresses long-distance dependencies, (2) exploiting the label-label correlations to guide the discriminative EEG representation learning, for that we design an Inter-Emotion Mask for guiding the Multi-Head Attention to learn the inter-emotion correlations, and (3) constructing an attention score vector from the representation-label correlation matrix to refine emotion-relevant EEG features. To evaluate the ability of our model for multi-label positive emotion classification, we demonstrate our model on a state-of-the-art positive emotion database CPED. Extensive experimental results show that our proposed method achieves superior performance over the competitive approaches.","1949-3045","","10.1109/TAFFC.2022.3221554","Beijing Natural Science Foundation(grant numbers:4212029); Tsinghua University Initiative Scientific Research Program(grant numbers:20211080093); National Natural Science Foundation of China(grant numbers:62272447,61725204); Newton Prize 2019 China(grant numbers:NP2PB/100047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946368","Multi-channel EEG;positive emotions;affective computing;multi-label classification;transformer encoder","Motion pictures;Electroencephalography;Brain modeling;Emotion recognition;Correlation;Transformers;Computational modeling","","10","","62","IEEE","11 Nov 2022","","","IEEE","IEEE Journals"
"A Graph Neural Network for EEG-Based Emotion Recognition With Contrastive Learning and Generative Adversarial Neural Network Data Augmentation","S. S. Gilakjani; H. A. Osman","Department of Electrical and Computer Engineering, University of Ottawa, Ottawa, ON, Canada; Department of Electrical and Computer Engineering, University of Ottawa, Ottawa, ON, Canada",IEEE Access,"3 Jan 2024","2024","12","","113","130","The limited size of existing datasets and signal variability have hindered EEG-based emotion recognition. In this paper, we present a solution that simultaneously addresses both problems. Generative Adversarial Networks (GANs) have recently shown notable data augmentation (DA) success. Therefore, we leverage a GAN-based DA technique to enhance the robustness of our proposed emotion recognition model by synthetically increasing the size of our datasets. Moreover, we employ contrastive learning to improve the quality of the learned representations from EEG signals and mitigate the adverse impact of inter-subject and intra-subject variability in signals corresponding to the same stimuli or emotions. We do so by maximizing the similarity in the representation of such EEG signals. We perform EEG-based emotion classification using a Graph Neural Network (GNN), which learns the relationship between the extracted EEG features. We compare the proposed model with several recent state-of-the-art emotion recognition models on the DEAP and MAHNOB datasets. The experimental results demonstrate that the proposed model outperforms previous models with a 64.84% and 66.40% emotion classification accuracy on the test set of the DEAP dataset and a 66.98% and 71.69% emotion classification accuracy on the test set of the MAHNOB-HCI dataset for the valence and arousal emotional dimensions, respectively. We perform an ablation study to demonstrate how contrastive learning, GAN, and GNN contribute to improving the proposed solution’s performance.","2169-3536","","10.1109/ACCESS.2023.3344476","Create-Best; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10365153","Contrastive learning;data augmentation;emotion in human-computer interaction;graph neural network;machine learning","Brain modeling;Electroencephalography;Emotion recognition;Feature extraction;Data models;Convolutional neural networks;Graph neural networks","","5","","77","CCBYNCND","18 Dec 2023","","","IEEE","IEEE Journals"
"Deep Learning Classification of Neuro-Emotional Phase Domain Complexity Levels Induced by Affective Video Film Clips","S. Aydın","Department of Biophysics, Faculty of Medicine, University of Hacettepe, Ankara, Turkey",IEEE Journal of Biomedical and Health Informatics,"4 Jun 2020","2020","24","6","1695","1702","In the present article, a novel emotional complexity marker is proposed for classification of discrete emotions induced by affective video film clips. Principal Component Analysis (PCA) is applied to full-band specific phase space trajectory matrix (PSTM) extracted from short emotional EEG segment of 6 s, then the first principal component is used to measure the level of local neuronal complexity. As well, Phase Locking Value (PLV) between right and left hemispheres is estimated for in order to observe the superiority of local neuronal complexity estimation to regional neuro-cortical connectivity measurements in clustering nine discrete emotions (fear, anger, happiness, sadness, amusement, surprise, excitement, calmness, disgust) by using Long-Short-Term-Memory Networks as deep learning applications. In tests, two groups (healthy females and males aged between 22 and 33 years old) are classified with the accuracy levels of $\text{68.52}{\%}$ and $\text{79.36}{\%}$ through the proposed emotional complexity markers and and connectivity levels in terms of PLV in amusement. The groups are found to be statistically different ($p\ll 0.5$) in amusement with respect to both metrics, even if gender difference does not lead to different neuro-cortical functions in any of the other discrete emotional states. The high deep learning classification accuracy of $\text{98.00}{\%}$ is commonly obtained for discrimination of positive emotions from negative emotions through the proposed new complexity markers. Besides, considerable useful classification performance is obtained in discriminating mixed emotions from each other through full-band connectivity features. The results reveal that emotion formation is mostly influenced by individual experiences rather than gender. In detail, local neuronal complexity is mostly sensitive to the affective valance rating, while regional neuro-cortical connectivity levels are mostly sensitive to the affective arousal ratings.","2168-2208","","10.1109/JBHI.2019.2959843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933102","Brain biophysics;emotion recognition;affective neuroscience;deep learning","Electroencephalography;Complexity theory;Principal component analysis;Emotion recognition;Informatics;Trajectory","Adult;Algorithms;Brain;Deep Learning;Electroencephalography;Emotions;Female;Humans;Male;Motion Pictures;Pattern Recognition, Automated;Signal Processing, Computer-Assisted;Young Adult","68","","105","IEEE","16 Dec 2019","","","IEEE","IEEE Journals"
"CR-GAT: Consistency Regularization Enhanced Graph Attention Network for Semi-supervised EEG Emotion Recognition","J. Liu; H. Wu; L. Zhang","School of Computer Science, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, Shaanxi, China",2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"2 Jan 2023","2022","","","2017","2023","Electroencephalogram (EEG) emotion recognition has become a research focus in the field of human-computer interaction (HCI). However, the process of EEG signal collection requires lots of expertise, which makes the amount of labeled EEG data very limited. It constrains the performance of supervised methods which require large amounts of annotated data in some sense. Self-supervised learning paradigm, which aims to train models that do not require any labeled samples can make full use of a large amount of unlabeled EEG samples. But a drawback is that they fall short of learning class discriminative sample representations since no labeled information is utilized during training. To solve the above problem, we propose a semi-supervised model, named consistency regularization enhanced graph attention network (CR-GAT) for EEG emotion recognition. The CR-GAT mainly consists of three modules, namely the feature extraction and fusion (FEF) module, the feature graph building and augment (GBA) module as well as the consistency regularization (CR) module. Specifically, t he F EFm odule is to extract task-specific EEG features and highlight the most valuable features from the EEG signals. The GBA module is to build a sample-related graph representation of the EEG feature set. The CR module, which draws support samples from labeled samples and anchor samples from the entire sample set, intends to minimize the difference between the predicted class distributions from different graphs constructed by multi-views of the sample set to push samples that belong to the same class to be grouped together. We conduct our experiment on three real-world datasets, the experimental results show the method surpasses most of competitive models.","","978-1-6654-6819-0","10.1109/BIBM55620.2022.9994941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9994941","EEG;Emotion recognition;semi-supervised;consistency regularization;GAT","Human computer interaction;Training;Emotion recognition;Visualization;Biological system modeling;Self-supervised learning;Feature extraction","","4","","38","IEEE","2 Jan 2023","","","IEEE","IEEE Conferences"
"Wearable Ear EEG Device for Emotion Recognition in Human-Robot Interaction","N. -D. Mai; K. Go; X. Mao; W. -Y. Chung","Dept. Artificial Intelligence Convergence, Pukyong National University, Busan, Korea; Dept. Computer Science and Engineering, University of Yamanashi Kofu, Yamanashi, Japan; Dept. Computer Science and Engineering, University of Yamanashi Kofu, Yamanashi, Japan; Dept. Artificial Intelligence Convergence, Pukyong National University, Busan, Korea",2024 International Conference on Cyberworlds (CW),"17 Mar 2025","2024","","","323","327","Wearable electroencephalography (EEG) devices are emerging as crucial tools in human-robot interaction (HRI), enabling intuitive and effective communication between humans and robots. These devices non-invasively measure brain activity, providing real-time insights into a user’s mental state, intentions, and cognitive load. This paper explores the advancements and applications of wearable ear EEG technology in HRI, with a focus on detecting human intent, monitoring emotional and cognitive states, and delivering real-time feedback for adaptive robot behavior. The benefits of wearable EEG over traditional scalp EEG, such as enhanced user comfort, reduced setup time, and improved long-term wearability, are thoroughly examined. Additionally, the paper covers advanced applications, including emotion-aware adaptive interactions, neurofeedback training, and direct robot control via brain-computer interfaces (BCIs). A review of the latest advancements in device miniaturization, integration with other wearable sensors, and applications in virtual reality, gaming, and healthcare is provided. Future directions focus on expanding applications, enhancing human-AI collaboration, and improving accuracy and reliability in HRI. The studies discussed in this paper represent our state-of-the-art research across various fields and applications of wearable EEG systems. This study highlights the potential of wearable Ear-EEG devices to revolutionize human-robot interactions by enhancing customization, responsiveness, and overall efficacy.","2642-3596","979-8-3315-2717-4","10.1109/CW64301.2024.00045","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10918263","Ear EEG;human-robot interaction (HRI);brain-computer interfaces (BCIs)","Training;Scalp;Human-robot interaction;Ear;Virtual reality;Electroencephalography;Real-time systems;Brain-computer interfaces;Reliability;Biomedical monitoring","","","","13","IEEE","17 Mar 2025","","","IEEE","IEEE Conferences"
"Emotion recognition from peripheral physiological signals enhanced by EEG","S. Chen; Z. Gao; S. Wang","University of Science and Technology of China, School of Computer Science and Technology, Hefei, Anhui; University of Science and Technology of China, School of Computer Science and Technology, Hefei, Anhui; University of Science and Technology of China, Hefei, Anhui, CN","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 May 2016","2016","","","2827","2831","Current multi-modal emotion recognition from physiological signals requires electroencephalogram(EEG) signals and peripheral physiological signals during both training and test. Compared with the peripheral physiological signals, it is more difficult to obtain EEG signals in our daily life. Therefore, we propose a novel approach to recognize emotions from peripheral signals by using EEG features as privileged information, which is only available during training. During training, first, peripheral physiological features and EEG features are extracted. Then, we construct a new peripheral physiological feature space using canonical correlation analysis with the help of EEG features. Finally we train a support vector machine(SVM) to map the new peripheral physiological features to the emotion labels. During test, only peripheral physiological features are used to recognize emotions from the constructed peripheral physiological feature space with the trained SVM model. The experimental results on two benchmark databases show that our proposed approach using EEG features as privileged information outperforms the method which recognizes emotions merely from the peripheral physiological signals.","2379-190X","978-1-4799-9988-0","10.1109/ICASSP.2016.7472193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472193","Emotion Recognition;EEG;Peripheral Physiological Signals;Privileged Information;Canonical Correlation Analysis","Electroencephalography;Emotion recognition;Physiology;Feature extraction;Databases;Principal component analysis;Training","","12","","10","IEEE","19 May 2016","","","IEEE","IEEE Conferences"
"Emotion Recognition from Wearable EEG Devices Using Neural Networks","T. Nithya; V. Ezhilkumar; K. S. H. Supikssa; S. Harshini; R. Kishore","Information Technology, Velalar College of Engineering and Technology, Erode, Tamil Nadu, India; Department of Information Technology, Velalar College of Engineering and Technology, Erode, Tamil Nadu, India; Department of Information Technology, Velalar College of Engineering and Technology, Erode, Tamil Nadu, India; Department of Information Technology, Velalar College of Engineering and Technology, Erode, Tamil Nadu, India; Department of Information Technology, Velalar College of Engineering and Technology, Erode, Tamil Nadu, India",2024 13th International Conference on System Modeling & Advancement in Research Trends (SMART),"20 Feb 2025","2024","","","696","700","Recognizing emotions using wearable technology is an emerging field that leverages the breakthroughs in machine learning and neuroscience to conduct real-time analyses of human emotional states. This study explores the scope of wearable technology-based emotion recognition by incorporating EEG (Electroencephalogram) signals to capture the electrical activity of the brain. A Deep Neural Network (DNN) is deployed to sort emotions into three classes: positive, negative and neutral. Harnessing advanced feature extraction strategies for EEG processing brings about improved accuracy over conventional methods. The observations demonstrate that the proposed DNN-based model leads way to economical, non-invasive and mobile solution for affect monitoring. This research offers notable potential in fields like human-computer interaction, mental health tracking, and stress management at work. Scalable medical-grade equipment alternatives are attained by means of the easily accessible wearable devices, that could be used in much wider scale in real-world use cases. This study aims to offer a feasible strategy for emotion recognition using EEG through compact devices.","2767-7362","979-8-3503-8058-3","10.1109/SMART63812.2024.10882207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10882207","Emotion recognition;Deep neural networks;Electroencephalogram;Human-Computer Interaction;Neuroscience","Emotion recognition;Neuroscience;Artificial neural networks;Mental health;Electroencephalography;Real-time systems;Biomedical monitoring;Wearable devices;Biological neural networks;Monitoring","","","","10","IEEE","20 Feb 2025","","","IEEE","IEEE Conferences"
"Robust emotion classification using neural network models","S. Salari; A. Ansarian; H. Atrianfar","Electrical Engineering Department, Amirkabir university of Technology, Tehran, Iran; Electrical Engineering Department, Amirkabir university of Technology, Tehran, Iran; Electrical Engineering Department, Amirkabir university of Technology, Tehran, Iran",2018 6th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS),"12 Apr 2018","2018","","","190","194","In the field of Brain Computer Interface, Emotion recognition plays an increasingly crucial role. As psychological understanding of emotions progresses, feature extraction along with classification of electroencephalogram (EEG) representation of these emotions becomes a more important challenge. In this work, Neural Networks as a type of high accuracy robust statistical learning model was employed in order to classify human emotions from the DEAP [7] dataset containing the measured EEG signals for Emotion Classification research. We take advantage of two Neural Network based models the first one of which is the Deep Neural Network and the other one is the Convolutional Neural Network in order to classify Valence, Arousal, Dominance and liking into two categories of Yes or No (High and Low) and to classify Valence and Arousal into three categories of (High, Normal, Low), The achieved accuracy surpasses those achieved in other papers indicating that these models carry the ability to be used as a high achieving classifier for BCI signals.","","978-1-5386-2836-2","10.1109/CFIS.2018.8336626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8336626","Deep-Learning;Machine-Learning;Computational-Intelligence;Medical-Image-Analysis","Brain modeling;Biological neural networks;Electroencephalography;Emotion recognition;Computational modeling;Feature extraction;Convolutional neural networks","","9","","15","IEEE","12 Apr 2018","","","IEEE","IEEE Conferences"
"Multifaceted Discrete Emotion Recognition from EEG Physiological Signals via Machine Learning Techniques","A. Gupta; K. Nayyar; H. Chopra; N. Gahlan; D. Sethia","Dept. of Software Engineering, Delhi Technological University; Dept. of Electronics & Communication Engineering, Delhi Technological University; Dept. of Computer Engineering, Delhi Technological University; Dept. of Software Engineering, Delhi Technological University; Dept. of Software Engineering, Delhi Technological University",2024 5th International Conference for Emerging Technology (INCET),"26 Jul 2024","2024","","","1","8","Emotions, intricate mental states shaped by neu-rophysiological alterations influenced by cognitive processes, sensory inputs, behaviours, and diverse experiences, remain a compelling domain for exploration. The Electroencephalogram (EEG) is a potent tool that directly quantifies these emotional variations by capturing brain signals. Recent strides in emotion recognition have harnessed traditional machine learning classifiers to automate the identification of human emotions with remarkable success. This paper delves into the relatively underexplored Discrete Emotion Model, unveiling its capacity to achieve outstanding accuracy despite historical reservations regarding its effectiveness, using the ECSMP (Emotion, Cognition, Sleep, and Multi-model Physiological signals) dataset. It has two distinct environments, Video watching, and CANTAB-based cognitive assessment phases, enabling seamless data collection and analysis. This research effectively quantifies emotion for binary classification (classifies the type of emotion felt) and multiclass classification (classifies the intensity of emotion felt), elevating emotion recognition capabilities through the synergy of EEG technology. The exceptional performance of XGBoost, with a 96.5% accuracy rate in binary classification and 95% in multiclass emotion recognition, highlights its prowess compared to the other models tested.","","979-8-3503-6115-5","10.1109/INCET61516.2024.10592936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10592936","Emotion Recognition;Physiological signals","Emotion recognition;Accuracy;Sleep;Cognitive processes;Machine learning;Data collection;Brain modeling","","","","22","IEEE","26 Jul 2024","","","IEEE","IEEE Conferences"
"A Bi-LSTM Frequency Band Attention Network for EEG- Based Emotion Recognition","B. Hu; R. Liu; G. Liu","Southwest University, College of Electronic and Information Engineering, Chongqing, China; Southwest University, College of Electronic and Information Engineering, Chongqing, China; Southwest University, College of Electronic and Information Engineering, Chongqing, China","2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)","11 Jul 2024","2024","","","1708","1711","Emotion is a complex internal state of humans, involving physiological responses, behavioral expressions, and cognitive evaluations. Effective emotion recognition is crucial for human-computer interaction. Currently, research on emotion recognition based on electroencephalogram (EEG) primarily focuses on subject-dependent emotion recognition tasks. These studies have provided many valuable insights and technical approaches, but they are limited to applications within a single dataset and rarely explore how to perform emotion recognition across datasets. Therefore, we propose a hybrid model based on the Transformer architecture, utilizing Bidirectional Long Short-Term Memory networks (Bi-LSTM) to capture long-term dependencies and short-term variations in EEG signals. This model incorporates a frequency band attention mechanism to deeply understand the relationships between frequency bands, thereby more effectively extracting emotional features. Furthermore, this paper also designs a method based on deep model transfer, aiming to achieve cross-dataset emotion recognition. Experimental results on the SEED and DEAP datasets show that the proposed method performs excellently in reducing model training time and improving emotion recognition accuracy, providing a promising framework for future research on cross-dataset emotion recognition.","","979-8-3503-8555-7","10.1109/AINIT61980.2024.10581447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581447","Bi-directional Long Short-Term Memory;Transfer Learning;Emotion recognition;Electroencephalogram","Training;Seminars;Emotion recognition;Accuracy;Transfer learning;Brain modeling;Feature extraction","","","","10","IEEE","11 Jul 2024","","","IEEE","IEEE Conferences"
"A Dual-Branch Dynamic Graph Convolution Based Adaptive TransFormer Feature Fusion Network for EEG Emotion Recognition","M. Sun; W. Cui; S. Yu; H. Han; B. Hu; Y. Li","Department of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Engineering Medicine, Beihang University, Beijing, China; Beijing Aerospace Measurement & Control Technology Co. Ltd, Beijing, China; Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, Gansu, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Department of Automation Science and Electrical Engineering, Beihang University, Beijing, China",IEEE Transactions on Affective Computing,"28 Nov 2022","2022","13","4","2218","2228","Electroencephalograph (EEG) emotion recognition plays an important role in the brain-computer interface (BCI) field. However, most of recent methods adopted shallow graph neural networks using a single temporal feature, leading to the limited emotion classification performance. Furthermore, the existing methods generally ignore the individual divergence between different subjects, resulting in poor transfer performance. To address these deficiencies, we propose a dual-branch dynamic graph convolution based adaptive transformer feature fusion network with adapter-finetuned transfer learning (DBGC-ATFFNet-AFTL) for EEG emotion recognition. Specifically, a dual-branch graph convolution network (DBGCN) is firstly designed to effectively capture the temporal and spectral characterizations of EEG simultaneously. Second, the adaptive Transformer feature fusion network (ATFFNet) is conducted by integrating the obtained feature maps with the channel-weight unit, leading to significant difference between different channels. Finally, the adapter-finetuned transfer learning method (AFTL) is applied in cross-subject emotion recognition, which proves to be parameter-efficient with few samples of the target subject. The competitive experimental results on three datasets have shown that our proposed method achieves the promising emotion classification performance compared with the state-of-the-art methods. The code of our proposed method will be available at: https://github.com/smy17/DANet.","1949-3045","","10.1109/TAFFC.2022.3199075","National Natural Science Foundation of China(grant numbers:U1809209,61671042,61403016); Beijing Municipal Education Commission - Natural Science Foundation; Beijing United Imaging Research Institute of Intelligent Imaging Foundation(grant numbers:KZ202110025036,CRIBJQY202103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857970","EEG;emotion recognition;graph neural network;Transformer;transfer learning","Electroencephalography;Feature extraction;Emotion recognition;Convolution;Brain modeling;Transformers;Transfer learning","","59","","40","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Recognition of emotional states induced by music videos based on nonlinear feature extraction and SOM classification","S. Hatamikia; A. M. Nasrabadi","Department of biomedical engineering, Science and Research branch, Islamic Azad University, Tehran, Iran; Department of biomedical engineering, Shahed University, Tehran, Iran",2014 21th Iranian Conference on Biomedical Engineering (ICBME),"19 Feb 2015","2014","","","333","337","This research aims at investigating the relationship between Electroencephalogram (EEG) signals and human emotional states. A subject-independent emotion recognition system is proposed using EEG signals collected during emotional audio-visual inductions to classify different classes of continuous valence-arousal model. First, four feature extraction methods based on Approximate Entropy, Spectral entropy, Katz's fractal dimension and Petrosian's fractal dimension were used; then, a two-stage feature selection method based on Dunn index and Sequential forward feature selection algorithm (SFS) algorithm was used to select the most informative feature subsets. Self-Organization Map (SOM) classifier was used to classify different emotional classes with the use of 5-fold cross-validation. The best results were achieved using combination of all features by average accuracies of %68.92 and %71.25 for two classes of valence and arousal, respectively. Furthermore, a hierarchical model which was constructed of two classifiers was used for classifying 4 emotional classes of valence and arousal levels and the average accuracy of %55.15 was achieved.","","978-1-4799-7418-4","10.1109/ICBME.2014.7043946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7043946","Emotion recognition;Nonlinear analysis;Sequential forward feature selection algorithm (SFS);Dunn index;Self Organization Map (SOM)","Feature extraction;Accuracy;Emotion recognition;Electroencephalography;Fractals;Biomedical engineering;Entropy","","26","","13","IEEE","19 Feb 2015","","","IEEE","IEEE Conferences"
"The Correlate of Emotion and Gender Classification Using EEG Signals","Y. F. Alharbi; Y. A. Alotaibi","Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia",2021 IEEE 6th International Conference on Signal and Image Processing (ICSIP),"28 Jan 2022","2021","","","790","794","Many current applications depend on human-computer interaction systems for service delivery. These applications need to know the personal features of users, such as their gender, to improve the user experience. Notably, therefore, it is possible to categorize gender based on speech and facial characteristics. Additionally, instead of considering body part features, gender prediction based on physiological measurements such as electroencephalogram (EEG) signals provides a strong and reliable method for an automatic gender classification system. EEG signals record brain activity from the outer scalp, which means they are contaminated by external and internal noise. Therefore, designing an effective gender detection mechanism based on EEG data is challenging because gender-related EEG features must be captured accurately. In this paper, we investigated the effect of emotions on a gender prediction system using EEG data in negative and positive emotional states. We proposed a model that extracts the power spectral density features of EEG signals in emotional states and predicts gender using three classifiers: decision tree, random forest, and multilayer perceptron. Furthermore, we studied the effect of omitting a single electrode and multiple electrodes in the EEG data on the proposed system. The experimental results demonstrate the effectiveness of using emotional state EEG data to identify gender. Furthermore, the random forest classifier achieved the lowest percentage error of 7% based on negative emotion EEG signal. Finally, the results revealed that the brain’s frontal lobe has a high level of success in enabling differentiation between males and females.","","978-1-6654-3904-6","10.1109/ICSIP52628.2021.9688884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688884","Classification;gender;emotions;random forest;MLP;EEG signals","Electrodes;Frontal lobe;Biometrics (access control);Databases;Multilayer perceptrons;Feature extraction;Brain modeling","","4","","25","IEEE","28 Jan 2022","","","IEEE","IEEE Conferences"
"A Real-Time and Two-Dimensional Emotion Recognition System Based on EEG and HRV Using Machine Learning","Y. Wei; Y. Lil; M. Xu; Y. Hua; Y. Gong; K. Osawa; E. Tanaka","Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information Production and Systems, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan",2023 IEEE/SICE International Symposium on System Integration (SII),"15 Feb 2023","2023","","","1","6","With the research on mental health, rehabilitation training and other fields, obtaining people's real emotion feelings is frequently required in many fields. Emotion recognition method based on physiological signals can directly obtain people's emotion states and avoid pretending expression and emotional expression disorder. In physiological signals, Electroencephalogram (EEG) signal is commonly used in the emotion evaluation, and Heart Rate Variability (HRV) signal is related to people's excited feeling. This paper proposed an emotion recognition method based on EEG and HRV to do the emotion recognition work. This method aims to solve the accuracy problem of instant emotion recognition, and achieve a higher accuracy. According to Russell's model of emotion, the system in this paper use two dimensions, “valence” and “arousal”, to describe people's emotion. The emotion recognition system we proposed combines more advanced neural network models and eigenvalues closely related to emotional states. This system uses DenseNet as the neural network model for machine learning process, which is more accurate than the general deep neural network. Using differential entropy as the main eigenvalue makes the system's ability to analyze emotions based on EEG more efficient.","2474-2325","979-8-3503-9868-7","10.1109/SII55687.2023.10039222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10039222","","Training;Emotion recognition;Neural networks;System integration;Brain modeling;Electroencephalography;Physiology","","4","","21","IEEE","15 Feb 2023","","","IEEE","IEEE Conferences"
"Mental State Evaluation with Machine Learning by utilizing Brain Signals","H. M. Mallikarjun","Dept.of EIE RNSIT, Bengaluru, India",2022 Fourth International Conference on Cognitive Computing and Information Processing (CCIP),"31 Mar 2023","2022","","","1","6","Mental State of the subject is evaluated by using EEG signals. EEG signal from the brain is taken by using Mind wave kit, which gives the raw EEG waves by the non-invasive method only using single electrode. To induce emotion in to the subject different emotional videos are shown and respective emotion EEGs are collected. so the electrical different EEG wave Alpha, Beta, Delta, Gama and theta varies, different waves having its nativity according to the emotional changes. Lucid scribe toolkit support to collection of data from the Mobile mind wave, the data exported to the excel and by finding the minimum and maximum value of every EEG wave, this is in the numerical values with known Mental states are set with numerical values like 0 neutral, 1 Happy, 2 Disgust, 3 Sad, 4 Angry. In this work Mental state evaluation using signal processing is carried by preparing 280 datasets are prepared by showing them different videos related to respective emotions. By using neuro sky's Mindwave kit brain waves are recorded at the forehead values are tabulated accordingly. 280 datasets are fed into Orange, open-source machine learning and data visualization module and algorithms are compared by extracting confusion matrices.","","978-1-6654-5648-7","10.1109/CCIP57447.2022.10058684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10058684","EEG;ALPHA;BETA;GAMA;THETA;REM;Orange;Lucid scribe","Electrodes;Machine learning algorithms;Forehead;Signal processing algorithms;Data visualization;Machine learning;Information processing","","2","","4","IEEE","31 Mar 2023","","","IEEE","IEEE Conferences"
"Brian-computer interface-based Emotion Features Recognition using Convolutional Neural Networks","Y. Ma","School of Architecture, University of Southern California, Los Angeles, California, United States",2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC),"27 Mar 2023","2022","","","555","558","Recently, the brain-computer interface technique has developed in the emotion recognition, it can acquire brain signals to decode human intention. However, lower classification performance is its limitation. Thus, the objective of this work is to decode emotion features using convolutional neural networks (CNN) in the field of brain-computer interface, to realize higher accuracy to classify different emotion features. In the processing of this work, electroencephalograph (EEG) was utilized to acquire brain's signals, some bandpass and wavelet filters were used to filter brain's signals, and finally, the CNN was proposed to automatically extract signal features and classify characteristics of intention. For results of this study, the 89.5% accuracy was realized to classify different conditions consisting of positive, negative and natural emotions. In the future, the proposed algorithm called CNN can be used to solve and improve lower recognition accuracy, and further to developed artificial intelligent algorithm application to brain-computer interface.","","979-8-3503-2195-1","10.1109/ICFTIC57696.2022.10075249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075249","emotion features;convolutional neural networks;brain-computer interface;EEG","Band-pass filters;Emotion recognition;Filtering algorithms;Feature extraction;Brain-computer interfaces;Electroencephalography;Classification algorithms","","","","18","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"EEG Emotion Recognition Based on an Implicit Emotion Regulatory Mechanism","D. Li; Z. Jin; Y. Shen; Z. Wang; S. Jiang","Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Psychiatry, Wenzhou Medical University, Wenzhou, China",IEEE Transactions on Artificial Intelligence,"","2025","PP","99","1","13","One of the main challenges in electroencephalography (EEG) emotion recognition is the lack of understanding of the biological properties of the brain and how they relate to emotions. To address this issue, this paper proposes an implicit emotion regulatory mechanism inspired contrastive learning framework (CLIER) for EEG emotion recognition. The framework simulates the complex relationship between emotions and the underlying neurobiological processes; to achieve this, the mechanism is mainly simulated through three parts. First, to leverage the inter-individual variability of emotional expression, the emotion features of the individual are captured by a dynamic connection graph in the subject-dependent setting. Subsequently, reverse regulation is simulated by contrast learning based on label information and data augmentation to capture more biologically specific emotional features. Finally, caused by the asymmetry between the left and right hemispheres of the human brain in response to emotions, brain lateralization mutual learning facilitates the fusion of the hemispheres in determining emotions. Experiments on SEED, SEED-IV, SEED-V, and EREMUS datasets show impressive results: 93.4% accuracy on SEED, 90.2% on SEED-IV, 82.46% on SEED-V, and 41.63% on EREMUS. Employing an identical experimental protocol, our model demonstrated superior performance relative to the majority of existing methods, thus showcasing its effectiveness in the realm of EEG emotion recognition.","2691-4581","","10.1109/TAI.2025.3560593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10964737","Emotion recognition;Implicit emotion regulatory mechanism;Dynamic graph convolution;Contrastive learning;Mutual learning","Electroencephalography;Emotion recognition;Regulation;Brain modeling;Human computer interaction;Feature extraction;Biology;Artificial intelligence;Contrastive learning;Training","","","","","IEEE","14 Apr 2025","","","IEEE","IEEE Early Access Articles"
"Unsupervised Time-Aware Sampling Network With Deep Reinforcement Learning for EEG-Based Emotion Recognition","Y. Zhang; Y. Pan; Y. Zhang; M. Zhang; L. Li; L. Zhang; G. Huang; L. Su; H. Liu; Z. Liang; Z. Zhang","School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China",IEEE Transactions on Affective Computing,"5 Sep 2024","2024","15","3","1090","1103","Recognizing human emotions from complex, multivariate, and non-stationary electroencephalography (EEG) time series is essential in affective brain-computer interface. However, because continuous labeling of ever-changing emotional states is not feasible in practice, existing methods can only assign a fixed label to all EEG timepoints in a continuous emotion-evoking trial, which overlooks the highly dynamic emotional states and highly non-stationary EEG signals. To solve the problems of high reliance on fixed labels and ignorance of time-changing information, in this paper we propose a time-aware sampling network (TAS-Net) using deep reinforcement learning (DRL) for unsupervised emotion recognition, which is able to detect key emotion fragments and disregard irrelevant and misleading parts. Specifically, we formulate the process of mining key emotion fragments from EEG time series as a Markov decision process and train a time-aware agent through DRL without label information. First, the time-aware agent takes deep features from a feature extractor as input and generates sample-wise importance scores reflecting the emotion-related information each sample contains. Then, based on the obtained sample-wise importance scores, our method preserves top-X continuous EEG fragments with relevant emotion and discards the rest. Finally, we treat these continuous fragments as key emotion fragments and feed them into a hypergraph decoding model for unsupervised clustering. Extensive experiments are conducted on three public datasets (SEED, DEAP, and MAHNOB-HCI) for emotion recognition using leave-one-subject-out cross-validation, and the results demonstrate the superiority of the proposed method against previous unsupervised emotion recognition methods. The proposed TAS-Net has great potential in achieving a more practical and accurate affective brain-computer interface in a dynamic and label-free circumstance.","1949-3045","","10.1109/TAFFC.2023.3319397","National Natural Science Foundation of China(grant numbers:82272114,62276169,61906122,62071310); Medical-Engineering Interdisciplinary Research Foundation of Shenzhen University; Shenzhen University-Lingnan University Joint Research Programme; Shenzhen-Hong Kong Institute of Brain Science-Shenzhen Fundamental Research Institutions(grant numbers:2022SHIBS0003); Shenzhen Science and Technology Research and Development Fund for Sustainable Development Project(grant numbers:KCXFZ20201221173613036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10264207","Electroencephalography;affective brain-computer interface;emotion recognition;deep reinforcement learning;unsupervised learning","Electroencephalography;Feature extraction;Emotion recognition;Deep learning;Brain modeling;Convolutional neural networks;Reinforcement learning","","3","","63","IEEE","26 Sep 2023","","","IEEE","IEEE Journals"
"Subject-Independent Emotion Recognition of EEG Signals Based on Dynamic Empirical Convolutional Neural Network","S. Liu; X. Wang; L. Zhao; J. Zhao; Q. Xin; S. -H. Wang","College of Electronic and Information Engineering, Hebei University, Baoding, China; College of Electronic and Information Engineering, Hebei University, Baoding, China; College of Electronic and Information Engineering, Hebei University, Baoding, China; College of Electronic and Information Engineering, Hebei University, Baoding, China; Key Laboratory of Digital Medical Engineering of Hebei Province, Baoding, China; Machine Vision Engineering Research Center of Hebei Province, Baoding, China",IEEE/ACM Transactions on Computational Biology and Bioinformatics,"7 Oct 2021","2021","18","5","1710","1721","Affective computing is one of the key technologies to achieve advanced brain-machine interfacing. It is increasingly concerning research orientation in the field of artificial intelligence. Emotion recognition is closely related to affective computing. Although emotion recognition based on electroencephalogram (EEG) has attracted more and more attention at home and abroad, subject-independent emotion recognition still faces enormous challenges. We proposed a subject-independent emotion recognition algorithm based on dynamic empirical convolutional neural network (DECNN) in view of the challenges. Combining the advantages of empirical mode decomposition (EMD) and differential entropy (DE), we proposed a dynamic differential entropy (DDE) algorithm to extract the features of EEG signals. After that, the extracted DDE features were classified by convolutional neural networks (CNN). Finally, the proposed algorithm is verified on SJTU Emotion EEG Dataset (SEED). In addition, we discuss the brain area closely related to emotion and design the best profile of electrode placements to reduce the calculation and complexity. Experimental results show that the accuracy of this algorithm is 3.53 percent higher than that of the state-of-the-art emotion recognition methods. What's more, we studied the key electrodes for EEG emotion recognition, which is of guiding significance for the development of wearable EEG devices.","1557-9964","","10.1109/TCBB.2020.3018137","National Natural Science Foundation of China(grant numbers:61401308,61572063); Natural Science Foundation of Hebei Province(grant numbers:F2020201025,F2019201151,QN2017306,F2018210148); Science research project of Hebei Province(grant numbers:QN2020030,QN2016085); Foundation of President of Hebei University(grant numbers:XZJJ201909); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9174858","Convolutional neural network;dynamic differential entropy;empirical mode decomposition;subject-independent emotion recognition","Electroencephalography;Emotion recognition;Feature extraction;Classification algorithms;Heuristic algorithms;Brain modeling;Frequency-domain analysis","Algorithms;Brain;Electroencephalography;Emotions;Entropy;Female;Humans;Male;Neural Networks, Computer;Signal Processing, Computer-Assisted","71","","56","IEEE","24 Aug 2020","","","IEEE","IEEE Journals"
"EEG-Based Emotion Classification Using Spiking Neural Networks","Y. Luo; Q. Fu; J. Xie; Y. Qin; G. Wu; J. Liu; F. Jiang; Y. Cao; X. Ding","School of Electronic Engineering, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; Department of Security, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China; Business School, The University of Edinburgh, Edinburgh, U.K.; School of Computing, Engineering, and Intelligent Systems, Ulster University, Londonderry, U.K.",IEEE Access,"12 Mar 2020","2020","8","","46007","46016","A novel method of using the spiking neural networks (SNNs) and the electroencephalograph (EEG) processing techniques to recognize emotion states is proposed in this paper. Three algorithms including discrete wavelet transform (DWT), variance and fast Fourier transform (FFT) are employed to extract the EEG signals, which are further taken by the SNN for the emotion classification. Two datasets, i.e., DEAP and SEED, are used to validate the proposed method. For the former dataset, the emotional states include arousal, valence, dominance and liking where each state is denoted as either high or low status. For the latter dataset, the emotional states are divided into three categories (negative, positive and neutral). Experimental results show that by using the variance data processing technique and SNN, the emotion states of arousal, valence, dominance and liking can be classified with accuracies of 74%, 78%, 80% and 86.27% for the DEAP dataset, and an overall accuracy is 96.67% for the SEED dataset, which outperform the FFT and DWT processing methods. In the meantime, this work achieves a better emotion classification performance than the benchmarking approaches, and also demonstrates the advantages of using SNN for the emotion state classifications.","2169-3536","","10.1109/ACCESS.2020.2978163","National Natural Science Foundation of China(grant numbers:61976063,61762018); Natural Science Foundation of Guangxi Province(grant numbers:2017GXNSFAA198180); Overseas 100 Talents Program of Guangxi Higher Education(grant numbers:F-KA16035,F-KA16016); Guilin University of Electronic Technology(grant numbers:GXZDSY2016-03); Research Fund of Guangxi Key Lab of Multi-Source Information Mining and Security(grant numbers:18-A-02-02); Innovation Project of Guangxi Graduate Education(grant numbers:YCSW2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9024211","Emotion classification;spiking neural network;EEG signal","Electroencephalography;Biological neural networks;Feature extraction;Emotion recognition;Videos;Data processing;Physiology","","115","","44","CCBY","4 Mar 2020","","","IEEE","IEEE Journals"
"Single-Channel Selection for EEG-Based Emotion Recognition Using Brain Rhythm Sequencing","J. W. Li; S. Barma; P. U. Mak; F. Chen; C. Li; M. T. Li; M. I. Vai; S. H. Pun","State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau, China; Department of Electronics and Communication Engineering, Indian Institute of Information Technology Guwahati, Guwahati, India; Department of Electrical and Computer Engineering, University of Macau, Macau, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau, China; State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau, China",IEEE Journal of Biomedical and Health Informatics,"3 Jun 2022","2022","26","6","2493","2503","Recently, electroencephalography (EEG) signals have shown great potential for emotion recognition. Nevertheless, multichannel EEG recordings lead to redundant data, computational burden, and hardware complexity. Hence, efficient channel selection, especially single-channel selection, is vital. For this purpose, a technique termed brain rhythm sequencing (BRS) that interprets EEG based on a dominant brain rhythm having the maximum instantaneous power at each 0.2 s timestamp has been proposed. Then, dynamic time warping (DTW) is used for rhythm sequence classification through the similarity measure. After evaluating the rhythm sequences for the emotion recognition task, the representative channel that produces impressive accuracy can be found, which realizes single-channel selection accordingly. In addition, the appropriate time segment for emotion recognition is estimated during the assessments. The results from the music emotion recognition (MER) experiment and three emotional datasets (SEED, DEAP, and MAHNOB) indicate that the classification accuracies achieve 70–82% by single-channel data with a 10 s time length. Such performances are remarkable when considering minimum data sources as the primary concerns. Furthermore, the individual characteristics in emotion recognition are investigated based on the channels and times found. Therefore, this study provides a novel method to solve single-channel selection for emotion recognition.","2168-2208","","10.1109/JBHI.2022.3148109","National Key R&D Program of China(grant numbers:2020YFB1313502); Shenzhen-Hong Kong-Macau S&T Program(grant numbers:SGDX20201103094002009); Universidade de Macau(grant numbers:MYRG2018-00146-AMSV,MYRG2019-00056-AMSV); Science and Technology Development Fund(grant numbers:088/2016/A2,0144/2019/A3,0022/2020/AFJ); FDCT-funded; SKL-AMSV-ADDITIONAL FUND; SKL-AMSV(UM)-2020-2022); Shenzhen Sustainable; High-level University(grant numbers:20200925154002001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9705159","Brain rhythm sequencing (BRS);electroencephalography (EEG);emotion recognition;single-channel selection;sequence classification","Electroencephalography;Emotion recognition;Rhythm;Feature extraction;Electronic mail;Channel estimation;Bioinformatics","Brain;Electroencephalography;Emotions;Humans;Information Storage and Retrieval","34","","52","IEEE","4 Feb 2022","","","IEEE","IEEE Journals"
"Positional-Spectral-Temporal Attention in 3D Convolutional Neural Networks for EEG Emotion Recognition","J. Liu; Y. Zhao; H. Wu; D. Jiang","School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an, Shaanxi, China",2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"3 Feb 2022","2021","","","305","312","Recognizing the feelings of human beings plays a critical role in our daily communication. Neuroscience has demonstrated that different emotion states present different degrees of activation in different brain regions, EEG frequency bands and temporal stamps. In this paper, we propose a novel structure to explore the informative EEG features for emotion recognition. The proposed module, denoted by PST-Attention, consists of Positional, Spectral and Temporal Attention modules to explore more discriminative EEG features. Specifically, the Positional Attention module is to capture the activate regions stimulated by different emotions in the spatial dimension. The Spectral and Temporal Attention modules assign the weights of different frequency bands and temporal slices respectively. Our method is adaptive as well as efficient which can be fit into 3D Convolutional Neural Networks (3D-CNN) as a plug-in module. We conduct experiments on two real-world datasets. 3D-CNN combined with our module achieves promising results and demonstrate that the PST-Attention is able to capture stable patterns for emotion recognition from EEG.","2640-0103","978-988-14768-9-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689261","EEG;attention;emotion recognition;3D-CNN","Emotion recognition;Time-frequency analysis;Three-dimensional displays;Neuroscience;Sleep;Time series analysis;Feature extraction","","4","","41","","3 Feb 2022","","","IEEE","IEEE Conferences"
"Emotion Classification of subjects while watching video, using Recurrent Neural Network and Tensorflow","A. R. Singh; G. Singh; N. Saluja","Chitkara Uiversity Institute of Engineering and Technology Chitkara University, Punjab, India; Chitkara Uiversity Institute of Engineering and Technology Chitkara University, Punjab, India; Chitkara Uiversity Institute of Engineering and Technology Chitkara University, Punjab, India",2024 3rd International Conference for Innovation in Technology (INOCON),"6 May 2024","2024","","","1","5","In the fast-evolving realm of contemporary computing, the challenge of accurately classifying emotions based on physiological signals remains a central focal point. This research embarks on an exploration of the utilization of Re-current Neural Networks (RNN), a potent machine learning technique, for the interpretation of human emotions derived from electroencephalogram (EEG) data. The impetus driving this inquiry stems from the growing demand for non-invasive, real-time emotion recognition systems capable of enhancing human-computer interactions. The EEG data collected from participants who were exposed to emotion-evoking stimuli form the cornerstone of this study. The RNN model undergoes meticulous training and evaluation, leveraging feature extraction from EEG signals. The primary objective is to effectively distinguish between fundamental emotional states, including reactions to positive, negative, and neutral stimuli. Our research outcomes underscore a promising level of accuracy in emotion classification, thus emphasizing the potential of RNN within this specialized domain. Furthermore, these results pave the way for future research directions, such as the integration of multi-modal data and the adoption of more advanced machine learning models to further amplify performance. This study contributes significantly to the field by demonstrating the effectiveness of RNN in EEG-based emotion classification, thereby laying the groundwork for the development of more intuitive and empathetic computing systems.","","979-8-3503-8193-1","10.1109/INOCON60754.2024.10511918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10511918","EEG signals;Machine learning;Deep Analysis;Python;Exploratory Data Analysis (EDA);Long-Short Term Memory (LSTM);Recurrent Neural Network (RNN)","Human computer interaction;Emotion recognition;Recurrent neural networks;Computational modeling;Machine learning;Brain modeling;Electroencephalography","","","","22","IEEE","6 May 2024","","","IEEE","IEEE Conferences"
"Deep Learning in EEG: Advance of the Last Ten-Year Critical Period","S. Gong; K. Xing; A. Cichocki; J. Li","College of Life Sciences, Sichuan University, Chengdu, China; College of Life Sciences, Sichuan University, Chengdu, China; Computational and Data Science and Engineering, Skolkovo Institute of Science and Technology, Moscow, Russia; School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.",IEEE Transactions on Cognitive and Developmental Systems,"10 Jun 2022","2022","14","2","348","365","Deep learning has achieved excellent performance in a wide range of domains, especially in speech recognition and computer vision. Relatively less work has been done for electroencephalogram (EEG), but there is still significant progress attained in the last decade. Due to the lack of a comprehensive and topic widely covered survey for deep learning in EEG, we attempt to summarize recent progress to provide an overview, as well as perspectives for future developments. We first briefly mention the artifacts removal for EEG signal and then introduce deep learning models that have been utilized in EEG processing and classification. Subsequently, the applications of deep learning in EEG are reviewed by categorizing them into groups, such as brain–computer interface, disease detection, and emotion recognition. They are followed by the discussion, in which the pros and cons of deep learning are presented and future directions and challenges for deep learning in EEG are proposed. We hope that this article could serve as a summary of past work for deep learning in EEG and the beginning of further developments and achievements of EEG studies based on deep learning.","2379-8939","","10.1109/TCDS.2021.3079712","National Natural Science Foundation of China(grant numbers:61806149); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515010991); Ministry of Education and Science of the Russian Federation(grant numbers:14.756.31.0001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9430619","Brain–computer interface (BCI);classification;deep learning;disease;electroencephalogram (EEG);emotion;mental state;sleep","Deep learning;Electroencephalography;Brain modeling;Diseases;Brain-computer interfaces;Computational modeling;Mathematical model","","64","","277","CCBY","13 May 2021","","","IEEE","IEEE Journals"
"Hypercomplex Multimodal Emotion Recognition from EEG and Peripheral Physiological Signals","E. Lopez; E. Chiarantano; E. Grassucci; D. Comminiello","Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy; Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy; Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy; Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy","2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)","2 Aug 2023","2023","","","1","5","Multimodal emotion recognition from physiological signals is receiving an increasing amount of attention due to the impossibility to control them at will unlike behavioral reactions, thus providing more reliable information. Existing deep learning-based methods still rely on extracted handcrafted features, not taking full advantage of the learning ability of neural networks, and often adopt a single-modality approach, while human emotions are inherently expressed in a multimodal way. In this paper, we propose a hypercomplex multimodal network equipped with a novel fusion module comprising parameterized hypercomplex multiplications. Indeed, by operating in a hypercomplex domain the operations follow algebraic rules which allow to model latent relations among learned feature dimensions for a more effective fusion step. We perform classification of valence and arousal from electroencephalogram (EEG) and peripheral physiological signals, employing the publicly available database MAHNOB-HCI surpassing a multimodal state-of-the-art network. The code of our work is freely available at https://github.com/ispamm/MHyEEG.","","979-8-3503-0261-5","10.1109/ICASSPW59220.2023.10193329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193329","Hypercomplex Neural Networks;Hypercomplex Algebra;EEG;Multimodal Emotion Recognition","Learning systems;Emotion recognition;Databases;Neural networks;Speech recognition;Feature extraction;Physiology","","9","","28","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"FedEmo: A Privacy-Preserving Framework for Emotion Recognition using EEG Physiological Data","M. A. Anwar; M. Agrawal; N. Gahlan; D. Sethia; G. K. Singh; R. Chaurasia","Dept. of Computer Science and Engineering, Delhi Technological University, Delhi, India; Dept. of Computer Science and Engineering, Delhi Technological University, Delhi, India; Dept. of Software Engineering, Delhi Technological University, Delhi, India; Dept. of Software Engineering, Delhi Technological University, Delhi, India; Dept. of Software Engineering, Delhi Technological University, Delhi, India; Dept. of Computer Science and Engineering, Delhi Technological University, Delhi, India",2023 15th International Conference on COMmunication Systems & NETworkS (COMSNETS),"15 Feb 2023","2023","","","119","124","Emotions are intricate mental states triggered by neurophysiological adjustments linked to ideas, sensations, behavioral reactions, and a level of pleasure or annoyance. These changes are best traced with the physiological signal Electroencephalogram (EEG), as it records the direct sensations sent by the brain. Recent research on emotion classification methods employs conventional machine learning classifiers to access human emotions and perform automatic emotion recognition tasks. However, they lack in securing users' privacy and sensitive information because they need access to all data. A newly introduced framework Federated Learning (FL), can resolve this problem. It is an approach that aims to create a global model classifier without requiring access to users' local data. This study proposes a novel FL framework, Federated learning for Emotion recognition (FedEmo), for emotion state classification from physiological signal EEG while preserving users' data privacy. It uses Artificial Neural Network (ANN) as a baseline model for classifying emotional states: Arousal, Valence, and Dominance. Adding the concept of federated learning to build a framework FedEmo prevents loss of privacy as it enables the local training on the client's end with an updated model from the global server without compromising privacy. The proposed FedEmo framework approach achieves accuracies of 63.3%, 56.7%, and 52.2% for Valence, Arousal, and Dominance, respectively, using the well-known DREAMER dataset. These results are comparable to the basic centralized ANN model with the additional development of privacy preservation.","2155-2509","978-1-6654-7706-2","10.1109/COMSNETS56262.2023.10041308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041308","Emotion Recognition;Physiological signals;Fed-erated Learning;Data Privacy","Training;Emotion recognition;Data privacy;Privacy;Federated learning;Artificial neural networks;Brain modeling","","8","","30","IEEE","15 Feb 2023","","","IEEE","IEEE Conferences"
"Enhanced Feature Extraction with Superlet Transformation for EEG Emotion Classification","Q. Guan; L. Zou; Z. Hao; J. Li; X. Qian; L. Zhang; H. Zhou","School of Automation, Nanjing University of Science and Technology, China; School of Automation, Nanjing University of Science and Technology, China; School of Automation, Nanjing University of Science and Technology, China; School of Automation, Nanjing University of Science and Technology, China; School of Automation, Nanjing University of Science and Technology, China; Department of Rehabilitation, Jiangsu province official hospital; School of Automation, Nanjing University of Science and Technology, China",2024 17th International Convention on Rehabilitation Engineering and Assistive Technology (i-CREATe),"11 Dec 2024","2024","","","1","5","In the field of emotion recognition, electroencephalography(EEG) technology can accurately capture emotion, and has been widely used in psychology, public safety and other fields. In order to improve the accuracy and efficiency of emotion recognition, a new feature extraction method based on Superlets (SL) is developed in this paper. We designed an emotional experimental paradigm to induce positive, neutral and negative emotions. Further more, we extracted features using Superlets from the F7, F8, T7 and Pz electrode channels and classified three emotion states with SVM and DNN classifiers. In order to validate the performance of the proposed model, the collected EEG data set from our lab and SEED data set were used in this study. With SL features and DNN model, the average recognition accuracy from the 11 subjects was 85.75%, and the average recognition accuracy of SEED data set was 85.67%. The experiment results show that superlet transform could be used as a reliable feature extraction method in emotion recognition from EEG signal.","","979-8-3503-5515-4","10.1109/i-CREATe62067.2024.10776343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10776343","","Support vector machines;Emotion recognition;Time-frequency analysis;Accuracy;Transforms;Feature extraction;Brain modeling;Electroencephalography;Data models;Reliability","","","","19","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"A Hierarchical Bidirectional GRU Model With Attention for EEG-Based Emotion Classification","J. X. Chen; D. M. Jiang; Y. N. Zhang","School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China",IEEE Access,"30 Aug 2019","2019","7","","118530","118540","In this paper, we propose a hierarchical bidirectional Gated Recurrent Unit (GRU) network with attention for human emotion classification from continues electroencephalogram (EEG) signals. The structure of the model mirrors the hierarchical structure of EEG signals, and the attention mechanism is used at two levels of EEG samples and epochs. By paying different levels of attention to content with different importance, the model can learn more significant feature representation of EEG sequence which highlights the contribution of important samples and epochs to its emotional categories. We conduct the cross-subject emotion classification experiments on DEAP data set to evaluate the model performance. The experimental results show that in valence and arousal dimensions, our model on 1-s segmented EEG sequences outperforms the best deep baseline LSTM model by 4.2% and 4.6%, and outperforms the best shallow baseline model by 11.7% and 12% respectively. Moreover, with increase of the epoch's length of EEG sequences, our model shows more robust classification performance than baseline models, which demonstrates that the proposed model can effectively reduce the impact of long-term non-stationarity of EEG sequences and improve the accuracy and robustness of EEG-based emotion classification.","2169-3536","","10.1109/ACCESS.2019.2936817","National Natural Science Foundation of China(grant numbers:61806118 (Project Name: Research on EEG Based Emotion Recognition With Semi-Supervised Deep Generative Adversarial Network, Project Principal: J. X. Chen)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809690","Hierarchical;bidirectional GRU;attention;EEG;emotion classification","Electroencephalography;Brain modeling;Logic gates;Feature extraction;Data models;Adaptation models;Deep learning","","126","","36","CCBY","22 Aug 2019","","","IEEE","IEEE Journals"
"Recognizing affective state patterns using regularized learning with nonlinear dynamical features of EEG","M. Fan; C. -A. Chou","Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, USA; Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, USA",2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),"9 Apr 2018","2018","","","137","140","In the present work, we aim to classify human emotional states categorized based on the arousal-valence model [1] by applying logistic regression (the original and L1 - regularized LR model) to nonlinear features extracted from electroencephalographic (EEG) signals. Recurrence quantification analysis (RQA) [2] was employed to effectively capture the underlying dynamics behind the complex reactivity corresponding to affective phenomenon. A benchmark dataset, DEAP [3], was used for our two-fold objectives: (1) to investigate the suitability of RQA measures and regularized learning method for emotion recognition, and (2) to compare the performances as well as topographic patterns of important channels for classifying emotional states with previous studies. The results demonstrated that our proposed method with selected RQA measures has better performance (test accuracy = 75.7% and F1 score = 78.1% on average) comparing to previous studies, and L1-regularized model is less over-fitted comparing to the LR.","","978-1-5386-2405-0","10.1109/BHI.2018.8333388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8333388","","Electroencephalography;Feature extraction;Brain modeling;Indexes;Emotion recognition;Task analysis","","8","","19","IEEE","9 Apr 2018","","","IEEE","IEEE Conferences"
"Machine Learning Algorithm to Detect EEG based Emotion states using Virtual-Video stimuli","T. S; R. B. N; M. K. R","Dept. of ETE, B M S Institute of Technology and Management, Bengaluru, Karnataka, India; Dept. of CSE, K S School of Engineering and Management, Bengaluru, Karnataka, India; Dept. of ETE, B M S Institute of Technology and Management, Bengaluru, Karnataka, India","2023 International Conference on Advances in Electronics, Communication, Computing and Intelligent Information Systems (ICAECIS)","7 Jul 2023","2023","","","151","156","The interpretation and detection of human emotions are crucial functions of the center-nervous system. Physiological signals are used widely to develop emotion recognition systems in recent years. Detection of emotions using electroencephalogram is a challenging task since they exhibit nonlinear nature. In this paper, three popular machine-learning models are implemented on an acquired database using a Novel approach. The EEG signals are acquired by showing Virtual Reality Video clips for eight discrete emotions. Thirty-four features are extracted in time and frequency domains. The frequency bands decomposition is carried out using 4-level Discrete-Wavelet-Transforms. The generated feature vectors are used on KNN, SVM, and ANN algorithm-based classifiers to classify the data into four, three, and two emotion states. The overall accuracy using ANN, SVM, and KNN classifiers is 85.50%, 73.50%, and 66.75% respectively for 4 classes. The performance of ANN classifiers gave a better accuracy compared to other models.","","979-8-3503-4805-7","10.1109/ICAECIS58353.2023.10170069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10170069","ANN;EEG;SVM;Emotion detection;KNN","Support vector machines;Solid modeling;Machine learning algorithms;Virtual reality;Feature extraction;Brain modeling;Electroencephalography","","2","","21","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"EEG Brain Emotion Classification Using AI","S. Tyagi; A. Chauhan; S. Ratna; P. K. Ram","Artificial Intelligence, Amity University, Noida, India; Artificial Intelligence, Amity University, Noida, India; Artificial Intelligence, Amity University, Noida, India; Artificial Intelligence, Amity University, Noida, India","2024 International Conference on Computing, Sciences and Communications (ICCSC)","15 Jan 2025","2024","","","1","5","Emotion recognition from EEG brain signals is a challenging yet crucial task in affective computing and neuro informatics. In paper, we file a DNN-based approach for automatic emotion classification using EEG data. Our method utilizes a multilayered architecture comprising CNNs and RNNs to absorb ranked representations from raw EEG signals. We explore various preprocessing techniques to enhance the discriminative power of the model and employ transfer learning strategies to hold instructed neural network models for feature extraction. The framework is resulted on standard datasets, demonstrating promising results in accurately classifying emotional states. Our findings highlight the potential of deep neural networks in grasping difficult patterns in data and the field of affective computing towards real-world applications in mental health monitoring and humancomputer interaction.","","979-8-3503-5364-8","10.1109/ICCSC62048.2024.10830373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10830373","Deep neural network;Deep learning;Data pre- processing;Comparative analysis;PSD;EEG","Deep learning;Computational modeling;Collaboration;Mental health;Computer architecture;Brain modeling;Electroencephalography;Data models;Monitoring;Biological neural networks","","","","7","IEEE","15 Jan 2025","","","IEEE","IEEE Conferences"
"Emotional State Recognition Using Advanced Machine Learning Techniques on EEG Data","K. Giannakaki; G. Giannakakis; C. Farmaki; V. Sakkalis","School of Medicine, University of Crete, Iraklio, Greece; Foundation for Research and Technology Hellas, Institute of Computer Science, Iraklio, Greece; School of Medicine, University of Crete, Iraklio, Greece; School of Medicine, University of Crete, Iraklio, Greece",2017 IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS),"13 Nov 2017","2017","","","337","342","This study investigates the discrimination between calm, exciting positive and exciting negative emotional states using EEG signals. Towards this direction, a publicly available dataset from eNTERFACE Workshop 2006 was used having as stimuli emotionally evocative images. At first, EEG features were extracted based on literature review. Then, a computational framework is proposed using machine learning techniques, performing feature selection and classification into two at a time emotional states. The procedure described in this paper investigates and assess the effectiveness of selection and classification techniques providing improved classification accuracy. The proposed methodology is able to obtain accuracy of 75.12% in classifying the two emotional states comparing with similar studies using the same dataset.","2372-9198","978-1-5386-1710-6","10.1109/CBMS.2017.156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104214","emotion recognition;EEG;feature selection;classification;machine learning","Feature extraction;Electroencephalography;Radio frequency;Support vector machines;Vegetation;Artificial neural networks;Emotion recognition","","21","","38","IEEE","13 Nov 2017","","","IEEE","IEEE Conferences"
"An 8 Channel Patient Specific Neuromorphic Processor for the Early Screening of Autistic Children through Emotion Detection","A. R. Aslam; M. A. B. Altaf","Lahore University of Management Sciences, Lahore, Pakistan; Lahore University of Management Sciences, Lahore, Pakistan",2019 IEEE International Symposium on Circuits and Systems (ISCAS),"1 May 2019","2019","","","1","5","Autism Spectrum Disorder (ASD) is a neurodevelopment disorder that affects children's development and can lead to handicap life if remain untreated. Scalp Electroencephalography (EEG) data can be used as a biomarker to characterize the human emotions on the valence-arousal scale. This work presents a machine learning patient-specific emotion detection (PSED) classification processor based on an eight-channel EEG signal. The proposed PSED classification processor integrates a hardware-efficient feature extraction engine and patient-specific support vector machine (SVM) classifier to discriminate the emotions in real-time. To utilize minimal hardware resources a hardware realizable feature set comprising of power spectral density (PSD), an absolute difference of inter-hemispheric power asymmetry (IHPD), and the scaled inter-hemispheric power asymmetry ratio (SIHPR) of eight electrode pairs are evaluated. To avoid high overhead of area and power consumption for an integer divider for SIHPR; simple LUT based divider is proposed that calculates the approximated value of SIHPR with a minimal overhead of 64 Bytes. The classification is performed using a Linear SVM and resulted in an accuracy of 63% and 60% for valence and arousal, respectively, based on the database for emotion analysis using physiological signals (DEAP). The PSED processor is synthesized using a 65nm CMOS technology with an overall energy efficiency of 10uJ/classification.","2158-1525","978-1-7281-0397-6","10.1109/ISCAS.2019.8702738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8702738","arousal;autism spectrum disorder (ASD);electroencephalogram (EEG);emotion;a look-up table (LUT);support vector machine (SVM);valence","Electroencephalography;Support vector machines;Hardware;Pediatrics;Autism;Table lookup;Feature extraction","","19","","28","IEEE","1 May 2019","","","IEEE","IEEE Conferences"
"Channel Selection of EEG Emotion Recognition using Stepwise Discriminant Analysis","E. S. Pane; A. D. Wibawa; M. H. Pumomo","Departement of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Indonesia; Departement of Electrical Engineering & Departement of Computer Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Departement of Electrical Engineering & Departement of Computer Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2018 International Conference on Computer Engineering, Network and Intelligent Multimedia (CENIM)","13 May 2019","2018","","","14","19","EEG has been used by many applications recently, not only in the field of medicine but also telemarketing, games, and cybernetics. Measuring brain signal by involving EEG is complicated and delicate work because it involves many channels, frequency bands, and features. An efficient and effective method in EEG measurement is then becoming crucial among the scientists. This paper proposed a channel selection study for emotion recognition based on the EEG signal by using Stepwise Discriminant Analysis (SDA). SDA is the extension of statistical tool for discriminant analysis that include stepwise technique. In this paper, the data was obtained from the public emotion EEG dataset which was recorded using 62 channels of EEG devices for three target emotions (i.e., positive, negative and neutral). In order to handle high dimensionality in EEG signals, we extracted differential entropy feature from five frequency bands: delta, theta, alpha, beta, and gamma. The selection criteria in SDA was based on Wilks Lambda score to get the optimal channel. In order to measure the performance of selected channels, we fed the features vector of the EEG signal to the LDA classifier. We conducted several scenarios from the different number of selected channels in experiments, such as 3, 4, 7, and 15 channels. The highest accuracy of 99.85% was obtained from 15 channels scenario in all combinations of frequency bands. Our results also confirm that alpha, beta, and gamma frequency bands are reliable for EEG emotion recognition.","","978-1-5386-7509-0","10.1109/CENIM.2018.8711196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8711196","electrode selection;identifying emotion;Wilks lambda;brain signal","Electroencephalography;Emotion recognition;Feature extraction;Entropy;Covariance matrices;Signal processing algorithms;Frequency measurement","","12","","12","IEEE","13 May 2019","","","IEEE","IEEE Conferences"
"Machine Learning Algorithms for Emotion Detection using EEG Signals","M. S. Hossain; S. K. Biswas; D. M. Thounaojam; A. Khan; K. N. e. A. Siddiquee","CSE Dept., NIT Slichar, Assam, India; CSE Dept., NIT Slichar, Assam, India; CSE Dept., NIT Slichar, Assam, India; CSE Dept., NIT Slichar, Assam, India; Faculty of Engineering, Multimedia University, Cyberjaya, Malaysia","2024 IEEE International Conference on Computing, Applications and Systems (COMPAS)","19 Dec 2024","2024","","","1","6","Assessment of clinical subjects’ cognitive functions and state is essential to e-healthcare delivery and developing novel human-machine interfaces. With the advancements in brain physiology research, electroencephalogram (EEG)-based emotion detection has emerged as an intriguing and popular study area. The emotion detection model can be implemented in human mental health monitoring, therapeutic support, law enforcement agencies, driver monitoring, etc. Numerous studies have been conducted in the last few decades to use EEG waves to detect emotions. In these research articles, an exhaustive study was not shown to find the best Machine Learning (ML) algorithms for multi-class emotion detection. Moreover, most of these researchers used a large number of electrodes in the experiment, which creates complexity in real-time setup. Therefore, the primary objective of this study is to identify the most effective ML algorithm that gives the expected accuracy using a single electrode. The proposed research has been done on the SEED-IV dataset to classify four common types of emotions: Happy, Fear, Sad, and Neutral. Different types of standard ML classifiers, including ensemble ML classifiers, have been used in this experiment. The performance of this study has been assessed by splitting and 10-fold cross-validation. Linear Discriminant Analysis (LDA) shows a better classification rate of 72.22% and 72.31% among all the classifiers according to Accuracy and F1 score. This study’s results anticipate opening up new possibilities for neuroscience by demonstrating that single-channel EEG data alone is sufficient for categorizing emotions.","","979-8-3315-2976-5","10.1109/COMPAS60761.2024.10796614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10796614","Brain-Computer Interfaces (BCIs);Human Machine Interactions (HCIs);Electroencephalography (EEG);Emotion Detection;Machine Learning (ML);Ensemble Machine Learning (EML);SEED-IV Dataset","Electrodes;Emotion recognition;Machine learning algorithms;Accuracy;Machine learning;Electroencephalography;Real-time systems;Object recognition;Monitoring;Standards","","","","36","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Principal Component Analysis and Support Vector Machine","M. A. Tran; L. H. Nguyen","The University of Danang – University of Science and Technology, Danang, Vietnam; The University of Danang – University of Science and Technology, Danang, Vietnam","2024 IEEE International Conference on Control & Automation, Electronics, Robotics, Internet of Things, and Artificial Intelligence (CERIA)","13 Mar 2025","2024","","","1","5","Recognition of human's emotional states is one of crucial issues in many Brain-Computer-Interface (BCI) applications. In the current study, the problem of emotion recognition based on EEG data will be addressed. The public SEED-V EEG dataset acquired during affect elicitation by audio-visual stimuli was first presented. Then, the preprocessing techniques including noise filtering, down sampling, trail separation were employed on the raw data. To reduce the size of data as well as remove less valuable data, the Principal Component Analysis (PCA) was proposed. Next, the Power Spectral Density (PSD) method was used to extract emotional features. Finally, we utilized the Support Vector Machine (SVM) for emotion classification. The obtained results show that a high classification accuracy (up to 95.6%) can be achieved. In addition, the obtained results also demonstrate that the proposed PCA technique greatly contributes to improving classification accuracy.","","979-8-3315-1116-6","10.1109/CERIA64726.2024.10914782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10914782","emotional state;electroencephalogram;principal component analysis;power spectral density;support vector machine","Support vector machines;Emotion recognition;Accuracy;Filtering;Noise;Feature extraction;Electroencephalography;Internet of Things;Robots;Principal component analysis","","","","14","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Fine-Grained Emotion Recognition from EEG Signal Using Fast Fourier Transformation and CNN","M. Hasan; Rokhshana-Nishat-Anzum; S. Yasmin; T. S. Pias","Department of Computer Science & Engineering, University of Asia Pacific, Dhaka, Bangladesh; Department of Computer Science & Engineering, University of Asia Pacific, Dhaka, Bangladesh; Department of Computer Science & Engineering, University of Asia Pacific, Dhaka, Bangladesh; Department of Computer Science, Virginia Tech, Blacksburg, USA","2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)","18 Oct 2021","2021","","","1","9","Emotions are mental states originating in the human brain, and this is closely related to the activities of the nervous system. Electroencephalogram (EEG) is a well-established approach to record neuron activities which is reliable for emotion recognition compared to the non-physiological clues. So far, there have been reports of various researches searching for active patterns involving different emotions. However, most of the previously published system could only classify 4 human emotions using the technique of binary classification but humans have more and complex emotions which couldn't be captured with only 4 classes. So, we proposed a fine-grained emotion classification technique which can classify 64 emotions including all the complex emotions. Hence, this paper presents convolutional neural network (CNN) models working on the DEAP dataset, and it contains emotional states which are arousal, valence, dominance and liking. Our binary models achieved 96.63% and 96.18% accuracy respectively for valence and arousal. Only four emotions are found with binary classification whereas 8-class classification can precisely recognize 64 emotions. The 8-class classification achieves a promising accuracy of 93.83 % and 93.79% respectively for valence and arousal. For both cases, Fast Fourier Transformation (FFT) has been used as the feature extraction method and all the four classification models are created under 1D-CNN using the same architecture.","","978-1-6654-4923-6","10.1109/ICIEVicIVPR52578.2021.9564204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564204","EEG;emotion recognition;CNN;DEAP;FFT","Emotion recognition;Neurons;Brain modeling;Feature extraction;Electroencephalography;Pattern recognition;Convolutional neural networks","","8","","27","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Deep Learning Models","A. Meghana; B. Vanshika; K. VedaSamhitha; K. Murali; R. P. Singh; S. Palaniswamy","Department of Computer Science & Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India; Department of Computer Science & Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India; Department of Computer Science & Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India; Department of Computer Science & Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India; Department of Computer Science & Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India; Department of Computer Science & Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India","2024 2nd International Conference on Recent Trends in Microelectronics, Automation, Computing and Communications Systems (ICMACC)","26 Feb 2025","2024","","","770","775","Emotion recognition is an emerging field of research and modalities like images, videos, speech, text, physiological signals, and EEG are used for the same. In this work, EEG based emotion recognition has been proposed using deep learning models. The recognition of emotion from EEG has attracted a lot of interest and has been applied in numerous fields, particularly in health, technology, and computing. In this work, two methods are proposed for emotion recognition from EEG signals, viz., a signal-based approach and an image conversion approach. the accuracy achieved by the models CNN+GRU, 2D- CNN and LSTM are 96.54%, 90.58%, and 87%, respectively. This work also addresses some of the issues related to model deployment, how to deal with logical and causal relations, and how to implement the outcomes in a real-time environment.","","979-8-3503-6657-0","10.1109/ICMACC62921.2024.10894376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10894376","CNN;LSTM;GRU;Efficient Net;ResNet;Signal-based approach;Image-based approach;Continuous Wavelet Transform (CWT);Neurotechnology","Deep learning;Emotion recognition;Computational modeling;Speech recognition;Brain modeling;Electroencephalography;Real-time systems;Physiology;Microelectronics;Videos","","","","17","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Transformer-Based Bidirectional Encoder Representations for Emotion Detection from Text","A. K. J; E. Cambria; T. E. Trueman","Information Science and Technology, Anna University, Chennai, India; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Information Science and Technology, Anna University, Chennai, India",2021 IEEE Symposium Series on Computational Intelligence (SSCI),"24 Jan 2022","2021","","","1","6","Social media influences internet users to share their sentiments, feelings, or emotions about entities. In particular, sentiment analysis classifies a text into positive, negative, or neutral. It does not capture the state of mind of an individual like happiness, anger, and fear. Therefore, emotion detection plays an important role in user-generated content for capturing the state of mind. Moreover, researchers adopted traditional machine learning and deep learning models to capture emotions from the text. Recently, transformers-based architectures achieve better results in various natural language processing tasks. Therefore, we propose a transformer-based emotion detection system, which uses context-dependent features and a one-cycle learning rate policy for a better understanding of emotions from the text. We evaluate the proposed emotion detection model using error matrix, learning curve, precision, recall, F1-score, and their micro and macro averages. Our results indicate that the system achieves a 6 % accuracy over existing models.","","978-1-7281-9048-8","10.1109/SSCI50451.2021.9660152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660152","Sentiment analysis;emotion detection;transformers;BERT","Deep learning;Emotion recognition;Sentiment analysis;Social networking (online);Computational modeling;User-generated content;Transformers","","5","","24","IEEE","24 Jan 2022","","","IEEE","IEEE Conferences"
"Adaptive Spatiotemporal Dynamic Graph Modeling for Robust EEG-Based Emotion Recognition","G. Hou; Q. Yu","College of Intelligent Science and Engineering, Harbin Engineering University, Harbin, China; College of Intelligent Science and Engineering, Harbin Engineering University, Harbin, China","2024 4th International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI)","18 Feb 2025","2024","","","99","106","Non-invasive electroencephalogram (EEG) signals contain valuable bioinformation, capturing emotion-related brain activity and enabling machines to interpret human behavior for improved human-computer interaction. EEG-based emotion recognition models face three main challenges: EEG noise and individual differences, limited spatiotemporal dynamics, and unclear classification boundaries during emotional state transition. To address these, we propose NODEs-STDGCN, a spatiotemporal dynamic graph convolutional network with parameter uncertainty and an end-to-end training paradigm based on Advantage Actor-Critic (A2C). NODEs-STDGCN combines an autoregressive model (AR) with neural ordinary differential equations (NODEs) to reduce information loss from discretization: AR captures long-term dependencies, while NODEs local spatiotemporal dynamics. Final features are fed into an MLP and SoftMax for classification. Here, each parameter follows a 2D Gaussian distribution, allowing for adaptively selecting weight configuration for each instance. The A2C-based training paradigm further contributes to balance multiple task requirements by guiding gradient updates during backpropagation. Extensive experiments on the SEED dataset demonstrates that A2C-based training paradigm, uncertainty in parameters, and effective model architecture enable NODEs-STDGCN to outperform state-of-the-art baselines. Notably, the A2C-based training offers a promising approach to improving EEG-based emotion recognition in complex conditions.","","979-8-3503-7854-2","10.1109/CEI63587.2024.10871591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10871591","bioinformation;spatiotemporal dynamic graph model;electroencephalogram;Advantage Actor-Critic;emotion recognition","Training;Emotion recognition;Adaptation models;Uncertainty;Computational modeling;Biological system modeling;Brain modeling;Electroencephalography;Spatiotemporal phenomena;Bioinformatics","","","","21","IEEE","18 Feb 2025","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition During Mobile Gameplay","S. H. Khan; A. Raheel; M. Majid; S. M. Anwar; A. Arsalan","Department of Computer Engineering, UET, Taxila, Pakistan; Department of Computer Engineering, UET, Taxila, Pakistan; Department of Computer Engineering, UET, Taxila, Pakistan; Department of Software Engineering, UET, Taxila, Pakistan; Department of Software Engineering, FJWU, Rawalpindi, Pakistan",2024 International Conference on Engineering & Computing Technologies (ICECT),"8 Jul 2024","2024","","","1","6","The gaming industry, encompassing both console and mobile platforms, has experienced remarkable growth in recent years. Assessing emotional responses during gameplay presents a significant challenge. This study employs electroen-cephalography (EEG) signals to discern players' emotions while engaged in mobile gaming. Specifically, two popular mobile games, Temple Run and Subway Surfers, were chosen to represent diverse gaming environments. Each game underwent three separate trials, during which EEG data was collected using a commercially available 4-channel MUSE headband. Following each trial, players indicated their emotional state on a 9-point valence and arousal scale. Statistical analysis, including a t-test, was conducted on the valence and arousal scores for both games, revealing a significant difference in emotional responses (p 0.05). Player data was labeled using the majority voting technique based on the three trials. Features were extracted from both the frequency domain and time domain which were used to classify valence and arousal into two classes using support vector machine (SVM) and k-nearest neighbor (KNN) classifiers. The KNN classifier achieved the highest accuracy rates, with 80.55","","979-8-3503-4971-9","10.1109/ICECT61618.2024.10581280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581280","Electroencephalography;emotion recognition;feature extraction;mobile game;classification","Support vector machines;Emotion recognition;Statistical analysis;Games;Feature extraction;Electroencephalography;Emotional responses","","","","31","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"Domain Adaptation Techniques for EEG-Based Emotion Recognition: A Comparative Study on Two Public Datasets","Z. Lan; O. Sourina; L. Wang; R. Scherer; G. R. Müller-Putz","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Institute of Neural Engineering, Graz University of Technology, Graz, Austria; Institute of Neural Engineering, Graz University of Technology, Graz, Austria",IEEE Transactions on Cognitive and Developmental Systems,"8 Mar 2019","2019","11","1","85","94","Affective brain-computer interface (aBCI) introduces personal affective factors to human-computer interaction. The state-of-the-art aBCI tailors its classifier to each individual user to achieve accurate emotion classification. A subject-independent classifier that is trained on pooled data from multiple subjects generally leads to inferior accuracy, due to the fact that electroencephalography patterns vary from subject to subject. Transfer learning or domain adaptation techniques have been leveraged to tackle this problem. Existing studies have reported successful applications of domain adaptation techniques on SEED dataset. However, little is known about the effectiveness of the domain adaptation techniques on other affective datasets or in a cross-dataset application. In this paper, we focus on a comparative study on several state-of-the-art domain adaptation techniques on two datasets: 1) DEAP and 2) SEED. We demonstrate that domain adaptation techniques can improve the classification accuracy on both datasets, but not so effective on DEAP as on SEED. Then, we explore the efficacy of domain adaptation in a cross-dataset setting when the data are collected under different environments using different devices and experimental protocols. Here, we propose to apply domain adaptation to reduce the intersubject variance as well as technical discrepancies between datasets, and then train a subject-independent classifier on one dataset and test on the other. Experiment results show that using domain adaptation technique in a transductive adaptation setting can improve the accuracy significantly by 7.25%-13.40% compared to the baseline accuracy where no domain adaptation technique is used.","2379-8939","","10.1109/TCDS.2018.2826840","National Research Foundation Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8337789","Affective brain–computer interface (aBCI);cross dataset;domain adaptation;electroencephalography (EEG);emotion recognition;transfer learning","Electroencephalography;Feature extraction;Emotion recognition;Task analysis;Motion pictures;Training;Brain-computer interfaces","","264","","36","IEEE","13 Apr 2018","","","IEEE","IEEE Journals"
"Decoding Emotions From EEG Responses Elicited by Videos Using Machine Learning Techniques on Two Datasets","E. C. S. Neverlien; R. Lu; M. Kumar; M. Molinas","Department of Engineering Cybernetics, Norwegian University of Science and Technology, Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology, Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology, Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology, Trondheim, Norway",2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"11 Dec 2023","2023","","","1","4","In recent times, we have seen extensive research in the field of EEG-based emotion identification. The majority of solutions suggested by current literature use sophisticated deep learning techniques for the identification of human emotions. These models are very complex and need huge resources to implement. Hence, in this work, a method for human emotion recognition is proposed which is based on much simpler architecture. For that, two publicly available datasets SEED and DEAP are used to perform experiments. First, the EEG signals of the two datasets are segmented into epochs of 1second duration. The epochs are also decomposed into different brain rhythms. The features computation is performed in two different ways, one is directly from the epochs and the other way is from the brain rhythms obtained after the decomposition of the epochs. Several features and their combination are examined with different classifiers. For the DEAP dataset baseline features are also utilised. It is observed that the support vector machine (SVM) has shown the best performance for the DEAP dataset when baseline feature correction and epoch decomposition are implemented together. The best achieved average accuracy is 96.50% and 96.71% for high versus low valence classes and high versus low arousal classes, respectively. For the SEED dataset, the best average accuracy of 86.89% is achieved using the multilayer perceptron (MLP) with 2 hidden layers.Clinical relevance— This work can be further explored to develop an automated mental health monitor which can assist doctors in their primary screening.","2694-0604","979-8-3503-2447-1","10.1109/EMBC40787.2023.10341106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341106","","Support vector machines;Deep learning;Emotion recognition;Biological system modeling;Multilayer perceptrons;Brain modeling;Feature extraction","Humans;Electroencephalography;Emotions;Brain;Machine Learning;Neural Networks, Computer","","","17","CCBY","11 Dec 2023","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition via Transformer Neural Architecture Search","C. Li; Z. Zhang; X. Zhang; G. Huang; Y. Liu; X. Chen","Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Chongqing Key Laboratory of Human Embryo Engineering, Chongqing, China; Reproductive and Genetic Institute, Chongqing Health Center for Women and Children, Chongqing, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Neurosurgery, The First Affiliated Hospital of USTC, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China",IEEE Transactions on Industrial Informatics,"24 Mar 2023","2023","19","4","6016","6025","Emotion recognition based on electroencephalogram (EEG) plays an increasingly important role in the field of brain–computer interfaces. Recently, deep learning has been widely applied to EEG decoding owning to its excellent capabilities in automatic feature extraction. Transformer holds great superiority in processing time-series signals due to its long-term dependencies extraction ability. However, most existing transformer architectures are designed manually by human experts, which is a time-consuming and resource-intensive process. In this article, we propose an automatic transformer neural architectures search (TNAS) framework based on multiobjective evolution algorithm (MOEA) for the EEG-based emotion recognition. The proposed TNAS conducts the MOEA strategy that considers both accuracy and model size to discover the optimal model from well-trained supernet for the emotion recognition. We conducted extensive experiments to evaluate the performance of the proposed TNAS on the DEAP and DREAMER datasets. The experimental results showed that the proposed TNAS outperforms the state-of-the-art methods.","1941-0050","","10.1109/TII.2022.3170422","National Key Research and Development Program of China(grant numbers:2019YFA0706203); National Natural Science Foundation of China(grant numbers:61922075,41901350,62176081,32150017); Chongqing Municipal Health Commission Medical Research Project(grant numbers:2022WSJK094); USTC Research Funds of the Double First-Class Initiative(grant numbers:KY2100000123); Provincial Natural Science Foundation of Anhui(grant numbers:2008085QF285); Fundamental Research Funds for the Central Universities(grant numbers:JZ2021HGTB0078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763316","Deep learning (DL);electroencephalogram (EEG);emotion recognition;transformer neural architecture search (TNAS)","Emotion recognition;Electroencephalography;Transformers;Feature extraction;Brain modeling;Task analysis;Computer architecture","","48","","47","IEEE","26 Apr 2022","","","IEEE","IEEE Journals"
"FBSTCNet: A Spatio-Temporal Convolutional Network Integrating Power and Connectivity Features for EEG-Based Emotion Decoding","W. Huang; W. Wang; Y. Li; W. Wu","Research Center for Brain-Computer Interface, Pazhou Lab, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Research Center for Brain-Computer Interface, Pazhou Lab, Guangzhou, China; Shanghai Key Laboratory of Emotions and Affective Disorders, School of Medicine, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Affective Computing,"26 Nov 2024","2024","15","4","1906","1918","Electroencephalography (EEG)-based emotion recognition plays a key role in the development of affective brain-computer interfaces (BCIs). However, emotions are complex and extracting salient EEG features underlying distinct emotional states is inherently limited by low signal-to-noise ratio (SNR) and low spatial resolution of practical EEG data, which is further compounded by the lack of effective spatio-temporal filter optimization approaches for generic EEG features. To address these challenges, this study proposes a set of neural networks termed the Filter-Bank Spatio-Temporal Convolutional Networks (FBSTCNets) for performing end-to-end multi-class emotion recognition via robust extraction of power and/or connectivity features from EEG. First, a filter bank is employed to construct a multiview spectral representation of EEG data. Next, a temporal convolutional layer, followed by a depth-wise spatial convolutional layer, performs spatio-temporal filtering, transforming EEG into latent signals with higher SNR. A feature extraction layer then extracts power and/or connectivity features from the latent signals. Finally, a fully connected layer with a cropped decoding strategy predicts the emotional state. Experimental results on two public emotion EEG datasets, SEED and SEED-IV, demonstrate that FBSTCNets outperform previous benchmark methods in decoding accuracy. Our approach provides a principled emotion decoding framework for designing high-performance spatio-temporal filtering networks tailored to specific EEG feature types.","1949-3045","","10.1109/TAFFC.2024.3385651","STI 2030–Major Projects(grant numbers:2022ZD0211700,2022ZD0208900); Key R&D Program of Guangdong Province(grant numbers:2018B030339001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493141","Brain-computer interface (BCI);electroencephalography (EEG);emotion recognition;neural network;connectivity","Electroencephalography;Feature extraction;Decoding;Convolution;Emotion recognition;Biological neural networks;Signal to noise ratio","","1","","60","IEEE","5 Apr 2024","","","IEEE","IEEE Journals"
"Deep Learning Model for Analyzing EEG Signal Analysis","V. Gupta; V. Kumar; Prince; S. Singh; Y. -S. Lee; I. -H. Ra","National Institute of Technology, Department of Electronics and Communication, Sikkim, INDIA; Department of Electrical and Electronics Engineering, National Institute of Technology, Sikkim, INDIA; Department of Industrial & Systems Engineering, Dongguk University, Seoul, Republic of Korea; Department of AI and Bigdata, Woosong University, Daejeon, Republic of Korea; School of Computer & Software, Kunsan National University, Gunsan, Republic of Korea; School of Software, Kunsan National University, Gunsan, Republic of Korea",IEEE Access,"","2025","PP","99","1","1","To analyze the physiological information within the acquired EEG signal is very cumbersome due to the possibility of several factors, viz. noise and artifacts, complexity of brain dynamics, and inter-subject variability. To address these issues, this paper compares a U-shaped encoder-decoder network (UNET) and Bat-based UNET signal analysis (BUSA) techniques to classify depression rates in the Electroencephalogram (EEG) datasets. The main objective of including these two techniques is to reveal their effectiveness. It comprises pre-processing, feature extraction, feature selection, and classification stages. The framework excels at noise reduction during pre-processing, enhancing dataset integrity. Feature extraction leverages band power and correlation dimension to extract crucial features. Furthermore, feature selection optimizes classification accuracy by refining the fitness function of bats in the classification layer. The performance of UNET and BUSA are compared based on the following performance evaluating parameters viz. accuracy (Acc), Area Under the Curve (AUC), precision (P), and recall (R) (or sensitivity (Se)). The results indicated that the BUSA technique outperforms the UNET technique.","2169-3536","","10.1109/ACCESS.2025.3563760","Regional Innovation Strategy (RIS)(grant numbers:2023RIS-008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10974958","Bat-based UNET signal analysis (BUSA);Band Power;correlation dimension;EEG datasets;U-shaped encoder-decoder network (UNET)","Electroencephalography;Feature extraction;Brain modeling;Brain;Accuracy;Emotion recognition;Signal analysis;Electrodes;Classification algorithms;Convolutional neural networks","","","","","CCBY","23 Apr 2025","","","IEEE","IEEE Early Access Articles"
"A Novel Dual-Task Model for EEG-Based Emotion and Cognition Recognition","Z. Jia; Y. Ouyang; X. Kong; Y. Guo; Z. Li; H. Zeng","School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China",IEEE Transactions on Instrumentation and Measurement,"9 Jan 2025","2025","74","","1","14","Human cognition and emotion states are interrelated. Constructing electroencephalography (EEG)-based deep learning models to simultaneously recognize the cognitive and emotional states of an individual contributes to cognitive and emotional interaction studies. However, most of the existing models focus on cognition or emotion recognition, neglecting to exploit their shared features in EEG and weakening the generalization performance of the models. In this study, we propose a dual-task learning model DT-EEGNet, where emotion and cognition are considered as related tasks for joint analysis. In DT-EEGNet, EEGNet is employed as a base network responsible for extracting shared features between cognition and emotion recognition tasks. Then, a multiscale ECAWeight attention (MSEA) module is introduced for obtaining key information from these shared features. Finally, a dual-task loss function named dynamic weight average (DWA) is used to balance the training rates of emotion and cognition recognition tasks for better overall training performance. Experimental results on our self-constructed emotion and cognition EEG dataset (ECED) and the public DEAP dataset show that our proposed DT-EEGNet has better dual-task recognition performance. The proposed method can provide a new idea for cognitive impairment assessment research as well.","1557-9662","","10.1109/TIM.2024.3522413","National Natural Science Foundation of China(grant numbers:62076083); National Key Research and Development Program of China(grant numbers:2022YFE0199300); Natural Science Foundation of Zhejiang Province(grant numbers:ZCLZ24F0301); “Leading Goose” Research and Development Program of Zhejiang(grant numbers:2023C03026); Hangzhou Artificial Intelligence Major Technological Innovation Project(grant numbers:2022AIZD0159); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10816104","Deep learning;DT-EEGNet;dual-task learning;EEG cognition recognition;electroencephalography (EEG) emotion recognition","Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Cognition;Data models;Convolution;Training;Context modeling;Accuracy","","1","","60","IEEE","25 Dec 2024","","","IEEE","IEEE Journals"
"Band-Level Adaptive Fusion Network for Cross-Subject EEG Emotion Recognition","Y. Wang; L. Zhang; Y. Zhang","School of Computer Science and Technology, Soochow University, Suzhou, Jiangsu, China; School of Computer Science and Technology, Soochow University, Suzhou, Jiangsu, China; School of Computer Science and Technology, Soochow University, Suzhou, Jiangsu, China",IEEE Transactions on Instrumentation and Measurement,"19 Mar 2025","2025","74","","1","12","Electroencephalogram (EEG)-based emotion recognition has been studied extensively. However, existing approaches struggle with cross-subject EEG emotion recognition due to the individual differences (individuality) of subjects and insufficient relationship information between frequency bands in EEG signals. This article proposes a novel band-level adaptive fusion network (BAFNet) for cross-subject EEG emotion recognition. BAFNet consists of five feature-extraction modules, one feature-fusion module, and one classification module. All of these modules except classification contain a modified Transformer, called a long short-term memory Transformer (LSTMTrans), and a modified adaptive graph convolutional network (MAGCN). First, BAFNet utilizes five feature-extraction modules to handle five frequency bands separately and constructs adaptive graphs to represent the time-varying electrical channel relationship, which can reflect the individuality of subjects. Next, BAFNet uses the feature-fusion module to combine the outputs of five feature-extraction modules. Thus, an adaptive graph is constructed to represent the relationship between frequency bands. Moreover, all six adaptive graphs contain general information that can be shared and individual information that represents the differences between subjects. To verify the superiority of BAFNet, experiments are conducted on two public emotion recognition datasets, SEED and SEED-IV. In these experiments, we compare BAFNet with several state-of-the-art (SOTA) cross-subject emotion recognition methods. The average accuracy of BAFNet is 1.34% and 5.09% higher than that of the best baseline on the SEED and SEED-IV datasets, respectively. The experimental results indicate that BAFNet has promising performance for cross-subject EEG emotion recognition.","1557-9662","","10.1109/TIM.2025.3544334","Natural Science Foundation of Jiangsu Higher Education Institutions of China(grant numbers:23KJB520033); Natural Science Foundation of Jiangsu Province in China(grant numbers:BK20230483); Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10904095","Adaptive graph;cross-subject;electroencephalogram (EEG);emotion recognition;graph convolutional network (GCN)","Feature extraction;Electroencephalography;Emotion recognition;Brain modeling;Transformers;Data mining;Adaptation models;Adaptive systems;Training;Accuracy","","","","33","IEEE","25 Feb 2025","","","IEEE","IEEE Journals"
"Application of Convolutional Neural Network in Multimodal Emotion Recognition","G. Qin; Y. Zhu; Z. Wu; Q. Jiang; J. Yin; J. Sun; X. Chen; Y. Wang","Shandong University of Science and Technology, Jinan, China; Independent Researcher, California, United States; Google Inc, California, United States; Chengxian College, Southeast University, Nanjing, China; Zhejiang University, Hangzhou, China; Shandong University of Science and Technology, Jinan, China; Jimei University, Xiamen, China; Beijing University of Posts and Telecommunication, Beijing, China",2024 9th International Symposium on Computer and Information Processing Technology (ISCIPT),"16 Sep 2024","2024","","","440","444","Human emotions are complex and multimodal. Multimodal recognition of human emotions has a profound impact on interpersonal communication, mental health, and the development of intelligent technology. At present, the integration of artificial intelligence and deep learning for multimodal human emotion recognition plays an important role, especially the 3D-CNN learning model. Based on the current problem of lack of data, this paper proposes a deep learning model based on 3D-CNN to enhance robustness and accuracy through transfer learning and data augmentation stage fusion. The multimodal recognition of human emotions in the database was mainly carried out through three methods: EEG signal recognition, face recognition and fusion recognition. It turned out to be. The recognition accuracy of the wake-up class proposed in this paper is as high as 96.79%, and it has excellent recognition effect.","","979-8-3503-8840-4","10.1109/ISCIPT61983.2024.10672685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10672685","Emotion recognition;multimodal 3D-CNN EEG signals;face recognition;fusion recognition","Deep learning;Emotion recognition;Accuracy;Face recognition;Transfer learning;Mental health;Learning (artificial intelligence)","","1","","10","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Detection Using Roberts Similarity and PSO Feature Selection","M. Hussein Mohammed; M. Noaman Kadhim; D. Al-Shammary; A. Ibaida","Computer Techniques Engineering Department, Al-Mustaqbal University, Babylon, Iraq; College of Computer Science and Information Technology, University of Al-Qadisiyah, Al Dewaniyah, Iraq; College of Computer Science and Information Technology, University of Al-Qadisiyah, Al Dewaniyah, Iraq; Intelligent Technology Innovation Laboratory, Victoria University, Melbourne, VIC, Australia",IEEE Access,"8 May 2025","2025","13","","79353","79366","In this paper, a novel classifier based on Robert’s similarity measure is introduced for emotion detection using electroencephalogram (EEG) signals. Traditional machine learning classifiers machine learning such as k-Nearest Neighbors (KNN), Support Vector Machine (SVM), Decision Tree (DT), Logistic Regression (LR), and Random Forest (RF), often struggle to accurately capture both linear and nonlinear patterns in EEG signals and face limitations in handling high-dimensional datasets. The proposed classifier addresses these challenges by segmenting EEG signals into block sizes categorized as small (1 to 10 samples), medium (20 to 100 samples), and large (200 to 1,000 samples), demonstrating particularly strong performance with medium and large block sizes to capture essential features. Integration of Particle Swarm Optimization (PSO) for feature selection, with Robert’s similarity as the fitness function, effectively refines the feature set, boosting classification accuracy and computational efficiency. Evaluation on an EEG brainwave dataset demonstrated that the method achieved an accuracy of 98.75% with feature selection, compared to 94.04% without it in emotional state detection. The results demonstrate that the proposed classifier is a valuable tool for diverse fields, including healthcare by detecting patient stress, education by assessing student engagement, customer service by monitoring satisfaction, and smart environments by enabling adaptive responses. Furthermore, the classifier has potential for broader industrial applications, such as improving workplace productivity by monitoring employee stress and enhancing safety in autonomous vehicle systems, making it a versatile solution for emotionally-aware systems across multiple domains.","2169-3536","","10.1109/ACCESS.2025.3555526","Al-Mustaqbal University, Babylon, Iraq, and Victoria University, Australia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10943121","Roberts similarity;electroencephalography (EEG) signal;emotion recognition;particle swarm optimization (PSO);machine learning classifiers","Electroencephalography;Feature extraction;Emotion recognition;Accuracy;Brain modeling;Machine learning;Support vector machines;Nearest neighbor methods;Long short term memory;Adaptation models","","","","35","CCBY","28 Mar 2025","","","IEEE","IEEE Journals"
"Using the center loss function to improve deep learning performance for EEG signal classification","W. Zhang; Q. Liu","School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of Mathematics, Southeast University, Nanjing, China",2018 Tenth International Conference on Advanced Computational Intelligence (ICACI),"11 Jun 2018","2018","","","578","582","Electroencephalography (EEG) signal classification is an increasingly interesting task in Brain-computer interface (BCI) systems, but how to learn the pattern from EEG signals and design a general model for EEG classification is still a challenge. In this paper, we combine convolutional neural network and Long Short-Term Memory (CNN+LSTM) with center loss function to classify EEG signals. The 1 × 3 kernels as convolutional filters in 2D CNNs will be used, which is different from other models. Deep metric learning is usually used for image classification such as face recognition. In this paper, we use center loss function to learn more discriminative features to improve classification accuracy. Meanwhile, we experiment our methods on multiple EEG datasets. We have achieved a classification accuracy of about 94% on cognitive load recognition and 86.52% on emotion recognition. On EEG dataset for visual object analysis, our method produces a comparable result compared with who publicly release this dataset. The good performance indicates that our model with the center loss function is widely suitable on EEG signal classification.","","978-1-5386-4362-4","10.1109/ICACI.2018.8377524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377524","Electroencephalogram (EEG);Convolutional Neural Network (CNN);Long Short-Term Memory (LSTM);center loss function","Electroencephalography;Brain modeling;Convolution;Kernel;Logic gates;Neural networks;Two dimensional displays","","5","","25","IEEE","11 Jun 2018","","","IEEE","IEEE Conferences"
"Integrating Deep Metric Learning, Semi Supervised Learning, and Domain Adaptation for Cross-Dataset EEG-Based Emotion Recognition","H. Razzaq Abed Alameer; P. Salehpour; H. S. Aghdasi; M. -R. Feizi-Derakhshi","Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran",IEEE Access,"6 Mar 2025","2025","13","","38914","38924","The variability of EEG signals produced by different trial conditions and different devices presents significant challenges in developing practical EEG-based emotion recognition systems. Much of the research on developing a generalizable EEG emotion recognition approach focuses on cross-subject and cross-session contexts. Although current cross-subject methods yield satisfactory outcomes on certain EEG emotion datasets, their effectiveness diminishes in cross-dataset scenarios. Additionally, the performance of existing cross-dataset methods remains inferior compared to methods that are trained and evaluated within the same dataset. To address these challenges, inspired by the effective application of deep metric learning (DML) in zero-shot and few-shot learning tasks, this paper introduces a cross-dataset emotion recognition method. The proposed approach integrates DML, domain-specific batch normalization (DSBN), shared batch normalization statistics, and adversarial learning. Specifically, our method extracts cross-domain features from the input signals using DSBN and shared batch normalization statistics. Then, the proposed DML loss minimizes intra-class variations of EEG features across different subjects and domains, while maximizing differences between different classes. Moreover, it captures the semantic order of emotions in the learned embedding. To further improve the generalization of the feature encoder, we employ adversarial learning with domain and subject discriminators. We evaluate our method on six cross-dataset scenarios. The results show that it consistently outperforms peer methods across the scenarios. For example, our method achieves an accuracy of 63.49% on SEED  $\to $  SEED-IV, improving the state-of-the-art result by 2.25%.","2169-3536","","10.1109/ACCESS.2025.3536549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10858132","EEG-based emotion recognition;deep metric learning;cross-dataset learning;domain generalization;adversarial learning","Electroencephalography;Feature extraction;Brain modeling;Emotion recognition;Accuracy;Batch normalization;Adversarial machine learning;Adaptation models;Semantics;Training","","","","31","CCBY","29 Jan 2025","","","IEEE","IEEE Journals"
"Emotion Recognition using EEG Signal Classification of seed Dataset","S. K; S. D; G. N M; L. S R; S. K A K","Department of Information Science and Engineering, Kumaraguru College of Technology, Coimbatore, India; Department of Artificial Intelligence and Data Science, Kumaraguru College of Technology, Coimbatore, India; Information Science and Engineering Department, Kumaraguru College of Technology, Coimbatore, India; Information Science and Engineering Department, Kumaraguru College of Technology, Coimbatore, India; Information Science and Engineering Department, Kumaraguru College of Technology, Coimbatore, India","2023 2nd International Conference on Advancements in Electrical, Electronics, Communication, Computing and Automation (ICAECA)","7 Aug 2023","2023","","","1","6","Emotions were considered an important component. Everyday lives need emotions on a regular basis. Electroencephalogram (EEG)-based emotion identification was gaining popularity quickly. Signals from electroencephalograms (EEGs) are one of the key resources. The biggest benefit of using EEG signals is that they accurately represent real feelings and it could be prepared by computer systems. EEG is a physiological marker that may be captured from the brain activity within the context of scalp-transmitted brain waves which was used to collect brain signals. With this test, waves representing the brain's activity are recorded. The SEED-IV dataset was utilized to categorize emotions as happy, sad, fear, and neutral. Sampling techniques and classification techniques were used to raise the level of performance of the algorithm. One of the most important advantages of using EEG signals is that is accurately prepared by computer systems and portrayed the real experience. The performance of the emotion recognition systems by brain signals depended on the efficiency of the algorithms used. The instability of the brain's impulses is one of the reasons why this process is seen as challenging. So, pre-process was done to remove noisy data. The proposed method focuses on implementing an algorithm that accurately classified emotions into the said four categories.","","979-8-3503-0681-1","10.1109/ICAECA56562.2023.10199350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10199350","EEG;Seed;Stratified sampling;ANN;Affinity Propagation;Classification;Emotions;Svm;Clustering","Emotion recognition;Automation;Pattern classification;Feature extraction;Brain modeling;Electroencephalography;Physiology","","2","","20","IEEE","7 Aug 2023","","","IEEE","IEEE Conferences"
"Emotion Recognition Based on Time-Frequency-Spatial Network of EEG Signals","Y. Jing; Z. Liu; D. Gao; M. Wang","School of Electronic Engineering, Chengdu University of Information Technology, Chengdu, China; School of Electronic Engineering, Chengdu University of Information Technology, Chengdu, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, China",2023 6th International Conference on Artificial Intelligence and Big Data (ICAIBD),"10 Aug 2023","2023","","","783","787","The study of brain activity and deciphering information in electroencephalographic (EEG) signals has become an emerging area of research, and substantial progress has been made in EEG-based emotion classification. However, distinguishing between different emotions using different EEG features and complementarity remains a challenge. Most existing models extract a single feature from EEG signals or directly input raw data into the model, ignoring the complementarity between features, which limits the classification ability of the model. To address this problem, we propose a Time-Frequency-Spatial Network (TFSN) for emotion recognition classification. Our proposed network consists of a deep over-parameterized convolutional layer (Do-Conv) and a long short-term memory network (LSTM). The experimental results demonstrate that our model achieves high performance.","2769-3554","978-1-6654-9125-9","10.1109/ICAIBD57115.2023.10206288","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10206288","Emotion recognition;EEG SIGNAL;Do-conv;LSTM","Emotion recognition;Time-frequency analysis;Brain modeling;Feature extraction;Electroencephalography;Data models;Entropy","","1","","14","IEEE","10 Aug 2023","","","IEEE","IEEE Conferences"
"Towards Context-aware EEG-based Emotion Recognition Models: Personality and Emotional Intelligence as Context","K. K; N. R. Verma; J. Shukla","Department of CSE, IIITDM Kancheepuram, Tamil Nadu, India; Department of CSE, IIIT Delhi, New Delhi, India; Department of CSE, IIIT Delhi, New Delhi, India","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Emotion recognition is a critical component of affective computing technologies, enabling machines to understand and respond to human emotions more effectively. While traditional models rely on physiological signals, the inclusion of contextual factors, such as personality traits (PT) and emotional intelligence (EI), enhances the precision of these systems. In this paper, we propose a context-aware emotion recognition model using EEG signals, where PT and EI are integrated as additional inputs. Features were extracted using autoencoders, and models were tested with 64, 128, 256, and 512 feature sizes. XGBoost classifiers were employed for classification, and experiments were conducted in two phases: baseline models (using only EEG as input), and context-aware models (incorporating personality and EI scores along with EEG as input). Results show that context-aware models significantly outperformed baseline models, with the highest accuracy of 88.24% and F1-score of 0.8319 achieved when both contexts were included. Statistical tests confirmed a significant improvement in model performance with context, validating our hypothesis that context enhances emotion recognition accuracy. These findings highlight the importance of context awareness in advancing emotion recognition models for more accurate and reliable affective computing systems, with potential applications in mental health monitoring, personalized learning, and human-computer interaction.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890528","Affective computing;EEG signal processing;Context Awareness;Emotion Recognition;Personality Traits;Emotional Intelligence","Emotion recognition;Affective computing;Accuracy;Computational modeling;Speech recognition;Context awareness;Brain modeling;Feature extraction;Electroencephalography;Context modeling","","","","22","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"A Survey of EEG-Based Stress Detection Using Machine Learning and Deep Learning Techniques","S. R; S. J. J; M. H. M; A. K. R","Dept of Information Science, BMS College of Engineering, Bangalore, India; Dept of Information Science, BMS College of Engineering, Bangalore, India; Dept of Information Science, BMS College of Engineering, Bangalore, India; Dept of Information Science, BMS College of Engineering, Bangalore, India",2024 1st International Conference on Communications and Computer Science (InCCCS),"23 Jul 2024","2024","","","1","6","This survey paper delves into the emerging realm of stress detection through Electroencephalogram (EEG) signals to employ Deep Learning algorithms. Identifying stress is pivotal for promoting mental well-being, and EEG signals provide a non-invasive method to explore neural activity. The paper assesses various approaches, including preprocessing methods, feature extraction, and various deep-learning models applied to EEG data. It sheds light on the obstacles and opportunities linked with stress detection using EEG signals, underscoring the importance of robust models in practical contexts. This thorough survey aims to assist researchers, practitioners, and technology enthusiasts in grasping the current fostering progress and shaping the trajectory of EEG-based stress detection with Deep Learning.","","979-8-3503-5885-8","10.1109/InCCCS60947.2024.10593592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10593592","Electroencephalogram;non-invasive;Machine Learning;Deep Learning","Deep learning;Surveys;Support vector machines;Machine learning algorithms;Recurrent neural networks;Human factors;Brain modeling","","","","24","IEEE","23 Jul 2024","","","IEEE","IEEE Conferences"
"Determining Positive-Negative Emotions in Male and Female Based on EEG Signals using Machine Learning Algorithms","Y. Pamungkas; E. N. Njoto","Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Medicine, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2024 11th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)","12 Dec 2024","2024","","","27","32","Emotionsare vital in everyday human life as a controller of behavior, decision-making and as a means to determine product marketing strategy/ market research. All of these things are very dependent on human emotional conditions. In addition, in the development of the computational affective field, brain signal-based emotion recognition (EEG) has become a trending topic of current research. Therefore, we attempted to compare positive-negative emotions in men and women based on EEG using a Machine Learning algorithm in this study. A total of 20 male and 20 female participants recorded their EEG signals in the frontopolar and frontal areas of the brain. Then the EEG data is processed by filtering, removing artifact, and decomposing it into three sub-bands (alpha, beta, and gamma). The extracted signal features are Mean Absolute Deviation and Power Spectral Density. Based on the signal feature analysis results, it is known that the signal feature values (MAD and PSD) for women tend to be higher than for men. Meanwhile, several algorithms are used to classify positive and negative emotions, such as Naive Bayes, K-Nearest Neighbor, Support Vector Machine, and Random Forest. Based on the results of classification, the best accuracy rate was 95.8% (on positive emotions for male & female gender), 92.2% (on negative emotions for male & female gender), and 79.8% (on positive-negative emotions for male & female gender) using Random Forest algorithm.","","979-8-3503-5531-4","10.1109/EECSI63442.2024.10776411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10776411","Emotion Recognition;EEG;Machine Learning;Gender;Mean Absolute Deviation;Power Spectral Density","Support vector machines;Emotion recognition;Machine learning algorithms;Accuracy;Nearest neighbor methods;Feature extraction;Market research;Electroencephalography;Classification algorithms;Random forests","","","","25","IEEE","12 Dec 2024","","","IEEE","IEEE Conferences"
"Emotion Classification Through EEG Signals: The EmoSense Model","Y. K. Chuah; C. K. Toa; S. K. Goh; C. -K. Chan; H. Takada","School of Electrical Engineering and Artificial Intelligence, Xiamen University Malaysia, Selangor, Malaysia; School of Computing and Data Science, Xiamen University Malaysia, Selangor, Malaysia; School of Electrical Engineering and Artificial Intelligence, Xiamen University Malaysia, Selangor, Malaysia; Department of Biomedical Engineering, Faculty of Engineering, Universiti Malaya, Malaysia; Department of Human & Artificial Intelligent Systems, Graduate School of Engineering, University of Fukui, Fukui, Japan","2024 International Conference on Computing Innovation, Intelligence, Technologies and Education (CIITE)","9 May 2025","2024","","","1","6","In the Emotion research, there has been extensive work on emotion classification using electroencephalogram (EEG) signals. Despite most studies having achieved high accuracy, there is a potential for improvement in the model, particularly in training time reduction to optimize the modeling efficiency. Hence, our study aims to develop an efficient EEG-based emotion classification model that achieves high accuracy with a reduced number of training epochs. The proposed model, named EmoSense, employs a hybrid deep learning neural network architecture, which comprises a combination of Convolutional Neural Network (CNN), Deep Autoencoder (DAE), Bidirectional Long Short-Term Memory (BiLSTM), and an Attention mechanism. EEG features are first extracted using Differential Entropy (DE) on the EEG channels, which are then structured to form a four-dimensional (4D) representation of features with temporal, spatial, and frequency information. Subsequently, the extracted features will be used as input to the proposed model for training. Experimental evaluation of the publicly available DEAP dataset demonstrated that the proposed model achieved an accuracy of 94.11% with only 35 epochs. This performance is comparable with the result of the Cascaded Convolutional Recurrent Neural Networks model developed by Meng et al., which reported an accuracy of 94.64% with 100 epochs. The primary reason for the comparison with Meng et al. is that our overall process, including feature extraction and formation, is similar to their study, with the only difference being the deep learning architecture. Our finding indicates that the EmoSense model is not only able to achieve comparable accuracy but also reduces the number of training epochs, demonstrating the potential to learn the EEG features in a shorter time and lowering the computational cost. This study will benefit EEG-based emotion recognition to be more accessible and practical for real-world applications.","","979-8-3503-7728-6","10.1109/CIITE62244.2024.10987655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10987655","Attention Mechanism;Bidirectional Long Short-Term Memory;Convolutional Neural Network;Deep Autoencoder;Electroencephalogram;Emotion Classification","Training;Emotion recognition;Accuracy;Attention mechanisms;Computational modeling;Brain modeling;Feature extraction;Electroencephalography;Convolutional neural networks;Tuning","","","","16","IEEE","9 May 2025","","","IEEE","IEEE Conferences"
"EEG-based Brain Wave Recognition using GRU and LSTM","R. Shashidhar; S. S. Tippannavar; K. B. Bhat; N. Sharma; M. Rashid; A. Rana","Department of Electronics and Communication Engineering, JSS Science and Technology University, Mysuru, India; Department of Electronics and Communication Engineering, JSS Science and Technology University, Mysuru, India; Department of Electronics and Communication Engineering, JSS Science and Technology University, Mysuru, India; Department of Computer Science & Engineering, Chitkara University Institute of Engineering and Technology, Chitkara University, Rajpura, Punjab, India; Department of Computer Engineering, Faculty of Science & Technology, Vishwakarma University, Pune, India; Uttaranchal Institute of Technology, Uttaranchal University, Dehradun, Uttarakhand, India",2022 5th International Conference on Contemporary Computing and Informatics (IC3I),"22 Mar 2023","2022","","","1083","1087","The human brain is made up of millions of neurons, each of which plays a crucial part in directing the behaviour of the human body in response to internal/external motor-impulses. These neurons will act as data conduits between the brain and the body. Studying brain signals or images may help us better understand the cognitive behaviour of the brain. Motor and sensory states including eye movement, lip movement, memory, concentration, hand clutching, and others may be used to depict human behaviour. These states are associated with certain signal frequencies, which aids in understanding the functional behaviour of complex brain structures. In this study, the characterization of EEG signals in connection to various bodily states is the main topic of study. It also talks about the experimental design used for the EEG analysis. On a dataset of physiological signals, in this case the AMIGOS dataset, including electrocardiogram and galvanic skin reaction, this study uses a deep convolutional neural network. In order to characterize a person’s emotional state, these physiological signals are combined with the dataset’s arousal and valence data. In addition, a tool for detecting emotions based on traditional machine learning techniques is provided in order to extract the properties of physiological data in the time, frequency, and nonlinearity domains. For autonomous feature extraction of physiological inputs in this application, a convolutional neural network is used. Fully connected network layers are then used to predict emotions. Compared to the original authors of this dataset, the experimental results on the AMIGOS dataset show that the approach suggested in this study provides more accurate emotional state categorization.","","979-8-3503-9826-7","10.1109/IC3I56241.2022.10072419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10072419","EEG;Deep Learning;Machine Learning;AMIGOS dataset;Neural network","Time-frequency analysis;Lips;Neurons;Machine learning;Feature extraction;Physiology;Electroencephalography","","8","","18","IEEE","22 Mar 2023","","","IEEE","IEEE Conferences"
"Emotion Recognition From Few-Channel EEG Signals by Integrating Deep Feature Aggregation and Transfer Learning","F. Liu; P. Yang; Y. Shu; N. Liu; J. Sheng; J. Luo; X. Wang; Y. -J. Liu","MOE-Key Laboratory of Pervasive Computing, and Department of Computer Science and Technology, Tsinghua University, Beijing, China; MOE-Key Laboratory of Pervasive Computing, and the Department of Computer Science and Technology, Tsinghua University, Beijing, China; MOE-Key Laboratory of Pervasive Computing, and the Department of Computer Science and Technology, Tsinghua University, Beijing, China; MOE-Key Laboratory of Pervasive Computing, and the Department of Computer Science and Technology, Tsinghua University, Beijing, China; MOE-Key Laboratory of Pervasive Computing, and the Department of Computer Science and Technology, Tsinghua University, Beijing, China; BrainUp Research Lab, Beijing, China; BrainUp Research Lab, Beijing, China; MOE-Key Laboratory of Pervasive Computing, and the Department of Computer Science and Technology, Tsinghua University, Beijing, China",IEEE Transactions on Affective Computing,"5 Sep 2024","2024","15","3","1315","1330","Electroencephalogram (EEG) signals have been widely studied in human emotion recognition. The majority of existing EEG emotion recognition algorithms utilize dozens or hundreds of electrodes covering the whole scalp region (denoted as full-channel EEG devices in this paper). Nowadays, more and more portable and miniature EEG devices with only a few electrodes (denoted as few-channel EEG devices in this paper) are emerging. However, emotion recognition from few-channel EEG data is challenging because the device can only capture EEG signals from a portion of the brain area. Moreover, existing full-channel algorithms cannot be directly adapted to few-channel EEG signals due to the significant inter-variation between full-channel and few-channel EEG devices. To address these challenges, we propose a novel few-channel EEG emotion recognition framework from the perspective of knowledge transfer. We leverage full-channel EEG signals to provide supplementary information, available online, for few-channel signals via a transfer learning-based model CD-EmotionNet, which consists of a base emotion model for efficient emotional feature extraction and a cross-device transfer learning strategy. This strategy helps to enhance emotion recognition performance on few-channel EEG data by utilizing knowledge learned from full-channel EEG data. To evaluate our cross-device EEG emotion transfer learning framework, we construct an emotion dataset containing paired 18-channel and 5-channel EEG signals from 25 subjects, as well as 5-channel EEG signals from 13 other subjects. Extensive experiments show that our framework outperforms state-of-the-art EEG emotion recognition methods by a large margin.","1949-3045","","10.1109/TAFFC.2023.3336531","National Natural Science Foundation of China(grant numbers:62332019,U2336214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10328701","Emotion recognition;few-channel EEG;transfer learning;cross-device","Electroencephalography;Emotion recognition;Electrodes;Brain modeling;Feature extraction;Transfer learning;Task analysis","","5","","72","IEEE","24 Nov 2023","","","IEEE","IEEE Journals"
"A Hybrid End-to-End Spatiotemporal Attention Neural Network With Graph-Smooth Signals for EEG Emotion Recognition","S. Sartipi; M. Torkamani-Azar; M. Cetin","Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA; A. I. Virtanen Institute for Molecular Sciences, University of Eastern Finland, Kuopio, Finland; Department of Electrical and Computer Engineering and the Goergen Institute for Data Science, University of Rochester, Rochester, NY, USA",IEEE Transactions on Cognitive and Developmental Systems,"4 Apr 2024","2024","16","2","732","743","Recently, physiological data such as electroencephalography (EEG) signals have attracted significant attention in affective computing. In this context, the main goal is to design an automated model that can assess emotional states. Lately, deep neural networks have shown promising performance in emotion recognition tasks. However, designing a deep architecture that can extract practical information from raw data is still a challenge. Here, we introduce a deep neural network that acquires interpretable physiological representations by a hybrid structure of spatiotemporal encoding and recurrent attention network blocks. Furthermore, a preprocessing step is applied to the raw data using graph signal processing tools to perform graph smoothing in the spatial domain. We demonstrate that our proposed architecture exceeds state-of-the-art results for emotion classification on the publicly available DEAP data set. To explore the generality of the learned model, we also evaluate the performance of our architecture toward transfer learning (TL) by transferring the model parameters from a specific source to other target domains. Using DEAP as the source data set, we demonstrate the effectiveness of our model in performing cross-modality TL and improving emotion classification accuracy on DREAMER and the emotional English word (EEWD) data sets, which involve EEG-based emotion classification tasks with different stimuli.","2379-8939","","10.1109/TCDS.2023.3293321","National Science Foundation (NSF)(grant numbers:CCF-1934962,DGE-1922591); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10175378","Electroencephalography;emotion;graph filtering;recurrent attention network (RAN);spatiotemporal encoding (STE);transfer learning (TL)","Electroencephalography;Brain modeling;Feature extraction;Task analysis;Electrodes;Emotion recognition;Deep learning","","11","","61","IEEE","7 Jul 2023","","","IEEE","IEEE Journals"
"A Comprehensive Study on Machine Learning Approaches for Emotion Recognition","N. Kumar; N. Gupta","Department of Mathematics and Scientific Computing, National Institute of Technology, Hamirpur, Himachal Pradesh, India; Department of Mathematics and Scientific Computing, National Institute of Technology, Hamirpur, Himachal Pradesh, India",2022 2nd International Conference on Artificial Intelligence and Signal Processing (AISP),"25 Apr 2022","2022","","","1","5","Emotion recognition is the process to study about the emotions in a human being. This is a research field where one can understand and recognize the feelings of human and ability of expression which varies from each other at great extent. Several methods have been developed to study emotions such as facial expression, speech method, textual method and EEG signal. In this study work, we have reviewed several methods to find an efficiency of emotions up to accurate observations. Several papers on emotion recognition from the year 2007 to 2021 are been explored in this paper to observe the accuracy 95.20% using electroencephalogram (EEG) signal and 95% using EEG signals with statistical features and neural network. The average accuracy ranges in between 63% to 73% using EEG signal and facial expressions, both.","2640-5768","978-1-6654-4290-9","10.1109/AISP53593.2022.9760652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760652","Machine Learning;Emotion Recognition;Classification;EEG;Facial Expression;Artificial Intelligence","Support vector machines;Emotion recognition;Pain;Neural networks;Machine learning;Signal processing;Electroencephalography","","1","","14","IEEE","25 Apr 2022","","","IEEE","IEEE Conferences"
"Emotion Identification Based on EEG Rhythms Separated using Improved Eigenvalue Decomposition of Hankel Matrix","A. Nalwaya; V. K. Singh; R. B. Pachori","Department of Electrical Engineering, Indian Institute of Technology Indore, Indore, Madhya Pradesh, India; Department of Electrical Engineering, Indian Institute of Technology Indore, Indore, Madhya Pradesh, India; Department of Electrical Engineering, Indian Institute of Technology Indore, Indore, Madhya Pradesh, India",2023 9th International Conference on Signal Processing and Communication (ICSC),"26 Feb 2024","2023","","","562","567","This paper presents a framework for identifying emotional state of humans using their electroencephalogram (EEG) signals. The accurate and efficient identification of multiple classes of emotion using non-stationary EEG signals is a challenging task. The emotion identification framework can be developed by applying proper signal processing and machine learning algorithms on the EEG signals. The improved eigenvalue decomposition of Hankel matrix (IEVDHM) is used to decompose the EEG signal into various components. The rhythms are obtained by adding the components together whose mean frequency falls in the range of the respective rhythm. Then from each rhythm, two features are computed namely, permutation min-entropy and Katz fractal dimension measures. Using ensemble subspace k-nearest neighbor (KNN), the feature values are classified into different emotional states namely, happy, sad, fear, and neutral. For testing the performance of the proposed framework, the 10-channel EEG signals were recorded from 39 subjects which include 19 females and 20 males. The average accuracy of 91.30 % is obtained for the proposed framework for human emotion identification using EEG signals. Such emotion identification algorithm can help in obtaining emotional state of the user in brain-computer interface (BCI).","2643-444X","979-8-3503-8320-1","10.1109/ICSC60394.2023.10441313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441313","EEG signals;EEG rhythms;IEVDHM technique;classification methods;human emotion identification","Signal processing algorithms;Signal processing;Electroencephalography;Fractals;Eigenvalues and eigenfunctions;Matrix decomposition;Testing","","1","","31","IEEE","26 Feb 2024","","","IEEE","IEEE Conferences"
"Real-Time On-Chip Machine-Learning-Based Wearable Behind-The-Ear Electroencephalogram Device for Emotion Recognition","N. -D. Mai; H. -T. Nguyen; W. -Y. Chung","Department of Artificial Convergence, Pukyong National University, Busan, South Korea; Department of Artificial Convergence, Pukyong National University, Busan, South Korea; Department of Artificial Convergence, Pukyong National University, Busan, South Korea",IEEE Access,"19 May 2023","2023","11","","47258","47271","In this study, we propose an end-to-end emotion recognition system using an ear-electroencephalogram (EEG)-based on-chip device that is enabled using the machine-learning model. The system has an integrated device that gathers EEG signals from electrodes positioned behind the ear; it is more practical than the conventional scalp-EEG method. The relative power spectral density (PSD), which is the feature used in this study, is derived using the fast Fourier transform over five frequency bands. Directly on the embedded device, data preprocessing and feature extraction were carried out. Three standard machine learning models, namely, support vector machine (SVM), multilayer perceptron (MLP), and one-dimensional convolutional neural network (1D-CNN), were trained on these rich emotion classification features. The traditional approach, which integrates a model into the application software on a personal computer (PC), is cumbersome and lacks mobility, which makes it challenging to use in real-life applications. Besides, the PC-based system is not sufficiently real-time because of the connection latency from the EEG data acquisition device. To overcome these limitations, we propose a wearable device capable of performing on-chip machine learning and signal processing on the EEG data immediately after the acquisition task for the real-time result. In order to perform on-chip machine learning for the real-time prediction of emotions, 1D-CNN was chosen as a pre-trained model using the relative PSD characteristics as input based on the evaluation of the set results. Additionally, we developed a smartphone application that alerted the user whenever a negative emotional state was identified and displayed the information in real life. Our test results demonstrated the feasibility and practicability of our embedded system for real-time emotion recognition.","2169-3536","","10.1109/ACCESS.2023.3276244","National Research Foundation of Korea (NRF) Grant funded by the Korea Government through Ministry of Science and ICT (MSIT)(grant numbers:NRF-2019R1A2C1089139); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124208","Electroencephalogram (EEG);emotion recognition;tiny machine learning;real-time EEG system;power spectral density (PSD);multilayer perceptron (MLP);support vector machine (SVM);one-dimensional convolutional neural network (1D-CNN)","Electroencephalography;Support vector machines;Brain modeling;Real-time systems;Machine learning;Electrodes;Emotion recognition;Convolutional neural networks","","5","","41","CCBYNCND","15 May 2023","","","IEEE","IEEE Journals"
"EEG Data Augmentation for Emotion Recognition Using a Conditional Wasserstein GAN","Y. Luo; B. -L. Lu","Shanghai Jiao Tong University, Shanghai, CN; Shanghai Jiao Tong University, Shanghai, CN",2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"28 Oct 2018","2018","","","2535","2538","Due to the lack of electroencephalography (EEG) data, it is hard to build an emotion recognition model with high accuracy from EEG signals using machine learning approach. Inspired by generative adversarial networks (GANs), we introduce a Conditional Wasserstein GAN (CWGAN) framework for EEG data augmentation to enhance EEG-based emotion recognition. A Wasserstein GAN with gradient penalty is adopted to generate realistic-like EEG data in differential entropy (DE) form. Three indicators are used to judge the qualities of the generated high-dimensional EEG data, and only high quality data are appended to supplement the data manifold, which leads to better classification of different emotions. We evaluate the proposed CWGAN framework on two public EEG datasets for emotion recognition, namely SEED and DEAP. The experimental results demonstrate that using the EEG data generated by CWGAN significantly improves the accuracies of emotion recognition models.","1558-4615","978-1-5386-3646-6","10.1109/EMBC.2018.8512865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8512865","","Electroencephalography;Gallium nitride;Training;Generators;Emotion recognition;Generative adversarial networks;Brain modeling","Data Accuracy;Electroencephalography;Emotions;Entropy;Machine Learning","132","","18","IEEE","28 Oct 2018","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Identification Using 1-D Deep Residual Shrinkage Network With Microstate Features","J. Wang; Y. Song; Z. Mao; J. Liu; Q. Gao","Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Technical College for the Deaf, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control {Theory} and Applications in Complicated Systems, TUT Maritime College, Tianjin University of Technology, Tianjin, China",IEEE Sensors Journal,"28 Feb 2023","2023","23","5","5165","5174","Previous studies on emotion identification from electroencephalogram (EEG) mostly focused on normal and depressed people. However, hearing-impaired subjects may require emotional identification due to their chronic lack of perception of auditory information. In this article, we designed an experiment to collect EEG signals from 15 hearing-impaired subjects when they are watching the four kinds of emotional movie clips (happiness, calmness, sadness, and fear). The novel  ${K}$ -means method is used to extract the ten kinds of microstates (as A, B, C, D, E, F, G, H, I, and J) from the raw EEG signal, and then the new EEG single will be retrofitted by those ten microstates. For feature extraction, six kinds of microstate features (global explained variance (GEV), GEV total (GEVT), global field power (GFP), coverage, duration, and occurrence) are calculated. To classify the microstate features, a 1-D deep residual shrinkage network (1-D-DRSN) is utilized, which can filter the emotional irrelevant noise information, and capture emotional representational information. Experimental results show that the proposed model can significantly improve performance compared with other machine learning methods, with an average accuracy of 87.48%. Moreover, we explore different combinations of microstate features to reduce redundant information, and the combination of occurrence, GEV, and coverage reaches 90.15%. From the exploration of each microstate, we find that microstate C has the advantage with an average accuracy of 49.07%.","1558-1748","","10.1109/JSEN.2023.3239507","National Natural Science Foundation of China(grant numbers:62103299); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10027865","Deep learning;electroencephalogram (EEG);emotion identification;hearing-impaired subjects;microstate features","Electroencephalography;Feature extraction;Motion pictures;Sensors;Emotion recognition;Time-frequency analysis;Time-domain analysis","","11","","52","IEEE","27 Jan 2023","","","IEEE","IEEE Journals"
"A Hierarchical Three-Dimensional MLP-Based Model for EEG Emotion Recognition","W. Li; Y. Tian; J. Dong; C. Fang","School of Instrument Science and Engineering, Southeast University, Nanjing, China; College of Software Engineering, Southeast University, Suzhou, China; College of Software Engineering, Southeast University, Suzhou, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China",IEEE Sensors Letters,"11 Sep 2023","2023","7","10","1","4","Electroencephalogram (EEG) sensor data are useful and important for emotion recognition. However, cross-subject EEG emotion recognition suffers from the challenging problems of individual difference and noise disturbance. To cope with these problems, we propose a hierarchical 3-D MLP-based neural network (HMNN). This method consists of multiple hierarchical layers of 3D-MLPBlocks and a noise optimization module. The 3D-MLPBlock is designed to extract the multiperiod features of common emotional patterns across different individuals; the noise optimization module is devised to enhance the network robustness to noise disturbance. Experimental results on public benchmarks DEAP, DREAMER, and SEED-IV have demonstrated the superiority of HMNN over the related advanced approaches. Specifically, HMNN obtains the accuracies of 63.69%/60.03% for valence/aoursal classification on DEAP, 62.51%/64.49% for valence/arousal classification on DREAMER, and 62.29% for emotion classification on SEED-IV.","2475-1472","","10.1109/LSENS.2023.3307111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225679","Sensor signal processing;3D-MLPBlock;electroencephalogram (EEG);emotion recognition;hierarchical three-dimensional MLP-based Neural Network (HMNN);noise optimization module","Electroencephalography;Feature extraction;Optimization;Emotion recognition;Sensors;Robustness;Brain modeling","","5","","16","IEEE","22 Aug 2023","","","IEEE","IEEE Journals"
"Improving EEG-based Emotion Recognition by Fusing Time-Frequency and Spatial Representations","K. Zhu; X. Zhang; J. Wang; N. Cheng; J. Xiao","Ping An Technology (Shenzhen) Co., Ltd., China; Ping An Technology (Shenzhen) Co., Ltd., China; Ping An Technology (Shenzhen) Co., Ltd., China; Ping An Technology (Shenzhen) Co., Ltd., China; Ping An Technology (Shenzhen) Co., Ltd., China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Using deep learning methods to classify EEG signals can accurately identify people’s emotions. However, existing studies have rarely considered the application of the information in another domain’s representations to feature selection in the time-frequency domain. We propose a classification network of EEG signals based on the cross-domain feature fusion method, which makes the network more focused on the features most related to brain activities and thinking changes by using the multi-domain attention mechanism. In addition, we propose a two-step fusion method and apply these methods to the EEG emotion recognition network. Experimental results show that our proposed network, which combines multiple representations in the time-frequency domain and spatial domain, outperforms previous methods on public datasets and achieves state-of-the-art at present.","2379-190X","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10097171","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097171","electroencephalogram;graph convolution network;feature fusion;emotion recognition","Deep learning;Time-frequency analysis;Emotion recognition;Federated learning;Speech recognition;Brain modeling;Feature extraction","","3","","19","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Gender-Sensitive EEG Channel Selection for Emotion Recognition Using Enhanced Genetic Algorithm","D. -T. Duan; B. Sun; Q. Yang; W. Zhong; L. Ye; Q. Zhang; J. Zhang","Key Laboratory of Media Audio & Video, Ministry of Education, Communication University of China, Beijing, China; College of Artificial Intelligence, Nankai University, Tianjin, China; School of Artificial Intelligence, Nanjing University of Information Science and Technology, Nanjing, China; State Key Laboratory of Media Convergence and Communication, Communication, University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication, University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication, University of China, Beijing, China; Key Laboratory of Intelligent Education Technology and Application of Zhejiang Province, Zhejiang Normal University, Jinhua, China","2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","29 Jan 2024","2023","","","3253","3258","EEG channel selection aims to choose informative and representative channels to reduce data redundancy. It is very beneficial for improving the utility and efficiency of emotion recognition. Previous studies on EEG channel selection have not considered the influence of genders despite long-standing belief in gender differences with respect to emotion analysis. In this paper, we collected EEG signals from 20 subjects containing 10 males and 10 females by letting them watch short emotional videos. Then, to reduce data redundancy, we propose an enhanced genetic algorithm to select the optimal channel subsets separately for male and female subjects by incorporating a novel evolution operation. Experimental results show that the proposed algorithm achieves higher accuracy in terms of emotion recognition than several compared methods with a smaller channel subset. Besides, experimental results also indicate that the gender differences in neural patterns indeed exist. Through this study, the gender-sensitive channel selection offers a new avenue for further development of EEG based emotion recognition.","2577-1655","979-8-3503-3702-0","10.1109/SMC53992.2023.10393902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10393902","EEG based emotion recognition;channel selection;genetic algorithm;gender difference;evolutionary algorithms","Emotion recognition;Adaptive systems;Redundancy;Electroencephalography;Genetic algorithms;Videos","","1","","48","IEEE","29 Jan 2024","","","IEEE","IEEE Conferences"
"Frequency Band Power-based EEG Channel Selection for Emotion Recognition","P. Kar; J. Hazarika","Electronics and Instrumentation Engineering, National Institute of Technology Silchar, Silchar, India; Electronics and Instrumentation Engineering, National Institute of Technology Silchar, Silchar, India",2024 3rd International Conference for Advancement in Technology (ICONAT),"10 Dec 2024","2024","","","1","6","Effective management of human emotion is of vital importance in maintaining general health and well-being. Consequently, with the growing frequency of human-machine interaction in everyday life, the presence of a computer system capable of adapting to human emotions could be of immense importance. Furthermore, an emotion-adaptive Brain-Computer Interface (BCI) system has the capacity to not only improve the healthcare system but also boost the overall quality of life for individuals. Currently, most emotion-adaptive Brain-Computer Interface (BCI) systems primarily rely on Electroencephalogram (EEG) components due to their non-invasive nature and greater temporal resolution. However, most of the recent efforts in the area of EEG-based emotion detection are primarily focused on improving the effectiveness of the classifier for data collected from a substantially large number of EEG channels. Nevertheless, choosing a smaller set of EEG channels that are pertinent to different emotional states can result in a more resilient and less computationally demanding emotion-adaptive BCI system. Thus, in this work, we have introduced a channel selection technique based on EEG band power to classify positive and negative emotions. To assess the effectiveness of the proposed technique, we used a publicly available dataset and conducted a comparison with the approach that does not include channel selection and also with state-of-the-art literature. The results showed that the suggested method outperformed the state-of-the-art approaches.","","979-8-3503-5417-1","10.1109/ICONAT61936.2024.10774746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10774746","BCI;EEG;channel selection;emotion recognition","Deep learning;Human computer interaction;Emotion recognition;Correlation;Neural networks;Medical services;Electroencephalography;Brain-computer interfaces;Complexity theory;Character recognition","","","","26","IEEE","10 Dec 2024","","","IEEE","IEEE Conferences"
"Research on Emotion Classification Recognition Method Based on Vision Transformer Model","C. Zhang; M. Jing; W. Liu; X. Wang; F. Wang","Faculty of robot, Northeastern University; Faculty of robot, Northeastern University; Faculty of robot, Northeastern University; Bussiness School, Liaoning University; Department of Engineering, Northeastern University","2024 IEEE 14th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","14 Nov 2024","2024","","","587","592","In this study, we propose a novel approach for processing EEG signals. We applied band-pass filtering to EEG signals across multiple frequency bands, expanded the two-dimensional EEG data into three dimensions, and performed a step of artificial feature extraction to obtain an EEG feature matrix with a structure similar to an image. Additionally, we developed an emotion recognition model based on the EEG signals using the Vision Transformer model. This model utilizes its self-attention mechanism to explore the global dependencies among different frequency bands and electrode channels, aiming to capture the emotional valence and arousal information contained within the three-dimensional EEG feature matrix, thereby determining the true emotional state of the subjects. After training, our model achieved relatively high accuracy on both the training set and the test set we collected","2642-6633","979-8-3315-0605-6","10.1109/CYBER63482.2024.10749440","Northeastern University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749440","","Training;Electrodes;Computer vision;Emotion recognition;Accuracy;Brain modeling;Transformers;Feature extraction;Electroencephalography;Data models","","","","12","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"HiRENet: Novel Convolutional Neural Network Architecture Using Hilbert-Transformed and Raw Electroencephalogram for Subject-Independent Emotion Classification","M. Kim; C. -H. Im","dept. Electronic Engineering, Hanyang University, Seoul, Republic of Korea; dept. Biomedical Engineering, Hanyang University, Seoul, Republic of Korea",2025 13th International Conference on Brain-Computer Interface (BCI),"27 Mar 2025","2025","","","1","2","This study introduces a novel convolutional neural networks (CNN) architecture called the Hilbert-transformed (HT) and raw EEG network (HiRENet), which incorporates both raw and HT EEG as inputs. The HiRENet model was developed using two CNN frameworks: ShallowFBCSPNet and a CNN with a residual block (ResCNN). The performance of the HiRENet model was assessed using a lab-made EEG database to classify human emotions, comparing three input modalities: raw EEG, HT EEG, and a combination of both signals. The HiRENet model based on ResCNN achieved the highest classification accuracy, with 86.03% for valence and 84.01% for arousal classifications, surpassing traditional CNN methodologies.","2572-7672","979-8-3315-2192-9","10.1109/BCI65088.2025.10931755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10931755","electroencephalogram;Hilbert transform;deep learning;convolutional neural network;emotion classification","Deep learning;Electric potential;Accuracy;Databases;Transforms;Brain modeling;Electroencephalography;Brain-computer interfaces;Decoding;Convolutional neural networks","","","","5","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"Stress & Emotion Recognition Using Sentiment Analysis With Brain Signal","A. A. Bamanikar; R. V. Patil; L. V. Patil","SKNCOE Vadgaon bk’, Pune; PDEA’S College of Engineering Manjari Bk, Pune; SKNCOE Vadgaon bk’, Pune",2022 IEEE 2nd International Conference on Mobile Networks and Wireless Communications (ICMNWC),"7 Feb 2023","2022","","","1","4","Stress, in common parlance, is the negative emotional state that arises when people feel overwhelmed by their responsibilities and cannot successfully do their daily tasks. There may be temporary benefits to stress. The negative effects of stress on health are well-documented, and they worsen with duration. Personality disorders, anxiety, and depression are all possible outcomes. The health issues stress creates can be mitigated if you have a firm grasp of how stress works in the body. One of the most reliable methods for determining human emotion and stress is through the analysis of brain signals. This signal-based or brain-wave-based technology can help diagnose a wide range of illnesses and impairments, much like EEG does. Emotion and mental strain can be detected using sentiment analysis. Therefore, a reliable, accurate, and precise system is essential. The purpose of this research is to create a more precise and reliable system for detecting stress in real time utilizing Electroencephalography (EEG) data. The human brain’s electrical activity (EEG) can be used as a reliable, noninvasive stress gauge.","","978-1-6654-9111-2","10.1109/ICMNWC56175.2022.10031835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10031835","EEG Signal;Stress;Emotion;Sentiment analysis","Wireless communication;Sentiment analysis;Soft sensors;Human factors;Feature extraction;Electroencephalography;Real-time systems","","1","","21","IEEE","7 Feb 2023","","","IEEE","IEEE Conferences"
"Classification of emotion primitives from EEG signals using visual and audio stimuli","Y. Daşdemir; S. Yıldırım; E. Yıldırım","Enformatik Mustafa Kemal Üniversitesi, Hatay, Türkiye; Bilgisayar Mühendisliği, Mustafa Kemal Üniversitesi, Hatay, Türkiye; Bilgisayar Mühendisliği, Mustafa Kemal Üniversitesi, Hatay, Türkiye",2015 23nd Signal Processing and Communications Applications Conference (SIU),"22 Jun 2015","2015","","","2250","2253","Emotion recognition from EEG signals has an important role in designing Brain-Computer Interface. This paper compares effects of audio and visual stimuli, used for collecting emotional EEG signals, on emotion classification performance. For this purpose EEG data from 25 subjects are collected and binary classification (low/high) for valence and activation emotion dimensions are performed. Wavelet transform is used for feature extraction and 3 classifiers are used for classification. True positive rates of 71.7% and 78.5% are obtained using audio and video stimuli for valence dimension 71% and 82% are obtained using audio and video stimuli for arousal dimension, respectively.","2165-0608","978-1-4673-7386-9","10.1109/SIU.2015.7130325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130325","EEG;Arousal;Valence;Emotion Primitive Classification","Electroencephalography;Brain modeling;Emotion recognition;Visualization;Films;Finite impulse response filters;Brain-computer interfaces","","4","","18","IEEE","22 Jun 2015","","","IEEE","IEEE Conferences"
"Emotion Recognition of Subjects With Hearing Impairment Based on Fusion of Facial Expression and EEG Topographic Map","D. Li; J. Liu; Y. Yang; F. Hou; H. Song; Y. Song; Q. Gao; Z. Mao","Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Zhuhai, Macau; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, TUT Maritime College, Tianjin University of Technology, Tianjin, China; Technical College for the Deaf, Tianjin University of Technology, Tianjin, China",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"1 Feb 2023","2023","31","","437","445","Emotion analysis has been employed in many fields such as human-computer interaction, rehabilitation, and neuroscience. But most emotion analysis methods mainly focus on healthy controls or depression patients. This paper aims to classify the emotional expressions in individuals with hearing impairment based on EEG signals and facial expressions. Two kinds of signals were collected simultaneously when the subjects watched affective video clips, and we labeled the video clips with discrete emotional states (fear, happiness, calmness, and sadness). We extracted the differential entropy (DE) features based on EEG signals and converted DE features into EEG topographic maps (ETM). Next, the ETM and facial expressions were fused by the multichannel fusion method. Finally,  ${a}$  deep learning classifier CBAM_ResNet34 combined Residual Network (ResNet) and Convolutional Block Attention Module (CBAM) was used for subject-dependent emotion classification. The results show that the average classification accuracy of four emotions recognition after multimodal fusion achieves 78.32%, which is higher than 67.90% for facial expressions and 69.43% for EEG signals. Moreover, visualization by the Gradient-weighted Class Activation Mapping (Grad-CAM) of ETM showed that the prefrontal, temporal and occipital lobes were the brain regions closely related to emotional changes in individuals with hearing impairment.","1558-0210","","10.1109/TNSRE.2022.3225948","National Natural Science Foundation of China(grant numbers:62103299); 2021 Tianjin Postgraduate Research and Innovation Project(grant numbers:2021YJSS092); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9968039","Emotion recognition;facial expression;electroencephalogram topographic map;individuals with hearing impairment","Feature extraction;Electroencephalography;Emotion recognition;Auditory system;Brain modeling;Face recognition;Deep learning","Humans;Facial Expression;Electroencephalography;Emotions;Brain;Hearing Loss","13","","47","CCBY","1 Dec 2022","","","IEEE","IEEE Journals"
"EEG Emotion Recognition Based on Granger Causality and CapsNet Neural Network","J. GUO; F. Fang; W. Wang; F. Ren","Hefei University of Technology, Hefei, Anhui, CN; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei 230601, China; Hefei University of Technology, Hefei, Anhui, CN; Hefei University of Technology, Hefei, Anhui, CN",2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS),"14 Apr 2019","2018","","","47","52","Emotion recognition is a very challenging task in the brain-computer interface field, and it is of great significance in medical, education, military, and other fields. The classification problem is the key to the field of emotion recognition research. In this paper, a classification model based on CapsNet neural network is proposed. By extracting the granger causality feature of original EEG signals, sparse group lasso algorithm is used for feature screening, and the obtained high-relevance feature subset is taken as the input of the network to achieve the final emotional classification. The experimental results show that by adjusting the model parameters and network structure, the constructed CapsNet neural network performs emotional classification on EEG signals, and obtains 88.09% and 87.37% average classification accuracy under valence and arousal emotion dimensions, compared with SVM and CNN. The classification system can obtain better results and significantly improve the performance of EEG emotional classification. This is the first time that CapsNet is applied to EEG emotional classification.","","978-1-5386-6005-8","10.1109/CCIS.2018.8691230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691230","EEG;Emotion recognition;CapsNet;Granger causality","","","7","","22","IEEE","14 Apr 2019","","","IEEE","IEEE Conferences"
"Identifying Stable Patterns over Time for Emotion Recognition from EEG","W. -L. Zheng; J. -Y. Zhu; B. -L. Lu","Department of Computer Science and Engineering, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Affective Computing,"4 Sep 2019","2019","10","3","417","429","In this paper, we investigate stable patterns of electroencephalogram (EEG) over time for emotion recognition using a machine learning approach. Up to now, various findings of activated patterns associated with different emotions have been reported. However, their stability over time has not been fully investigated yet. In this paper, we focus on identifying EEG stability in emotion recognition. We systematically evaluate the performance of various popular feature extraction, feature selection, feature smoothing and pattern classification methods with the DEAP dataset and a newly developed dataset called SEED for this study. Discriminative Graph regularized Extreme Learning Machine with differential entropy features achieves the best average accuracies of 69.67 and 91.07 percent on the DEAP and SEED datasets, respectively. The experimental results indicate that stable patterns exhibit consistency across sessions; the lateral temporal areas activate more for positive emotions than negative emotions in beta and gamma bands; the neural patterns of neutral emotions have higher alpha responses at parietal and occipital sites; and for negative emotions, the neural patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites. The performance of our emotion recognition models shows that the neural patterns are relatively stable within and between sessions.","1949-3045","","10.1109/TAFFC.2017.2712143","National Natural Science Foundation of China(grant numbers:61673266,61272248); National Basic Research Program of China (973 Program)(grant numbers:2013CB329401); Major Basic Research Program of Shanghai Science and Technology Committee(grant numbers:15JC1400103); ZBYY-MOE Joint Funding(grant numbers:6141A02022604); Technology Research and Development Program of China Railway Corporation(grant numbers:2016Z003-B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938737","Affective computing;affective brain-computer interaction;emotion recognition;EEG;stable EEG patterns;machine learning;extreme learning machine","Electroencephalography;Emotion recognition;Stability analysis;Feature extraction;Brain modeling;Time-frequency analysis;Support vector machines","","608","","66","IEEE","5 Jun 2017","","","IEEE","IEEE Journals"
"ST-GCN: EEG Emotion Recognition via Spectral Graph and Temporal Analysis with Graph Convolutional Networks","C. Tang; L. Yang; G. Cao; J. Du; Q. Zhang","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China",2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"10 Jan 2025","2024","","","3748","3751","Emotion recognition from electroencephalogram (EEG) signals is a key application in brain-computer interfaces (BCIs), but the high dimensionality and noise in EEG data pose significant challenges. Many existing approaches fail to adequately filter irrelevant information or fully capture complex inter-channel relationships and temporal dynamics, leading to suboptimal emotional representation.To address these challenges, we propose ST-GCN, a novel model that integrates spectral and temporal domain features using graph convolution for robust EEG emotion recognition. ST-GCN employs a channel information reconstruction layer, channel aggregation, and temporal feature extraction to learn discriminative representations across EEG channels and time. Evaluated on the DEAP dataset with a cross-validation setup, ST-GCN achieves state-of-the-art performance, with 98.43% accuracy for valence and 98.69% for arousal, demonstrating its effectiveness in EEG-based emotion recognition.","2156-1133","979-8-3503-8622-6","10.1109/BIBM62325.2024.10822712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10822712","EEG;Emotion recognition;Graph convolutional network;Channel reconstruction","Emotion recognition;Convolution;Graph convolutional networks;Noise;Information filters;Feature extraction;Brain modeling;Electroencephalography;Brain-computer interfaces;Bioinformatics","","","","27","IEEE","10 Jan 2025","","","IEEE","IEEE Conferences"
"EEG-Based Motor Imagery Classification with Deep Multi-Task Learning","Y. Song; D. Wang; K. Yue; N. Zheng; Z. -J. M. Shen","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Industrial Engineering and Operations Research and Department of Civil and Environmental Engineering, University of California, Berkeley, Berkeley, USA",2019 International Joint Conference on Neural Networks (IJCNN),"30 Sep 2019","2019","","","1","8","In the past decade, Electroencephalogram (EEG) has been applied in many fields, such as Motor Imagery (MI) and Emotion Recognition. Traditionally, for classification tasks based on EEG, researchers would extract features from raw signals manually which is often time consuming and requires adequate domain knowledge. Besides that, features manually extracted and selected may not generalize well due to the limitation of human. Convolutional Neural Networks (CNNs) plays an important role in the wave of deep learning and achieve amazing results in many areas. One of the most attractive features of deep learning for EEG-based tasks is the end-to-end learning. Features are learned from raw signals automatically and the feature extractor and classifier are optimized simultaneously. There are some researchers applying deep learning methods to EEG analysis and achieving promising performances. However, supervised deep learning methods often require large-scale annotated dataset, which is almost impossible to acquire in EEG-based tasks. This problem limits the further improvements of deep learning models for classification based on EEG. In this paper, we propose a novel deep learning method DMTL-BCI based on Multi-Task Learning framework for EEG-based classification tasks. The proposed model consists of three modules, the representation module, the reconstruction module and the classification module. Our model is proposed to improve the classification performance with limited EEG data. Experimental results on benchmark dataset, BCI Competition IV dataset 2a, show that our proposed method outperforms the state-of-the-art method by 3.0%, which demonstrates the effectiveness of our model.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8852362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852362","EEG;Deep Learning;Multi-Task Learning","Feature extraction;Task analysis;Electroencephalography;Deep learning;Brain modeling;Image reconstruction;Training","","16","","32","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"Multimodal Emotion Recognition: Emotion Classification Through the Integration of EEG and Facial Expressions","S. Erdem Güler; F. Patlar Akbulut","Department of Computer Engineering, Istanbul Kültür University, İstanbul, Türkiye; Department of Computer Engineering, Istanbul Kültür University, İstanbul, Türkiye",IEEE Access,"10 Feb 2025","2025","13","","24587","24603","Despite advances in the field of emotion recognition, the research field still faces two main limitations: the use of deep models for increasingly complex calculations and the identification of emotions through various data types. This study aims to advance the knowledge on multimodal emotion recognition by combining electroencephalography (EEG) signals with facial expressions, using advanced models such as Transformer, Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). The results validate the effectiveness of this approach, demonstrating the high accuracy of the Gated Recurrent Unit (GRU) model, which achieved an average of 91.8% classification accuracy on unimodal (EEG-only) data and an average of 97.8% classification accuracy on multimodal (EEG and facial expressions) datasets in the multi-class emotion categories. The findings emphasize that by applying a multi-class classification framework, multimodal approaches offer significant improvements over traditional unimodal techniques. This work presents a framework that captures complex neural dynamics and visible emotional cues, enhancing the robustness and accuracy of emotion recognition systems. These results have important practical implications, showing how integrating various data sources with advanced models can overcome the limitations of single-modality systems.","2169-3536","","10.1109/ACCESS.2025.3538642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10870204","Human computer interaction;emotion recognition;deep learning;EEG signals;facial expressions","Electroencephalography;Accuracy;Brain modeling;Emotion recognition;Data models;Videos;Long short term memory;Logic gates;Human computer interaction;Soft sensors","","","","39","CCBY","4 Feb 2025","","","IEEE","IEEE Journals"
"Using Attentive Network Layers for Identifying Relevant EEG channels for Subject-Independent Emotion Recognition Approaches","C. E. Valderrama","Applied Computer Department, University of Winnipeg, Winnipeg, Canada",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","5","Emotion recognition approaches using electroencephalographic (EEG) signals can be evaluated using two distinct approaches: subject-dependent and subject-independent. While subject-independent models are more generalizable and practical than subject-dependent models, they face challenges due to the high variability of EEG signals among individuals. One solution is identifying shared patterns during emotional processing. As deep learning has become a common practice for emotion recognition, using attentive network layers can help identify shared predictive patterns. This study explores this approach by using attentive network layers to identify brain areas relevant for predicting four emotions evoked by video clips in 15 individuals. The model achieved an average accuracy of 46% (95% CI: 41.3-50.7%) among subjects, indicating that the EEG channels in the right hemisphere were more relevant for predicting happy and neutral emotions, while those in the left hemisphere were more relevant for sadness and fear. These findings highlight the importance of including EEG channels from both hemispheres to ensure the prediction of different emotion types in subject-independent approaches. Clinical relevance— This study identifies shared neuronal patterns in emotion prediction, supporting the development of generalizable emotion recognition systems that can help diagnose and treat disorders such as depression, anxiety, and neurodegenerative diseases.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10781782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781782","","Deep learning;Emotion recognition;Face recognition;Anxiety disorders;Predictive models;Brain modeling;Depression;Electroencephalography;Engineering in medicine and biology;Diseases","Humans;Electroencephalography;Emotions;Male;Female;Adult;Signal Processing, Computer-Assisted;Deep Learning;Young Adult","","","18","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Feature selection and comparison for the emotion recognition according to music listening","S. -W. Byun; S. -P. Lee; H. S. Han","Department of Computer Science, Graduate school of SangMyung Univ, Seoul, Korea; Department of Computer Science, Graduate school of SangMyung Univ, Seoul, Korea; Department of Computer Science, Graduate school of SangMyung Univ, Seoul, Korea",2017 International Conference on Robotics and Automation Sciences (ICRAS),"19 Oct 2017","2017","","","172","176","Recently, researches on analyzing relationship between the state of emotion and musical stimuli using EEG are increasing. These research shows that a selection of feature vectors is very important for the performance of EEG pattern classifiers. In this paper, we apply feature extraction methods, which were reviewed in the previous, to DEAP data for the emotion recognition. We limit to analysis features in time-domain for this research. To evaluate the feature vectors, the Relief algorithm and the Bhattacharyya distance are used. According to result, the power of signal is better for the emotion recognition than the other feature.","","978-1-5386-3995-5","10.1109/ICRAS.2017.8071939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071939","EEG;emotion recognition;feature selection","Electroencephalography;Feature extraction;Emotion recognition;Music;Videos;Electrodes;Classification algorithms","","6","","7","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Soft Voting Strategy for Multi-Modal Emotion Recognition Using Deep-learning- Facial Images and EEG","U. Chinta; J. Kalita; A. Atyabi","Department of Computer Science, University of Colorado at Colorado Springs; Department of Computer Science, University of Colorado at Colorado Springs; Department of Computer Science, University of Colorado at Colorado Springs",2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC),"18 Apr 2023","2023","","","0738","0745","Emotion recognition is an important factor in social communication and has a wide range of applications from retail to healthcare. In psychology, emotion recognition focuses on emotional states within non-verbal visual and auditory cues. It is essential to the human ability to associate meaning with events rather than treating them as mere facts. Studies of emotion recognition often utilize data gathered in response to non-verbal cues using modalities such as eye tracking, Electroencephalo-gram (EEG), and facial video and build classification models capable of differentiating responses to various emotions and cues. The accuracy of these emotion recognition models largely depends on feature representation and the suitability of the chosen features in magnifying the differences between patterns of various emotions. Single-modal feature extraction methods are limited in capturing between-group differences and often result in reduced classification performance. To address this problem, this paper proposes a multi-modal approach in the representation of response to emotional cues involving EEG recording and facial video data. The study utilizes the dataset containing frontal face video recordings and EEG data of 22 participants. A novel deep neural network architecture within the feature level is used to efficiently predict emotions using EEG and facial video data. The experimental result indicates 97.5% accuracy in identifying facial expressions and categorizing them into two classes, arousal (class 0) and valence (class 1), surpassing state-of-the-art for the DEAP dataset.","","979-8-3503-3286-5","10.1109/CCWC57344.2023.10099070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10099070","EEG;feature extraction;emotion analysis;multi-modal integration;Gated Recurrent Unit","Emotion recognition;Visualization;Neural networks;Psychology;Medical services;Gaze tracking;Brain modeling","","5","","57","IEEE","18 Apr 2023","","","IEEE","IEEE Conferences"
"Emotion Specific Network with Multi-dimension Features in Emotion Recognition","G. Jia; X. Liu; H. Wang; Y. Hu; F. Wan","Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau; Department of Electrical and Computer Engineering, Faculty of Science and Technology Centre for Cognitive and Brain Sciences Institute of Collaborative Innovation, University of Macau, Macau; Faculty of Intelligent Manufacturing, Wuyi University, Jiangmen, China; Department of Orthopaedics and Traumatology, Li Ka Shing Faculty of Medicine, The University of Hong Kong, Hong Kong; Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau",2021 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA),"27 Jul 2021","2021","","","1","6","Emotion recognition has been recognized as an important issue in terms of human-computer interaction. Various studies showed brain regional cooperation changed with mental state. However, the important role of specific brain channels and their topology during the emotion activity is still unclear. In this paper, we extracted the multi-dimension EEG features to achieve emotion specific network construction and emotion recognition. The dataset is from the 2020 World Robot Conference-Brain-Computer Interfaces (BCI) Contest, provided by Shanghai Jiaotong University. There are 24 sessions included in this paper. The power spectrum density (PSD), Hjorth parameter, and functional connectivity were extracted from each session. A data-driven critical channel selection strategy was performed by sorting the classification accuracy of each channel. Meanwhile, the emotion specific network was established by the top 10 channels. Finally, we mixed the features in the emotion specific network to recognize three emotion types (positive, neutral, and negative). We found the reorganization of the brain network mostly focused on the right hemisphere. Moreover, the classification accuracy (70.53% ± 4.61% (mean ± std)) showed the feasibility of emotion specific network. Our study indicated the emotion specific network is critical for emotion alteration and provides a new insight for emotion recognition.","2377-9322","978-1-6654-1249-0","10.1109/CIVEMSA52099.2021.9493578","Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493578","emotion recognition;electroencephalogram;multi-dimension features;functional network;emotion specific network","Human computer interaction;Emotion recognition;Network topology;Virtual environments;Feature extraction;Electroencephalography;Topology","","2","","20","IEEE","27 Jul 2021","","","IEEE","IEEE Conferences"
"Improve the generalization of the cross-task emotion classifier using EEG based on feature selection and SVR","S. Liu; W. Wu; S. Zhai; X. Liu; Y. Ke; X. An; D. Ming","Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China",2019 IEEE 10th International Conference on Awareness Science and Technology (iCAST),"5 Dec 2019","2019","","","1","5","Emotion is a state that comprehensively represents human feeling, thought and behavior. In our daily life, emotion has played an increasingly important role, and emotion recognition has become a research focus. What' more, the application has a broader perspective at home and abroad. Most existing studies identified emotion under specific tasks, but emotion classifiers are required to recognize emotion under any conditions in practice. Therefore, cross-task emotion recognition is a necessary step to move from the laboratory to the practical use. In this work, we designed three different induced tasks, picture-induced, music-induced and video-induced tasks. 13 (8 females and 5 males) participants were recruited and evoked to be positive, neutral and negative states respectively. The results using support vector regression highlighted that the correlation coefficient was higher for inter-task classification in video-induced and music-induced tasks, while deteriorated significantly in cross-task classification. Combining recursive feature screening and support vector regression to optimize features, the optimal feature set had better performance than all features employed, obtaining above 0.8 for correlation coefficient. These results indicated that SVR could achieve a better performance of cross-task emotion recognition, partly because it avoided the problem of emotion intensity mismatch in different tasks.","2325-5994","978-1-7281-3821-3","10.1109/ICAwST.2019.8923256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8923256","EEG;emotion recognition;cross-task;emotion classifiers;recursive feature screening;SVR","Task analysis;Emotion recognition;Correlation coefficient;Electroencephalography;Feature extraction;Support vector machines;Brain modeling","","1","","20","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Emotion Recognition From EEG Using Higher Order Crossings","P. C. Petrantonakis; L. J. Hadjileontiadis","Signal Processing and Biomedical Technology Unit, Telecommunications Laboratory, Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Signal Processing and Biomedical Technology Unit, Telecommunications Laboratory, Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece",IEEE Transactions on Information Technology in Biomedicine,"15 Mar 2010","2010","14","2","186","197","Electroencephalogram (EEG)-based emotion recognition is a relatively new field in the affective computing area with challenging issues regarding the induction of the emotional states and the extraction of the features in order to achieve optimum classification performance. In this paper, a novel emotion evocation and EEG-based feature extraction technique is presented. In particular, the mirror neuron system concept was adapted to efficiently foster emotion induction by the process of imitation. In addition, higher order crossings (HOC) analysis was employed for the feature extraction scheme and a robust classification method, namely HOC-emotion classifier (HOC-EC), was implemented testing four different classifiers [quadratic discriminant analysis (QDA), k-nearest neighbor, Mahalanobis distance, and support vector machines (SVMs)], in order to accomplish efficient emotion recognition. Through a series of facial expression image projection, EEG data have been collected by 16 healthy subjects using only 3 EEG channels, namely Fp1, Fp2, and a bipolar channel of F3 and F4 positions according to 10-20 system. Two scenarios were examined using EEG data from a single-channel and from combined-channels, respectively. Compared with other feature extraction methods, HOC-EC appears to outperform them, achieving a 62.3% (using QDA) and 83.33% (using SVM) classification accuracy for the single-channel and combined-channel cases, respectively, differentiating among the six basic emotions, i.e., happiness , surprise, anger, fear, disgust, and sadness. As the emotion class-set reduces its dimension, the HOC-EC converges toward maximum classification rate (100% for five or less emotions), justifying the efficiency of the proposed approach. This could facilitate the integration of HOC-EC in human machine interfaces, such as pervasive healthcare systems, enhancing their affective character and providing information about the user's emotional status (e.g., identifying user's emotion experiences, recurring affective states, time-dependent emotional trends).","1558-0032","","10.1109/TITB.2009.2034649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291724","Electroencephalogram (EEG);emotion recognition;higher order crossings analysis;$k$-nearest neighbor ( $k$-NN);Mahalanobis distance (MD);mirror neuron system;quadratic discriminant analysis;support vector machines (SVMs)","Emotion recognition;Electroencephalography;Feature extraction;Support vector machines;Support vector machine classification;Mirrors;Neurons;Robustness;Testing;Humans","Adult;Algorithms;Discriminant Analysis;Electroencephalography;Emotions;Facial Expression;Female;Humans;Imitative Behavior;Male;Models, Neurological;Recognition (Psychology);Reproducibility of Results;Signal Processing, Computer-Assisted","537","1","43","IEEE","23 Oct 2009","","","IEEE","IEEE Journals"
"Brain Emotion Perception Inspired EEG Emotion Recognition With Deep Reinforcement Learning","D. Li; L. Xie; Z. Wang; H. Yang","Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, China",IEEE Transactions on Neural Networks and Learning Systems,"3 Sep 2024","2024","35","9","12979","12992","Inspired by the well-known Papez circuit theory and neuroscience knowledge of reinforcement learning, a double dueling deep  $Q$  network (DQN) is built incorporating the electroencephalogram (EEG) signals of the frontal lobe as prior information, which is named frontal lobe double dueling DQN (FLD3QN). The framework of FLD3QN is constructed in accord with the brain emotion mechanism which takes the frontal lobe and the thalamus as the core, in which the part of the Papez circuit is simulated by the bifrontal lobe residual convolution neural network (BiFRCNN). Moreover, a step penalty factor is designed to constrain the number of mistakes of the agent. The ablation studies results on the public EEG emotion dataset DEAP verified the important roles of the frontal lobe and the Papez circuit in modeling the procedure of learning rewards during the perception of emotions, with a great increase in the average accuracies by 25.24% and 23.31% in valence and arousal dimensions.","2162-2388","","10.1109/TNNLS.2023.3265730","Natural Science Foundation of China(grant numbers:62276098,62076094); Shanghai Science and Technology Program— Federated-based cross-domain and cross-task incremental learning(grant numbers:21511100800); Shanghai Science and Technology Program—Distributed and generative few-shot algorithm and theory research(grant numbers:20511100600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113206","Electroencephalogram (EEG);emotion perception;emotion recognition;reinforcement learning","Thalamus;Frontal lobe;Reinforcement learning;Electroencephalography;Emotion recognition;Hypothalamus;Brain modeling","Humans;Emotions;Electroencephalography;Deep Learning;Reinforcement, Psychology;Neural Networks, Computer;Brain;Frontal Lobe;Perception","20","","57","IEEE","1 May 2023","","","IEEE","IEEE Journals"
"Survey on Signal Processing and Classification Algorithms for Depression Using Electroencephalogram Signals","S. N; U. A","Department of Electronics and Communication Engineering, PSG College of Technology, Coimbatore, India; Department of Electronics and Communication Engineering, PSG College of Technology, Coimbatore, India","2024 International Conference on Smart Systems for Electrical, Electronics, Communication and Computer Engineering (ICSSEECC)","3 Sep 2024","2024","","","489","494","This survey paper presents various functional neuro-imaging techniques, revealing the excellence of EEG signals in terms of high temporal resolution, expensiveness, non-invasive, ease of use, flexibility, safety and portable when compared to position emission tomography, magneto encephalogram, functional MRI and transcranial magnetic stimulation methods. It also delves into different frequency bands present in EEG signals. The primary objective of this survey paper is to examine findings, related to depression using EEG signals. Subsequently, it discusses preprocessing methods, extraction of features from the preprocessed signal, post processing and result investigation utilizing classification algorithms and statistical testing methods. Finally it identifies research gaps and outlines, potential future findings in depression-related studies.","","979-8-3503-7817-7","10.1109/ICSSEECC61126.2024.10649522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10649522","EEG;Functional neuro-imaging techniques;Signal processing;Survey;Investigation;Classification;Research gap;Depression","Surveys;Magnetic resonance imaging;Machine learning;Feature extraction;Depression;Brain modeling;Electroencephalography","","","","24","IEEE","3 Sep 2024","","","IEEE","IEEE Conferences"
"EEG-based emotion recognition using Recurrence Plot analysis and K nearest neighbor classifier","F. Bahari; A. Janghorbani","Department of Biomedical Engineering, Amirkabir University of Technology, Tehran, Iran; Department of Biomedical Engineering, Amirkabir University of Technology, Tehran, Iran",2013 20th Iranian Conference on Biomedical Engineering (ICBME),"3 Apr 2014","2013","","","228","233","Electroencephalogram (EEG)-based emotion recognition has been a rapidly growing field. However, accurate and sufficient performance rates are yet to be obtained. This paper presents the classification of EEG correlates on emotion using the relatively new non-linear feature extraction method, namely, Recurrence Plot analysis to extract thirteen non-linear features. This method is compared with feature extraction method based on spectral power analysis. The K nearest neighbor is applied to classify extracted features into the emotional states based on arousal-valence (high/low arousal, valence) plane with the addition of liking axis (positive/negative). Leading to performance rates of 58.05%, 64.56% and 67.42% for 3 classes of valence, arousal and liking; which confirm the advantage of a non-linear feature extraction method over previous frequency based feature extraction techniques.","","978-1-4799-3232-0","10.1109/ICBME.2013.6782224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6782224","Emotion Recognition;Chaos;Non-linear Analysis;EEG;Recurrence Plot;K Nearest Neighbor","Feature extraction;Electroencephalography;Emotion recognition;Accuracy;Trajectory;Biomedical engineering;Educational institutions","","53","","31","IEEE","3 Apr 2014","","","IEEE","IEEE Conferences"
"An EEG Data Processing Approach for Emotion Recognition","G. Li; D. Ouyang; Y. Yuan; W. Li; Z. Guo; X. Qu; P. Green","Institute of Human Factors and Ergonomics, College of Mechatronics and Control Engineering, Shenzhen University, Shenzhen, China; Institute of Human Factors and Ergonomics, College of Mechatronics and Control Engineering, Shenzhen University, Shenzhen, China; Institute of Human Factors and Ergonomics, College of Mechatronics and Control Engineering, Shenzhen University, Shenzhen, China; State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Tsinghua University, Beijing, China; School of Transportation and Logistics, Southwest Jiaotong University, Chengdu, China; Institute of Human Factors and Ergonomics, College of Mechatronics and Control Engineering, Shenzhen University, Shenzhen, China; Department of Industrial and Operations Engineering, University of Michigan Transportation Research Institute (UMTRI), University of Michigan, Ann Arbor, MI, USA",IEEE Sensors Journal,"30 May 2022","2022","22","11","10751","10763","As the most direct way to measure the true emotional states of humans, EEG-based emotion recognition has been widely used in affective computing applications. In this paper, we aim to propose a novel emotion recognition approach that relies on a reduced number of EEG electrode channels and at the same time overcomes the negative impact of individual differences to achieve a high recognition accuracy. According to the statistical significance results of EEG power spectral density (PSD) features obtained from the SJTU Emotion EEG Dataset (SEED), six candidate sets of EEG electrode channels are determined. An experiment-level batch normalization (BN) is proposed and applied on the features from the candidate sets, and the normalized features are then used for emotion recognition across individuals. Eleven well-accepted classifiers are used for emotion recognition. The experimental results show that the recognition accuracy when using a small portion of the available electrodes is almost the same or even better than that when using all the channels. Based on the reduced number of electrode channels, the application of experiment-level BN can help further improve the recognition accuracy, specifically from 73.33% to 89.63% when using the LR classifier. These results demonstrate that better and easier emotion recognition performance can be achieved based on the batch normalized features from fewer channels, indicating promising applications of our proposed method in real-time emotion recognition applications in intelligent systems.","1558-1748","","10.1109/JSEN.2022.3168572","NSF China(grant numbers:51805332,52072320); Shenzhen Fundamental Research and Discipline Layout project(grant numbers:JCYJ20190808142613246,20200803015912001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761248","Electroencephalogram (EEG);emotion recognition;electrode channels selection;batch normalization;individual difference","Emotion recognition;Electroencephalography;Electrodes;Feature extraction;Sensors;Neural networks;Deep learning","","39","","67","IEEE","21 Apr 2022","","","IEEE","IEEE Journals"
"Multi-Channel EEG Based Emotion Recognition Using Temporal Convolutional Network and Broad Learning System","X. Jia; T. Zhang; C. L. Philip Chen; Z. Liu; L. Chen; G. Wen; B. Hu","Sch. of Computer Science&Engineering, South China University of Technology, Guangzhou, China; Sch. of Computer Science&Engineering, South China University of Technology, Guangzhou, China; Sch. of Computer Science&Engineering, South China University of Technology, Guangzhou, China; Sch. of Computer Science&Engineering, South China University of Technology, Guangzhou, China; Dept. of Computer and Inform., Science University of Macau, Macau, China; Sch. of Computer Science&Engineering, South China University of Technology, Guangzhou, China; Sch. of Information Science&Engineering, Lanzhou University, Lanzhou, China","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","2452","2457","Automatic real-time emotion recognition based on multi-channel EEG signals is a significant and challenging task in neurology and psychiatry. In recent years, deep learning has been used in EEG emotion recognition. However, many existing deep learning based methods still require complex pre-processing or additional feature extraction, which make it difficult to achieve real-time emotion recognition. In this paper, an end-to-end model named Temporal Convolutional Broad Learning System (TCBLS) was designed for multi-channel EEG based emotion recognition. The TCBLS takes one-dimensional EEG signals as input, then extracts emotion-related features of EEG automatically. In this model, the Temporal Convolutional Network (TCN) is designed to extract EEG temporal features and deep abstract features simultaneously, then Broad Learning System (BLS) is used to map the features to a more discriminative space and further enhance the features. We evaluated our method on DEAP database, performing 10-fold cross-validation on each subject to obtain the classification accuracy. Experimental results indicate that the performance of TCBLS is better than other comparison methods, and the mean accuracy of TCBLS is 99.5755% and 99.5781% on valence and arousal classification task respectively. The results demonstrate the effectiveness and robustness of TCBLS in EEG emotion recognition.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283159","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283159","EEG;emotion recognition;temporal convolutional network (TCN);broad learning system (BLS);temporal convolutional broad learning system (TCBLS)","Learning systems;Emotion recognition;Convolution;Feature extraction;Brain modeling;Electroencephalography;Task analysis","","14","","27","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"Hybrid Network Using Dynamic Graph Convolution and Temporal Self-Attention for EEG-Based Emotion Recognition","C. Cheng; Z. Yu; Y. Zhang; L. Feng","Department of Computer Science and Technology, Dalian University of Technology, Dalian, China; Department of Computer Science and Technology, Dalian University of Technology, Dalian, China; Department of Information Engineering, Huzhou University, Huzhou, China; Department of Computer Science and Technology, Dalian University of Technology, Dalian, China",IEEE Transactions on Neural Networks and Learning Systems,"2 Dec 2024","2024","35","12","18565","18575","The electroencephalogram (EEG) signal has become a highly effective decoding target for emotion recognition and has garnered significant attention from researchers. Its spatial topological and time-dependent characteristics make it crucial to explore both spatial information and temporal information for accurate emotion recognition. However, existing studies often focus on either spatial or temporal aspects of EEG signals, neglecting the joint consideration of both perspectives. To this end, this article proposes a hybrid network consisting of a dynamic graph convolution (DGC) module and temporal self-attention representation (TSAR) module, which concurrently incorporates the representative knowledge of spatial topology and temporal context into the EEG emotion recognition task. Specifically, the DGC module is designed to capture the spatial functional relationships within the brain by dynamically updating the adjacency matrix during the model training process. Simultaneously, the TSAR module is introduced to emphasize more valuable time segments and extract global temporal features from EEG signals. To fully exploit the interactivity between spatial and temporal information, the hierarchical cross-attention fusion (H-CAF) module is incorporated to fuse the complementary information from spatial and temporal features. Extensive experimental results on the DEAP, SEED, and SEED-IV datasets demonstrate that the proposed method outperforms other state-of-the-art methods.","2162-2388","","10.1109/TNNLS.2023.3319315","National Natural Science Foundation of China(grant numbers:62306317); China Postdoctoral Science Foundation(grant numbers:2023M733738); LiaoNing Revitalization Talents Program(grant numbers:XLYC1806006); Fundamental Research Funds for the Central Universities(grant numbers:DUT19RC(3)012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10285039","Dynamic graph convolution;electroencephalogram (EEG);emotion recognition;hierarchical cross-attention fusion (H-CAF);temporal self-attention representation (TSAR)","Electroencephalography;Brain modeling;Emotion recognition;Feature extraction;Convolutional neural networks;Topology;Graph convolutional networks","Electroencephalography;Humans;Emotions;Neural Networks, Computer;Attention;Algorithms;Time Factors;Brain","8","","58","IEEE","13 Oct 2023","","","IEEE","IEEE Journals"
"Convolving Emotions: A Compact CNN for EEG-Based Emotion Recognition","M. A. Cardoso-Moreno; C. Macias; T. Alcantara; M. Soto; H. Calvo; C. Yañez-Marquez","Computational Cognitive Sciences Laboratory, CIC Instituto Politécnico Nacional Mexico City, Mexico; Computational Cognitive Sciences Laboratory, CIC Instituto Politécnico Nacional Mexico City, Mexico; Computational Cognitive Sciences Laboratory, CIC Instituto Politécnico Nacional Mexico City, Mexico; Computational Cognitive Sciences Laboratory, CIC Instituto Politécnico Nacional Mexico City, Mexico; Computational Cognitive Sciences Laboratory, CIC Instituto Politécnico Nacional Mexico City, Mexico; Intelligent Computing Laboratory, CIC Instituto Politécnico Nacional Mexico City, Mexico",2023 IEEE Symposium Series on Computational Intelligence (SSCI),"1 Jan 2024","2023","","","1472","1476","Emotion Recognition is a research area that has had a surge in interest, since areas such as mental health, psychological diagnosis, e-Learning and assistance for people who are not capable of communicating their feelings, depend on certain level, on the capacities of computer systems to reliably identify emotions. There are several approaches to this task, for instance, analyzing facial expressions, speech, and physiological signals (electrocardiogram, galvanic skin response, electroencephalogram, among others). Electroencephalogram is one of the preferred methods due, in part, to is great temporal resolution. Therefore, in this paper we used the EEG Brainwave Dataset as benchmark to test our model, which is a four layer, one dimensional convolutional neural network. After the preprocessing pipeline, consisting on considering certain features of the dataset as signals and processing them accordingly, by creating several channels by two decomposition methods, our model achieved accuracy values of 98.36% and 95.31 %, which is competitive with what is found on the state of the art, while being a significantly less complex model.","2472-8322","978-1-6654-3065-4","10.1109/SSCI52147.2023.10372040","Instituto Politécnico Nacional; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10372040","Emotion Recognition;EEG;Convolutional Neural Network;CNN;Classification;Deep Learning","Radio frequency;Emotion recognition;Brain modeling;Electroencephalography;Skin;Convolutional neural networks;Proposals","","1","","25","IEEE","1 Jan 2024","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition Using Multi-Dimensional Convolutional Neural LSTM via Attention Mechanism","Y. Yang; D. Wang; Y. Zheng; Y. Yang","College of Electrical Engineering, Qingdao University, Qingdao, China; College of Electrical Engineering, Qingdao University, Qingdao, China; College of Electrical Engineering, Qingdao University, Qingdao, China; College of Electrical Engineering, Qingdao University, Qingdao, China",2023 IEEE 6th International Electrical and Energy Conference (CIEEC),"10 Jul 2023","2023","","","579","584","For emotion recognition, firstly, we transform the differential entropy and power spectral density features extracted from different channels of EEG signals into a spatial representation with a multi-dimensional structure. Secondly, a convolutional neural network (CNN) and a neural network with bidirectional long short-term memory (Bi-LSTM) are combined together to form a deep learning model. Among them, CNN extracts the effective frequency and spatial information in each time segment of the input EEG signal, and the Bi-LSTM strengthens the temporal dependence of the output information from CNN. Further, the attention enhancement mechanism is fused into the Bi-LSTM module to extract more discriminative spatial-temporal features. The proposed model is extensively trained and tested on DEAP dataset to verify its advantages in different aspects. The experimental findings show that the accuracy of emotion recognition is also enhanced to some degree.","","979-8-3503-4667-1","10.1109/CIEEC58067.2023.10167160","National Natural Science Foundation of China(grant numbers:62273190); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167160","EEG;Emotion recognition;multi-dimensional Feature;attention-additional bi-directional long short-term memory;convolutional neural network","Emotion recognition;Time-frequency analysis;Transforms;Feature extraction;Brain modeling;Electroencephalography;Entropy","","1","","16","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Comparing Recognition Performance and Robustness of Multimodal Deep Learning Models for Multimodal Emotion Recognition","W. Liu; J. -L. Qiu; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Brain Science and Technology Research Center, Center for Brain-Like Computing and Machine Intelligence, Qing Yuan Research Institute, Shanghai Jiao Tong University, Shanghai, China; Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Brain and Cognitive Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Computer Science and Engineering, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Brain Science and Technology Research Center, Center for Brain-Like Computing and Machine Intelligence, Qing Yuan Research Institute, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Cognitive and Developmental Systems,"10 Jun 2022","2022","14","2","715","729","Multimodal signals are powerful for emotion recognition since they can represent emotions comprehensively. In this article, we compare the recognition performance and robustness of two multimodal emotion recognition models: 1) deep canonical correlation analysis (DCCA) and 2) bimodal deep autoencoder (BDAE). The contributions of this article are threefold: 1) we propose two methods for extending the original DCCA model for multimodal fusion: a) weighted sum fusion and b) attention-based fusion; 2) we systemically compare the performance of DCCA, BDAE, and traditional approaches on five multimodal data sets; and 3) we investigate the robustness of DCCA, BDAE, and traditional approaches on SEED-V and DREAMER data sets under two conditions: 1) adding noises to multimodal features and 2) replacing electroencephalography features with noises. Our experimental results demonstrate that DCCA achieves state-of-the-art recognition results on all five data sets: 1) 94.6% on the SEED data set; 2) 87.5% on the SEED-IV data set; 3) 84.3% and 85.6% on the DEAP data set; 4) 85.3% on the SEED-V data set; and 5) 89.0%, 90.6%, and 90.7% on the DREAMER data set. Meanwhile, DCCA has greater robustness when adding various amounts of noises to the SEED-V and DREAMER data sets. By visualizing features before and after DCCA transformation on the SEED-V data set, we find that the transformed features are more homogeneous and discriminative across emotions.","2379-8939","","10.1109/TCDS.2021.3071170","National Natural Science Foundation of China(grant numbers:61976135); National Key Research and Development Program of China(grant numbers:2017YFB1002501); SJTU Transmed Awards Research(grant numbers:WF540162605); Fundamental Research Funds for the Central Universities; Higher Education Discipline Innovation Project; China Southern Power Grid(grant numbers:GDKJXM20185761); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9395500","Bimodal deep autoencoder (BDAE);deep canonical correlation analysis (DCCA);electroencephalography (EEG);eye movement;multimodal deep learning;multimodal emotion recognition;robustness","Emotion recognition;Electroencephalography;Robustness;Deep learning;Correlation;Brain modeling;Computational modeling","","175","","61","IEEE","5 Apr 2021","","","IEEE","IEEE Journals"
"Discriminative-CCA Promoted By EEG signals For Physiological-based Emotion Recognition","W. Zhao; Z. Zhao; C. Li","College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China",2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia),"23 Sep 2018","2018","","","1","6","Recently, multi-modal signals such as physiological and EEG signals are increasingly utilized because of their consistency and complementarity in emotional representation. Compared with physiological signal, however, EEG signal is more difficult to obtain and more expensive so that it still cannot be utilized effectively and sufficiently. In this paper, we propose a method to enhance the performance of physiological-based emotion recognition system, where EEG signal is considered as privileged information and only available during training. Discriminative canonical correlation analysis (DCCA) is used to capture the consistency and complementarity from between physiological and EEG features, and then, a new emotional-relevant discriminative space can be constructed. During training period, physiological and EEG features are projected into an emotional discriminative space by using DCCA with the help of EEG signals. And then, machine learning techniques are utilized to build emotion recognizer for projected affective samples. During testing period, only peripheral signals are used for emotion recognition. The experimental results on two databases demonstrate that our proposed method achieves better recognition performance than existing methods.","","978-1-5386-5311-1","10.1109/ACIIAsia.2018.8470373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8470373","Emotion recognition;EEG signals;Peripheral physiological signals;Discriminative Canonical Correlation Analysis","Physiology;Electroencephalography;Correlation;Brain modeling;Emotion recognition;Databases;Feature extraction","","9","","15","IEEE","23 Sep 2018","","","IEEE","IEEE Conferences"
"Inter-Subject Emotion Detection Using Empirical Mode Decomposition on EEG Data","A. Srivastava; C. Charan; A. J. Patil; S. K. Maddheshiya; P. Bhavsar; D. Kumar","School of Biomedical Engineering, Indian Institute of Technology (BHU), Varanasi, India; School of Biomedical Engineering, Indian Institute of Technology (BHU), Varanasi, India; School of Biomedical Engineering, Indian Institute of Technology (BHU), Varanasi, India; School of Biomedical Engineering, Indian Institute of Technology (BHU), Varanasi, India; Department of Electronics Engineering, Visvesvaraya National Institute of Technology Nagpur, Nagpur, India; School of Biomedical Engineering, Indian Institute of Technology (BHU), Varanasi, India",2024 IEEE International Conference on Intelligent Signal Processing and Effective Communication Technologies (INSPECT),"27 Feb 2025","2024","","","1","6","Accurate identification of human emotional states can be achieved through various methods, including subjective self-reports and obj ective measurements like facial expressions, vocal patterns, and physiological signals. Among these, use of physiological signal such as electroencephalogram (EEG) is optimal for emotion detection due to its direct measurement of brain activity, providing insights into neural processes underpinning emotions. However, detecting emotions using EEG signals is challenging in subj ect-independent scenarios, due to the fluctuating and unpredictable nature of these brain wave recordings. This study addresses these challenges by utilizing empirical mode decomposition (EMD) to analyze EEG signals. EMD is an iterative method that decomposes signals into intrinsic mode functions (IMFs) without needing predefined basic functions. A 4-second sliding window with a 2-second overlap is utilized to divide the EEG signal into segments. Each segment is decomposed into five IMFs, and nonlinear features such as Shannon entropy, differential entropy, collision entropy, Hjorth parameters, and Higuchi's fractal dimension are extracted. These features serve as 2D inputs to an EEGN et classifier, a convolutional neural network designed for EEG signals. The study employs the DEAP dataset, which includes 32-channel EEG recordings from 32 participants who rated their arousal and valence levels after viewing music video excerpts. These ratings are mapped to high versus low arousal and valence categories. The study focuses on inter-subj ect emotion recognition and the findings from our experiments showcase average classification accuracies of 65.46% (±0.09 (SD)) for arousal and 62.5% (±0.06 (SD)) for valence in inter-subject emotion recognition.","","979-8-3503-7952-5","10.1109/INSPECT63485.2024.10896236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10896236","DEAP;EEG;EEGNet;Emotion;Empirical mode decomposition;Intrinsic mode functions","Emotion recognition;Accuracy;Empirical mode decomposition;Feature extraction;Brain modeling;Electroencephalography;Entropy;Physiology;Data models;Recording","","","","15","IEEE","27 Feb 2025","","","IEEE","IEEE Conferences"
"BioCNN: A Hardware Inference Engine for EEG-Based Emotion Detection","H. A. Gonzalez; S. Muzaffar; J. Yoo; I. M. Elfadel","Chair of Highly Parallel VLSI-Systems and Neuro-Microelectronics, Technische Universität Dresden, Dresden, Germany; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates",IEEE Access,"7 Aug 2020","2020","8","","140896","140914","EEG-based emotion classifiers have the potential of significantly improving the social integration of patients suffering from neurological disorders such as Amyotrophic Lateral Sclerosis or the acute stages of Alzheimer's disease. Emotion classifiers have historically used software on general-purpose computers and operating under off-line conditions. Yet the wearability of such classifiers is a must if they are to enable the socialization of critical-care patients. Such wearability requires the use of low-power hardware accelerators that would enable near real-time classification and extended periods of operations. In this article, we architect, design, implement, and test a handcrafted, hardware Convolutional Neural Network, named BioCNN, optimized for EEG-based emotion detection and other bio-medical applications. The EEG signals are generated using a low-cost, off-the-shelf device, namely, Emotiv Epoc+, and then denoised and pre-processed ahead of their use by BioCNN. For training and testing, BioCNN uses three repositories of emotion classification datasets, including the publicly available DEAP and DREAMER datasets, along with an original dataset collected in-house from 5 healthy subjects using standard visual stimuli. A subject-specific training approach is used under TensorFlow to train BioCNN, which is implemented using the Digilent Atlys Board with a low-cost Spartan-6 FPGA. The experimental results show a competitive energy efficiency of 11 GOps/W, a throughput of 1.65 GOps that is in line with the real-time specification of a wearable device, and a latency of less than 1 ms, which is smaller than the 150 ms required for human interaction times. Its emotion inference accuracy is competitive with the top software-based emotion detectors.","2169-3536","","10.1109/ACCESS.2020.3012900","Research Ethics Committee at Khalifa University(grant numbers:#H17-003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9151874","Emotion recognition;EEG;FPGA;machine learning;hardware accelerator;edge AI;convolutional neural networks;hardware parallelism;pipelining","Electroencephalography;Hardware;Feature extraction;Real-time systems;Engines;Training;Machine learning","","31","","31","CCBY","29 Jul 2020","","","IEEE","IEEE Journals"
"A Novel Emotion Recognition Method Based on the Feature Fusion of Single-Lead EEG and ECG Signals","X. Wang; J. Zhang; C. He; H. Wu; L. Cheng","School of Computer, Guangdong University of Technology, Guangzhou, China; School of Computer, Guangdong University of Technology, Guangzhou, China; School of Computer, Guangdong University of Technology, Guangzhou, China; School of Automation, Guangdong University of Technology, Guangzhou, China; School of Computer, Guangdong University of Technology, Guangzhou, China",IEEE Internet of Things Journal,"21 Feb 2024","2024","11","5","8746","8756","Emotions are complex, and people vary greatly in their accuracy in recognizing their own emotions and those of others. With advances in computer science and neuroscience, there is a desire to use automated techniques to help people identify emotions. Bio-electrical signals have been proven effective for emotion detection, but the acquisition of conventional electrocardiogram (ECG) and EEG requires medical-specific equipment, which is very expensive, uncomfortable, and inconvenient due to the large number of electrodes and the hair-covered scalp. In this article, a novel emotion recognition method based on the feature fusion of single-lead EEG and ECG signals is proposed, using the long short term memory (LSTM)-MLP-based model and the CNN-based model for feature fusion and classification, respectively, with fivefold cross-validation for validation. The ECG and EEG signals of 15 participants were collected in five states: 1) happy; 2) relaxed; 3) calm; 4) sad; and 5) afraid, each of which was stimulated using the participants’ own proposed music. Various time-domain features, frequency-domain features, and nonlinear features were extracted from the ECG and EEG signals. Experimental results demonstrate that the accuracy of emotion recognition and classification of signals captured by the proposed device can reach 92.08% using the CNN model. While using the LSTM-MLP feature fusion model, the accuracy figure can be improved to 95.07%. The results of the ablation experiment indicate that the feature fusion approach does improve the accuracy of recognition. It is demonstrated that the proposed device and emotional recognition approach are effective and feasible.","2327-4662","","10.1109/JIOT.2023.3320269","National Natural Science Foundation of China(grant numbers:U22A2012,62104047,62173098); Natural Science Foundation of Guangdong Province(grant numbers:2023A1515010291); Basic and Applied Basic Research Project of Guangzhou Basic Research Program(grant numbers:2023A04J1707); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266684","Electrocardiogram (ECG);EEG;emotion recognition;feature fusion networks;HRV;wearable convenience devices","Electroencephalography;Electrocardiography;Emotion recognition;Frequency-domain analysis;Feature extraction;Internet of Things;Wavelet packets","","7","","40","IEEE","28 Sep 2023","","","IEEE","IEEE Journals"
"Brain Multi-Region Information Fusion using Attentional Transformer for EEG Based Affective Computing","M. Asif; A. Gupta; A. Aditya; S. Mishra; U. S. Tiwary","Department of Information Technology, Indian Institute of Information Technology, Allahabad, Prayagraj, India; Department of Information Technology, Indian Institute of Information Technology, Allahabad, Prayagraj, India; Department of Information Technology, Indian Institute of Information Technology, Allahabad, Prayagraj, India; Department of Cognitive Science, Indian Institute of Technology, Kanpur, Kanpur, India; Department of Information Technology, Indian Institute of Information Technology, Allahabad, Prayagraj, India",2023 IEEE 20th India Council International Conference (INDICON),"27 Feb 2024","2023","","","771","775","In this paper, we worked on the fusion of multiple brain regions in order to combine information from different brain regions. The idea is that considering the dynamic processing of emotional video stimulus will involve different brain regions, and hence, fusion of information from these brain regions can increase emotion recognition accuracy significantly. We utilized EEG datasets from Indian (DENS) and European (DEAP) populations, comprising 128 and 32 channels, respectively. We categorized EEG channels into different brain regions and divided them into five sub-regions. Then, we used five transformer models to extract information from these sub-regions and concatenated the results to obtain a fused information vector for classification. The idea resembles the theoretical findings that brain association areas that fuse information from different brain lobes contribute significantly to emotion processing. For the DENS dataset, we achieved 97.78% accuracy in valence and 95.74% accuracy in arousal. For the DEAP dataset, we achieved 90.07% accuracy in valence and 84.52% accuracy in arousal. We propose that deep learning models simulating information fusion in the association regions of the brain can enhance emotion recognition accuracy.","2325-9418","979-8-3503-0559-3","10.1109/INDICON59947.2023.10440791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440791","Affective Computing;Brain Region Interaction;DENS Dataset;EEG;Emotion Recognition;Self Attention;Transformers","Emotion recognition;Sociology;Transformers;Brain modeling;Electroencephalography;Vectors;Statistics","","1","","22","IEEE","27 Feb 2024","","","IEEE","IEEE Conferences"
"Emotion recognition system design using multi-physiological signals","H. Jiang; G. Yang; X. Gui; N. Wu; T. Zhang","MinZu University of China, Beijing, China; MinZu University of China, Beijing, China; MinZu University of China, Beijing, China; MinZu University of China, Beijing, China; MinZu University of China, Beijing, China",2012 IEEE 11th International Conference on Cognitive Informatics and Cognitive Computing,"24 Sep 2012","2012","","","499","503","A physiological signal-based emotion recognition system was designed. The system was developed to operate as a use r-independent system, based on three physiological signals databases obtained from multiple ethnic objections. The input signals were EEG, eye activity and facial expressions, all of which were acquired synchronous. The whole system will be comfort from the subjects' body surface, and can reflect the influence of emotion on the autonomic nervous system. The system consisted of preprocessing, feature extraction and pattern classification stages. Preprocessing and feature extraction methods were devised so that emotion-specific characteristics could be extracted.","","978-1-4673-2795-4","10.1109/ICCI-CC.2012.6311199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311199","Multi-physiological;Ethnic emotion;Pattern recognition","Emotion recognition;Electroencephalography;Biomedical monitoring;Feature extraction;Speech recognition;Physiology;Face recognition","","2","","25","IEEE","24 Sep 2012","","","IEEE","IEEE Conferences"
"Fine-Grained Interpretability for EEG Emotion Recognition: Concat-Aided Grad-CAM and Systematic Brain Functional Network","B. Liu; J. Guo; C. L. P. Chen; X. Wu; T. Zhang","School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China",IEEE Transactions on Affective Computing,"30 May 2024","2024","15","2","671","684","EEG emotion recognition plays a significant role in various mental health services. Deep learning-based methods perform excellently, but still suffer from interpretability. Although methods such as Gradient-weighted Class Activation Mapping(Grad-CAM) can cope with the above problem, their coarse granularity cannot accurately reveal the mechanism to promote emotional intelligence. In this paper, fine-grained interpretability is proposed, called Concat-aided Grad-CAM. Specifically, the multi-level feature mapping before the fully connected layer is concatenated to obtain the gradients of the target concept so that the discriminant information can be directly located in the high-precision area. Unlike coarse-grained interpretability methods applied in EEG emotion recognition, it can accurately highlight the EEG channels related to emotion rather than an obscure area. In addition, a systematic brain functional network is proposed to reveal the relationship between those channels and to further improve emotion recognition performance. The channels with greater contributions are connected, and those connections are learned by dynamic graph convolutional networks, while the others are independent to eliminate interference. Experiments on four EEG emotion recognition datasets manifest that Concat-aided Grad-CAM can be interpreted by the fine-grained. In addition, it has been shown that the learned brain functional network can improve the performance of the baselines. Significantly, the experiment results achieve state-of-the-art performance in multiple experiments.","1949-3045","","10.1109/TAFFC.2023.3288885","National Key Research and Development Program of China(grant numbers:2019YFA0706200); National Natural Science Foundation of China(grant numbers:62076102,62222603,92267203); Guangdong Natural Science Funds for Distinguished Young Scholar(grant numbers:2020B1515020041); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2019ZT08X214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160113","Brain functional network;dynamic graph convolutional networks;EEG emotion recognition;interpretability","Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Convolutional neural networks;Heating systems;Physiology","","22","","62","IEEE","23 Jun 2023","","","IEEE","IEEE Journals"
"Cognitive Emotion Measures of Brain","R. Kaur; R. Gill; J. Singh","Department of CSE, Institute of Engineering and Technology, Chitkara University, Rajpura, Punjab, INDIA; Department of CSE, Institute of Engineering and Technology, Chitkara University, Rajpura, Punjab, INDIA; Department of CSE, Institute of Engineering and Technology, Chitkara University, Rajpura, Punjab, INDIA",2019 6th International Conference on Computing for Sustainable Global Development (INDIACom),"13 Feb 2020","2019","","","290","294","The Cognitive analysis of brain is much more powerful emotion response than face. Amygdala and frontal cortex are two main parts of brain which are highly responsible for cognition and emotion. Now days, human inner feelings reside inside and try to hide its natural response. The human predictive emotion using artificial intelligence, pattern recognition is useful for analyzing neuromarketing behavior. Neuromarketing application area is used to study consumer behavior in order to take review of any product. Earlier, people will collect reviews in groups, surveys etc. Cognitive analysis technique is more beneficial than conventional methods. In this research work, proposed dataset is used to analyse human response while watching advertisement videos. In advertisement videos, emotions of human drive them to purchase and donate money. So, the human response is collected by using EEG device. This response is useful for researchers in neuromarketing, to analyse which product requires improvement or which is in demand. This paper provides accuracy metrics of proposed dataset using deep learning (DL) and support vector machine (SVM) classifiers. More over deep learning network gives more accurate results than SVM.","","978-9-3805-4434-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8991258","Cognition;Emotion;Neurodynamics;Electroencephalography (EEG);Arousal;Valence;Neuromarketing;Deep learning(DL);Support Vector Machine(SVM)","","","","","35","","13 Feb 2020","","","IEEE","IEEE Conferences"
"Multimodal emotion recognition using EEG and eye tracking data","W. -L. Zheng; B. -N. Dong; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"6 Nov 2014","2014","","","5040","5043","This paper presents a new emotion recognition method which combines electroencephalograph (EEG) signals and pupillary response collected from eye tracker. We select 15 emotional film clips of 3 categories (positive, neutral and negative). The EEG signals and eye tracking data of five participants are recorded, simultaneously, while watching these videos. We extract emotion-relevant features from EEG signals and eye tracing data of 12 experiments and build a fusion model to improve the performance of emotion recognition. The best average accuracies based on EEG signals and eye tracking data are 71.77% and 58.90%, respectively. We also achieve average accuracies of 73.59% and 72.98% for feature level fusion strategy and decision level fusion strategy, respectively. These results show that both feature level fusion and decision level fusion combining EEG signals and eye tracking data can improve the performance of emotion recognition model.","1558-4615","978-1-4244-7929-0","10.1109/EMBC.2014.6944757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944757","","Electroencephalography;Feature extraction;Emotion recognition;Accuracy;Brain modeling;Videos;Entropy","Algorithms;Brain;Electroencephalography;Emotions;Eye Movements;Female;Humans;Iris;Male;Models, Psychological;Multimodal Imaging;Pattern Recognition, Automated;Young Adult","64","1","11","IEEE","6 Nov 2014","","","IEEE","IEEE Conferences"
"Identifying Stable EEG Patterns in Manipulation Task for Negative Emotion Recognition","Y. Pei; S. Zhao; L. Xie; Z. Luo; D. Zhou; C. Ma; Y. Yan; E. Yin","Defense Innovation Institute, Academy of Military Sciences (AMS) and Intelligent Game and Decision Laboratory, Beijing, China; Defense Innovation Institute, Academy of Military Sciences (AMS) and Intelligent Game and Decision Laboratory, Beijing, China; Defense Innovation Institute, Academy of Military Sciences (AMS) and Intelligent Game and Decision Laboratory, Beijing, China; Defense Innovation Institute, Academy of Military Sciences (AMS) and Intelligent Game and Decision Laboratory, Beijing, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; Defense Innovation Institute, Academy of Military Sciences (AMS) and Intelligent Game and Decision Laboratory, Beijing, China; Defense Innovation Institute, Academy of Military Sciences (AMS) and Intelligent Game and Decision Laboratory, Beijing, China; Defense Innovation Institute, Academy of Military Sciences (AMS) and Intelligent Game and Decision Laboratory, Beijing, China",IEEE Transactions on Affective Computing,"","2025","PP","99","1","15","Negative emotion recognition during manipulation task plays crucial role in human-machine interaction, where diverse cognitive variables coexist and influence each other. However, traditional emotion experiments often overemphasize emotion induction while overlooking other practical cognitive tasks, which leads participants to suffer from simplistic emotional experiences and ultimately compromises the real-world applicability of the emotional data collected. To incorporate critical cognitive variables into emotion elicitation, we utilize joystick-based real-time emotion annotation to encourage subjects to continuously feel emotional intensity, to advisedly decide when to manipulate the joystick, and to physically operate it. Consequently, at least two essential cognitive variables—decision-making and action—are integrated into emotion perception. Following this, we develop a novel negative emotion dataset called CRED, which includes a variety of physiological data, particularly Electroencephalograph (EEG). To assess the stability of emotional EEG patterns, we employ strict statistical analysis and a dual-branch transformer (DBT) with the gradient-based attribution method on the proposed CRED. Additionally, two well-known public datasets (SEED, and SEED-V) are used to verify the DBT. Compared to traditional methods, DBT improves classification accuracy by approximately 5% on CRED and by around 2% on the public datasets. The experimental results indicate that the occipital lobe plays a crucial role in the discrimination of negative emotions; the critical frequency bands vary between the five emotions in the CRED. Specifically, the low-delta rhythm is associated with anger, while fear is influenced by both theta and alpha rhythms; disgust is found to be significant in the theta rhythm; and for neutral emotions, both low-delta and alpha rhythms are identified as crucial. In summary, our findings demonstrate the existence of stable emotional EEG patterns when additional cognitive variables are involved. The CRED and the source codes will be released at https://huggingface.co/datasets/peiyu999/CRED.","1949-3045","","10.1109/TAFFC.2025.3551330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10925902","Emotion Recognition;EEG;Transformer;Real-Time Emotion Annotation","Emotion recognition;Electroencephalography;Real-time systems;Rhythm;Annotations;Brain modeling;Analysis of variance;Videos;Decision making;Accuracy","","","","","IEEE","14 Mar 2025","","","IEEE","IEEE Early Access Articles"
"EEG based emotion recognition system using MFDFA as feature extractor","S. Paul; A. Mazumder; P. Ghosh; D. N. Tibarewala; G. Vimalarani","School of Bioscience & Engineering, Jadavpur University, Kolkata, India; School of Bioscience & Engineering, Jadavpur University, Kolkata, India; School of Bioscience & Engineering, Jadavpur University, Kolkata, India; School of Bioscience & Engineering, Jadavpur University, Kolkata, India; Department of Electronics & Instrumentation, Hindustan University, Chennai, India","2015 International Conference on Robotics, Automation, Control and Embedded Systems (RACE)","30 Apr 2015","2015","","","1","5","Emotion is a complex set of interactions among subjective and objective factors governed by neural/hormonal systems resulting in the arousal of feelings and generate cognitive processes, activate physiological changes such as behavior. Emotion recognition can be correctly done by EEG signals. Electroencephalogram (EEG) is the direct reflection of the activities of hundreds and millions of neurons residing within the brain. Different emotion states create distinct EEG signals in different brain regions. Therefore EEG provides reliable technique to identify the underlying emotion information. This paper proposes a novel approach to recognize users emotions from electroencephalogram (EEG) signals. Audio signals are used as stimuli to elicit positive and negative emotions of subjects. For eight healthy subjects, EEG signals are acquired using seven channels of an EEG amplifier. The result reveal that frontal, temporal and parietal regions of the brain are relevant to positive emotion recognition and frontal and parietal regions are activated in case of negative emotion identification. After proper signal processing of the raw EEG, for the whole frequency bands the features are extracted from each channel of the EEG signals by Multifractral Detrended Fluctuation Analysis (MFDFA) method. We introduce an effective classifier named Support Vector Machine (SVM) to categorize the EEG feature space related to various emotional states into their respective classes. Next, we compare Support Vector Machine (SVM) with various other methods like Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA) and K Nearest Neighbor (KNN). The average classification accuracy of SVM for positive emotions on the whole frequency bands is 84.50%, while the accuracy of QDA is 76.50% and with LDA 75.25% and KNN is only 69.625% whereas, for negative emotions it is 82.50%, while for QDA is 72.375% and with LDA 65.125% and KNN is only 70.50%.","","978-8-1925-9743-0","10.1109/RACE.2015.7097247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7097247","EEG;MFDFA;SVM;BCI;Emotion","Electroencephalography;Feature extraction;Support vector machines;Fractals;Fluctuations;Emotion recognition;Accuracy","","34","","23","","30 Apr 2015","","","IEEE","IEEE Conferences"
"Epileptic Seizure Analysis using Scalp EEG Signals with Deep Learning","B. M. Devi; R. Srilekha; G. Harshitha; B. Sushma","Department of Computer Science and Engineering; Computer Science and Engineering, Institute of Aeronautical Engineering; Computer Science and Engineering, Institute of Aeronautical Engineering; Computer Science and Engineering, Institute of Aeronautical Engineering",2023 International Conference on Sustainable Computing and Smart Systems (ICSCSS),"7 Jul 2023","2023","","","195","201","There has been an increase in interest in using Electroencephalogram (EEG) data for emotion detection in recent years, with possible applications in industries including affective computing and medicine. It has been discovered that persons with depression, a common mental health disorder, have differential spatial responses to neurophysiological signals when exposed to positive and negative stimuli that differ from those in healthy people. Our investigation contrasts previous research in the field and focuses on important elements essential to emotion recognition, such as themes, extracted features, classifiers, and more. This research study develops an emotion activation curve to more clearly depict the activation process of emotions. The proposed study employs machine learning to classify emotions by extracting features from EEG signals. It leverages data from several experiments to train a suggested model, which is then tested for its effectiveness in detecting emotions. The main goal of our project is to identify emotions from brain signals by applying a novel adaptive channel selection method that can distinguish between how brain activity operates in various people and emotional states.The results of our investigation show a significant improvement in the proposed method’s ability to correctly categorize positive or negative emotions in depressed patients.","","979-8-3503-3360-2","10.1109/ICSCSS57650.2023.10169286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10169286","Epilepsy;Seizure detection using Electroencephalogram(EEG);Deep learning and Classification","Emotion recognition;Time-frequency analysis;Mood;Scalp;Mental health;Feature extraction;Depression","","","","30","IEEE","7 Jul 2023","","","IEEE","IEEE Conferences"
"Fusing Acoustic and Electroencephalographic Modalities for User-Independent Emotion Prediction","S. Ntalampiras; F. Avanzini; L. A. Ludovico","Department of Computer Science, University of Milan Milan, Italy; Department of Computer Science, University of Milan Milan, Italy; Department of Computer Science, University of Milan Milan, Italy",2019 IEEE International Conference on Cognitive Computing (ICCC),"29 Aug 2019","2019","","","36","41","Search and retrieval of multimedia content based on the evoked emotion comprises an interesting scientific field with numerous applications. This paper proposes a method that fuses two heterogeneous modalities, i.e. music and electroencephalographic signals, both for predicting emotional dimensions in the valence-arousal plane and for addressing four binary classification tasks, namely i.e. high/low arousal, positive/negative valence, high/low dominance, high/low liking. The proposed solution exploits Mel-scaled and EEG spectrograms feeding a k-medoids clustering scheme based on canonical correlation analysis. A thorough experimental campaign carried out on a publicly available dataset confirms the efficacy of such an approach. Despite its low computational cost, it was able to surpass state of the art results, and most importantly, in a user-independent manner.","","978-1-7281-2711-8","10.1109/ICCC.2019.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816973","music emotion prediction;EEG emotion prediction;music EEG fusion;canonical correlation analysis;k-medoids clustering algorithm","Electroencephalography;Feature extraction;Multiple signal classification;Correlation;Clustering algorithms;Prediction algorithms;Music","","2","","28","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Statistical Analysis of Multi-channel EEG Signals for Digitizing Human Emotions","A. Roshdy; S. Alkork; A. S. Karar; H. Mhalla; T. Beyrouthy; Z. Al Barakeh; A. Nait-ali","College of Engineering and Technology, American University of the Middle East, Kuwait; College of Engineering and Technology, American University of the Middle East, Kuwait; College of Engineering and Technology, American University of the Middle East, Kuwait; College of Engineering and Technology, American University of the Middle East, Kuwait; College of Engineering and Technology, American University of the Middle East, Kuwait; College of Engineering and Technology, American University of the Middle East, Kuwait; University-Paris-Est, LiSSi, (UPEC), Vitry-sur-Seine, France",2021 4th International Conference on Bio-Engineering for Smart Technologies (BioSMART),"21 Jan 2022","2021","","","1","4","The primary objective of this research is the sta-tistical analysis of multi-channel electroencephalogram (EEG) signals for the purpose of emotion recognition performed in the valence-arousal space. The spatial information offered by the sensor location of the multi-channel EEG, is of critical importance as it does not only contain latent information, but also provides insights into the regions of the brain which are active during the expression of the targeted emotions. In particular, the linear correlation between the EEG channel features and the emotion value on the valence-arousal axes is obtained over different frequency ranges using the Pearson method. The five different features utilized in this study are the power of each sensor, power difference between symmetric sensors, power ratio between symmetric differences, average of the sensors readings, and standard deviation of the sensors readings. The statistical analysis was performed using the standard DEAP data set valence, arousal, and dominance values along with raw multi-channel EEG data. Preliminary results indicate that it is possible to optimize the number of sensors used in capturing the EEG signal, while maintaining a high degree of emotion detection accuracy. The standard deviation was found to be the most optimum metric for detecting valence emotion, while the beta frequency range is the better suited for detecting arousal with any of the devised metrics.","","978-1-6654-0810-3","10.1109/BioSMART54244.2021.9677741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9677741","Channels optimisation;human-robot interaction;EEG Channels;Emotiv;Heat Brain Map;EEG Features","Measurement;Headphones;Emotion recognition;Correlation;Statistical analysis;Feature extraction;Electroencephalography","","8","","14","IEEE","21 Jan 2022","","","IEEE","IEEE Conferences"
"Identifying Functional Brain Connectivity Patterns for EEG-Based Emotion Recognition","X. Wu; W. -L. Zheng; B. -L. Lu","Center for Brain-Like Computing and Machine Intelligence, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, Shanghai Jiao Tong University, Shanghai, China",2019 9th International IEEE/EMBS Conference on Neural Engineering (NER),"20 May 2019","2019","","","235","238","Previous studies on EEG-based emotion recognition mainly focus on single-channel analysis, which neglect the functional connectivity between different EEG channels. This paper aims to explore the emotion associated functional brain connectivity patterns among different subjects. We proposed a critical subnetwork selection approach and extracted three topological features (strength, clustering coefficient, and eigenvector centrality) based on the constructed brain connectivity networks. The experimental results of 5-fold cross validation on a public emotion EEG dataset called SEED indicate that the common connectivity patterns associated with different emotions do exist, where the coherence connectivity is significantly higher at frontal site in the alpha, beta and gamma bands for the happy emotion, at parietal and occipital sites in the delta band for the sad emotion, and at frontal site in the delta band for the neutral emotion. In addition, the results demonstrate that the topological features considerably outperform the conventional power spectral density feature, and the decision-level fusion strategy achieves the best classification accuracy of 87.04% and the corresponding improvement of 3.78% in comparison with the state-of-the-art using the differential entropy feature on the same dataset.","1948-3554","978-1-5386-7921-0","10.1109/NER.2019.8717035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8717035","","Electroencephalography;Coherence;Feature extraction;Correlation;Training;Indexes;Emotion recognition","","44","","11","IEEE","20 May 2019","","","IEEE","IEEE Conferences"
"Hierarchical Attention-Based Temporal Convolutional Networks for Eeg-Based Emotion Recognition","C. Li; B. Chen; Z. Zhao; N. Cummins; B. W. Schuller","College of Computer and Information Engineering, Tianjin Normal University, China; College of Computer and Information Engineering, Tianjin Normal University, China; College of Computer and Information Engineering, Tianjin Normal University, China; Department of Biostatistics and Health Informatics, IoPPN, Kings College London, UK; College of Computer and Information Engineering, Tianjin Normal University, China","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","1240","1244","EEG-based emotion recognition is an effective way to infer the inner emotional state of human beings. Recently, deep learning methods, particularly long short-term memory recurrent neural networks (LSTM-RNNs), have made encouraging progress for in the field of emotion recognition. However, the LSTM-RNNs are time-consuming and have difficulty avoiding the problem of exploding/vanishing gradients when during training. In addition, EEG-based emotion recognition often suffers due to the existence of silent and emotional irrelevant frames from intra-channel. Not all channels carry the same emotional discriminative information. In order to tackle these problems, a hierarchical attention-based temporal convolutional networks (HATCN) for efficient EEG-based emotion recognition is proposed. Firstly, a spectrogram representation is generated from raw EEG signals in each channel to capture their time and frequency information. Secondly, temporal convolutional networks (TCNs) are utilised to automatically learn more robust/intrinsic long-term dynamic characters in emotion response. Next, a hierarchical attention mechanism is investigated that aggregates the emotional information at both the frame and channel level. The experimental results on the DEAP dataset show that our method achieves an average recognition accuracy of 0.716 and an F1-score of 0.642 over four emotional dimensions and outperforms other state-of-the-art methods in a user-independent scenario.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413635","National Natural Science Foundation of China; National Science Fund for Distinguished Young Scholars; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413635","emotion recognition;EEG signals;temporal convolutional networks;hierarchical attention mechanism","Training;Emotion recognition;Time-frequency analysis;Recurrent neural networks;Convolution;Speech recognition;Brain modeling","","25","","20","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Spatial Feature Extraction based EEG Stress Detection System: Review","K. R. Hole; D. Anand","School of Computer Science, Engineering Lovely Professional University; School of Computer Science, Engineering Lovely Professional University",2022 4th International Conference on Inventive Research in Computing Applications (ICIRCA),"29 Dec 2022","2022","","","1","10","In every country, stress is faced by individuals. Stress is a common issue faced across the globe due to employment issues, disappointments, employment conditions, etc. Short-term stress can be beneficial. However, long-term stress affects individual health to a great extent, such as cardiovascular disease, heart disease, high blood pressure, heart attack, and stroke. It can also lead to depression, anxiety and personality disorders. Consequently, stress recognition becomes helpful in controlling health-related issues generated from stress. Stress can be measured and evaluated dependent on perceptual, conduct and physiological reactions. A few researchers have proposed different methodologies based on feature extraction and classification techniques. It is established that some of these techniques are intricate in their applicability, and they give less precise outcomes in human stress analysis. Hence, there is a need for a system which is accurate, detailed, and reliable. With this motivation to achieve a more precise and reliable system, this research aims to detect real-time stress based on Electroencephalography (EEG) signals. EEG signals serve as a reliable tool to measure pressure using non-invasive ways. Selection of the most appropriate feature extraction method plays an important role in accurate classification.","","978-1-6654-9707-7","10.1109/ICIRCA54612.2022.9985679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985679","Signal processing;Brain-Computer Interface (BCI);Electroencephalography (EEG);Emotion detection;Stress detection","Anxiety disorders;Employment;Signal processing algorithms;Stroke (medical condition);Feature extraction;Electroencephalography;Reliability","","1","","46","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"Reservoir Splitting method for EEG-based Emotion Recognition","Anubhav; K. Fujiwara","Dept. of Mathematical Informatics, The University of Tokyo, Tokyo, Japan; Dept. of Mathematical Informatics, The University of Tokyo, Tokyo, Japan",2023 11th International Winter Conference on Brain-Computer Interface (BCI),"28 Mar 2023","2023","","","1","5","This paper presents a novel reservoir splitting method to train an efficient Reservoir Computing model for Emotion Recognition using Electroencephalogram (EEG) signals. Since different brain lobes have distinct functions and combining these functions results in the final decision, we propose splitting a single reservoir into multiple reservoirs dedicated to distinct lobes and integrating them to imitate the human brain functioning. We utilise the EEG signals from the publicly available GAMEEMO dataset for experiments. EEG signals are input to the single reservoir and proposed multiple reservoir configurations to obtain representations. Various classifiers: Ridge regression, Support Vector Machine (SVM), Gradient Boosted Classifier (GBC), and Random Forest are trained on the representations obtained from the reservoirs. We follow Leave-One-Subject-Out (LOSO) strategy to train these classifiers. Comparing the classification accuracy, we notice that representations from the proposed models of multiple reservoirs outperform the single reservoir model, and SVM performs better than the other classifiers. Furthermore, on increasing the reservoir size, the testing accuracy for all the classifiers attains a peak for both the Valence and Arousal domain. The proposed reservoir splitting method can be extended to explore diverse splitting configurations for future work.","2572-7672","978-1-6654-6444-4","10.1109/BCI57258.2023.10078629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10078629","EEG;Emotion Recognition;Reservoir Computing;Echo State Networks","Support vector machines;Emotion recognition;Computational modeling;Reservoirs;Brain modeling;Electroencephalography;Brain-computer interfaces","","1","","16","IEEE","28 Mar 2023","","","IEEE","IEEE Conferences"
"Emotion Decoding: An Extensive Examination of Electroencephalogram Signals Using Explainable Machine Learning","A. Nag; H. Mondal; S. R. Raihan Kabir; M. R. Islam; M. E. Ahmed; S. M. Hasan Jamil","Computer Science and Engineering Discipline, Khulna University, Khulna, Bangladesh; Department of Computer Science and Engineering, North Western University, Khulna, Bangladesh; Computer Science and Engineering Discipline, Khulna University, Khulna, Bangladesh; Department of Biomedical Engineering, Islamic University, Kushtia, Bangladesh; Computer Science and Engineering Discipline, Khulna University, Khulna, Bangladesh; Department of Computer Science and Engineering, North Western University, Khulna, Bangladesh",2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT),"23 May 2024","2024","","","604","609","Emotion plays a substantial impact on an individual's mental processes and social interactions. It functions as a connection between an individual's emotions and their behaviors, or it may be stated that it occasionally impacts one's life choices. Given the individual variations in emotional patterns and their manifestations, the investigation of these phenomena should rely on methodologies that are applicable across diverse populations. In order to improve accuracy and pick specific features, the process of emotion detection utilizing brain waves or EEG data necessitates the utilization of effective signal processing techniques. Researchers have been working on several methods of human-machine interface technologies for a considerable period. In recent years, they have achieved significant progress in the automated comprehension of emotions through brain signals. Our research involved the classification and testing of three emotional states using EEG signals collected from the widely accessible EEG Brainwave Dataset: Feeling Emotions from kaggle, utilizing seven machine learning techniques. This study presented a methodology that employed machine learning to identify emotions using the EEG Brainwave dataset. The study also conducted a thorough evaluation of several machine learning algorithms to assess their overall effectiveness. The study employed the Local Interpretable Model-Agnostic Explanations (LIME) to generate interpretable predictions and gain insights into the elements that influence the model's predictions. Despite the interpretability of the ML models, the XGBoost classifier had the greatest accuracy of 99% in recognition.","2769-5700","979-8-3503-8577-9","10.1109/ICEEICT62016.2024.10534518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534518","EEG;Emotion Recognition;Machine Learning;XGBoost;LIME","Support vector machines;Emotion recognition;Sociology;Predictive models;Signal processing;Brain modeling;Feature extraction","","1","","16","IEEE","23 May 2024","","","IEEE","IEEE Conferences"
"Unsupervised Learning in Reservoir Computing for EEG-Based Emotion Recognition","R. Fourati; B. Ammar; J. Sanchez-Medina; A. M. Alimi","Department of Computer Engineering and Applied Mathematics, National Engineering School of Sfax, Research Groups in Intelligent Machines, University of Sfax, Sfax, Tunisia; Department of Computer Engineering and Applied Mathematics, National Engineering School of Sfax, Research Groups in Intelligent Machines, University of Sfax, Sfax, Tunisia; Innovation Center for the Information Society, University of Las Palmas de Gran Canaria, Las Palmas, Spain; Department of Computer Engineering and Applied Mathematics, National Engineering School of Sfax, Research Groups in Intelligent Machines, University of Sfax, Sfax, Tunisia",IEEE Transactions on Affective Computing,"1 Jun 2022","2022","13","2","972","984","In real-world applications such as emotion recognition from recorded brain activity, data are captured from electrodes over time. These signals constitute a multidimensional time series. In this article, Echo State Network (ESN), a recurrent neural network with great success in time series prediction and classification, is optimized with different neural plasticity rules for classification of emotions based on electroencephalogram (EEG) time series. The developed network could automatically extract valid features from EEG signals. We use the filtered signals as the network input and do not take any feature extraction methods. Evaluated on two well-known benchmarks, the DEAP dataset, and the SEED dataset, the performance of the ESN with intrinsic plasticity greatly outperforms the feature-based methods and shows certain advantages compared with other existing methods. Thus, the proposed network can form a more complete and efficient representation, whilst retaining the advantages such as faster learning speed and more reliable performance.","1949-3045","","10.1109/TAFFC.2020.2982143","Ministry of Higher Education and Scientific Research of Tunisia(grant numbers:LR11ES48); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043472","Emotion recognition;electroencephalogram;echo state network;feature learning;synaptic plasticity;intrinsic plasticity","Feature extraction;Electroencephalography;Emotion recognition;Reservoirs;Brain modeling;Task analysis;Erbium","","34","","62","IEEE","20 Mar 2020","","","IEEE","IEEE Journals"
"EEG Emotion Classification Based On Baseline Strategy","J. Xu; F. Ren; Y. Bao","School of Computer and Information, Hefei University of Technology, Hefei 230009, China; Graduate School of Advanced Technology & Science, University of Tokushima, Tokushima 7708502, Japan; School of Computer and Information, Hefei University of Technology, Hefei 230009, China",2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS),"14 Apr 2019","2018","","","43","46","Electroencephalograph (EEG) emotional computing, as an important task of pattern recognition, has received more and more attention in recent years and is widely used in human-computer interaction, emotional computing and medical fields. Most researches have focused on finding particularly effective features and classifiers to achieve higher classification accuracy in some cases, while most methods are only effective under specific tasks or data and lack broad applicability. In this paper, we propose a novel baseline strategy that introducing emotional features to acquire a newly generated baseline and then, calibrate the individualized features in the emotional features, so that decrease the experimental errors and improve the versatility and effectiveness of the classification method. We performed a classification comparison experiment with baseline-strategy and no baseline-strategy on the DEAP dataset. The selected methods adopt different power spectral density (PSD) feature extraction methods and are classified by Support Vector Machine (SVM) and convolutional neural network (CNN) respectively. The results showed that the experiments with the baseline-strategy achieved better classification results.","","978-1-5386-6005-8","10.1109/CCIS.2018.8691174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691174","EEG;baseline strategy;PSD;SVM;CNN","","","8","","21","IEEE","14 Apr 2019","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Optimum Number of Channels","A. K. Dwivedi; O. P. Verma; S. Taran","Department of Electronics and Communication, Delhi Technological University, Delhi, India; Department of Electronics and Communication, Delhi Technological University, Delhi, India; Department of Electronics and Communication, Delhi Technological University, Delhi, India",2024 IEEE International Conference on Smart Power Control and Renewable Energy (ICSPCRE),"25 Nov 2024","2024","","","1","6","Electroencephalography (EEG) signals can detect an individual's emotional state, as emotions significantly influence communication. Brain-computer interfaces, or BCIs, use EEG signals to improve human-machine communication. By deciphering EEG data and applying it to various scenarios, emotion detection via EEG signals can efficiently ascertain the activity of particular brain areas. Using all EEG channels for categorization may lead to feature confusion and redundancy due to the specificity of these activation zones. This paper suggests a novel technique that uses the fewest possible EEG channels to accurately identify emotions. The suggested approach uses Pearson's correlation coefficient cutoff value to filter electrodes pertinent to emotion classification. The EEG data from the filtered electrodes is decomposed into sub-bands using Volt-Kalman order filtering. The decomposed EEG sub-bands are used to extract certain statistical properties, such as mean, skewness, kurtosis, and entropy. The MRMR (Minimum Redundancy Maximum Importance) algorithm was used to choose these features based on their importance. Several classification techniques were used to classify the retrieved information into four emotions further. The suggested framework outperforms both deep and shallow machine learning techniques, with cross-validation results demonstrating an 88% classification accuracy. A statistical analysis validates the importance of certain variables for emotion classification by confirming their significance with p-values < 0.05.","","979-8-3503-7700-2","10.1109/ICSPCRE62303.2024.10675012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10675012","EEG;Pearson's Correlation Coefficient;Volt Kalman Filtering Algorithm;Machine Learning Algorithms","Electrodes;Correlation coefficient;Emotion recognition;Accuracy;Filtering;Redundancy;Filtering algorithms","","","","22","IEEE","25 Nov 2024","","","IEEE","IEEE Conferences"
"Optimized Artifical Neural Network For EEG-Based Emotion Recognition","J. Khubani; S. Kulkarni","Department of Instrumentation Engineering, Ramrao Adik Institute of Technology, Navi Mumbai, India; Department of Instrumentation Engineering, Ramrao Adik Institute of Technology, Navi Mumbai, India",2022 6th International Conference on Computing Methodologies and Communication (ICCMC),"13 Apr 2022","2022","","","1624","1633","The Emotion provides a great impact in cognitive activities of the humans, such as communication and interaction. The emotion recognition is the hot research topic as the emotion plays a significant role in artificial human computer interaction. The EEG is considered as the best modality considered for the emotion recognition, as it directly receives the signal from the human brain. Hence, in this research a novel optimization for the efficient emotion recognition based on the EEG signals is proposed. The classification of the emotion process is carried out by the Optimized Artificial Neural Network, in which the Brain storm Optimization (BSO) is employed for tuning the hyper parameter. Further, the process, such as artifacts removal, feature extraction, electrode selection enhances the accuracy of the classifier. The experimental results show that the proposed EEG based emotion recognition process attains maximum accuracy, sensitivity and specificity of 94.23% , 93.83% and 95.60% respectively.","","978-1-6654-1028-1","10.1109/ICCMC53470.2022.9754136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754136","Emotion recognition;EEG signals;deep learning;Artificial Neural Network;BSO","Electrodes;Emotion recognition;Storms;Artificial neural networks;Sensitivity and specificity;Electroencephalography;Classification algorithms","","","","26","IEEE","13 Apr 2022","","","IEEE","IEEE Conferences"
"A novel approach for analyzing human emotions based on electroencephalography (EEG)","P. Lahane; M. Thirugnanam","School of Computer Science and Engineering, VIT University, Vellore, TN, India; School of Computer Science and Engineering, VIT University, Vellore, TN, India",2017 Innovations in Power and Advanced Computing Technologies (i-PACT),"4 Jan 2018","2017","","","1","6","In our day-to-day life Emotions play an vital role. They help in identifying a human state of mind. Information about the human state of mind can significantly help in human-machine interaction and brain-computer interface. Some of the existing researchers have used speech, text, gesture or facial expressions for emotion recognition. However, these factors vary across culture and nation. Because of which, it is difficult to detect emotions more accurately. Hence, present work considers EEG signals for emotion recognition which not only ignores external factors but also helps to detect real emotions arising directly from our brain. A benchmark DEAP (Dataset for Emotion Analysis using Physiological signals) dataset is used for emotion investigation. A novel feature extraction technique called Frequency Cepstral Coefficient (FCC) is proposed to extract features from DEAP dataset. FCC technique is compared with Kernel Density Estimation (KDE) on DEAP dataset for feature extraction. These extracted features are then classified into two emotional states - happy and sad using K-Nearest Neighbor (K-NN) classifier. The selection of most appropriate and reliable method of feature extraction greatly helps for accurate classification. Number of experiments was conducted to evaluate the efficiency of feature extraction techniques. The experimental results show that KDE gives 80% accuracy and FCC outperforms it by achieving 90% accuracy on DEAP.","","978-1-5090-5682-8","10.1109/IPACT.2017.8245056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8245056","Electroencephalography (EEG);DEAP;Feature extraction;Frequency Cepstral Coefficient (FCC);Kernel Density Estimation (KDE);k-nearest neighbor","Feature extraction;Electroencephalography;FCC;Emotion recognition;Filter banks;Cepstral analysis;Estimation","","9","","29","IEEE","4 Jan 2018","","","IEEE","IEEE Conferences"
"ADAPTER: Auto-Augmentation for Emotion Recognition in EEG - A Class and Subject Invariant Approach","C. Ahuja; D. Setia","Computer Science, Delhi Technological University, Delhi, India; Computer Science, Delhi Technological University, Delhi, India",2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),"23 Nov 2023","2023","","","1","6","Data Augmentation is essential to machine learning pipelines, particularly for developing robust models in emotion recognition tasks using EEG signals. The low signal-to-noise ratio of EEG signals and limitations on collecting large EEG datasets make data augmentation one of the easiest and most effective techniques for developing real-world emotion recognition models. It addresses class imbalance, distribution shifts, and lack of data, allowing the model to generalise and reduce overfitting. This paper introduces ADAPTER (Automatic Data Augmentation and PerTubation for Emotion Recognition), a novel automatic data augmentation strategy for emotion recognition using EEG signal classification. ADAPTER finds the best policy of geometric transformations across multiple classes in the classification dataset, such as SEED, without manually selecting from the pool of different transformations. It is tailored to be class and subject-invariant, ensuring that the resulting models are robust and generalisable across various tasks and subjects. The proposed technique outperforms the existing state-of-the-art model for SEED by 5% in terms of accuracy and 2% macro F1-score. ADAPTER showcases the potential of a flexible and adaptable data augmentation approach in enhancing the performance of EEG-based emotion recognition models while maintaining interoperability and control over the augmentation process.","2473-7674","979-8-3503-3509-5","10.1109/ICCCNT56998.2023.10308153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308153","EEG;Emotion Recognition;SEED;Data Augmentation;Reinforcement Learning;AutoML;AutoAugment","Emotion recognition;Adaptation models;Pipelines;Process control;Brain modeling;Data augmentation;Electroencephalography","","","","28","IEEE","23 Nov 2023","","","IEEE","IEEE Conferences"
"Emotion Recognition System Using Deep Learning Algorithm","A. Doe; M. Abdulgader; K. C. Nwosu; I. O. Osunmakinde","Department of Computer Science, Norfolk State University, Norfolk, VA, USA; Department of Computer Science, Norfolk State University, Norfolk, VA, USA; Department of Computer Science, Norfolk State University, Norfolk, VA, USA; Department of Computer Science, Norfolk State University, Norfolk, VA, USA",2024 International Conference on Computer and Applications (ICCA),"26 Mar 2025","2024","","","1","8","Emotions form a vital part of communication between humans. Being able to express emotions and the ability to recognize the emotions of others is essential for communication. Unfortunately, not everybody can express emotions in the most obvious ways, and this makes it difficult for others to recognize and understand how these people feel at given points in time. Some autistic patients and people with severe cases of Parkinson's disease are unable to express emotions through regular facial expressions, hindering communication with health practitioners and close relatives. Machine Learning, a subset of artificial intelligence, has over the years proven to be efficient over traditional software development methods in dealing with the dynamic nature of real-world problems, and given the right datasets, machine learning models can be trained to learn unique patterns and traits specific to different models which can ultimately lead to the prediction of emotions. This paper presents a machine-learning model for emotion recognition using brain signals collected through electroencephalography (EEG). The study utilizes the GAMEEMO dataset, which includes EEG data collected from 28 subjects playing different video games designed to evoke various emotions. Five models were imple-mented-deep Learning, Support Vector Machines, K-nearest neighbors, Decision Trees, and multiple linear regression-where the Deep Learning model achieved the best performance with a 96% testing accuracy. The results were validated using 10-fold cross-validation, confirming the robustness of the model.","","979-8-3503-6756-0","10.1109/ICCA62237.2024.10927877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10927877","Deep Learning;Emotion;Neural Network;Elec-troencephalography;Recognition","Deep learning;Support vector machines;Emotion recognition;Video games;Computational modeling;Predictive models;Brain modeling;Electroencephalography;Testing;Software development management","","","","26","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"EEG Interpretation for Healthcare: Emotional Analysis Using GRU and Activity Tracking with Epilepsy Diagnosis using Random Forest","J. L. Aldo Stalin; J. Jeba Emilyn; P. Monish; B. Jagan; P. Giridharan; K. Mukesh","Department of Information Technology, Sona College of Technology, Salem, India; Department of Information Technology, Sona College of Technology, Salem, India; Department of Information Technology, Sona College of Technology, Salem, India; Department of Information Technology, Sona College of Technology, Salem, India; Department of Information Technology, Sona College of Technology, Salem, India; Department of Information Technology, Sona College of Technology, Salem, India",2024 International Conference on Computing and Intelligent Reality Technologies (ICCIRT),"18 Mar 2025","2024","","","314","319","This paper deals with the design of a multi-functional system for EEG signal interpretation, with emphasis on emotional analysis and monitoring activities along with epilepsy diagnosis. The primary module is realized using the deep learning-based Gated Recurrent Units model in which EEG signals fall under several modes of the emotional state, like Sleep/Unconscious, Calm, and Active/Stress. We include a preprocessing step using FFT that will extract features from the frequency domain to enhance model performance. We also use Keras Tuner for hyperparameter tuning, as in the number of units in GRU and the learning rate, to get 83% accuracy. The second module for this system is a classifier using a Random Forest for monitoring activity and diagnosis of epilepsy. 11 activities from EEG signals, including walking and sitting are recognized during the day while determining whether they are epileptic or not. The extraction of statistical features along with frequency domain characteristics from the EEG data indicates an exceptionally high efficacy of Random Forest in the diagnosis of epilepsy, which has gained an accuracy rate of 83%. The datasets that will be utilized will comprise EEG recordings from healthy subjects and those who have a diagnosed case of epilepsy. This methodology utilizes statistical and frequency characteristics for identifying intricate patterns and, therefore, making possible upgraded predictions of emotional conditions and medical diagnoses.","","979-8-3315-1029-9","10.1109/ICCIRT59484.2024.10921934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10921934","GRU-based EEG Emotion Classifier;FFT-enhanced Feature Extraction;Keras Tuner Hyperparameter Optimization;Random Forest Activity Recognition;EEG-based Epilepsy Detection;Frequency and Statistical Feature Fusion;Single Electrode EEG Signal Analysis;Time-series EEG Emotion Analysis","Time-frequency analysis;Accuracy;Tuners;Fast Fourier transforms;Epilepsy;Feature extraction;Brain modeling;Electroencephalography;Random forests;Monitoring","","","","16","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Emotion recognition from EEG signals using back propagation neural network","R. Ghosh; N. Sinha; N. Singh","Dept. of Computer Science and Engineering, NIT Silchar, India; Dept. of Electrical Engineering, NIT Silchar, India; Dept. of Computer Science and Engineering, NIT Silchar, India","2019 2nd International Conference on Innovations in Electronics, Signal Processing and Communication (IESC)","18 Nov 2019","2019","","","188","191","The proposed work presents a discrete wavelet based feature extraction method in conjunction with a back propagation neural network (BPNN) for the classification of emotions from EEG recordings. The EEG recordings were used from DEAP dataset (A Database for emotion analysis using physiological signals) for evaluation of the method. Russell’s model was used for quantification of emotions in valence and arousal dimension space. The emotions were classified into four classes, namely – high arousal positive valence (HAPV), high arousal negative valence (HANV), low arousal positive valence (LAPV) and low arousal negative valence (LANV). We have used three mother wavelets Sym8, Coif5 and Db8 for extracting four features, namely entropy, mean, standard deviation and variance. Classification of the emotions was done using BPNN. An average classification accuracy of 57.9% was obtained in the classification of the above mentioned four classes using different mother wavelets.","","978-1-7281-0744-8","10.1109/IESPC.2019.8902418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902418","Back Propagation Neural Network (BPNN);Brain Computer Interface (BCI);EEG;Emotion Recognition;Russell’s Model;Wavelet Transform","","","1","","18","IEEE","18 Nov 2019","","","IEEE","IEEE Conferences"
"Optimizing Emotion Recognition in EEG Data: A Genetic Algorithm Approach with XAI Insights","N. Ali; M. Asif; A. Kaushal; U. Singh; U. S. Tiwary","Department of Electronics and Communication Engineering UIET H, Panjab University, Chandigarh, Panjab; Department of Information Technology, Indian Institute of Information Technology, Prayagraj, Allahabad, India; Department of Computer Science and Engineering UIET H, Panjab University, Chandigarh, Panjab; Department of Information Technology, Indian Institute of Information Technology, Prayagraj, Allahabad, India; Department of Information Technology, Indian Institute of Information Technology, Prayagraj, Allahabad, India",2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"4 Nov 2024","2024","","","1","6","Emotion recognition using electroencephalogram data presents a burgeoning area of research, yet navigates through intricate optimization hurdles, alongside the persistent challenge of rendering results interpretable. In this study, we employed a combination of Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks and utilized a Genetic Algorithm (GA) for optimization and enhancing the model’s performance and robustness in deciphering emotional states from EEG signals. Our methodology encompasses data pre-processing techniques, including Short-Time Fourier Transform (STFT) analysis, applied to EEG data for feature extraction and GA-driven hyperparameter optimization to identify an optimal neural network architecture. This architecture, consisting of Convolutional and Recurrent layers with dropout regularization, is adept at extracting temporal and spatial features from EEG signals while mitigating overfitting. Furthermore, we investigate explainable AI (XAI) strategies to get insight into the decision-making process of our GA-based optimized model. Additionally, rigorous cross-validation ensures the generalization performance of the optimized model across diverse datasets. Empirical results demonstrate the effectiveness of our approach, with the optimized CNN-LSTM hybrid model achieving an accuracy of $\mathbf{9 3. 2 8 \%}$ in classifying 24 different emotions. This study enhances our understanding of emotion recognition systems by examining the intricate interplay between EEG data analysis, CNN-LSTM networks, and Genetic Algorithm optimization. Additionally, it provides practical insights into the optimization of such systems, potentially influencing future advancements in affective computing technologies.","2473-7674","979-8-3503-7024-9","10.1109/ICCCNT61001.2024.10725514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10725514","CNN-LSTM;Genetic Algorithm (GA);Explainable AI;EEG;Emotions Recognition;Optimization","Emotion recognition;Explainable AI;Computer architecture;Brain modeling;Feature extraction;Electroencephalography;Convolutional neural networks;Optimization;Long short term memory;Genetic algorithms","","1","","28","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"Exploring Deep Learning Features for Automatic Classification of Human Emotion Using EEG Rhythms","F. Demir; N. Sobahi; S. Siuly; A. Sengur","Department of Electrical and Electronics Engineering, Faculty of Technology, Firat University, Elâziğ, Turkey; Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, Saudi Arabia; Institute for Sustainable Industries and Liveable Cities, Victoria University, Footscray, VIC, Australia; Department of Electrical and Electronics Engineering, Faculty of Technology, Firat University, Elâziğ, Turkey",IEEE Sensors Journal,"30 Jun 2021","2021","21","13","14923","14930","Emotion recognition (ER) from Electroencephalogram (EEG) signals is a challenging task due to the non-linearity and non-stationarity nature of EEG signals. Existing feature extraction methods cannot extract the deep concealed characteristics of EEG signals from different layers for efficient classification scheme and also hard to select appropriate and effective feature extraction methods for different types of EEG data. Hence this study intends to develop an efficient deep feature extraction based method to automatically classify emotion status of people. In order to discover reliable deep features, five deep convolutional neural networks (CNN) models are considered: AlexNet, VGG16, ResNet50, SqueezeNet and MobilNetv2. Pre-processing, Wavelet Transform (WT), and Continuous Wavelet Transform (CWT) are employed to convert the EEG signals into EEG rhythm images then five well-known pretrained CNN models are employed for feature extraction. Finally, the proposed method puts the obtained features as input to the support vector machine (SVM) method for classifying them into binary emotion classes: valence and arousal classes. The DEAP dataset was used in experimental works. The experimental results demonstrate that the AlexNet features with Alpha rhythm produces better accuracy scores (91.07% in channel Oz) than the other deep features for the valence discrimination, and the MobilNetv2 features yields the highest accuracy score (98.93% in Delta rhythm (with channel C3) for arousal discrimination.","1558-1748","","10.1109/JSEN.2021.3070373","Deanship of Scientific Research (DSR), at King Abdulaziz University, Jeddah(grant numbers:498-135-1442); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9393369","EEG based emotion classification;EEG rhythms;CWT;deep features;pretrained CNN models","Electroencephalography;Feature extraction;Support vector machines;Erbium;Brain modeling;Continuous wavelet transforms;Filtering","","66","","35","IEEE","1 Apr 2021","","","IEEE","IEEE Journals"
"Emotion Recognition from RGB Kinect Videos and Physiological Signals Using Pre-Trained CNN Model","S. Anand; L. Mookiah; B. V S; S. S. Surakshitha","Department of Computing Technologies, SRM Institute of Science and Technology, Kattankalathur, India; Independent Researcher, Nevada, USA; Department of Computing Technologies, SRM Institute of Science and Technology, Kattankalathur, India; Department of Computing Technologies, SRM Institute of Science and Technology, Kattankalathur, India",2024 International Conference on Intelligent Computing and Next Generation Networks (ICNGN),"14 Feb 2025","2024","","","01","05","This study presents an innovative approach to emotion recognition by integrating RGB Kinect video data with physiological signals, including electroencephalography (EEG), electrocardiography (ECG), and galvanic skin response (GSR). Using the AMIGOS dataset, which includes video recordings and self-assessment emotional ratings, we improve emotion prediction through a comprehensive multimodal analysis. Our method involves feature extraction and averaging from both video and physiological signals, emphasizing the connection between visual cues and emotional responses. A pre-trained ResNet model is used for advanced feature extraction from video data. Various machine learning algorithms are applied to evaluate predictive models against self-assessment labels. Results indicate that the combination of ResNet-based visual features and physiological signals enhances emotion prediction accuracy, underscoring the value of a multimodal approach in capturing complex emotional dynamics.","","979-8-3315-2922-2","10.1109/ICNGN63705.2024.10871530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10871530","ResNet;Electroencephalography;Electrocardio-graphy;Galvanic Skin Response;Self assessment;RGB Kinect Videos","Visualization;Emotion recognition;Machine learning algorithms;Predictive models;Electrocardiography;Brain modeling;Feature extraction;Physiology;Data models;Skin","","","","16","IEEE","14 Feb 2025","","","IEEE","IEEE Conferences"
"Feature Extraction and Selection for Emotion Recognition from EEG","R. Jenke; A. Peer; M. Buss","Institute of Automatic Control Engineering, Technische Universität München, Munich, Germany; Institute of Advanced Studies, Technische Universität München, Munich, Germany; Institute of Automatic Control Engineering, Technische Universität München, Munich, Germany",IEEE Transactions on Affective Computing,"20 May 2017","2014","5","3","327","339","Emotion recognition from EEG signals allows the direct assessment of the “inner” state of a user, which is considered an important factor in human-machine-interaction. Many methods for feature extraction have been studied and the selection of both appropriate features and electrode locations is usually based on neuro-scientific findings. Their suitability for emotion recognition, however, has been tested using a small amount of distinct feature sets and on different, usually small data sets. A major limitation is that no systematic comparison of features exists. Therefore, we review feature extraction methods for emotion recognition from EEG based on 33 studies. An experiment is conducted comparing these features using machine learning techniques for feature selection on a self recorded data set. Results are presented with respect to performance of different feature selection methods, usage of selected feature types, and selection of electrode locations. Features selected by multivariate methods slightly outperform univariate methods. Advanced feature extraction techniques are found to have advantages over commonly used spectral power bands. Results also suggest preference to locations over parietal and centro-parietal lobes.","1949-3045","","10.1109/TAFFC.2014.2339834","VERE(grant numbers:ICT-2010-257695); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6858031","Emotion recognition;EEG;feature extraction;feature selection;electrode selection;machine learning","Electrodes;Feature extraction;Electroencephalography;Emotion recognition;Discrete wavelet transforms;Time-frequency analysis","","752","","63","IEEE","17 Jul 2014","","","IEEE","IEEE Journals"
"EEG-Based Emotion Classification Using Graph Signal Processing","S. S. Saboksayr; G. Mateos; M. Cetin","Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA; Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA; Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","1065","1069","The key role of emotions in human life is undeniable. The question of whether there exists a brain pattern associated with a specific emotion is the theme of many affective neuroscience studies. In this work, we bring to bear graph signal processing (GSP) techniques to tackle the problem of automatic emotion recognition using brain signals. GSP is an extension of classical signal processing methods to complex networks where there exists an inherent relation graph. With the help of GSP, we propose a new framework for learning class-specific discriminative graphs. To that end, firstly we assume for each class of observations there exists a latent underlying graph representation. Secondly, we consider the observations are smooth on their corresponding class-specific sough graph while they are non-smooth on other classes’ graphs. The learned class-specific graph-based representations can act as sub-dictionaries and be utilized for the task of emotion classification. Applying the proposed method on an electroencephalogram (EEG) emotion recognition dataset indicates the superiority of our framework over other state-of-the-art methods.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414342","Graph learning;graph signal processing;discriminative transform learning;emotion recognition;EEG","Emotion recognition;Neuroscience;Signal processing algorithms;Speech recognition;Transforms;Signal processing;Linear programming","","16","","25","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Spatial-Temporal Graph Convolutional LSTM With Attention Mechanism","L. Feng; C. Cheng; M. Zhao; H. Deng; Y. Zhang","Department of Computer Science and Technology, Dalian University of Technology, Dalian, China; Department of Computer Science and Technology, Dalian University of Technology, Dalian, China; Department of Computer Science and Technology, Dalian University of Technology, Dalian, China; Department of Computer Science and Technology, Dalian University of Technology, Dalian, China; School of Information Engineering, Huzhou University, Huzhou, China",IEEE Journal of Biomedical and Health Informatics,"10 Nov 2022","2022","26","11","5406","5417","The dynamic uncertain relationship among each brain region is a necessary factor that limits EEG-based emotion recognition. It is a thought-provoking problem to availably employ time-varying spatial and temporal characteristics from multi-channel electroencephalogram (EEG) signals. Although deep learning has made remarkable achievements in emotion recognition, the biological topological information among brain regions does not fully exploit, which is vital for EEG-based emotion recognition. In response to this problem, we design a hybrid model called ST-GCLSTM, which comprises a spatial-graph convolutional network (SGCN) module and an attention-enhanced bi-directional Long Short-Term Memory (LSTM) module. The main advantage of ST-GCLSTM is that it can consider the biological topology information of each brain region to extract representative spatial-temporal features from multiple EEG channels. Specifically, we construct two layers SGCN by introducing adjacency matrices to adaptively learn the intrinsic connection among different EEG channels. Moreover, an attention-enhanced mechanism is placed into a bi-directional LSTM module to extract the crucial spatial-temporal features from sequential EEG data, and then these features serve as the input layer of the classifier to learn discriminative emotion-related features. Extensive experiments on the DEAP, SEED, and SEED-IV datasets demonstrate the effectiveness of the proposed ST-GCLSTM model, revealing that our model had an absolute performance improvement over state-of-the-art strategies.","2168-2208","","10.1109/JBHI.2022.3198688","National Natural Science Foundation of China(grant numbers:61972064,61772252); Liaoning Revitalization Talents Program(grant numbers:XLYC1806006); Fundamental Research Funds for the Central Universities(grant numbers:DU19RC(3)012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857562","Attention-enhanced bi-directional long short-term memory;biological topology;electroencephalogram;emotion recognition;spatial-graph convolutional network","Electroencephalography;Emotion recognition;Feature extraction;Brain modeling;Bidirectional control;Data mining;Convolutional neural networks","Humans;Electroencephalography;Emotions;Brain","69","","66","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Annotation Efficiency in Multimodal Emotion Recognition with Deep Learning","L. Zhu; P. Spachos","School of Engineering, University of Guelph, Guelph, ON, Canada; School of Engineering, University of Guelph, Guelph, ON, Canada",GLOBECOM 2022 - 2022 IEEE Global Communications Conference,"11 Jan 2023","2022","","","560","565","In the fast pace of life, emotion recognition systems are essential to help monitor mental health and well-being. The continuous development of the Internet of Things (IoT) and Human-Computer Interaction (HCI) improve the availability and accessibility to devices that can capture the facial expressions of a user, while wearable devices can also capture physiological signals and use them for emotion recognition. Meanwhile, machine learning and deep learning methods can provide emotion prediction models. However, the training of the models relies heavily on massive amounts of labeled data. The accuracy of data labels affects the success of the overall system. Research targeting emotion recognition uses the participants' self-reports as labels. However, participants often fail to give accurate self-reports, thus affecting the accuracy of the analysis. In this study, we examine the performance of the self-reports and external annotations for emotion recognition based on visual and physiological signals. Specifically, we use video data, as well as the Electrodermal Activity (EDA), Electroencephalogram (EEG), and Electrocardiogram (ECG) signals collected from wearable devices. We use two machine learning and three deep learning methods to process the signals and train the classifiers. The results show that the classifiers trained with external annotations offer better emotion recognition accuracy than self-reports. Also, the classifiers trained on facial expression offer better emotion prediction accuracy than the physiological signals, and the Deep Convolutional Network model shows the best results.","2576-6813","978-1-6654-3540-6","10.1109/GLOBECOM48099.2022.10000909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10000909","Affective computing;emotion recognition;deep learning;physiological signals;electrodermal activity (EDA);electroencephalogram (EEG);electrocardiogram (ECG)","Deep learning;Support vector machines;Emotion recognition;Visualization;Annotations;Wearable computers;Electrocardiography","","2","","23","IEEE","11 Jan 2023","","","IEEE","IEEE Conferences"
"Robust EEG-based Emotion Recognition Using an Inception and Two-sided Perturbation Model","S. Sartipi; M. Cetin","Department of Electrical and Computer Engineering, University of Rochester, Rochester, USA; Department of Electrical and Computer Engineering, Goergen Institute for Data Science, University of Rochester, Rochester, USA",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","Automated emotion recognition using electroencephalogram (EEG) signals has gained substantial attention. Although deep learning approaches exhibit strong performance, they often suffer from vulnerabilities to various perturbations, like environmental noise and adversarial attacks. In this paper, we propose an Inception feature generator and two-sided perturbation (INC-TSP) approach to enhance emotion recognition in brain-computer interfaces. INC-TSP integrates the Inception module for EEG data analysis and employs two-sided perturbation (TSP) as a defensive mechanism against input perturbations. TSP introduces worst-case perturbations to the model’s weights and inputs, reinforcing the model’s elasticity against adversarial attacks. The proposed approach addresses the challenge of maintaining accurate emotion recognition in the presence of input uncertainties. We validate INC-TSP in a subject-independent three-class emotion recognition scenario, demonstrating robust performance.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782060","National Science Foundation; National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782060","Adversarial training;Brain-computer interfaces;Emotion recognition;Inception module;Weight perturbation","Threat modeling;Emotion recognition;Uncertainty;Accuracy;Perturbation methods;Working environment noise;Brain modeling;Robustness;Electroencephalography;Generators","Electroencephalography;Emotions;Humans;Algorithms;Brain-Computer Interfaces;Signal Processing, Computer-Assisted;Deep Learning","","","21","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Advancing Emotion Detection through Dual-Channel EEG and Peripheral Physiological Signal Integration","S. Roy; S. M. Band; P. Ramesh; P. Bhavsar; A. Jac Fredo; D. Kumar","School of Biomedical Engineering, Indian Institute of Technology (BHU), Varanasi, India; School of Biomedical Engineering, Indian Institute of Technology (BHU), Varanasi, India; Department of Electronics and Communication Engineering, National Institute of Technology, Tiruchirappalli, India; Department of Electronics and Communication Engineering, National Institute of Technology, Tiruchirappalli, India; School of Biomedical Engineering, Indian Institute of Technology (BHU), Varanasi, India; School of Biomedical Engineering, Indian Institute of Technology (BHU), Varanasi, India","2024 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)","18 Feb 2025","2024","","","105","109","Wearable devices often struggle to capture the full range of emotions accurately through peripheral physiological signals like Electrodermal Activity (EDA), Photoplethysmogram (PPG), and Electromyogram (EMG), leading to reduced classification accuracy. While Electroencephalogram (EEG) signals are recognized for their superior emotion detection capabilities, past EEG-based studies have been confined to lab settings with high-end systems using 32-64 channels. There is a growing need for systems that can continuously monitor emotions outside the lab. Modern wearable EEG devices, with configurations from single to eight channels, offer a solution. Our study investigates if integrating EEG data from only two channels in commercial wearable EEG devices with peripheral signals can enhance emotion detection accuracy. Using the publicly available DEAP dataset, which provides EEG and peripheral signals from 32 participants exposed to emotional stimuli, we explore various preprocessing and feature extraction techniques for multimodal emotion detection. The DEAP dataset categorizes emotions based on valence and arousal ratings, converting them into binary class problems: High Valence vs. Low Valence and High Arousal vs. Low Arousal. Intra-class emotion classification is performed using a Random Forest classifier with $\mathbf{5}$-fold cross-validation. Results show that while single-modality peripheral signals achieve classification accuracies of 76-85%, the addition of EEG data significantly boosts accuracy up to $\mathbf{9 5 \%}$. Combining EEG data with peripheral signals also greatly improves individual subject valence and arousal classification accuracy and F1 scores.","","979-8-3315-3331-1","10.1109/MoSICom63082.2024.10881605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10881605","EDA;EEG;EMG;Emotion recognition;PPG;Random Forest","Emotion recognition;Accuracy;Forestry;Feature extraction;Electroencephalography;Electromyography;Biomedical monitoring;Wearable devices;Random forests;Monitoring","","","","24","IEEE","18 Feb 2025","","","IEEE","IEEE Conferences"
"Emotion recognition from EEG brain signals based on particle swarm optimization and genetic search","R. M. Mehmood; H. J. Lee",Division of Computer Science and Engineering; Center for Advanced Image and Information Technology,2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),"26 Sep 2016","2016","","","1","5","The purpose of this study is to classify the emotions from human brain signals using electroencephalography (EEG). EEG signals were acquired while subject were watching the emotional stimuli. Subjects were asked to watch four types of emotional stimulus such as, happy, calm, sad and scared. The EEG signals were recorded using 14-channel brain headset. We preprocessed the EEG recorded data with manual artifact rejection and independent component analysis (ICA). The total numbers of 21 subjects were participated in this experiment. We performed emotion recognition which was based on two feature selection methods such as, particle swarm optimization (PSO) and genetic search (GS). Further, the selected features were processed using support vector machine (SVM). Emotion recognition accuracy had shown the possibility of classification of EEG brain activity.","","978-1-5090-1552-8","10.1109/ICMEW.2016.7574682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574682","EEG;SVM;hjorth;Particle Swarm Optimization;Genetic Search","Electroencephalography;Bars;Conferences;Monitoring","","4","","26","IEEE","26 Sep 2016","","","IEEE","IEEE Conferences"
"Emotion recognition from EEG signal using ISO-FLANN with firefly algorithm","J. Preethi; S. Sowmiya","Department of Computer Science, Anna University Regional Campus Coimbatore; Department of Computer Science, Anna University Regional Campus Coimbatore",2016 International Conference on Communication and Signal Processing (ICCSP),"24 Nov 2016","2016","","","1932","1936","Emotion has a greater importance in communication, perception and decision making. The ability to identify the emotional states of people surrounding us is an essential part of natural communication. Human brain activity is used to analyze the emotions in different situations and brain activity is measured by EEG signal. The existing system has concluded a 6 layer biologically tested feed forward neural network which is used to extract the human emotions through EEG. This system delivered to a series of functionalities such as preprocessing, feature extraction, classification. In first phase the EEG preprocessing is done by band pass filter. The connectivity feature is estimated by magnitude square coherence estimation (MSCE) in second phase. In third phase nonnegative sparse principal component analysis (NSPCA) and radial basis function are used for feature selection and classifications respectively. In order to achieve improved accuracy and high true positive rate the proposed system introduced a Firefly algorithm for feature selection and ISO-FLANN for classification. In this proposed system, optimal features are selected by using accuracy as an objective function. Finally the optimally selected features are then classified using an ISO-FLANN to discriminate various states of emotions. From a practical observation it conveys that the proposed system achieves higher accuracy when compared to the existing system.","","978-1-5090-0396-9","10.1109/ICCSP.2016.7754508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7754508","Emotions;EEG signal;feature selection and classification","Electroencephalography;Feature extraction;Classification algorithms;Band-pass filters;Emotion recognition;Principal component analysis;Algorithm design and analysis","","8","","8","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"EEG-Based Multimodal Emotion Recognition: A Machine Learning Perspective","H. Liu; T. Lou; Y. Zhang; Y. Wu; Y. Xiao; C. S. Jensen; D. Zhang","School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; School of Cyber Engineering, Xidian University, Xi’an, China; Department of Computer Science, Aalborg University, Aalborg, Denmark; Department of Computer Science, Aalborg University, Aalborg, Denmark",IEEE Transactions on Instrumentation and Measurement,"1 Mar 2024","2024","73","","1","29","Emotion, a fundamental trait of human beings, plays a pivotal role in shaping aspects of our lives, including our cognitive and perceptual abilities. Hence, emotion recognition also is central to human communication, decision-making, learning, and other activities. Emotion recognition from electroencephalography (EEG) signals has garnered substantial attention due to advantages such as noninvasiveness, high speed, and high temporal resolution; driven also by the complementarity between EEG and other physiological signals at revealing emotions, recent years have seen a surge in proposals for EEG-based multimodal emotion recognition (EMER). In short, EEG-based emotion recognition is a promising technology in medical measurements and health monitoring. While reviews exist, which explore emotion recognition from multimodal physiological signals, they focus mostly on general combinations of modalities and do not emphasize studies that center on EEG as the fundamental modality. Furthermore, existing reviews take a methodology-agnostic perspective, primarily concentrating on the biomedical basis or experimental paradigms, thereby giving little attention to the methodological characteristics unique to this field. To address these gaps, we present a comprehensive review of current EMER studies, with a focus on multimodal machine learning models. The review is structured around three key aspects: multimodal feature representation learning, multimodal physiological signal fusion, and incomplete multimodal learning models. In doing so, the review sheds light on the advances and challenges in the field of EMER, thus offering researchers who are new to the field a holistic understanding. The review also aims to provide valuable insight that may guide new research in this exciting and rapidly evolving field.","1557-9662","","10.1109/TIM.2024.3369130","National Natural Science Foundation of China(grant numbers:62202367,62192781); Innovation Research Team of the Ministry of Education(grant numbers:IRT_17R86); Project of China Knowledge Centre for Engineering Science and Technology; Project of Chinese Academy of Engineering “The Online and Offline Mixed Educational Service System for ‘the Belt and Road’ Training in MOOC China.”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10443946","Electroencephalography (EEG);emotion recognition;machine learning;multimodal learning;multimodal physiological signal","Electroencephalography;Emotion recognition;Physiology;Electromyography;Electrooculography;Reviews;Biomedical monitoring","","16","","219","IEEE","23 Feb 2024","","","IEEE","IEEE Journals"
"Exploring deep learning features for automatic classification of human emotion using EEG Rhythms","",,2020 International Conference on Advanced Science and Engineering (ICOASE),"31 May 2021","2020","","","217","218","Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Electroencephalogram (EEG) signal data are very crucial for understanding the emotions of humans that provides a way to control and regulate behaviors. The classification of emotion from EEG signals is challenging task due to the non-linearity and non-stationarity nature of EEG signals. Existing feature extraction methods cannot extract the deep concealed characteristics of EEG signals from different layers for efficient classification scheme and also hard to select appropriate and effective feature extraction methods for different types of EEG data. Hence, this study intends to develop an efficient deep feature extraction based model to automatically classify emotion status of people. In order to discover reliable deep features, this study investigates five deep learning convolutional neural networks (CNN) models: AlexNet, VGG16, ResNet50, SqueezeNet and MobilNetv2. The proposed scheme consists of several steps: pre-processing by using low-pass filtering for noise removing and Wavelet Transform (WT) for EEG rhythm extraction; converting the extracted EEG rhythm signals to the EEG rhythm images employing the Continuous Wavelet Transform (CWT) method; then feeding EEG rhythm images to deep five well-known pretrained aforementioned CNN models for feature extraction separately; putting the obtained features as input to support vector machine (SVM) method for classifying into binary emotion classes: valence and arousal classes. The proposed methodology is tested on “DEAP dataset”. The experimental results demonstrate that the AlexNet features with Alpha rhythm produces better accuracy scores (91.07% in channel Oz) than the other deep features for the valence discrimination, and the MobilNetv2 features yields the highest accuracy score (98.93% in Delta rhythm (with channel C3) for arousal discrimination.","","978-1-6654-1579-8","10.1109/ICOASE51841.2020.9436580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9436580","","","","","","","IEEE","31 May 2021","","","IEEE","IEEE Conferences"
"Comparative Analysis of EEG-based Emotion Recognition between Male and Female Participants Using Hjorth Parameter","N. Fatih; A. D. Wibawa; M. H. Purnomo; A. Mas","Department of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia; Department of Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia; Department of Electrical Engineering Department of Computer Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia; Department of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia",2023 International Electronics Symposium (IES),"12 Sep 2023","2023","","","179","185","In recent years, scientists have investigated the potential of EEG for identifying emotional states. Analyzing the patterns and frequencies of brainwave activity makes it possible to detect and classify different emotional states, such as happiness, sadness, fear, or anger. In daily life, it is evident that there are differences in how men and women think and approach various aspects, such as perceiving and experiencing emotions. Given the scarcity of research on gender-based EEG studies, it is crucial to explore this area to gain valuable insights into the unique neurological aspects of gender. This study examines gender disparities in emotion recognition using the Hjorth Parameter analysis. This study recorded the data of 24 healthy subjects (22± 5.6 years old) using a wireless OpenBCI EEG device. The participants were stimulated by seeing happy and sad videos. The electrodes used in this study were placed in the frontal area channels F7, F8, FP1, and FP2, following the internationally recognized 10/20 EEG system. EEG pre-processing techniques include signal filtering and ICA for Artifact removal. The EEG signals were separated into alpha, beta, and gamma sub-bands using Butterworth filters. The analysis was to compute the Hjorth parameters: in activity, mobility, and complexity. We used the Fisher ratio to identify the most influential features, then processed and analyzed them using SVM, KNN, Naive Bayes, and Random Forest algorithms for emotion classification. This study has shown that Hjorth Activity outperforms other Hjorth parameters, such as Hjorth mobility and complexity, in accurately recognizing emotions, and the beta subband is the most sensitive in distinguishing between happy and sad emotions. On the gender aspect, differences in emotional responses between men and women are evident. Women tend to exhibit more similar EEG patterns among themselves, while men show more significant individual differences. The other intriguing finding is that the value of the gamma subband is higher for both men and women in the sad condition, indicating that the cognitive process appears more evolved than in the happy condition.","2687-8909","979-8-3503-1473-1","10.1109/IES59143.2023.10242538","Ministry of Communication and Information Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10242538","Cognitive Process;EEG;Emotion Recognition;Fisher Ratio;Gamma Subband;Gender-based;Hjorth Parameter","Wireless communication;Support vector machines;Emotion recognition;Wireless sensor networks;Face recognition;Filtering algorithms;Electroencephalography","","1","","30","IEEE","12 Sep 2023","","","IEEE","IEEE Conferences"
"Differential entropy feature for EEG-based emotion classification","R. -N. Duan; J. -Y. Zhu; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2013 6th International IEEE/EMBS Conference on Neural Engineering (NER),"2 Jan 2014","2013","","","81","84","EEG-based emotion recognition has been studied for a long time. In this paper, a new effective EEG feature named differential entropy is proposed to represent the characteristics associated with emotional states. Differential entropy (DE) and its combination on symmetrical electrodes (Differential asymmetry, DASM; and rational asymmetry, RASM) are compared with traditional frequency domain feature (energy spectrum, ES). The average classification accuracies using features DE, DASM, RASM, and ES on EEG data collected in our experiment are 84.22%, 80.96%, 83.28%, and 76.56%, respectively. This result indicates that DE is more suited for emotion recognition than traditional feature, ES. It is also confirmed that EEG signals on frequency band Gamma relates to emotional states more closely than other frequency bands. Feature smoothing method- linear dynamical system (LDS), and feature selection algorithm- minimal-redundancy-maximal-relevance (MRMR) algorithm also help to increase the accuracies and efficiencies of EEG-based emotion classifiers.","1948-3554","978-1-4673-1969-0","10.1109/NER.2013.6695876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6695876","","Electroencephalography;Accuracy;Support vector machines;Emotion recognition;Smoothing methods;Motion pictures;Brain modeling","","659","","9","IEEE","2 Jan 2014","","","IEEE","IEEE Conferences"
"GFIL: A Unified Framework for the Importance Analysis of Features, Frequency Bands, and Channels in EEG-Based Emotion Recognition","Y. Peng; F. Qin; W. Kong; Y. Ge; F. Nie; A. Cichocki","School of Computer Science and Technology and the Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology and the Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou Dianzi University, Hangzhou, China; Key Laboratory of Advanced Perception and Intelligent Control of High-End Equipment of Ministry of Education, Anhui Polytechnic University, Wuhu, China; Center for Optical Imagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China; Center for Computational and Data-Intensive Science and Engineering, Skolkov Institute of Science and Technology, Moscow, Russia",IEEE Transactions on Cognitive and Developmental Systems,"8 Sep 2022","2022","14","3","935","947","Accurately and automatically recognizing the emotional states of human beings is the central task in affective computing. The electroencephalography (EEG) data, generated from the neural activities in brain cortex, provide us with a reliable data source to perform emotion recognition. Besides the recognition accuracy, it is also necessary to explore the importance of different EEG features, frequency bands, and channels in emotion expression. In this article, we propose a unified framework termed graph-regularized least square regression with feature importance learning (GFIL) to simultaneously achieve these goals by incorporating an autoweighting variable into the least square regression. Unlike the widely used trial-and-error manner, GFIL automatically completes the identification once it is trained. Specifically, GFIL can: 1) adaptively discriminate the contributions of different feature dimensions; 2) automatically identify the critical frequency bands and channels; and 3) quantitatively rank and select the features by the learned autoweighting variable. From the experimental results on the SEED_IV data set, we find GFIL obtained improved accuracies based on the feature autoweighting strategy, which are 75.33%, 75.03%, and 79.17% corresponding to the three cross-session recognition tasks (session1->session2, session1->session3, session2->session3), respectively. Additionally, the Gamma band is identified as the most important one and the channels locating in the prefrontal and left/right central regions are more important.","2379-8939","","10.1109/TCDS.2021.3082803","National Natural Science Foundation of China(grant numbers:61971173,61972121,U1909202,U20B2074); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY21F030005); National Social Science Foundation of China(grant numbers:19ZDA348); Fundamental Research Funds for the Provincial Universities of Zhejiang(grant numbers:GK209907299001-008); China Postdoctoral Science Foundation(grant numbers:2017M620470); Key Laboratory of Advanced Perception and Intelligent Control of High-End Equipment of Ministry of Education (Anhui Polytechnic University)(grant numbers:GDSC202015); Jiangsu Provincial Key Laboratory for Computer Information Processing Technology (Soochow University)(grant numbers:KJS1841); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438698","Affective brain–computer interface (aBCI);channel;electroencephalography (EEG);emotion recognition;feature importance learning;frequency band","Emotion recognition;Electroencephalography;Feature extraction;Brain modeling;Time-frequency analysis;Time-domain analysis;Electrodes","","50","","40","IEEE","21 May 2021","","","IEEE","IEEE Journals"
"CEMOAE: A Dynamic Autoencoder with Masked Channel Modeling for Robust EEG-Based Emotion Recognition","Y. -T. Lan; W. -B. Jiang; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","1871","1875","Emotion recognition through electroencephalography (EEG) has been an area of active research, but the inherent sensitivity of EEG signals to noise and artifacts poses significant challenges, especially in real-world settings. These complications often necessitate the removal of corrupted channels, making it crucial to develop robust models capable of maintaining performance even when few channels are available. To address this, we propose the Corrupted EMOtion AutoEncoder (CEMOAE), an innovative approach that leverages masked channel modeling to maintain robust performance, achieved through three components: masked autoencoder pretraining for robust representation learning, random masked auxiliary task for implicit modeling of channel corruption, and masked auto-repair to explicitly narrow the data distribution gap between high-quality and corrupted EEG signals. Specifically, we first pretrain a masked autoencoder with the dynamic masking strategy for feature extractor initialization and channel recovery. During the finetuning stage, we mask EEG data using the auxiliary task to mimic real-world EEG corruption. We then employ the pretrained autoencoder to repair these signals and finetune the feature extractor for emotion recognition. Experiments on the SEED dataset demonstrate that CEMOAE achieves SOTA performance for emotion recognition under the random channel corruption simulation, validating the effectiveness of the proposed techniques.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10447463","National Natural Science Foundation of China; Shanghai Jiao Tong University; School of Medicine; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10447463","Masked channel modeling;robust EEG-based emotion recognition","Representation learning;Emotion recognition;Speech recognition;Signal processing;Brain modeling;Feature extraction;Electroencephalography","","2","","21","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Emo-regulator: An emotion-regulation training system fusing virtual reality and EEG-based neurofeedback","M. Yu; Y. Bai; Y. Li","School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Life Sciences, Shanghai University, Shanghai, China; School of Life Sciences, College of International Education, Shanghai University, Shanghai, China",2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"11 Dec 2023","2023","","","1","4","Good emotion-regulation ability (ERA) is a vital sign of psychological health; conversely, emotion dysregulation may lead to mental or neurological disorders, such as anxiety disorders and depression. This study developed an emotion-regulation training system, Emo-regulator, fusing virtual reality (VR) and EEG-based neurofeedback to enhance subjects' ability to down-regulate negative emotions. Emo-regulator first elicited negative emotions in subjects through VR scenarios and then asked them to regulate emotions using cognitive reappraisal to change the emotional responses elicited by the VRs. Meanwhile, EEG signals from the subjects were collected and analyzed in real time by machine learning to predict the emotional states of the subjects (negative or positive). Emo-regulator changed the VR scenarios according to the prediction results and completed the feedback. Eight subjects used Emo-regulator for two weeks, and the results showed it could help the subjects improve their emotion regulation, and its usability is above average.Clinical Relevance—Emo-regulator can help subjects improve their ability to down-regulate negative emotions and increase the frequency of cognitive reappraisal use during emotion regulation.","2694-0604","979-8-3503-2447-1","10.1109/EMBC40787.2023.10340975","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340975","","Training;Neurological diseases;Three-dimensional displays;Virtual reality;Machine learning;Regulation;Electroencephalography","Humans;Emotional Regulation;Neurofeedback;Emotions;Electroencephalography;Virtual Reality","2","","12","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Cross-Subject emotion recognition from EEG using Convolutional Neural Networks","X. Zhong; Z. Yin; J. Zhang","Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai, P. R. China; Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai, P. R. China; OsloMet Artificial Intelligence Lab, Oslo Metropolitan University, Oslo, Norway",2020 39th Chinese Control Conference (CCC),"9 Sep 2020","2020","","","7516","7521","Using electroencephalogram (EEG) signals for emotion detection has aroused widespread research concern. However, across subjects emotional recognition has become an insurmountable gap which researchers cannot step across for a long time due to the poor generalizability of features across subjects. In response to this difficulty, in this study, the moving average(MA) technology is introduced to smooth out short-term fluctuations and highlight longer-term trends or cycles. Based on the MA technology, an effective method for cross-subject emotion recognition was then developed, which designed a method of salient region extraction based on attention mechanism, with the purpose of enhancing the capability of representations generated by a network by modelling the interdependecices between the channels of its informative features. The effectiveness of our method was validated on a dataset for emotion analysis using physiological signals (DEAP) and the MAHNOB-HCI multimodal tagging database. Compared with recent similar works, the method developed in this study for emotion recognition across all subjects was found to be effective, and its accuracy was 66.23% for valence and 68.50% for arousal (DEAP) and 70.25% for valence and 73.27% for arousal (MAHNOB) on the Gamma frequency band. And benefiting from the strong representational learning capacity in the two-dimensional space, our method is efficient in emotion recognition especially on Beta and Gamma waves.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9189559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189559","Emotional recognition;Physiological signals;Deep learning;Classification;Machine learning;Cross-subject","Emotion recognition;Fluctuations;Databases;Tagging;Brain modeling;Feature extraction;Market research","","6","","23","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Double Domain Converter Transformer For Improving EEG-Based Emotion Recognition from Video to Game Scenarios","J. -Y. Pan; H. -L. Yin; W. -L. Zheng","SJTU Paris Elite Institute of Technology, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Emotion recognition (ER) plays an important role in the field of modern technology and human-computer interaction. Traditional emotion recognition approaches usually utilize videos as stimuli. However, watching video lacks interaction. Recently, more and more game-related stimuli have been used. To explore emotion recognition in a more natural state, this study pioneers the use of Genshin Impact game and gameplay-related videos to evoke positive and neutral emotions. As Electroencephalogram (EEG) feature distributions of video and game scenarios are very different, we propose the Double Domain Converter Transformer network (DDCT) to enhance EEG-based cross-scenario emotion recognition, making full use of information from different domains. Our model has two converters that can convert input data into the source domain and the target domain, diminishing distribution difference in two feature spaces. Experimental results demonstrate that our model achieves a remarkable prediction accuracy of 84.95% in video-game mix-scenario emotion recognition and 74.76% in video to game cross-scenario emotion recognition. Our code is available at https://github.com/AlbertMentat/DDCT.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889052","National Natural Science Foundation of China; Shanghai Jiao Tong University; Shanghai Jiao Tong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889052","cross-scenario emotion recognition;video games;domain adaptation;EEG;Transformer","Emotion recognition;Video games;Games;Speech recognition;Signal processing;Brain modeling;Transformers;Feature extraction;Speech processing;Videos","","","","30","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Extracting Features Using Computational Cerebellar Model for Emotion Classification","H. Yaacob; W. Abdul; N. Kamaruddin","Kulliyyah of Information & Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Kulliyyah of Information & Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Faculty of Computer and Mathematical Sciences, MARA University of Technology, Shah Alam, Selangor, Malaysia",2013 International Conference on Advanced Computer Science Applications and Technologies,"19 Jun 2014","2013","","","367","372","Several feature extraction techniques have been employed to extract features from EEG signals for classifying emotions. Such techniques are not constructed based on the understanding of EEG and brain functions, neither inspired by the understanding of emotional dynamics. Hence, the features are difficult to be interpreted and yield low classification performance. In this study, a new feature extraction technique using Cerebellar Model Articulation Controller (CMAC) is proposed. The features are extracted from the weights of data-driven self-organizing feature map that are adjusted during training to optimize the error obtained from the desired output and the calculated output. Multi-Layer Perceptron (MLP) classifier is then employed to perform classification on fear, happiness, sadness and calm emotions. Experimental results show that the average accuracy of classifying emotions from EEG signals captured on 12 children aged between 4 to 6 years old ranging from 84.18% to 89.29%. In addition, classification performance for features derived from other techniques such as Power Spectrum Density (PSD), Kernel Density Estimation (KDE) and Mel-Frequency Cepstral Coefficients (MFCC) are also presented as a standard benchmark for comparison purpose. It is observed that the proposed approach is able to yield accuracy of 33.77% to 55% as compared to the respective comparison features. The experimental results indicated that the proposed approach has potential for comparative emotion recognition accuracy when coupled with MLP.","","978-1-4799-2758-6","10.1109/ACSAT.2013.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6836608","CMAC;feature extraction;latent features;emotion classification;EEG","Feature extraction;Accuracy;Electroencephalography;Brain modeling;Mel frequency cepstral coefficient;Computational modeling;Training","","","","28","IEEE","19 Jun 2014","","","IEEE","IEEE Conferences"
"PARSE: Pairwise Alignment of Representations in Semi-Supervised EEG Learning for Emotion Recognition","G. Zhang; V. Davoodnia; A. Etemad","Department of Electrical and Computer Engineering, and Ingenuity Labs Research Institute, Queen's University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, and Ingenuity Labs Research Institute, Queen's University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, and Ingenuity Labs Research Institute, Queen's University, Kingston, ON, Canada",IEEE Transactions on Affective Computing,"28 Nov 2022","2022","13","4","2185","2200","We propose pairwise alignment of representations for semi-supervised Electroencephalogram (EEG) learning (PARSE), a novel semi-supervised architecture for learning reliable EEG representations for emotion recognition. To reduce the potential distribution mismatch between large amounts of unlabeled data and a limited number of labeled data, PARSE uses pairwise representation alignment. First, our model performs data augmentation followed by label guessing for large amounts of original and augmented unlabeled data. The model is then followed by sharpening the guessed labels and convex combinations of the unlabeled and labeled data. Finally, it performs representation alignment and emotion classification. To rigorously test our model, we compare PARSE to several state-of-the-art semi-supervised approaches, which we implement and adapt for EEG learning. We perform these experiments on four public EEG-based emotion recognition datasets, SEED, SEED-IV, SEED-V and AMIGOS (valence and arousal). The experiments show that our proposed framework achieves the overall best results with varying amounts of limited labeled samples in SEED, SEED-IV and AMIGOS (valence), while approaching the overall best result (reaching the second-best) in SEED-V and AMIGOS (arousal). The analysis shows that our pairwise representation alignment considerably improves the performance by performing the distribution alignment between unlabeled and labeled data, especially when only 1 sample per class is labeled. The source code of our article is publicly available at https://github.com/guangyizhangbci/PARSE.","1949-3045","","10.1109/TAFFC.2022.3210441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9904937","Semi-supervised learning;eeg representations learning;emotion recognition","Electroencephalography;Emotion recognition;Feature extraction;Brain modeling;Computational modeling;Training;Deep learning","","29","","54","IEEE","28 Sep 2022","","","IEEE","IEEE Journals"
"DCR-GAT: 3D Convolutional Residual Graph Attention Network for Emotion Classification","Y. Huang; T. Liu; Q. Wu","Xiamen University, Xiamen, China; Xiamen University, Xiamen, China; Xiamen University, Xiamen, China",2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"10 Jan 2025","2024","","","4363","4370","Electroencephalogram (EEG) emotion recognition has become a central research focus within brain-computer interface (BCI) studies, where efficient emotion classification models can enable intelligent emotion regulation. In this paper, we propose a novel model based on three-dimensional convolutional neural networks (3D-CNN), residual networks (ResNet), and graph attention networks (GAT), named 3DCR-GAT, for EEG emotion recognition. The main advantage of the 3DCR-GAT model is its ability to simultaneously process multi-channel EEG signals, capturing complex spatiotemporal dependencies and local features within a unified network framework. Additionally, the ResNet employs skip connections to address the vanishing gradient problem during deep network training, improving the stability and expressiveness of the model. The attention mechanism within the model dynamically adjusts feature weights, captures global features, and enhances the accuracy of EEG emotion classification. Experimental results on public dataset, demonstrate that 3DCR-GAT significantly outperforms traditional methods in classification accuracy and feature extraction capability. Ablation studies reveal that each component of the model contributes substantially to its overall performance. Specifically, our model achieve an average accuracy of 92% on SEED for multi-state emotion recognition tasks. The 3DCR-GAT model represents an advanced and effective tool for EEG emotion classification, with broad potential applications.","2156-1133","979-8-3503-8622-6","10.1109/BIBM62325.2024.10822676","Natural Science Foundation of Fujian Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10822676","Electroencephalography;3D Convolutional Neural Network;Graph Attention Network;Residual Network;Emotion Classification","Emotion recognition;Solid modeling;Accuracy;Three-dimensional displays;Brain modeling;Feature extraction;Electroencephalography;Stability analysis;Spatiotemporal phenomena;Convolutional neural networks","","","","31","IEEE","10 Jan 2025","","","IEEE","IEEE Conferences"
"An Improved Dual Stream DGCNN Method for Fine-Grained Positive Emotion Classification","S. Hu; Z. Yu; E. Xu; Y. Wang; Y. Zhang; B. Guo","School of Computer Science, Northwestern Polytechnical University, Xian, China; Harbin Engineering University, Harbin, China; Department of Physics, Hong Kong Baptist University, Hong Kong SAR, China; School of Computer Science, Northwestern Polytechnical University, Xian, China; School of Computer Science, Northwestern Polytechnical University, Xian, China; School of Computer Science, Northwestern Polytechnical University, Xian, China",2024 11th International Conference on Behavioural and Social Computing (BESC),"12 Dec 2024","2024","","","1","6","Recent advances in EEG-based emotion computation have garnered significant interest from the fields of neuroscience and computer science. Despite this, the bulk of EEG-based emotion classification research has concentrated on the dimensions of arousal and valence, with scant attention to the classification of positive emotions as experienced in daily life. Addressing this gap, our study focuses on the classification of four specific positive emotions using the open-source FACED dataset. We processed EEG data by segmenting it, extracting features, and then designing and training the DualStreamDGCNN deep learning model, which achieved the best accuracy of 60.88%. Furthermore, we visualized the adjacency matrices of the four emotions from our model, offering insights from a neuroscientific perspective. This research is pivotal for advancing our understanding of the types and dynamics of positive emotions in practical applications, such as enhancing educational environments and boosting employee efficiency.","2689-8284","979-8-3315-3190-4","10.1109/BESC64747.2024.10780604","National Natural Science Foundation of China(grant numbers:61960206008,62272390); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10780604","Electroencephalogram (EEG);Positive Emotion Classification;Graph Neural Network (GNN);Deep Learning","Training;Productivity;Social computing;Accuracy;Neuroscience;Neural networks;Employment;Brain modeling;Feature extraction;Monitoring","","","","28","IEEE","12 Dec 2024","","","IEEE","IEEE Conferences"
"EEG Emotion Recognition Based on CNN+LSTM","X. Zhu; Y. Song; D. Li","School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China",2024 IEEE 25th China Conference on System Simulation Technology and its Application (CCSSTA),"1 Oct 2024","2024","","","144","148","With the rapid development of computer science, the field of emotion recognition using electroencephalogram (EEG) is getting more and more attention. In this study, we will use CNN+LSTM method for EEG emotion recognition. After the EEG data are pre-processed accordingly, we convert the standard electrode map into corresponding rectangular mapping map and extract the differential entropy (DE) feature of four different frequency bands in the corresponding channels. After that, in feature learning and classification, CNN is used to learn frequency and spatial domain feature, and LSTM is responsible for extracting the correlation of temporal information and generating classification results. We evaluated this approach on the DEAP dataset and achieved satisfactory classification results on the emotion (95.82%) and arousal (95.96%) dimensions, respectively.","","979-8-3503-6660-0","10.1109/CCSSTA62096.2024.10691696","Tianjin University; Tianjin University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691696","EEG;emotion recognition;CNN;LSTM","Representation learning;Electrodes;Emotion recognition;Frequency-domain analysis;Feature extraction;Frequency conversion;Electroencephalography;Data mining;Standards;Long short term memory","","","","9","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"MPED: A Multi-Modal Physiological Emotion Database for Discrete Emotion Recognition","T. Song; W. Zheng; C. Lu; Y. Zong; X. Zhang; Z. Cui","School of Information Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science, Ministry of Education, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science, Ministry of Education, Southeast University, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China",IEEE Access,"3 Feb 2019","2019","7","","12177","12191","To explore human emotions, in this paper, we design and build a multi-modal physiological emotion database, which collects four modal physiological signals, i.e., electroencephalogram (EEG), galvanic skin response, respiration, and electrocardiogram (ECG). To alleviate the influence of culture dependent elicitation materials and evoke desired human emotions, we specifically collect an emotion elicitation material database selected from more than 1500 video clips. By the considerable amount of strict man-made labeling, we elaborately choose 28 videos as standardized elicitation samples, which are assessed by psychological methods. The physiological signals of participants were synchronously recorded when they watched these standardized video clips that described six discrete emotions and neutral emotion. With three types of classification protocols, different feature extraction methods and classifiers (support vector machine and k-NearestNeighbor) were used to recognize the physiological responses of different emotions, which presented the baseline results. Simultaneously, we present a novel attention-long short-term memory (A-LSTM), which strengthens the effectiveness of useful sequences to extract more discriminative features. In addition, correlations between the EEG signals and the participants' ratings are investigated. The database has been made publicly available to encourage other researchers to use it to evaluate their own emotion estimation methods.","2169-3536","","10.1109/ACCESS.2019.2891579","National Basic Research Program of China (973 Program)(grant numbers:2015CB351704); National Natural Science Foundation of China(grant numbers:61572009,61772276); Key Research and Development Program of Jiangsu Province, China(grant numbers:BE2016616); Fundamental Research Funds for the Central Universities(grant numbers:2242018K3DN01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606087","Discrete emotion recognition;physiological signals;EEG;affective computing;machine learning;video-induced emotion;LSTM","Electroencephalography;Brain modeling;Physiology;Databases;Emotion recognition;Support vector machines;Feature extraction","","204","","57","OAPA","9 Jan 2019","","","IEEE","IEEE Journals"
"Study of EEG based Emotion Classification using Convolutional Neural Networks","D. Satuluri; B. Saiteja; S. K. Milinda; P. Singh","Department of ECE, NIT, Andhra Pradesh, Tadepalligudem, India; Department of ECE, NIT, Andhra Pradesh, Tadepalligudem, India; Department of ECE, NIT, Andhra Pradesh, Tadepalligudem, India; Department of ECE, NIT, Andhra Pradesh, Tadepalligudem, India",2022 8th International Conference on Signal Processing and Communication (ICSC),"12 Jan 2023","2022","","","508","513","The objective of this research work is to perform emotion classification using Electroencephalography (EEG) signals, which is the measure of electrical activity of the brain. The EEG data is obtained from the SEED IV (SJTU Emotion EEG Dataset) dataset. In this research, Power Spectral Density (PSD) and Differential Entropy (DE) values are extracted from the five rhythms of the EEG signal for different EEG channels and then images are formed from these PSD and DE values. These obtained PSD and DE images are then used to train the Convolutional Neural Network, which is used for the binary classification of different binary combinations of the four emotions. The emotions studied in this work are sad, fear, happy and neutral.","2643-444X","978-1-6654-5430-8","10.1109/ICSC56524.2022.10009458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009458","Emotion Classification;Electroencephalography;SEED IV;Power Spectral Density;Differential Entropy;Convolutional Neural Network","Electrodes;Electric variables measurement;Signal processing;Feature extraction;Electroencephalography;Entropy;Distance measurement","","1","","13","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"EEG-Based Multimodal Representation Learning for Emotion Recognition","K. Yin; H. -B. Shin; D. Li; S. -W. Lee","Dept. of Artificial Intelligence, Korea University, Seoul, Republic of Korea; Dept. of Brain and Cognitive Engineering, Korea University, Seoul, Republic of Korea; Dept. of Artificial Intelligence, Korea University, Seoul, Republic of Korea; Dept. of Artificial Intelligence, Korea University, Seoul, Republic of Korea",2025 13th International Conference on Brain-Computer Interface (BCI),"27 Mar 2025","2025","","","1","4","Multimodal learning has been a popular area of research, yet integrating electroencephalogram (EEG) data poses unique challenges due to its inherent variability and limited availability. In this paper, we introduce a novel multimodal framework that accommodates not only conventional modalities such as video, images, and audio, but also incorporates EEG data. Our framework is designed to flexibly handle varying input sizes, while dynamically adjusting attention to account for feature importance across modalities. We evaluate our approach on a recently introduced emotion recognition dataset that combines data from three modalities, making it an ideal testbed for multimodal learning. The experimental results provide a benchmark for the dataset and demonstrate the effectiveness of the proposed framework. This work highlights the potential of integrating EEG into multimodal systems, paving the way for more robust and comprehensive applications in emotion recognition and beyond.","2572-7672","979-8-3315-2192-9","10.1109/BCI65088.2025.10931743","National Research Foundation of Korea(grant numbers:2022-2-00975); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10931743","brain-computer interface;electroencephalogram;multimodal training;emotion recognition","Training;Representation learning;Emotion recognition;Benchmark testing;Transformers;Brain modeling;Electroencephalography;Brain-computer interfaces","","","","23","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"EEG-based attention recognition","Xiaowei Li; Bin Hu; Qunxi Dong; W. Campbell; P. Moore; Hong Peng","School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Department of Computing, Birmingham City University, UK; Department of Computing, Birmingham City University, UK; School of Information Science and Engineering, Lanzhou University, Lanzhou, China",2011 6th International Conference on Pervasive Computing and Applications,"19 Dec 2011","2011","","","196","201","Attention recognition (AR) is an essential component in many applications, however the focus of current research into AR is on `face detection', `eye center localization' and `eye center tracking techniques'. This paper describes a research project conducted to investigate the use of electroencephalography (EEG) signals to extend the current approaches and enrich AR. EEG processing and classification algorithms are applied to EEG data to identify a group of features that can be used to effectively implement AR. The experimental results reported in this paper are encouraging with correct classification rates achieved being: 51.9% where attention is divided into 5 classes and 63.9% where attention id divided into 3 classes. The distribution of the `training tuples' and `testing tuples' is discussed along with their impact on the reported results. The paper concludes with an overview of outstanding issues and consideration of projected future research.","","978-1-4577-0208-2","10.1109/ICPCA.2011.6106504","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106504","EEG;Attention Recognition;Classification Algrithm","Algorithm design and analysis;Open systems;Classification algorithms;Electroencephalography","","4","","13","IEEE","19 Dec 2011","","","IEEE","IEEE Conferences"
"Emotion recognition system based on EEG signals using relative wavelet energy features and a modified radial basis function neural networks","B. Krisnandhika; A. Faqih; P. Dewi Pumamasari; B. Kusumoputro","Department of Electrical Engineering, Universitas Indonesia, Depok, Indonesia; Department of Electrical Engineering, Universitas Indonesia, Depok, Indonesia; Universitas Indonesia, Depok, Jawa Barat, ID; Department of Electrical Engineering, Universitas Indonesia, Depok, Indonesia",2017 International Conference on Consumer Electronics and Devices (ICCED),"31 Aug 2017","2017","","","50","54","Emotion recognition is very important, especially on its application for patient monitoring and in the treatment management system of that patient. In this paper, an EEG-based emotion recognition system is developed that consists of a feature extraction subsystem and a classifier subsystem. In this research, we have studied the utilization of a relative wavelet energy as the feature extraction, and a modified radial basis function neural networks is then implemented as the classifier. Experimental result shows that the relative wavelet energy and the modified radial basis function neural networks achieved an average recognition rate of 76% when using a 50% of the data in the training stage.","","978-1-5386-0403-8","10.1109/ICCED.2017.8019990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019990","emotion recognition;emotion classification;relative wavelet energy;modified radial basis function neural networks","Emotion recognition;Feature extraction;Electroencephalography;Biological neural networks;Databases;Neurons;Wavelet transforms","","13","","12","IEEE","31 Aug 2017","","","IEEE","IEEE Conferences"
"Transformers for EEG-Based Emotion Recognition: A Hierarchical Spatial Information Learning Model","Z. Wang; Y. Wang; C. Hu; Z. Yin; Y. Song","School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China; Key Laboratory of Measurement and Control of CSE, Ministry of Education, School of Automation, Southeast University, Nanjing, China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology, Tianjin, China",IEEE Sensors Journal,"28 Feb 2022","2022","22","5","4359","4368","The spatial information of Electroencephalography (EEG) is essential for emotion recognition model to learn discriminative feature. The convolutional networks and recurrent networks are the conventional choices to learn the complex spatial dependencies through a number of electrodes and brain regions. However, these models have difficulty in capturing long-range dependencies due to the operations of local feature learning. To enhance EEG spatial dependencies capturing and improve the accuracy of emotion recognition, we propose a transformer- based model to hierarchically learn the discriminative spatial information from electrode level to brain-region-level. In the electrode-level spatial learning, the transformer encoders are adopted to integrate information within different brain regions. Next, in view of the different roles of brain regions in the emotion recognition, the self-attention within the transformer could emphasize the contributive brain regions. Hence, in the brain-region-level spatial learning, a transformer encoder is utilized to capture the spatial dependencies among the brain regions. Finally, to validate the effectiveness of the proposed model, the subject-independent experiments are conducted on the DEAP and MAHNOB-HCI database. The experimental results demonstrate that the proposed model achieves outstanding performance in emotion recognition with arousal and valence level. Moreover, the visualization of self-attention indicates that the proposed model could emphasize the discriminative spatial information from pre-frontal lobe, frontal lobe, temporal lobe and parietal lobe.","1558-1748","","10.1109/JSEN.2022.3144317","National Natural Science Foundation of China(grant numbers:62103299); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684393","Emotion recognition;EEG;transformer;self-attention","Electroencephalography;Electrodes;Transformers;Feature extraction;Brain modeling;Emotion recognition;Sensors","","102","","45","IEEE","18 Jan 2022","","","IEEE","IEEE Journals"
"A Comparative Study of Machine Learning Techniques for Emotion Recognition from Peripheral Physiological Signals","S. Vijayakumar; R. Flynn; N. Murray","Department of Computer and Software Engineering, Athlone Institute of Technology, Athlone, Co. Westmeath, Ireland; Department of Computer and Software Engineering, Athlone Institute of Technology, Athlone, Co. Westmeath, Ireland; Department of Computer and Software Engineering, Athlone Institute of Technology, Athlone, Co. Westmeath, Ireland",2020 31st Irish Signals and Systems Conference (ISSC),"31 Aug 2020","2020","","","1","6","Recent developments in wearable technology have led to increased research interest in using peripheral physiological signals for emotion recognition. The non-invasive nature of peripheral physiological signal measurement via wearables enables ecologically valid long-term monitoring. These peripheral signal measurements can be used in real-time in many ways including health and emotion classification. This paper investigates the utility of peripheral physiological signals for emotion recognition using the publicly available DEAP database. Using this database (which contains electroencephalogram (EEG) signals and peripheral signals), this paper compares eight machine learning models in the classification of valence and arousal emotion dimensions. These were applied to the peripheral physiological signals only. These models operate on three groupings of the peripheral data: (i) the raw peripheral physiological signals; (ii) individual feature sets extracted from each peripheral signal; and (iii) a fusion data set made of the combined features from the individual peripheral signals. The results indicate that support vector machine, linear discriminant analysis and logistic regression give the best recognition results on all three data groups considered. The feature fusion data set, which is made up by fusing all the features from the peripheral signals, gives the best recognition accuracy on both valence and arousal dimensions. In addition, subject dependency for emotion classification from peripheral signals is examined and significant individual variability is observed. The recognition rate varies between each participant from 10% to 87.5%.","2688-1454","978-1-7281-9418-9","10.1109/ISSC49989.2020.9180193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9180193","peripheral physiological signals;emotion recognition;machine learning;wearables;classification","Support vector machines;Emotion recognition;Databases;Machine learning;Feature extraction;Brain modeling;Physiology","","18","","18","IEEE","31 Aug 2020","","","IEEE","IEEE Conferences"
"Grop: Graph Orthogonal Purification Network for EEG Emotion Recognition","M. Wu; C. L. P. Chen; B. Chen; T. Zhang","Guangdong Provincial Key Laboratory of AI Large Model and Intelligent Cognition, the School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of AI Large Model and Intelligent Cognition, the School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of AI Large Model and Intelligent Cognition, the School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of AI Large Model and Intelligent Cognition, the School of Computer Science and Engineering, South China University of Technology, Guangzhou, China",IEEE Transactions on Affective Computing,"28 Feb 2025","2025","16","1","319","332","The existence of emotion-irrelevant representations and individual variability impedes the extraction of robust emotional representations, limiting the adaptability of EEG emotion recognition. Massive studies focus on the mining of emotion-aware information, overlooking emotion-agnostic information, which is insufficient for the extraction of emotion-relevant features against redundancy and variation. In this paper, Graph Orthogonal Purification Network (Grop) is proposed to enhance individual adaptability through improvements in the orthogonality and transferability between emotion-relevant and emotion-irrelevant features. Specifically, the proposed Grop utilized a graph representation extraction module to capture both emotion-relevant and emotion-irrelevant features by the dual graph. The representation orthogonal purification module is developed to eliminate redundant information through feature projection and feature purification. Moreover, the dual emotional space alignment module is imposed to align distribution discrepancies in different emotion feature spaces. To assess the effectiveness of the proposed Grop, various experiments are conducted on two public EEG emotion datasets, i.e., SEED and SEED-IV. The results achieve state-of-the-art performance, demonstrating the capability of the Grop to capture robust emotion features and alleviate the intra- and inter-subject discrepancies.","1949-3045","","10.1109/TAFFC.2024.3433613","National Key Research and Development Program of China(grant numbers:2019YFA0706200); National Natural Science Foundation of China(grant numbers:62222603,62076102,92267203); STI2030-Major Projects grant from the Ministry of Science and Technology of the People's Republic of China(grant numbers:2021ZD0200700); Key-Area Research and Development Program of Guangdong Province(grant numbers:2023B0303030001); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2019ZT08X214); Science and Technology Program of Guangzhou(grant numbers:2024A04J6310); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10609541","EEG emotion recognition;orthogonal purification;emotion-relevant feature;adaptability;Grop","Electroencephalography;Feature extraction;Emotion recognition;Purification;Convolution;Robustness;Redundancy","","3","","55","IEEE","25 Jul 2024","","","IEEE","IEEE Journals"
"Stocastic Multimodal Fusion Method for Classifying Emotions With Attention Mechanism Using Deep Learning","R. Selvi; C. Vijayakumaran","Departmentt of Computing Technologies, SRM Institute of Science and Technology, Chennai, India; Departmentt of Computing Technologies, SRM Institute of Science and Technology, Chennai, India",2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS),"5 May 2023","2023","1","","2347","2352","Deep multimodal Learning is a subfield of machine learning that seeks to train artificial intelligence models to handle and discover relationships between various modalities in order to minimize reader bias and is most likely to generate high-quality data due to the standardized training program's controlled and transparent setting, which includes interpretation methods and criterion as well as improving diagnostic accuracy compared to only one modality In order to accomplish multimodal emotion categorization, Using physiological data as input, propose an unique classification technique in this study called stochastic multimodal fusion with attention mechanism. To be more precise, this work utilizes an adversarial learning-based discriminator to drive the feature extraction technique to explicitly learn the linked information. Moreover, we developed a reconstruction method based on attention to motivate the feature transformation to master the feature maps associated with the emotional field. In this way, more perception information can be added to the features of each mode of operation. Our method considers both correlative and interrelated data from three modalities for multimodal fusion, in contrast to previous multi-modal approaches, which solely concentrate on learning complimentary features from temporal characteristics. For multimodal fusion, our technique considers both the correlations and the differences between the three modalities. This is unlike previous multimodal-based methodologies, which only focused on learning correlations from temporal characteristics. As part of our evaluation process, conduct extensive testing on the ASCERTAIN database, a publicly accessible multimodal dataset. The findings of the experiments show that our suggested strategy is preferable to recognize different emotions than the methods that are currently considered to be state-of-the-art and that it improves accuracy on average.","2575-7288","979-8-3503-9737-6","10.1109/ICACCS57279.2023.10113124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113124","emotion detection;fusion;attention;LSTMP;deep learning","Deep learning;Training;Emotion recognition;Correlation;Stochastic processes;Reconstruction algorithms;Physiology","","2","","15","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"An Efficient Machine Learning Framework for Sleep Stages Classification in Sleep Disorder Disease","S. K. Satapathy; C. Daga; M. Tripathi; R. Khandelwal; S. Mehta; A. Mathur","Information and Communication Technology, Pandit Deendayal Energy University, Gandhinagar, Gujarat; Information and Communication Technology, Pandit Deendayal Energy University, Gandhinagar, Gujarat; Information and Communication Technology, Pandit Deendayal Energy University, Gandhinagar, Gujarat; Information and Communication Technology, Pandit Deendayal Energy University, Gandhinagar, Gujarat; Information and Communication Technology, Pandit Deendayal Energy University, Gandhinagar, Gujarat; Information and Communication Technology, Pandit Deendayal Energy University, Gandhinagar, Gujarat","2024 2nd International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)","16 May 2025","2024","","","1","6","Sleep stage classification is of prime importance in understanding the sleep pattern and diagnostics of several sleep disorders. Accurate classification would require clinicians to detect abnormal conditions like sleep apnea and insomnia. With the recent progress in machine learning techniques, there is an increasing interest in automating sleep stage classification using EEG data. This project involves the use of EEG signals and machine learning algorithms for classifying sleep stages to improve diagnostic accuracy and efficiency. Preprocessing techniques involving filtering and normalization were applied to the EEG signals from the Sleep Physionet Dataset. Relevant features such as mean, variance, and standard deviation were extracted in the time domain to capture essential patterns in EEG data. The efficiency of different sleep stage classifications was tested on models. The Random Forest model turned out to be the best with an accuracy of 99.37% for two-class tasks. The experimentation shows that better performance is exhibited by ensemble methods than the single classifier for the sleep stage classification and therefore would be more appropriate for use in an automatic sleep staging system. This work will further contribute to the improvement of clinical diagnosis of sleep disorders by automating the sleep stage classification. Future work can include enhanced feature extraction methods or deeper learning techniques for further improvement in classification accuracy.","","979-8-3315-0645-2","10.1109/SCOPES64467.2024.10990881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10990881","Sleep Stage Classification;EEG;Machine Learning;Random Forest","Accuracy;Sleep;Transforms;Feature extraction;Brain modeling;Electroencephalography;Sleep apnea;Time-domain analysis;Random forests;Standards","","","","20","IEEE","16 May 2025","","","IEEE","IEEE Conferences"
"MetaEmotionNet: Spatial–Spectral–Temporal-Based Attention 3-D Dense Network With Meta-Learning for EEG Emotion Recognition","X. Ning; J. Wang; Y. Lin; X. Cai; H. Chen; H. Gou; X. Li; Z. Jia","Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Samueli School of Engineering, University of California at Los Angeles, Los Angeles, CA, USA; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Institute for Infocomm Research, A*STAR, Fusionopolis, Singapore; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China",IEEE Transactions on Instrumentation and Measurement,"22 Dec 2023","2024","73","","1","13","Emotion recognition has become an important area in affective computing. Emotion recognition based on multichannel electroencephalogram (EEG) signals has gradually become popular in recent years. However, on one hand, how to make full use of different EEG features and the discriminative local patterns among the features for various emotions is challenging. Existing methods ignore the complementarity among the spatial–spectral–temporal features and discriminative local patterns in all features, which limits the classification performance. On the other hand, when dealing with cross-subject emotion recognition, existing transfer learning (TL) methods need a lot of training data. At the same time, it is extremely expensive and time-consuming to collect the labeled EEG data, which is not conducive to the wide application of emotion recognition models for new subjects. To solve the above challenges, we propose a novel spatial–spectral–temporal-based attention 3-D dense network (SST-Net) with meta-learning, named MetaEmotionNet, for emotion recognition. Specifically, MetaEmotionNet integrates the spatial–spectral–temporal features simultaneously in a unified network framework through two-stream fusion. At the same time, the 3-D attention mechanism can adaptively explore discriminative local patterns. In addition, a meta-learning algorithm is applied to reduce dependence on training data. Experiments demonstrate that the MetaEmotionNet is superior to the baseline models on two benchmark datasets.","1557-9662","","10.1109/TIM.2023.3338676","National Natural Science Foundation of China(grant numbers:62306317,61603029); China Postdoctoral Science Foundation(grant numbers:2023M733738); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342627","Affective computing;attention mechanism;electroencephalogram (EEG);emotion recognition;meta-learning","Electroencephalography;Emotion recognition;Feature extraction;Three-dimensional displays;Metalearning;Brain modeling;Adaptation models","","13","","68","IEEE","4 Dec 2023","","","IEEE","IEEE Journals"
"Emotion Recognition Model With Class-wise Domain Adaptation Based on Few-Shot EEG Learning","S. Furukawa; T. Sakuma; S. Kato","Graduate School of Engineering, Nagoya Institute of Technology, Gokiso-cho, Showa-ku, Nagoya-city, Aichi, Japan; Graduate School of Engineering, Nagoya Institute of Technology, Gokiso-cho, Showa-ku, Nagoya-city, Aichi, Japan; Graduate School of Engineering, Nagoya Institute of Technology, Gokiso-cho, Showa-ku, Nagoya-city, Aichi, Japan",2022 IEEE 11th Global Conference on Consumer Electronics (GCCE),"18 Jan 2023","2022","","","697","698","In recent years, EEG-based emotion recognition technology has attracted attention due to its many advantages, such as understanding the emotional states of users who are disabled to express their emotions through facial expressions or voices. However, because of individual differences in EEG, it is difficult to learn an effective emotion recognition model using data from an unspecified number of users. Therefore, we use domain adaptation to reduce individual differences in EEG to construct an emotion recognition model suitable for the target user while minimizing the number of data required for training. In a previous study, the domain discriminator is trained so that they cannot discriminate between the domain of the target user and that of other users regardless of the emotion class, which may hurt the classification between data that differ in both domain and emotion class. Therefore, we proposed a method that class-wise domain adaptation by using a multi-discriminator and confirmed higher performance than the conventional method.","2378-8143","978-1-6654-9232-4","10.1109/GCCE56475.2022.10014236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10014236","EEG;Brain-computer Interface;Deep Neural Networks;Domain Adaptation","Training;Adaptation models;Emotion recognition;Brain modeling;Electroencephalography;Data models;Consumer electronics","","1","","5","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"A Channel-Fused Dense Convolutional Network for EEG-Based Emotion Recognition","Z. Gao; X. Wang; Y. Yang; Y. Li; K. Ma; G. Chen","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Tencent Jarvis Lab, Shenzhen, China; Department of Electronic Engineering, City University of Hong Kong, Hong Kong",IEEE Transactions on Cognitive and Developmental Systems,"10 Dec 2021","2021","13","4","945","954","Human emotion recognition could greatly contribute to human–computer interaction with promising applications in artificial intelligence. One of the challenges in recognition tasks is learning effective representations with stable performances from electroencephalogram (EEG) signals. In this article, we propose a novel deep-learning framework, named channel-fused dense convolutional network, for EEG-based emotion recognition. First, we use a 1-D convolution layer to receive weighted combinations of contextual features along the temporal dimension from EEG signals. Next, inspired by state-of-the-art object classification techniques, we employ 1-D dense structures to capture electrode correlations along the spatial dimension. The developed algorithm is capable of handling temporal dependencies and electrode correlations with the effective feature extraction from noisy EEG signals. Finally, we perform extensive experiments based on two popular EEG emotion datasets. Results indicate that our framework achieves prominent average accuracies of 90.63% and 92.58% on the SEED and DEAP datasets, respectively, which both receive better performances than most of the compared studies. The novel model provides an interpretable solution with excellent generalization capacity for broader EEG-based classification tasks.","2379-8939","","10.1109/TCDS.2020.2976112","National Natural Science Foundation of China(grant numbers:61922062,61873181); Hong Kong Research Grants Council through GRF(grant numbers:U-11200317); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9011570","Brain–computer interface (BCI);convolutional neural network (CNN);deep learning (DL);electroencephalogram (EEG);emotion recognition","Electroencephalography;Feature extraction;Emotion recognition;Brain-computer interfaces;Correlation;Brain modeling;Convolutional neural networks","","112","","80","IEEE","25 Feb 2020","","","IEEE","IEEE Journals"
"Scalp connectivity networks for analysis of EEG signal during emotional stimulation","M. K. Ahirwal; A. Kumar; N. D. Londhe; H. Bikrol","Department of Computer Applications, National Institute of Technology, Raipur, India; Design and Manufacturing Department of Electronics and Communication Engineering., PDPM-Indian Institute of Information Technology, Jabalpur, India; Department of Computer Applications, National Institute of Technology, Raipur, India; Department of Computer Applications, National Institute of Technology, Raipur, India",2016 International Conference on Communication and Signal Processing (ICCSP),"24 Nov 2016","2016","","","0592","0596","In this paper, analysis of correlation based functional connectivity network of electroencephalogram (EEG) signals has been done. Functional connectivity network explores the important facts about activation and synchronization among different areas of brain. Multi-channel EEG signals generated through audio-visual stimulation is used in this study. Correlated electrode pairs show the simultaneous activation of two different brain areas. Correlation strength and degree of each node explore information about involvement of particular brain area during stimulation. From the results, it is evident that on average, male and female brain responses differently for same type of audio-visual stimulation and right hemisphere is mainly involved in emotion generation.","","978-1-5090-0396-9","10.1109/ICCSP.2016.7754208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7754208","EEG signals;Emotion;Connectivity network;Correlation","Electroencephalography;Electrodes;Correlation;Synchronization;Scalp;Emotion recognition","","9","","12","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"Emotion recognition based on spatially smooth spectral features of the EEG","T. Balli; S. M. Deniz; B. Cebeci; M. Erbey; A. D. Duru; T. Demiralp","Department of Computer Engineering, Istanbul Kemerburgaz University, Istanbul, Turkey; Institute of Biomedical Engineering, The Scientific and Technological Research Council of Turkey (TUBITAK), Istanbul, Turkey; Department of Electronic Engineering, Kultur University, Istanbul, Turkey; Center for Lifespan Psychology, Max Planck Institute for Human Development, Berlin, Germany; Hulusi Behcet Life Sciences Center, Istanbul University, Istanbul, Turkey; Department of Physiology, Istanbul University, Istanbul, Turkey",2013 6th International IEEE/EMBS Conference on Neural Engineering (NER),"2 Jan 2014","2013","","","407","410","The primary aim of this study was to select the optimal feature subset for discrimination of three dimensions of emotions (arousal, valence, liking) from subjects using electroencephalogram (EEG) signals. The EEG signals were collected from 25 channels on 21 healthy subjects whilst they were watching movie segments with emotional content. The band power values extracted from eleven frequency bands, namely delta (0.5-3.5 Hz), theta (4-7.5 Hz), alpha (8-12 Hz), beta (13-30 Hz), gamma (30-50 Hz), low theta (4-6 Hz), high theta (6-8 Hz), low alpha (8-10 Hz), high alpha (10-12 Hz), low beta (13-18 Hz) and high beta (18-30 Hz) bands, were used as EEG features. The most discriminative features for classification of EEG feature sets were selected using sequential floating forward search (SFFS) algorithm and a modified version of SFFS algorithm, which imposes the topographical smoothness of spectral features, along with linear discriminant analysis (LDA) classifier. The best classification accuracies for three emotional dimensions were obtained for liking (72.22%) followed by arousal (67.50%) and valence (66.67%). SFFS-LDA and modified SFFS-LDA algorithms produced slightly different classification accuracies. However, the findings suggested that the use of modified SFFS-LDA algorithm provides more robust feature subsets for understanding of underlying functional neuroanatomic mechanisms corresponding to distinct emotional states.","1948-3554","978-1-4673-1969-0","10.1109/NER.2013.6695958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6695958","","Electroencephalography;Classification algorithms;Accuracy;Emotion recognition;Feature extraction;Algorithm design and analysis;Motion pictures","","","","20","IEEE","2 Jan 2014","","","IEEE","IEEE Conferences"
"Recognition and Research of Picture-Induced Emotion based on EEG Signals","H. Jin; Y. Song; Z. Mao; Z. Bai; Z. Li; Y. Chen; M. Cheng; Y. Liu","Department of School of Electrical, Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical, Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical, Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical, Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical, Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical, Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical, Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology; Department of School of Electrical, Engineering and Automation, Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin University of Technology",2022 IEEE International Conference on Mechatronics and Automation (ICMA),"22 Aug 2022","2022","","","444","449","In recent years, many researchers have explored electroencephalogram-based (EEG-based) emotion recognition in a variety of ways, but a few studies have investigated on facial emotion recognition for hearing-impaired and normal people. In order to compare and analyze the differences in facial emotion recognition between hearing-impaired and normal people, we established emotional EEG dataset of hearing-impaired subjects and normal subjects based on facial affective picture stimulation, which contains five kinds of emotion (happiness, neutral, sadness, fear and anger) with 15 hearing-impaired and 15 normal subjects. The collected EEG signals are filtered and artifact are removed by preprocessing. Then, differential entropy (DE), power spectral density (PSD) and wavelet entropy (WE) features were extracted, and the linear support vector machine (SVM-linear) as selected optimal classifier is used for ten-fold cross-validation emotion classification. The results show that the DE feature achieved the best emotion recognition accuracy for both hearing-impaired subjects (40.8%) and normal subjects (45.5%).","2152-744X","978-1-6654-0853-0","10.1109/ICMA54519.2022.9856357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856357","Electroencephalogram (EEG);Emotion recognition;Hearing-impaired","Support vector machines;Emotion recognition;Mechatronics;Automation;Face recognition;Feature extraction;Electroencephalography","","1","","24","IEEE","22 Aug 2022","","","IEEE","IEEE Conferences"
"Cross-Session Emotion Recognition by Joint Label-Common and Label-Specific EEG Features Exploration","Y. Peng; H. Liu; J. Li; J. Huang; B. -L. Lu; W. Kong","School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; HDU-IMTO Joint Institute, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.; School of Computer Science and Technology, Anhui University of Technology, Ma’anshan, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"2 Feb 2023","2023","31","","759","768","Since Electroencephalogram (EEG) is resistant to camouflage, it has been a reliable data source for objective emotion recognition. EEG is naturally multi-rhythm and multi-channel, based on which we can extract multiple features for further processing. In EEG-based emotion recognition, it is important to investigate whether there exist some common features shared by different emotional states, and the specific features associated with each emotional state. However, such fundamental problem is ignored by most of the existing studies. To this end, we propose a Joint label-Common and label-Specific Features Exploration (JCSFE) model for semi-supervised cross-session EEG emotion recognition in this paper. To be specific, JCSFE imposes the  $\ell _{\text {2,1}}$ -norm on the projection matrix to explore the label-common EEG features and simultaneously the  $\ell _{{1}}$ -norm is used to explore the label-specific EEG features. Besides, a graph regularization term is introduced to enforce the data local invariance property, i.e., similar EEG samples are encouraged to have the same emotional state. Results obtained from the SEED-IV and SEED-V emotional data sets experimentally demonstrate that JCSFE not only achieves superior emotion recognition performance in comparison with the state-of-the-art models but also provides us with a quantitative method to identify the label-common and label-specific EEG features in emotion recognition.","1558-0210","","10.1109/TNSRE.2022.3233109","National Science Foundation of China(grant numbers:61971173,U20B2074); National Science Foundation of Zhejiang Province(grant numbers:LY21F030005); Key Research and Development Project of Zhejiang Province(grant numbers:2023C03026,2021C03001,2021C03003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003248","EEG emotion recognition;graph regularization;label-common features;label-specific features;semi-supervised regression","Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Computer science;Sparse matrices;Linear programming","","19","","35","CCBY","29 Dec 2022","","","IEEE","IEEE Journals"
"Multi-Task CNN model for emotion recognition from EEG Brain maps","E. Rudakov; L. Laurent; V. Cousin; A. Roshdi; R. Fournier; A. Nait-ali; T. Beyrouthy; S. A. Kork","Faculty of Sciences and Technology, Univ Paris Est Creteil, Créteil, France; Faculty of Sciences and Technology, Univ Paris Est Creteil, Créteil, France; Faculty of Sciences and Technology, Univ Paris Est Creteil, Créteil, France; LISSI, Université Paris-Est (UPE), Vitry, France; LISSI, Université Paris-Est (UPE), Vitry, France; LISSI, Université Paris-Est (UPE), Vitry, France; American University of the Middle East, Kuwait, Lebanon; American University of the Middle East, Kuwait, Lebanon",2021 4th International Conference on Bio-Engineering for Smart Technologies (BioSMART),"21 Jan 2022","2021","","","1","4","Emotion identification plays a vital role in human interactions. For this purpose, Computer-vision methods for automatic emotion recognition is nowadays a widely studied topic. One of the most studied approaches for automatic emotion recognition is processing multi-channel Electroencephalogram signals (EEG). This paper presents a new model for emotion recognition using brain maps as input and providing emotion states in terms of arousal and valence as output. Brain maps are a spatial representation of features extracted from EEG signals. The proposed model, called Multi-Task Convolutional Neural Network (MT-CNN), is fed with stacked brain maps of four different waves of different frequency bands: alpha, beta, gamma and theta, using differential entropy and power spectra density and considering observation windows of 0.5s. This model is trained and tested on the DEAP dataset, a well-known dataset for comparison purposes. This work shows that the MT-CNN nerforms better than other methods.","","978-1-6654-0810-3","10.1109/BioSMART54244.2021.9677807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9677807","Convolutional Neural Network;EEG;Brain map;Emotion recognition;Multi-Task learning","Emotion recognition;Face recognition;Brain modeling;Feature extraction;Multitasking;Electroencephalography;Convolutional neural networks","","14","","15","IEEE","21 Jan 2022","","","IEEE","IEEE Conferences"
"Emotion Recognition with Multi-Channel EEG Signals Using Auditory Stimulus","C. Gunes; M. A. Ozdemir; A. Akan","Department of Biomedical Engineering, Electrophysiological Signals Laboratory Izmir Katip Celebi University, Izmir, Turkey; Department of Biomedical Engineering, Electrophysiological Signals Laboratory Izmir Katip Celebi University, Izmir, Turkey; Department of Biomedical Engineering, Electrophysiological Signals Laboratory Izmir Katip Celebi University, Izmir, Turkey",2019 Medical Technologies Congress (TIPTEKNO),"11 Nov 2019","2019","","","1","4","Emotions play a significant role in daily life by encouraging the individual in the survival, decision making, guessing, and communication processes. Through emotions can be explained with the activation of anatomical structures in certain regions of brain with nervous system the emotions can be understood by electroencephalogram (EEG) signals. In order to recognize emotions, the signal processing techniques were applied to recorded signals using 32-channels EEG device from the subjects during listening audios. The Self-Assessment Manikin (SAM) form was filled by 23 subjects to evaluate their feelings based on three emotion states and recorded their answers by designed Graphical User Interface (GUI) monitored in front of the subjects. In signal processing stage, the EEG signals were segmented into segmented files by cutting stimulus intervals from recorded signal and decomposed to Intrinsic Mode Functions (IMFs) by Empirical Mode Decomposition (EMD) method. Then, most meaningful IMFs has been selected by analyzing Power Spectral Density (PSD) to extract statistical and entropy-based features and then, classification algorithm has been applied to obtain feature vector to categorize states of emotion consisting of valence, arousal, dominance dimensions. It is aimed to find most useful selected IMFs, most active channels related to emotion, best suitable features for each dimension of emotion. Finally, the percentage of performance accuracy has been calculated and the best accuracy of 81.74% is found in channels ranged in frontal lobe (1–12) for valence state, 72.15% in channel TP7 for arousal state, 74.57% in channels ranged in frontal lobe (1–12) for dominance state by combining different features.","2687-7783","978-1-7281-2420-9","10.1109/TIPTEKNO.2019.8895124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8895124","EEG signal;Empirical Mode Decomposition;Emotion Recognition;Feature Extraction;Intrinsic Mode Function;Power Spectral Density;Support Vector Machine","","","3","","11","IEEE","11 Nov 2019","","","IEEE","IEEE Conferences"
"Discrimination of Genuine and Acted Emotional Expressions Using EEG Signal and Machine Learning","M. Alex; U. Tariq; F. Al-Shargie; H. S. Mir; H. A. Nashash","Biomedical Engineering Graduate Program, American University of Sharjah, Sharjah, United Arab Emirates; Biomedical Engineering Graduate Program, American University of Sharjah, Sharjah, United Arab Emirates; Biomedical Engineering Graduate Program, American University of Sharjah, Sharjah, United Arab Emirates; Biomedical Engineering Graduate Program, American University of Sharjah, Sharjah, United Arab Emirates; Biomedical Engineering Graduate Program, American University of Sharjah, Sharjah, United Arab Emirates",IEEE Access,"27 Oct 2020","2020","8","","191080","191089","We present here one of the first studies that attempt to differentiate between genuine and acted emotional expressions, using EEG data. We present the first EEG dataset (available here) with recordings of subjects with genuine and fake emotional expressions. We build our experimental paradigm for classification of smiles; genuine smiles, fake/acted smiles and neutral expression. We propose multiple methods to extract intrinsic features from three EEG emotional expressions; genuine, neutral, and fake/acted smile. We extracted EEG features using three time-frequency analysis methods: discrete wavelet transforms (DWT), empirical mode decomposition (EMD), and incorporating DWT into EMD (DWT-EMD) at three frequency bands. We then evaluated the proposed methods using several classifiers including, k-nearest neighbors (KNN), support vector machine (SVM), and artificial neural network (ANN). We carried out an experimental paradigm on 28-subjects underwent three types of emotional expressions, genuine, neutral and fake/acted. The results showed that incorporating DWT into EMD extracted more hidden features than sole DWT or sole EMD method. The power spectral feature extracted by DWT, EMD, and DWT-EMD showed different neural patterns across the three emotional expressions at all the frequency bands. We performed binary classification experiments and achieved acceptable accuracy reaching a maximum of 84% in all type of emotions, classifiers and bands using sole DWT or EMD. Meanwhile, a combination of DWT-EMD achieved the highest classification accuracy with ANN in classifying true emotional expressions from fake expressions in the alpha and beta bands with an average accuracy of 94.3% and 84.1%, respectively. Our results suggest combining DWT-EMD for future emotion studies and highlight the association of alpha and beta frequency bands with emotions.","2169-3536","","10.1109/ACCESS.2020.3032380","Biosciences and Bioengineering Research Institute (BBRI) Grant(grant numbers:EN0275-BBRI18-06); Enhanced Faculty Research Grant(grant numbers:EFRG18-BBR-CEN-02/EN0); Department of Electrical Engineering, American University of Sharjah, United Arab Emirates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233330","Emotion recognition;electroencephalogram (EEG);discrete wavelet transforms (DWT);empirical mode decomposition (EMD);classification","Electroencephalography;Discrete wavelet transforms;Feature extraction;Time-frequency analysis;Emotion recognition;Physiology","","27","","48","CCBY","20 Oct 2020","","","IEEE","IEEE Journals"
"Feature Extraction for Emotion Recognition and Modelling Using Neurophysiological Data","A. Samara; M. L. R. Menezes; L. Galway","School of Computing and Mathematics, Ulster University, Belfast, United Kingdom; Center for Applied Intelligent Systems Research, Halmstad University, Sweden; School of Computing and Mathematics, Ulster University, Belfast, United Kingdom",2016 15th International Conference on Ubiquitous Computing and Communications and 2016 International Symposium on Cyberspace and Security (IUCC-CSS),"23 Jan 2017","2016","","","138","144","The ubiquitous computing paradigm is becoming a reality; we are reaching a level of automation and computing in which people and devices interact seamlessly. However, one of the main challenges is the difficulty users have in interacting with these increasingly complex systems. Ultimately, endowing machines with the ability to perceive users' emotions will enable a more intuitive and reliable interaction. Consequently, using the electroencephalogram (EEG) as a bio-signal sensor, the affective state of a user can be modelled and subsequently utilised in order to achieve a system that can recognise and react to the users emotions. In this context, this paper investigates feature vector generation from EEG signals for the purpose of affective state modelling based on Russells Circumplex Model. Investigations are presented that aim to provide the foundation for future work in modelling user affect and interaction experiences through exploitation of different input modalities. The DEAP dataset was used within this work, along with a Support Vector Machine, which yielded reasonable classification accuracies for Valence and Arousal using feature vectors based on statistical measurements, band power from the , β, δ and θ waves, and High Order Crossing of the EEG signal.","","978-1-5090-5566-1","10.1109/IUCC-CSS.2016.027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828594","EEG;bio-signal sensor;affective state modelling;feature extraction","Electroencephalography;Feature extraction;Emotion recognition;Brain models;Support vector machines;Context","","24","","29","IEEE","23 Jan 2017","","","IEEE","IEEE Conferences"
"Time–Frequency Representation and Convolutional Neural Network-Based Emotion Recognition","S. K. Khare; V. Bajaj","Electronics and Communication Discipline, Indian Institute of Information Technology Design and Manufacturing, Jabalpur, India; Electronics and Communication Discipline, Indian Institute of Information Technology Design and Manufacturing, Jabalpur, India",IEEE Transactions on Neural Networks and Learning Systems,"6 Jul 2021","2021","32","7","2901","2909","Emotions composed of cognizant logical reactions toward various situations. Such mental responses stem from physiological, cognitive, and behavioral changes. Electroencephalogram (EEG) signals provide a noninvasive and nonradioactive solution for emotion identification. Accurate and automatic classification of emotions can boost the development of human–computer interface. This article proposes automatic extraction and classification of features through the use of different convolutional neural networks (CNNs). At first, the proposed method converts the filtered EEG signals into an image using a time–frequency representation. Smoothed pseudo-Wigner–Ville distribution is used to transform time-domain EEG signals into images. These images are fed to pretrained AlexNet, ResNet50, and VGG16 along with configurable CNN. The performance of four CNNs is evaluated by measuring the accuracy, precision, Mathew’s correlation coefficient, F1-score, and false-positive rate. The results obtained by evaluating four CNNs show that configurable CNN requires very less learning parameters with better accuracy. Accuracy scores of 90.98%, 91.91%, 92.71%, and 93.01% obtained by AlexNet, ResNet50, VGG16, and configurable CNN show that the proposed method is best among other existing methods.","2162-2388","","10.1109/TNNLS.2020.3008938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153955","Convolutional neural networks (CNNs);electroencephalogram (EEG);emotion recognition;smoothed pseudo-Wigner–Ville distribution (SPWVD)","Feature extraction;Electroencephalography;Support vector machines;Time-domain analysis;Wavelet transforms;Emotion recognition","Algorithms;Brain-Computer Interfaces;Electroencephalography;Emotions;False Positive Reactions;Female;Humans;Machine Learning;Male;Neural Networks, Computer;Pattern Recognition, Automated;Reproducibility of Results;Young Adult","180","","36","IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"Automatic Emotion Recognition Using Temporal Multimodal Deep Learning","B. Nakisa; M. N. Rastgoo; A. Rakotonirainy; F. Maire; V. Chandran","School of Information Technology, Deakin University, Geelong, VIC, Australia; School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, QLD, Australia; Centre for Accident Research and Road Safety-Queensland, Queensland University of Technology, Brisbane, QLD, Australia; School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, QLD, Australia; School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, QLD, Australia",IEEE Access,"24 Dec 2020","2020","8","","225463","225474","Emotion recognition using miniaturised wearable physiological sensors has emerged as a revolutionary technology in various applications. However, detecting emotions using the fusion of multiple physiological signals remains a complex and challenging task. When fusing physiological signals, it is essential to consider the ability of different fusion approaches to capture the emotional information contained within and across modalities. Moreover, since physiological signals consist of time-series data, it becomes imperative to consider their temporal structures in the fusion process. In this study, we propose a temporal multimodal fusion approach with a deep learning model to capture the non-linear emotional correlation within and across electroencephalography (EEG) and blood volume pulse (BVP) signals and to improve the performance of emotion classification. The performance of the proposed model is evaluated using two different fusion approaches - early fusion and late fusion. Specifically, we use a convolutional neural network (ConvNet) long short-term memory (LSTM) model to fuse the EEG and BVP signals to jointly learn and explore the highly correlated representation of emotions across modalities, after learning each modality with a single deep network. The performance of the temporal multimodal deep learning model is validated on our dataset collected from smart wearable sensors and is also compared with results of recent studies. The experimental results show that the temporal multimodal deep learning models, based on early and late fusion approaches, successfully classified human emotions into one of four quadrants of dimensional emotions with an accuracy of 71.61% and 70.17%, respectively.","2169-3536","","10.1109/ACCESS.2020.3027026","Queensland University of Technology (QUT) through the Centre for Robotics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206543","Emotion recognition;electroencephalography;blood volume pulse;convolutional neural network;long short-term memory;temporal multimodal fusion","Feature extraction;Brain modeling;Electroencephalography;Physiology;Emotion recognition;Deep learning;Sensors","","49","","41","CCBY","28 Sep 2020","","","IEEE","IEEE Journals"
"Emotion Recognition with Refined Labels for Deep Learning","S. Zhang; C. Guan","School of Computer Science & Engineering, Nanyang Technological University, Singapore; School of Computer Science & Engineering, Nanyang Technological University, Singapore",2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"27 Aug 2020","2020","","","108","111","The traditional emotion classification framework usually fits all the features segments of the same trial to a fixed annotation. Considering the fact that emotion is a reaction to stimuli that lasts for varied periods, we argue that the indiscriminate annotation is equivalent to taking the emotional state as fixed within the whole trial, leading to a decrease of the classification accuracy. In this study, we attempt to alleviate this issue by developing a thresholding scheme, converting the continuous emotional trace into a three-class annotation temporally. The features within a trial are therefore assigned to varied emotional states, resulting in an improvement in the accuracy. A long short term memory (LSTM) networks-based emotion classification framework is implemented, to which the proposed thresholding scheme is applied. A subset of MAHNOB-HCI dataset with continuous emotional annotation is used. The EEG signal and frontal facial video are used for feature extraction. The experiment results demonstrate that the proposed scheme provides statistically significant improvement to the three-class classification accuracy of the EEG feature-based LSTM network (p-value = 0.0329).","2694-0604","978-1-7281-1990-8","10.1109/EMBC44109.2020.9176111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9176111","","Electroencephalography;Feature extraction;Training;Face;Kernel;Emotion recognition;Microsoft Windows","Deep Learning;Electroencephalography;Emotions;Humans;Seizures;Neural Networks, Computer","3","","7","IEEE","27 Aug 2020","","","IEEE","IEEE Conferences"
"EEG-Based Alzheimer’s Disease Detection Using Deep Learning Techniques","A. S.A.; J. T. Panachakel","Dept. of Computer Science and Engineering, College of Engineering, Trivandrum, India; Dept. of Electronics and Comm. Engg., College of Engineering, Trivandrum, India",2024 IEEE 12th Region 10 Humanitarian Technology Conference (R10-HTC),"11 Dec 2024","2024","","","1","6","Alzheimer’s disease (AD) is a neurological condition characterized by the degeneration of neuron cells, leading to cognitive impairment. This project investigates the effectiveness of EEG-based features for detecting Alzheimer’s Disease using deep learning techniques. The dataset comprises EEG recordings from healthy controls, Frontotemporal Dementia (FTD) patients, and patients diagnosed with AD. Data preprocessing involved segmenting the EEG signals into fixed-length epochs and filtering the baseline EEG into alpha, beta, and theta frequency bands. Features such as Mean Phase Coherence (MPC), Differential Entropy (DE), and Mean Squared Coherence (MSC) were computed pairwise for each epoch and frequency band. A feature matrix was constructed for each subject by stacking these features, which were then converted into images and used as input data for a ResNet50-based pre-trained model. The model was tested in leave-one-out cross-validation setting and achieved results comparable to the state-of-the-art in distinguishing between EEG recordings from healthy and AD-affected subjects.","2572-7621","979-8-3503-4020-4","10.1109/R10-HTC59322.2024.10778722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778722","Alzheimer’s disease (AD);Front Temporal Dementia (FTD);deep learning;classification;electroencephalogram (EEG)","Deep learning;Computational modeling;Coherence;Brain modeling;Feature extraction;Electroencephalography;Data models;Recording;Reliability;Alzheimer's disease","","","","8","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition from Multi-Channel EEG through Parallel Convolutional Recurrent Neural Network","Y. Yang; Q. Wu; M. Qiu; Y. Wang; X. Chen","Software School, Xiamen University, Xiamen, China; Software School, Xiamen University, Xiamen, China; Software School, Xiamen University, Xiamen, China; Software School, Xiamen University, Xiamen, China; Software School, Xiamen University, Xiamen, China",2018 International Joint Conference on Neural Networks (IJCNN),"14 Oct 2018","2018","","","1","7","As a challenging pattern recognition task, automatic real-time emotion recognition based on multi-channel EEG signals is becoming an important computer-aided method for emotion disorder diagnose in neurology and psychiatry. Traditional machine learning approaches require to design and extract various features from single or multiple channels based on comprehensive domain knowledge. Consequently, these approaches may be an obstacle for non-domain experts. On the contrast, deep learning approaches have been used successfully in many recent literatures to learn features and classify different types of data. In this paper, baseline signals are considered and a simple but effective pre-processing method has been proposed to improve the recognition accuracy. Meanwhile, a hybrid neural network which combines 'Convolutional Neural Network (CNN)' and 'Recurrent Neural Network (RNN)' has been applied to classify human emotion states by effectively learning compositional spatial-temporal representation of raw EEG streams. The CNN module is used to mine the inter-channel correlation among physically adjacent EEG signals by converting the chain-like EEG sequence into 2D-like frame sequence. The LSTM module is adopted to mine contextual information. Experiments are carried out in a segment-level emotion identification task, on the DEAP benchmarking dataset. Our experimental results indicate that the proposed pre-processing method can increase emotion recognition accuracy by 32% approximately and the model achieves a high performance with a mean accuracy of 90.80% and 91.03% on valence and arousal classification task respectively.","2161-4407","978-1-5090-6014-6","10.1109/IJCNN.2018.8489331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489331","EEG;emotion recognition;deep learning;CNN;RNN","Electroencephalography;Feature extraction;Machine learning;Two dimensional displays;Electrodes;Emotion recognition;Recurrent neural networks","","206","","20","IEEE","14 Oct 2018","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition in the Investment Activities","N. I. M. Razi; M. Othman; H. Yaacob","International Islamic University Malaysia, Kuala Lumpur, Wilayah Persekutuan, MY; Kulliyyah of Information and Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Kulliyyah of Information and Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia",2016 6th International Conference on Information and Communication Technology for The Muslim World (ICT4M),"16 Jan 2017","2016","","","325","329","Emotion plays a significant role during a decision making process and greatly influence investor's behavior. This paper investigates investors' emotional perception and exemplify how these emotions may affect their judgement in investment activities. In our experiments, the participants' emotion were derived from electroencephalogram (EEG) while they were trading in the stock market. A negative emotion can reduce the rationality of the investor in decision making and lead them to lose money. However, positive emotion can help them to minimize the risk of negative consequences. Based on the EEG-based emotion recognition, this paper presents the result of an investor that was influenced by their emotion and behaviors while making an investment decisions.","","978-1-5090-4521-1","10.1109/ICT4M.2016.072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814925","Electroencephalography (EEG);neuro-finance;emotion recognition;decision making","Stock markets;Electroencephalography;Brain modeling;Investment;Decision making;Data models;Feature extraction","","5","","14","IEEE","16 Jan 2017","","","IEEE","IEEE Conferences"
"A General Multi-Scale Time and Channel Spatiotemporal Model for EEG Signal Classification","Y. Fu; X. Liu; Y. Cui; J. Liang","School of computer science, Beijing University of Posts and Telecommunications, Beijing, China; School of computer science, Beijing University of Posts and Telecommunications, Beijing, China; School of computer science, Beijing University of Posts and Telecommunications, Beijing, China; School of computer science, Beijing University of Posts and Telecommunications, Beijing, China",2024 5th International Conference on Artificial Intelligence and Computer Engineering (ICAICE),"6 Feb 2025","2024","","","885","889","In recent years, with the help of EEG, brain - computer interface (BCI) has played a significant role in brain - related fields. Therefore, indepth analysis of EEG signals has become particularly crucial. The analysis process of EEG signals generally consists of three steps: pre - processing, feature extraction and classification. Regarding EEG signals, we propose a spatiotemporal feature composite classification model with multiple time and channel scales based on convolutional neural networks. Firstly, the model utilizes two-dimensional convolution kernels of different scales to capture time features. Then, it adaptively adjusts the importance of channels and spatial positions through an optimized Convolutional Block Attention Module (CBAM). Next, it extracts spatial features by using depth convolution kernels of different scales. Finally, it employs Bidirectional Long Short-Term Memory (BiLSTM) to conduct indepth processing on the already extracted EEG feature sequences to explore their temporal dependency relationships, and then obtains the classification results through the fully connected layer. Tests across four diverse datasets were conducted, benchmarking our approach against both high-performing deep learning and conventional techniques. The outcomes show our model's superior accuracy and F1 score across the board.","","979-8-3315-2891-1","10.1109/ICAICE63571.2024.10864229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10864229","EEG;Deep learning;Multi-scale Time and Channel Spatiotemporal Model(MSTC)","Deep learning;Attention mechanisms;Convolution;Computational modeling;Bidirectional long short term memory;Feature extraction;Brain modeling;Electroencephalography;Spatiotemporal phenomena;Kernel","","","","12","IEEE","6 Feb 2025","","","IEEE","IEEE Conferences"
"An Efficient Graph Learning System for Emotion Recognition Inspired by the Cognitive Prior Graph of EEG Brain Network","C. Li; T. Tang; Y. Pan; L. Yang; S. Zhang; Z. Chen; P. Li; D. Gao; H. Chen; F. Li; D. Yao; Z. Cao; P. Xu","Clinical Hospital of Chengdu Brain Science Institute, the Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, the Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, the Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, the Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, the Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Bioinfomatics, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, the Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, the Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, the Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; STEM, University of South Australia, Adelaide, SA, Australia; Clinical Hospital of Chengdu Brain Science Institute, the Ministry of Education (MOE) Key Laboratory for Neuroinformation, and the School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China",IEEE Transactions on Neural Networks and Learning Systems,"4 Apr 2025","2025","36","4","7130","7144","Benefiting from the high-temporal resolution of electroencephalogram (EEG), EEG-based emotion recognition has become one of the hotspots of affective computing. For EEG-based emotion recognition systems, it is crucial to utilize state-of-the-art learning strategies to automatically learn emotion-related brain cognitive patterns from emotional EEG signals, and the learned stable cognitive patterns effectively ensure the robustness of the emotion recognition system. In this work, to realize the efficient decoding of emotional EEG, we propose a graph learning system [Graph Convolutional Network framework with Brain network initial inspiration and Fused attention mechanism (BF-GCN)] inspired by the brain cognitive mechanism to automatically learn graph patterns from emotional EEG and improve the performance of EEG emotion recognition. In the proposed BF-GCN, three graph branches, i.e., cognition-inspired functional graph branch, data-driven graph branch, and fused common graph branch, are first elaborately designed to automatically learn emotional cognitive graph patterns from emotional EEG signals. And then, the attention mechanism is adopted to further capture the brain activation graph patterns that are related to emotion cognition to achieve an efficient representation of emotional EEG signals. Essentially, the proposed BF-CGN model is a cognition-inspired graph learning neural network model, which utilizes the spectral graph filtering theory in the automatic learning and extracting of emotional EEG graph patterns. To evaluate the performance of the BF-GCN graph learning system, we conducted subject-dependent and subject-independent experiments on two public datasets, i.e., SEED and SEED-IV. The proposed BF-GCN graph learning system has achieved 97.44% (SEED) and 89.55% (SEED-IV) in subject-dependent experiments, and the results in subject-independent experiments have achieved 92.72% (SEED) and 82.03% (SEED-IV), respectively. The state-of-the-art performance indicates that the proposed BF-GCN graph learning system has a robust performance in EEG-based emotion recognition, which provides a promising direction for affective computing.","2162-2388","","10.1109/TNNLS.2024.3405663","STI 2030-Major Projects(grant numbers:2022ZD0208500,2022ZD0211400); National Natural Science Foundation of China(grant numbers:62103085,U19A2082,61961160705,61901077,62076209); Dr. Cao’s Australian Research Council (ARC) DECRA Fellowship(grant numbers:DE220100265); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549833","Affective computing;cognition-inspired learning;electroencephalogram (EEG) brain networks;emotion recognition;graph neural network","Electroencephalography;Emotion recognition;Learning systems;Brain modeling;Cognition;Feature extraction;Affective computing","Humans;Electroencephalography;Emotions;Neural Networks, Computer;Cognition;Brain;Algorithms;Pattern Recognition, Automated;Attention;Machine Learning;Adult","6","","70","IEEE","5 Jun 2024","","","IEEE","IEEE Journals"
"A MEMD Method of Human Emotion Recognition Based on Valence-Arousal Model","Y. He; Q. Ai; K. Chen","School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China",2017 9th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC),"21 Sep 2017","2017","2","","399","402","Since emotions play an important role in human-machine interaction and EEG based emotion recognition is a challenging study due to nonstationary behavior of the signals, sophisticated signal processing methods are required to extract the hidden patterns in the EEG. In this research, some promising results are presented for classification of emotions induced by watching music videos, a multivariate empirical mode decomposition(MEMD) based feature extraction method is proposed to identify emotional state as high/low arousal and high/low valence. Multi-channel EEG signals are decomposed into intrinsic mode functions (IMFs), the information-bearing IMFs are selected to extract power, volatility index, hjorth parameters and asymmetrical properties on the left and right hemispher. The performances of the proposed MEMD based methods are evaluated using the publicly available DEAP data set. High/low arousal and high/low valence states of participants are recognized using the MEMD based features with KNN and SVM classifier. For the recorded 8-channel EEG signals, extracted features with KNN have accuracy rates of 67.7% and 70.5%, while SVM yields the accuracy of 67.9% and 70.9 % for arousal and valence states.","","978-1-5386-3022-8","10.1109/IHMSC.2017.201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048185","emotion recognition;EEG;multivariate empirical mode decomposition;valence and arousal","Electroencephalography;Feature extraction;Emotion recognition;Support vector machines;Brain modeling;Indexes;Standards","","4","","12","IEEE","21 Sep 2017","","","IEEE","IEEE Conferences"
"A Deep Learning Model for Classification of EEG Signals for Neuromarketing","S. M. Usman; S. M. Ali Shah; O. C. Edo; J. Emakhu","Department of Creative Technologies, Faculty of Computing and AI, Air Univesity, Islamabad, Pakistan; Department of Computer Science, Shaheed Zulfikar Ali Bhutto Institure of Science and Technology, Islamabad, Pakistan; Department of Information Systems, Auburn University at Montgomery, Alabama, USA; Department of Public Health Sciences, Henry Ford Health, Michigan, USA",2023 International Conference on IT Innovation and Knowledge Discovery (ITIKD),"19 Apr 2023","2023","","","1","6","Advertising campaigns for marketing and advertisement of different consumer items is a well-known strategy to boost sales and public awareness. It can lead towards greater profit margins for the factories or companies. Reproduction of products typically depends on numerous factors, such as market usage, reviewer comments, ratings, etc. In neuromarketing a person is examined with the help of EEG signals generated in his/her brain so that his emotions can be recognized for making certain decisions. Therefore, research in this area is in high demand but has not yet achieved an adequate standard. We provide a predictive modelling framework to interpret consumer preferences for e-commerce goods by analyzing EEG data. In this research study, volunteers of varying ages and genders were asked to visually feel the effect of different packaging of products and the corresponding EEG signals generated inside their brains were monitored. Several experiments by varying approaches were performed on the dataset that contain the EEG signals of consumers. Two machine learning and a deep learning classifier were employed to evaluate the accuracy of the model. After conducting different experiments, it was observed that the proposed approach performs superior, and the framework can be leveraged to create a more effective business model.","","978-1-6654-6372-0","10.1109/ITIKD56332.2023.10100014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10100014","EEG;Deep Learning;SVM;DT;Neuromarketing","Deep learning;Technological innovation;Tracking;Neuromarketing;Sensitivity and specificity;Brain modeling;Electroencephalography","","4","","40","IEEE","19 Apr 2023","","","IEEE","IEEE Conferences"
"Multimodal Emotion Recognition in Response to Oil Paintings","S. Luo; Y. -T. Lan; D. Peng; Z. Li; W. -L. Zheng; B. -L. Lu","Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Key Laboratory of Shanghai Ed-ucation Commission for Intelligent Interaction and Cognitive Engineering, and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, Peoples Republic of China; Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Key Laboratory of Shanghai Ed-ucation Commission for Intelligent Interaction and Cognitive Engineering, and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, Peoples Republic of China; RuiJin-Mihoyo Laboratory, Clinical Neuroscience Center, RuiJin Hospital, Shanghai Jiao Tong University, School of Medicine, Shanghai, Peoples Re-public of China; Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Key Laboratory of Shanghai Ed-ucation Commission for Intelligent Interaction and Cognitive Engineering, and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, Peoples Republic of China; Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Key Laboratory of Shanghai Ed-ucation Commission for Intelligent Interaction and Cognitive Engineering, and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, Peoples Republic of China; RuiJin-Mihoyo Laboratory, Clinical Neuroscience Center, RuiJin Hospital, Shanghai Jiao Tong University, School of Medicine, Shanghai, Peoples Re-public of China",2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"8 Sep 2022","2022","","","4167","4170","Most previous affective studies use facial expression pictures, music or movie clips as emotional stimuli, which are either too simplified without contexts or too dynamic for emotion annotations. In this work, we evaluate the effectiveness of oil paintings as stimuli. We develop an emotion stimuli dataset with 114 oil paintings selected from subject ratings to evoke three emotional states (i.e., negative, neutral and positive), and acquire both EEG and eye tracking data from 20 subjects while watching the oil paintings. Furthermore, we propose a novel affective model for multimodal emotion recognition by 1) extracting informative features of EEG signals from both the time domain and the frequency domain, 2) exploring topological information embedded in EEG channels with graph neural networks (GNNs), and 3) combining EEG and eye tracking data with a deep autoencoder neural network. From the exper-iments, our model obtains an averaged classification accuracy of 94.72 % ± 1.47 %, which demonstrates the feasibility of using oil paintings as emotion elicitation material.","2694-0604","978-1-7281-2782-8","10.1109/EMBC48229.2022.9871630","National Natural Science Foundation of China(grant numbers:61976135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9871630","","Emotion recognition;Oils;Gaze tracking;Brain modeling;Feature extraction;Electroencephalography;Data models","Emotions;Humans;Music;Neural Networks, Computer;Paintings","6","","14","IEEE","8 Sep 2022","","","IEEE","IEEE Conferences"
"Quantitative Analysis for Emotion Recognition by Using EEG Signals","W. Khairunizam; Y. J. Lai; W. Y. Choong; W. A. Mustapha","Faculty of Electrical Engineering and Technology, Universiti Malaysia Perlis, Perlis, Malaysia; Faculty of Electrical Engineering and Technology, Universiti Malaysia Perlis, Perlis, Malaysia; Faculty of Electrical Engineering and Technology, Universiti Malaysia Perlis, Perlis, Malaysia; Faculty of Electrical Engineering and Technology, Perlis, Malaysia",2024 16th International Conference on Computer and Automation Engineering (ICCAE),"1 Jul 2024","2024","","","428","431","Electroencephalogram (EEG) signal is a recording of electrical activity across the scalps. It is a biological signal that often used as emotion recognition and has been widely adopted in medical, affective computing and others relevant field. The challenge in this research is to classify emotional states from the signals produced from the brain. A proper design of experiment required to make sure a correct emotion induced regarding the task given to the subject. Therefore, this research proposes a data acquisition protocol for inducing emotional states of the subject. Three type of stimuli (audio-visual, audio, visual) and three group of emotional states (Positive, Negative, Neutral) involve in the investigation. In the signal preprocessing stage, five features representing emotion signals are selected which is energy, mean, variance, entropy, and power. Moreover, for the emotion classification K-Nearest Neighbors (KNN) and Probabilistic Neural Network (PNN) are used as the classifier. From the result, energy and power features give the best performance among the five features selected. KNN produces the average accuracy 67.15%. Moreover, PNN produces the average accuracy of 64.87%.","2154-4360","979-8-3503-7005-8","10.1109/ICCAE59995.2024.10569859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10569859","EEG;emotion recognition;classifier","Emotion recognition;Visualization;Protocols;Accuracy;Statistical analysis;Data acquisition;Scalp","","1","","13","IEEE","1 Jul 2024","","","IEEE","IEEE Conferences"
"An EEG Signal Processing System Design with Approximate Operations","Z. Yin; W. Shi; K. Liu","College of Electronic and Information Engineering, Shenzhen University; College of Electronic and Information Engineering, Shenzhen University; College of Electronic and Information Engineering, Shenzhen University","2023 IEEE International Conference on Integrated Circuits, Technologies and Applications (ICTA)","28 Dec 2023","2023","","","166","167","This paper presents an EEG signal processing system with an application to emotional recognition, which includes three primary modules: preprocessing, feature extraction, and emotion classification. A hardware stream control mode tailored for the system was devised to minimize redundant logic computations. Additionally, we used a piecewise linear approximation method for three non-linear operations utilized in the neural network (tanh activation function, sigmoid activation function, and square root function). The resulting hardware circuit achieves a dual-label classification of 16 emotions, with an average accuracy of 64.5% on the DEAP dataset. Compared to other state-of-art systems developed using DEAP dataset, this design exhibits superior real-time processing capabilities (latency of 0.02ms), while recognizing the maximum number of emotion classes without compromising additional power and area.","2831-3968","979-8-3503-4428-8","10.1109/ICTA60488.2023.10364288","National Natural Science Foundation of China(grant numbers:61974095); Natural Science Foundation of Guangdong Province, China(grant numbers:2023A1515010761,2021A1515011162); Shenzhen Science and Technology Program(grant numbers:ZDSYS20220527171402005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10364288","EEG;biomedical signal processing;brain-computer interface;deep learning;integrated circuit","Integrated circuit technology;Emotion recognition;Piecewise linear approximation;Neural networks;Signal processing;Feature extraction;Hardware","","","","7","IEEE","28 Dec 2023","","","IEEE","IEEE Conferences"
"Expression-Guided EEG Representation Learning for Emotion Recognition","S. Rayatdoost; D. Rudrauf; M. Soleymani","Swiss Center for Affective Sciences (CISA) and Computer Science Department, University of Geneva; Department of Psychology, University of Geneva; USC Institute for Creative Technologies, University of Southern California","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","3222","3226","Learning a joint and coordinated representation between different modalities can improve multimodal emotion recognition. In this paper, we propose a deep representation learning approach for emotion recognition from electroencephalogram (EEG) signals guided by facial electromyogram (EMG) and electrooculogram (EOG) signals. We recorded EEG, EMG and EOG signals from 60 participants who watched 40 short videos and self-reported their emotions. A cross-modal encoder that jointly learns the features extracted from facial and ocular expressions and EEG responses was designed and evaluated on our recorded data and MAHOB-HCI, a publicly available database. We demonstrate that the proposed representation is able to improve emotion recognition performance. We also show that the learned representation can be transferred to a different database without EMG and EOG and achieve superior performance. Methods that fuse behavioral and neural responses can be deployed in wearable emotion recognition solutions, practical in situations in which computer vision expression recognition is not feasible.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053004","EEG signals;emotion recognition;facial expression;deep learning","Emotion recognition;Electrooculography;Databases;Speech recognition;Feature extraction;Electroencephalography;Electromyography","","9","","20","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Developing an EEG-Based Emotion Recognition Using Ensemble Deep Learning Methods and Fusion of Brain Effective Connectivity Maps","S. Bagherzadeh; A. Shalbaf; A. Shoeibi; M. Jafari; R. -S. Tan; U. R. Acharya","Department of Biomedical Engineering, Islamic Azad University Science and Research Branch, Tehran, Iran; Department of Biomedical Engineering and Medical Physics, School of Medicine, Shahid Beheshti University of Medical Sciences, Tehran, Iran; Data Science and Computational Intelligence Institute, University of Granada, Granada, Spain; School of Mathematics, Physics and Computing, University of Southern Queensland, Toowoomba, QLD, Australia; National Heart Centre Singapore, Hospital Drive, Singapore; School of Mathematics, Physics and Computing, University of Southern Queensland, Toowoomba, QLD, Australia",IEEE Access,"15 Apr 2024","2024","12","","50949","50965","The objective of this paper is to develop a novel emotion recognition system from electroencephalogram (EEG) signals using effective connectivity and deep learning methods. Emotion recognition is an important task for various applications such as human-computer interaction and, mental health diagnosis. The paper aims to improve the accuracy and robustness of emotion recognition by combining different effective connectivity (EC) methods and pre-trained convolutional neural networks (CNNs), as well as long short-term memory (LSTM). EC methods measure information flow in the brain during emotional states using EEG signals. We used three EC methods: transfer entropy (TE), partial directed coherence (PDC), and direct directed transfer function (dDTF). We estimated a fused image from these methods for each five-second window of 32-channel EEG signals. Then, we applied six pre-trained CNNs to classify the images into four emotion classes based on the two-dimensional valence-arousal model. We used the leave-one-subject-out cross-validation strategy to evaluate the classification results. We also used an ensemble model to select the best results from the best pre-trained CNNs using the majority voting approach. Moreover, we combined the CNNs with LSTM to improve recognition performance. We achieved the average accuracy and F-score of 98.76%, 98.86%, 98.66 and 98.88% for classifying emotions using DEAP and MAHNOB-HCI datasets, respectively. Our results show that fused images can increase the accuracy and that an ensemble and combination of pre-trained CNNs and LSTM can achieve high accuracy for automated emotion recognition. Our model outperformed other state-of-the-art systems using the same datasets for four-class emotion classification.","2169-3536","","10.1109/ACCESS.2024.3384303","Shahid Beheshti University of Medical Sciences(grant numbers:43004477); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488403","Effective connectivity;electroencephalography;emotion recognition;long short-term memory;transfer learning","Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Long short term memory;Time-frequency analysis;Deep learning;Transfer learning","","7","","72","CCBYNCND","2 Apr 2024","","","IEEE","IEEE Journals"
"Correlation of emotion to film rating classification using EEG signal analysis","V. S. M. Dimalanta; B. C. R. Hubilla; J. C. J. S. Marquez; V. P. T. Quiambao; K. L. Tungala; S. V. Prado","University of Santo Tomas, Manila, PH; University of Santo Tomas, Manila, PH; University of Santo Tomas, Manila, PH; University of Santo Tomas, Manila, PH; Department of Electronics Engineering, University of Santo Tomas, Philippines; Department of Electronics Engineering, University of Santo Tomas, Philippines",2017 International Electrical Engineering Congress (iEECON),"23 Oct 2017","2017","","","1","4","This paper describes a research project conducted to correlate emotion to film rating classification using EEG signals measured by Emotiv device. Certain films contain inappropriate content that call for the need to classify their ratings. Every Television of Film Review Board rates a certain film based on their general criteria. Since emotion plays a huge role in decision making, this paper conducts a study of the correlation of emotion to film classification. This research is made since decision-making can be greatly affected by emotion and no research paper has studied a correlation between the two. To do this study, firstly, literature research has been performed to establish a proper approach on acquisition and processing of EEG signals. Second is the gathering of data in a controlled environment and putting this data into practice during the experimental phase. EEG data were processed to extract the features of a certain EEG. Pearson's Correlation Coefficient was used to correlate data of emotions and film ratings.","","978-1-5090-4666-9","10.1109/IEECON.2017.8075866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075866","Electroencephalogram;Film Rating Classification;Correlation","Correlation;Electroencephalography;TV;Decision making;Band-pass filters;Discrete wavelet transforms;IIR filters","","4","","16","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Emotion Recognition Through EEG Signals With Movie Clips Stimuli","J. A. C. Vazquez; J. Y. M. Pérez; R. R. Herrera","Robotics and mechatronics laboratory IPN – CIC, CDMX, México; Robotics and mechatronics laboratory IPN – CIC, CDMX, México; Department of Computer Science and Engineering IPN – ESCOM, CDMX, México",2024 4th Interdisciplinary Conference on Electrics and Computer (INTCEC),"30 Jul 2024","2024","","","1","6","This study proposes a multimodal emotion recognition method acquiring electroencephalogram (EEG) data and using movie clips to express emotional stimuli. The study included 12 participants who aimed to identify three emotions: happy, sad, and neutral. For the experiments, the Emotiv Epoc+ device was used for data collection, including data preprocessing, time and frequency domain feature extraction, and classification. Emotion recognition uses different machine learning algorithms, including support vector machines (SVM), k-nearest neighbors (k-NN), random forests, and neural networks. The algorithm is trained and evaluated using feature vectors extracted from participants' emotional responses to movie videos. This method achieves a competitive accuracy of 79.09% using an SVM(POL) classifier and features including time and frequency domains.","","979-8-3503-4945-0","10.1109/INTCEC61833.2024.10602937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10602937","EEG Signals;Emotion Recognition;Signal Processing;Emotion Classification","Support vector machines;Emotion recognition;Visualization;Accuracy;Machine learning algorithms;Frequency-domain analysis;Feature extraction","","","","18","IEEE","30 Jul 2024","","","IEEE","IEEE Conferences"
"Modified Earthworm Optimization With Deep Learning Assisted Emotion Recognition for Human Computer Interface","F. Alrowais; N. Negm; M. Khalid; N. Almalki; R. Marzouk; A. Mohamed; M. Al Duhayyim; A. a. Alneil","Department of Computer Sciences, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Department of Computer Science, College of Science and Art at Mahayil, King Khalid University, Riyadh, Saudi Arabia; Department of Computer Science, College of Computer and Information Systems, Umm Al-Qura University, Makkah, Saudi Arabia; Department of Special Education, College of Education, King Saud University, Riyadh, Saudi Arabia; Department of Information Systems, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; Research Centre, Future University in Egypt, New Cairo, Egypt; Department of Computer Science, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, Al-Kharj, Saudi Arabia; Department of Computer and Self Development, Preparatory Year Deanship, Prince Sattam Bin Abdulaziz University, Al-Kharj, Saudi Arabia",IEEE Access,"13 Apr 2023","2023","11","","35089","35096","Among the most prominent field in the human-computer interface (HCI) is emotion recognition using facial expressions. Posed variations, facial accessories, and non-uniform illuminations are some of the difficulties in the emotion recognition field. Emotion detection with the help of traditional methods has the shortcoming of mutual optimization of feature extraction and classification. Computer vision (CV) technology improves HCI by visualizing the natural world in a digital platform like the human brain. In CV technique, advances in machine learning and artificial intelligence result in further enhancements and changes, which ensures an improved and more stable visualization. This study develops a new Modified Earthworm Optimization with Deep Learning Assisted Emotion Recognition (MEWODL-ER) for HCI applications. The presented MEWODL-ER technique intends to categorize different kinds of emotions that exist in the HCI applications. To do so, the presented MEWODL-ER technique employs the GoogleNet model to extract feature vectors and the hyperparameter tuning process is performed via the MEWO algorithm. The design of automated hyperparameter adjustment using the MEWO algorithm helps in attaining an improved emotion recognition process. Finally, the quantum autoencoder (QAE) model is implemented for the identification and classification of emotions related to the HCI applications. To exhibit the enhanced recognition results of the MEWODL-ER approach, a wide-ranging simulation analysis is performed. The experimental values indicated that the MEWODL-ER technique accomplishes promising performance over other models with maximum accuracy of 98.91%.","2169-3536","","10.1109/ACCESS.2023.3264260","Deanship of Scientific Research at King Khalid University(grant numbers:RGP2/96/44); Princess Nourah bint Abdulrahman University Researchers(grant numbers:PNURSP2023R77); Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; King Saud University, Riyadh, Saudi Arabia(grant numbers:RSPD2023R521); Prince Sattam bin Abdulaziz University(grant numbers:PSAU/2023/R/1444); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10091537","Human-computer interaction;artificial intelligence;deep learning;emotion recognition;earthworm optimization algorithm","Feature extraction;Human computer interaction;Emotion recognition;Brain modeling;Convolutional neural networks;Computational modeling;Deep learning","","9","","31","CCBYNCND","3 Apr 2023","","","IEEE","IEEE Journals"
"Emotional State Classification with Distributed Random Forest, Gradient Boosting Machine and Naïve Bayes in Virtual Reality Using Wearable Electroencephalography and Inertial Sensing","N. S. Bin Suhaimi; J. Mountstephens; J. Teo","Faculty of Computing and Informatics, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Faculty of Computing and Informatics, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Faculty of Computing and Informatics, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia",2020 IEEE 10th Symposium on Computer Applications & Industrial Electronics (ISCAIE),"5 Jun 2020","2020","","","12","17","Among the various neurophysiological signal devices used for emotion classification, the collection of the human brain signal using an EEG device is the most effective way of measuring since it is portable, easy to set up and it is low-cost. The EEG device can record the different rhythmic bands (Delta, Theta, Alpha, Beta, Gamma) as well as provide inertial sensing data (gyroscope and accelerometer) which was used as this study's dataset. Furthermore, this study uses virtual reality as the platform to deliver 360-video stimuli that were designed and stitched according to the Arousal-Valence Space (AVS) model which focuses on four emotions selected from each quadrant that were representative to these emotions namely happy, angry, boring and calm which encompasses the high and low arousal states and negative and positive valences. The dataset was then classified using Distributed Random Forest (DRF), Gradient Boosting Machine (GBM) and Naïve Bayes (NB). The performance of the classifiers were compared using the five rhythmic bands with and without the inertial sensing data. The study shows that in the subject-dependent approach, classification performance improved when inertial sensing data were included as additional sensor modalities to serve as input features in the dataset that was fed to the machine learning classifiers with GBM and NB obtained classification accuracy of 67.04% and 36.24% respectively, DRF achieved classification accuracy of 82.49%.","","978-1-7281-5033-8","10.1109/ISCAIE47305.2020.9108821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108821","Machine Learning;EEG;Virtual Reality;Emotion Recognition;Neuroinformatics","","","4","","28","IEEE","5 Jun 2020","","","IEEE","IEEE Conferences"
"STA-Net: Deep Spatial-Temporal Attention Network for Emotion Detection using EEG","P. Gan","School of Mathematics and Science, China University of Geosciences Beijing, Beijing, China","2024 4th International Conference on Neural Networks, Information and Communication Engineering (NNICE)","22 Apr 2024","2024","","","142","149","EEG signals are captured from the cerebral cortex, and neuropsychological studies have confirmed that EEG signals are related to human emotions. To address the problem of undermining temporal and frequency features of EEG signals, we propose a deep learning framework, Deep Spatial-Temporal Attention Network (STA-Net), for detecting human emotions from electroencephalograms. STA-Net utilizes temporal, spatial, and channel attention mechanisms to simultaneously extract discriminative features in the temporal and spatial domains of EEG signals and improve the network’s feature representation by explicitly modeling the interdependencies between channels. This paper also proposes the use of wavelet denoising and sliding window algorithms to preprocess EEG data. The method was experimented on the DEAP database to evaluate the performance of the proposed deep learning network in the classification of low emotional arousal states and high emotional arousal states, achieving a classification accuracy of 93%, which significantly outperforms methods such as SVM and LSTM (p-0.05).","","979-8-3503-9437-5","10.1109/NNICE61279.2024.10498977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498977","deep learning;convolutional neural network;EEG;attention;emotional arousal","Deep learning;Support vector machines;Emotion recognition;Noise reduction;Feature extraction;Electroencephalography;Physiology","","","","43","IEEE","22 Apr 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition under Sleep Deprivation Using a Multimodal Residual LSTM Network","L. -Y. Tao; B. -L. Lu","Department of Computer Science and Engineering, Center for Brain-like Computing and Machine Intelligence, Shanghai, China; Department of Computer Science and Engineering, Center for Brain-like Computing and Machine Intelligence, Shanghai, China",2020 International Joint Conference on Neural Networks (IJCNN),"28 Sep 2020","2020","","","1","8","Emotion recognition under sleep deprivation is instructive for the study of mental disorders such as major depressive disorder. Previous studies on emotion recognition under sleep deprivation have been mainly based on psychological research techniques. In this paper, we introduce a multilayer weight-sharing multimodal residual LSTM network for emotion recognition under sleep deprivation. The advantage of our proposed method is that it allows for the combination of three different features: the electroencephalography (EEG) single-channel differential entropy (DE) features, EEG functional strength features with topological correlation connectivity, and eye movement features. The experiments under the conditions of sleep deprivation, sleep recovery and baseline are designed and conducted. The experimental results demonstrate that the proposed method significantly enhances the performance compared with the simple concatenation of the features of different modalities, and the best mean accuracies of 86.86% and 82.03% are achieved for four emotions (happiness, sadness, fear, and neutral) in subject-dependent and cross-subject emotion recognition tasks under 30 hours of sleep deprivation, respectively. The classification accuracy of the happiness emotion is obviously impaired under sleep deprivation, indicating that sleep deprivation impairs the stimulation of the happiness emotion, and one night of sleep recovery can reactivate the elicitation of the happiness emotion to the baseline level. Furthermore, we study the brain neural patterns of the four emotional states. The prefrontal area becomes less activated for the happiness emotion and sadness emotion in the gamma band under sleep deprivation, while the neural pattern of the fear emotion is highly robust with respect to sleep deprivation.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9206957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206957","emotion recognition;sleep deprivation;long short-term memory network;electroencephalography (EEG)","Feature extraction;Electroencephalography;Sleep;Emotion recognition;Correlation;Physiology;Task analysis","","7","","34","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"MECAM: A Novel Multi-Axis EEG Channel Attention Model for Emotion Recognition","F. Hou; W. Meng; L. Ma; K. Chen; Q. Ai; Q. Liu; S. Q. Xie","School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Electronic and Electrical Engineering, University of Leeds, Leeds, U.K.",IEEE Transactions on Instrumentation and Measurement,"24 Jul 2024","2024","73","","1","13","With the constant evolution of information technology, the paradigm of human-computer interaction has progressively transitioned from emphasizing command behavior to a novel approach that prioritizes natural language and emotional communication. In response to this trend, the development of an effective model for emotion recognition has become imperative. Consequently, this article introduces a novel multi-axis EEG channel attention model (MECAM) designed for emotion recognition. To enhance the model’s capabilities, we employ a split depth-wise convolution with a larger convolutional kernel, resulting in the creation of four parallel branches. This not only reduces the computational complexity but also expands the receptive field of the model. The incorporation of a multi-axis attention mechanism, capturing both global and local information from the output features of parallel branches, further elevates the network’s receptive field through the MBConv block. The primary objective of MECAM is to adeptly extract discriminative features from electroencephalogram (EEG) data, thereby enhancing the performance of emotion recognition. The model underwent rigorous validation using multiple datasets, including database for emission analysis using physical signals (DEAPs), SJTU emission EEG dataset (SEED), and SJTU emission EEG dataset with four emission (SEED-IV). In subject-dependent experiments, the accuracy of binary and ternary emotion classification tasks exceeded 95%, while the accuracy of the quaternary classification task surpassed 93%. In subject-independent experiments, the accuracy of most emotion classification tasks also exceeded 90%, with the exception of quaternary classification on DEAP, achieving an accuracy of 86.64%. The experimental results unequivocally underscore the superior performance of MECAM in EEG-based emotion recognition tasks.","1557-9662","","10.1109/TIM.2024.3428607","National Natural Science Foundation of China(grant numbers:52075398,52275029); Key Research and Development Program of Hubei Province(grant numbers:2022BAA066); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10599286","Electroencephalogram (EEG);emotion recognition;multi-axis attention mechanism;parallel branch;split depth-wise convolution","Brain modeling;Emotion recognition;Electroencephalography;Convolution;Computational modeling;Feature extraction;Transformers","","","","40","IEEE","15 Jul 2024","","","IEEE","IEEE Journals"
"Emotion Recognition and Channel Selection Based on EEG Signal","L. Tong; J. Zhao; W. Fu","Computer and Electronic Information College, Guangxi University, China; Computer and Electronic Information College, Guangxi University, China; Computer and Electronic Information College, Guangxi University, China",2018 11th International Conference on Intelligent Computation Technology and Automation (ICICTA),"28 Oct 2018","2018","","","101","105","As an important research direction in the field of artificial intelligence, emotion recognition has become a hot topic in current research. Because of the use of multi-channel EEG acquisition equipment nowadays, which brings many problems in practical use and post-calculation, we have also studied channel selection. In this paper, the DEAP database is used as EEG data. The multi-feature fusion in a time domain and the composite features based on wavelet feature and information entropy are used as EEG features for emotion recognition. The average recognition accuracy reached 72.03% and 71.7% respectively. We also use the ReliefF algorithm to select EEG channels. Under the premise of a slight loss of emotional recognition preparation rate, we selected the optimal combination of 6 and 13 channels, and the brain data was reduced from 32 channels to 13 channels. It lays a foundation for the development of portable, wearable devices.","","978-1-5386-8308-8","10.1109/ICICTA.2018.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8512076","Emotional Recognition;EEG;Multi-feature Fusion;RelieF;Channel Selection","Handheld computers;Automation","","35","","10","IEEE","28 Oct 2018","","","IEEE","IEEE Conferences"
"EEG-based emotion estimation using adaptive tracking of discriminative frequency components","S. Liu; D. Zhang; J. Tong; F. He; H. Qi; L. Zhang; D. Ming","Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"14 Sep 2017","2017","","","2231","2234","EEG-based emotion recognition has received increasing attention in the past few decades. The frequency components that give effective discrimination between different emotion states are subject specific. Identification of these subject-specific discriminative frequency components (DFCs) is important for the accurate classification of emotional activities. This paper investigated the potential of adaptive tracking of DFCs as an effective method for choosing the discriminative bands of EEG patterns and improving emotion recognition performance. 13 healthy volunteers were emotionally elicited by pictures selected from the International Affective Picture System (IAPS). Discriminative frequency components were tracked and analyzed for each subject and classification of three emotions (pleasant/high arousal, neutral, unpleasant/high arousal) was performed by employing a Hidden Markov Model (HMM) and a Support Vector Machine (SVM). Our results showed that adaptive tracking of DFCs improved classification accuracies significantly and the highest average accuracy of 82.85% was achieved by SVM.","1558-4615","978-1-5090-2809-2","10.1109/EMBC.2017.8037298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037298","Emotion recognition;Electroencephalographic (EEG);Discriminative band;Hidden Markov Model;Support vector machine (SVM)","Electroencephalography;Hidden Markov models;Support vector machines;Emotion recognition;Brain modeling;Bandwidth","Arousal;Electroencephalography;Emotions;Humans;Support Vector Machine","2","","21","IEEE","14 Sep 2017","","","IEEE","IEEE Conferences"
"Emotion Distribution Learning from Multi-Channel EEG Signals for Human Centred Service","X. Du; H. Qin; C. Ma; H. Wang","Department of Computer Science, University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science, University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science, University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science, University of Chinese Academy of Sciences, Beijing, China",2021 26th International Conference on Automation and Computing (ICAC),"15 Nov 2021","2021","","","1","6","It is generally accepted that emotion recognition plays an essential role not only in the field of affective computing, but also in the design process. Emotional design focuses on products/arts that attract user’s attention and induce positive emotional responses to increase the performance of a specific behaviour. Therefore, it is crucial to understand user’s emotional feedback on the product in the first instance. Electroencephalography (EEG) is a direct response to emotions, and the studies on EEG-based emotion recognition in the area of human-computer/robots interaction has received increasing attention. With more fine-grained emotion categories considered in subtle HCI applications, emotional design and intelligent human centered service systems, it is found that the emotional arousal of human beings is often a mixture of multiple fine-grained emotions. Therefore, learning user’s emotion distribution when they are engaged in the applications is significant. In this work, we propose an efficient deep neural network consists of the co-attention mechanism and LSTM modules. Specifically, the co-attention mechanism aims to learn the correlation between pair of brain regions, and the LSTM modules to learn the EEG representation. We evaluate the method on CPED (Chinese Positive Emotion Database), and experimental results show that our method can achieve the-state-of-art results.","","978-1-86043-557-7","10.23919/ICAC50006.2021.9594207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594207","Emotional design;Emotion recognition;EEG signals;human-centered service","Deep learning;Emotion recognition;Affective computing;Correlation;Automation;Databases;Electroencephalography","","1","","34","","15 Nov 2021","","","IEEE","IEEE Conferences"
"A Bi-Hemisphere Domain Adversarial Neural Network Model for EEG Emotion Recognition","Y. Li; W. Zheng; Y. Zong; Z. Cui; T. Zhang; X. Zhou","Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Southeast University, Nanjing, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Southeast University, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Information Science and Engineering Technology, Nanjing, Jiangsu, China",IEEE Transactions on Affective Computing,"27 May 2021","2021","12","2","494","504","In this paper, we propose a novel neural network model, called bi-hemisphere domain adversarial neural network (BiDANN) model, for electroencephalograph (EEG) emotion recognition. The BiDANN model is inspired by the neuroscience findings that the left and right hemispheres of human's brain are asymmetric to the emotional response. It contains a global and two local domain discriminators that work adversarially with a classifier to learn discriminative emotional features for each hemisphere. At the same time, it tries to reduce the possible domain differences in each hemisphere between the source and target domains so as to improve the generality of the recognition model. In addition, we also propose an improved version of BiDANN, denoted by BiDANN-S, for subject-independent EEG emotion recognition problem by lowering the influences of the personal information of subjects to the EEG emotion recognition. Extensive experiments on the SEED database are conducted to evaluate the performance of both BiDANN and BiDANN-S. The experimental results have shown that the proposed BiDANN and BiDANN models achieve state-of-the-art performance in the EEG emotion recognition.","1949-3045","","10.1109/TAFFC.2018.2885474","National Key Research and Development Program of China(grant numbers:2015CB351704); National Natural Science Foundation of China(grant numbers:61572009,61772276); Key Research and Development Program of Jiangsu Province, China(grant numbers:BE2016616); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8567966","EEG emotion recognition;long short term memory (LSTM);cerebral hemisphere asymmetry;adversarial network","Electroencephalography;Feature extraction;Brain modeling;Emotion recognition;Biological neural networks;Neuroscience;Data models;Emotion recognition;Generative adversarial networks","","194","","52","IEEE","7 Dec 2018","","","IEEE","IEEE Journals"
"Emotion Recognition using Deep Convolutional Neural Network on Temporal Representations of Physiological Signals","G. Singh; K. Verma; N. Sharma; A. Kumar; A. Mantri","Chitkara University Institute of Technology, Chitkara University, Punjab, India; Chitkara University Institute of Technology, Chitkara University, Punjab, India; Chitkara University Institute of Technology, Chitkara University, Punjab, India; Chitkara University Institute of Technology, Chitkara University, Punjab, India; Chitkara University Institute of Technology, Chitkara University, Punjab, India",2020 IEEE International Conference on Machine Learning and Applied Network Technologies (ICMLANT),"23 Feb 2021","2020","","","1","6","Human emotion recognition using Electroencephalography (EEG) signals has become paramount in the field of affective computing. These signals received considerable recognition from researchers, as it delivers low cost and simple results for recognizing emotions manually as well as automatic. In this research paper, we present a novel approach for emotion classification based on electroencephalographic signals taken from the AMIGOS dataset. The proposed work extract features 14 channels of EEG signals using a variant of a deep convolutional neural network on spectrogram images of signals containing time and frequency. The proposed model outperforms the previous deep learning models. Accuracy of arousal comes out to be 0.875 with an f1 score of 0.75789 while the accuracy of valence comes out 0.750 with an f1 score of 0.49693.","","978-1-7281-8885-0","10.1109/ICMLANT50963.2020.9355990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355990","electroencephalogram;spectrograms;physiological signals;DCNN","Deep learning;Emotion recognition;Brain modeling;Feature extraction;Electroencephalography;Convolutional neural networks;Spectrogram","","2","","18","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"Distillation-Based Domain Generalization for Cross-Dataset EEG-Based Emotion Recognition","W. Li; S. Wang; S. Shao; K. Huang","School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; Department of Electrical and Computer Engineering and Data Science Research Center, Duke Kunshan University, Kunshan, China",IEEE Transactions on Emerging Topics in Computational Intelligence,"","2024","PP","99","1","17","Electroencephalogram (EEG)-based emotion recognition has gradually become a research hotspot with extensive real-world applications. Differences in EEG signals across subjects usually lead to the unsatisfactory performance in subject-independent emotion recognition. To handle this challenge, many researchers have paid attention to the development of transfer learning techniques, which yield promising results. Currently, most researchers focus on transfer learning between subjects within one single dataset. However, cross-dataset transfer learning presents a great challenge, because, in this case, the data collected from different environments and equipments have much more severe variations. Domain Generalization (DG) has great potential to handle the unseen data without involving them in training. Besides, Knowledge Distillation (KD), which can transfer the knowledge learned from the teacher to the student model, has shown promise in generalization for the student model. Inspired by DG and KD, we propose a novel and effective method, Distillation-Based Domain Generalization (DBDG), for cross-dataset EEG-based emotion recognition. Specifically, DBDG contains the modules of feature extraction, online distillation and self-distillation. The feature extraction module can learn the discriminative emotional features from EEG signals; the online distillation and self-distillation modules can enhance the method generalizability. Experimental results on three public benchmark datasets, SEED, SEED-IV and DEAP, have demonstrated the effectiveness of our method for cross-dataset EEG-based emotion recognition.","2471-285X","","10.1109/TETCI.2024.3449926","Basic Research Project of Leading Technology of Jiangsu Province(grant numbers:BK20192004); State Key Laboratory of Multimodal Artificial Intelligence Systems(grant numbers:MAIS2024108); Big Data Computing Center of Southeast University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10700820","Cross-dataset emotion recognition;domain generalization;electroencephalogram (EEG);online distillation;self-distillation","Brain modeling;Emotion recognition;Electroencephalography;Training;Feature extraction;Transfer learning;Data models;Computational modeling;Knowledge engineering;Time series analysis","","","","","IEEE","30 Sep 2024","","","IEEE","IEEE Early Access Articles"
"Partial Label Learning for Emotion Recognition from EEG","G. Zhang; A. Etemad","Department of Electrical and Computer Engineering, Queen's University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, ON, Canada",IEEE Transactions on Affective Computing,"","2025","PP","99","1","15","Fully supervised learning has recently achieved promising performance in various electroencephalography (EEG) learning tasks by training on large datasets with ground truth labels. However, labeling EEG data for affective experiments is challenging, as it can be difficult for participants to accurately distinguish between similar emotions, resulting in ambiguous labeling (reporting multiple emotions for one EEG instance). This notion could cause model performance degradation, as the ground truth is hidden within multiple candidate labels. To address this issue, Partial Label Learning (PLL) has been proposed to identify the ground truth from candidate labels during the training phase, and has shown good performance in the computer vision domain. However, PLL methods have not yet been adopted for EEG representation learning or implemented for emotion recognition tasks. In this paper, we adapt and re-implement six state-of-the-art PLL approaches for emotion recognition from EEG on two large emotion datasets (SEED-IV and SEED-V). These datasets contain four and five categories of emotions, respectively. We evaluate the performance of all methods in classical, circumplex-based and real-world experiments. The results show that PLL methods can achieve strong results in affective computing from EEG and achieve comparable performance to fully supervised learning. We also investigate the effect of label disambiguation, a key step in many PLL methods. The results show that in most cases, label disambiguation would benefit the model when the candidate labels are generated based on their similarities to the ground truth rather than obeying a uniform distribution. This finding suggests the potential of using label disambiguation-based PLL methods for circumplex-based and real-world affective tasks.","1949-3045","","10.1109/TAFFC.2025.3562027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10967242","Partial label learning;deep learning;electroencephalography;emotion recognition","Electroencephalography;Phase locked loops;Emotion recognition;Brain modeling;Training;Predictive models;Labeling;Deep learning;Computer vision;Supervised learning","","","","","IEEE","16 Apr 2025","","","IEEE","IEEE Early Access Articles"
"Single-trial EEG-based emotion recognition using kernel Eigen-emotion pattern and adaptive support vector machine","Y. -H. Liu; C. -T. Wu; Y. -H. Kao; Y. -T. Chen","Department of Mechanical Engineering, Chung Yuan Christian University, Zhongli, Taiwan; School of Occupational Therapy, College of Medicine, National Taiwan University, Taipei, Taiwan; Department of Mechanical Engineering, Chung Yuan Christian University, Zhongli, Taiwan; School of Occupational Therapy, College of Medicine, National Taiwan University, Taipei, Taiwan",2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"26 Sep 2013","2013","","","4306","4309","Single-trial electroencephalography (EEG)-based emotion recognition enables us to perform fast and direct assessments of human emotional states. However, previous works suggest that a great improvement on the classification accuracy of valence and arousal levels is still needed. To address this, we propose a novel emotional EEG feature extraction method: kernel Eigen-emotion pattern (KEEP). An adaptive SVM is also proposed to deal with the problem of learning from imbalanced emotional EEG data sets. In this study, a set of pictures from IAPS are used for emotion induction. Results based on seven participants show that KEEP gives much better classification results than the widely-used EEG frequency band power features. Also, the adaptive SVM greatly improves classification performance of commonly-adopted SVM classifier. Combined use of KEEP and adaptive SVM can achieve high average valence and arousal classification rates of 73.42% and 73.57%. The highest classification rates for valence and arousal are 80% and 79%, respectively. The results are very promising.","1558-4615","978-1-4577-0216-7","10.1109/EMBC.2013.6610498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610498","","Electroencephalography;Emotion recognition;Kernel;Support vector machines;Principal component analysis;Feature extraction;Brain modeling","Adult;Arousal;Electroencephalography;Emotions;Humans;Photic Stimulation;Software;Support Vector Machine;Task Performance and Analysis;Young Adult","11","","21","IEEE","26 Sep 2013","","","IEEE","IEEE Conferences"
"Research on Hearing-Impaired EEG Emotion Recognition Based on Deep Residual Shrinkage Networks","M. Cheng; Q. Gao; Z. Bai; Y. Chen; Y. Liu; Y. Song","School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, Tianjin; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin",2023 42nd Chinese Control Conference (CCC),"18 Sep 2023","2023","","","8677","8682","With the development of affective computing and computer science, electroencephalogram (EEG) based emotion recognition has attracted much more attention. In this paper, we collected EEG signals from fifteen healthy people and fifteen hearing-impaired people when they were watching five kinds of emotional pictures (fear, anger, sadness, happiness and neutral). The collected EEG signals are preprocessed to remove artifacts by independent component analysis (ICA). Then the differential power spectral density, entropy, and wavelet entropy features were extracted (PSD, DE, WE). The Deep Residual Shrinkage Networks (DRSNs) were composed of residual building units (RBU s) and an attention mechanism was used to capture the representative features, and then made a classification. The classification results prove that using DE as a feature for classification is better than other features, with an average accuracy being 76.35% and 80.47% for the hearing-impaired group and the normal group, respectively. Moreover, from the brain topography, we found that there are certain differences in brain function between hearing-impaired and healthy people.","1934-1768","978-988-75815-4-3","10.23919/CCC58697.2023.10239923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10239923","Electroencephalogram (EEG);Deep Residual Shrinkage Networks (DRSNs);Emotion Recognition;Hearing-impaired","Computer science;Emotion recognition;Affective computing;Buildings;Independent component analysis;Surfaces;Data collection","","","","25","","18 Sep 2023","","","IEEE","IEEE Conferences"
"Decoding Emotions: Integrating EEG Signals and Facial Expressions for Advanced Multimodal Emotion Recognition","K. Moussaoui; M. Farah","Riadi Laboratory, Univ. Manouba, ISAMM, RIADI LR99ES26, University Campus, Manouba; Riadi Laboratory, Univ. Manouba, ISAMM, RIADI LR99ES26, University Campus, Manouba","2024 IEEE 7th International Conference on Advanced Technologies, Signal and Image Processing (ATSIP)","23 Aug 2024","2024","1","","530","535","Emotion recognition is widely applied in medicine, education, and human-computer interaction, with three main approaches: non-physiological, physiological, and hybrid signals. Hybrid methods show promise while non-physiological signals are easily manipulable. Our study proposes a hybrid approach that combines EEG and facial expression data using decision-level fusion. Validating our approach using the DEAP database, we focused on binary classifications for alertness and valence. Our model classifies emotions into four categories: HVLA, HVHA, LVLA, and LVHA. From the dataset, we extracted 17 features from 32 EEG channels across 5 frequency bands for each subject. We applied SVM with the RBF kernel and achieved an accuracy of $54.49 \%$. For facial expression classification, we preprocessed frames from the tests of each subject and used CNN to obtain a validation accuracy of 68.36%. In the fusion step, we combined the predicted probabilities of the four labels from the two unimodal classifiers using weighted averaging to calculate the average predicted probabilities for the final emotion classification. Our thorough approach and strong results make a meaningful contribution to the field of emotion computing and emotion recognition.","2687-878X","979-8-3503-5148-4","10.1109/ATSIP62566.2024.10638977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638977","Emotion recognition;Affective computing;Electroencephalography (EEG);Facial expressions;Decision-level fusion;DEAP database","Support vector machines;Emotion recognition;Accuracy;Image recognition;Thresholding (Imaging);Databases;Probability","","","","47","IEEE","23 Aug 2024","","","IEEE","IEEE Conferences"
"CPED: A Chinese Positive Emotion Database for Emotion Elicitation and Analysis","Y. Zhang; G. Zhao; Y. Shu; Y. Ge; D. Zhang; Y. -J. Liu; X. Sun","CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; Department of Psychology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China",IEEE Transactions on Affective Computing,"30 May 2023","2023","14","2","1417","1430","Positive emotions are of great significance to people's daily life, such as human-computer/robot interaction. However, the structure of extensive positive emotions is not clear yet and effective standardized inducing materials containing as many positive emotional categories as possible are lacking. Thus, this article aims to establish a Chinese positive emotion database (CPED) to (1) effectively elicit positive emotion categories as many as possible, (2) provide both the subjective feelings of different positive emotions and a corresponding peripheral physiological database, and (3) explore the structure and framework of positive emotion categories. 42 video clips of 16 positive emotion categories were screened from 1000+ online clips. Then a total of 312 participants watched and rated these video clips during which GSR and PPG signals were recorded. 34 video clips that met hit rate and intensity standards were systemically clustered into four emotion categories (empathy, fun, creativity and esteem). Eventually, 22 film clips of these four major categories formed the CPED database. A total of 84 features from GSR and PPG signals were extracted and entered into RF, SVM, DBN and LSTM classifiers that serves as baseline classification methods. A classification accuracy of 44.66 percent for four major categories of positive emotions was achieved.","1949-3045","","10.1109/TAFFC.2021.3088523","National Natural Science Foundation of China(grant numbers:U1736220,31771226); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9453134","Positive emotion;emotional film clip;emotion elicitation;PPG;GSR","Emotion recognition;Databases;Feature extraction;Electroencephalography;Physiology;Motion pictures;Electronic mail","","8","","93","IEEE","11 Jun 2021","","","IEEE","IEEE Journals"
"EEG-Based Emotion Recognition via Channel-Wise Attention and Self Attention","W. Tao; C. Li; R. Song; J. Cheng; Y. Liu; F. Wan; X. Chen","Department of Biomedical Engineering, Hefei University of Technology, Hefei, Anhui, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, Anhui, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, Anhui, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, Anhui, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, Anhui, China; Department of Electrical and Computer Engineering, University of Macau, Macau, China; Department of Neurosurgery, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, Anhui, China",IEEE Transactions on Affective Computing,"28 Feb 2023","2023","14","1","382","393","Emotion recognition based on electroencephalography (EEG) is a significant task in the brain-computer interface field. Recently, many deep learning-based emotion recognition methods are demonstrated to outperform traditional methods. However, it remains challenging to extract discriminative features for EEG emotion recognition, and most methods ignore useful information in channel and time. This article proposes an attention-based convolutional recurrent neural network (ACRNN) to extract more discriminative features from EEG signals and improve the accuracy of emotion recognition. First, the proposed ACRNN adopts a channel-wise attention mechanism to adaptively assign the weights of different channels, and a CNN is employed to extract the spatial information of encoded EEG signals. Then, to explore the temporal information of EEG signals, extended self-attention is integrated into an RNN to recode the importance based on intrinsic similarity in EEG signals. We conducted extensive experiments on the DEAP and DREAMER databases. The experimental results demonstrate that the proposed ACRNN outperforms state-of-the-art methods.","1949-3045","","10.1109/TAFFC.2020.3025777","National Key Research and Development Program of China(grant numbers:2019YFA0706203); National Natural Science Foundation of China(grant numbers:61922075,41901350,61701160); Provincial Natural Science Foundation of Anhui(grant numbers:2008085QF285,1808085QF186); Anhui Provincial Key Research and Development Plan(grant numbers:1804h08020244); Fundamental Research Funds for the Central Universities(grant numbers:JZ2019HGBZ0151,JZ2020HGPA0111); Science and Technology Development Fund, Macau SAR(grant numbers:055/2015/A2,0045/2019/AFJ,0018/2019/AKP); Anhui Huami Information Technology Co., Ltd.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204431","Electroencephalogram (EEG);emotion recognition;channel-wise attention;self-attention","Electroencephalography;Feature extraction;Emotion recognition;Data mining;Task analysis;Convolution;Databases","","268","","56","IEEE","22 Sep 2020","","","IEEE","IEEE Journals"
"EEG-based Happy and Sad Emotions Classification using LSTM and Bidirectional LSTM","M. Pratiwi; A. D. Wibawa; M. H. Purnomo","Dept. of Electrical Engineering, Faculty of Intelligent Electrical and Informatics Technology (ELECTICS), Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Dept. of Electrical Engineering and Computer Engineering, Faculty of Intelligent Electrical and Informatics Technology (ELECTICS), Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; University Center of Excellence on Artificial Intelligence for Healthcare and Society (UCE AIHeS)",2021 3rd International Conference on Electronics Representation and Algorithm (ICERA),"20 Sep 2021","2021","","","89","94","Emotion recognition is currently a topic that researchers widely have discussed. This is due to the significant influence of emotions on everyday life. It is not only affecting human health, but also playing an essential role in decision making. Electroencephalogram (EEG) is one of the physiological signals that can be used to measure and recognize emotions based on data from human brain activity. In this study, EEG-based happy and sad emotions classification were performed using time-domain features in the range of EEG bands low alpha(8-10Hz), high alpha(10-13Hz), low beta(13-20Hz), and high beta(20-30Hz) from four different channels namely Fp1, Fp2, F7 and F8 in 10/20 EEG system. The EEG data was obtained from 25 adult subjects. Statistical features such as Mean Absolute Value (MAV), maximum, and variance are used to distinguish between happy and sad emotions. Four classification models were tested and analyzed, namely: logistic regression, SVM, Long-Short Term Memory (LSTM), and bidirectional LSTM. The experiment result showed that the MAV feature contributed to the highest accuracy (95%) using bidirectional LSTM compared to other features. By applying the MAV feature on four different classifiers mentioned above, the highest accuracy was obtained in beta high EEG band.","","978-1-6654-3400-3","10.1109/ICERA53111.2021.9538698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9538698","emotion classification;happy and sad emotions;EEG signal;LSTM;bidirectional LSTM","Support vector machines;Emotion recognition;Brain modeling;Feature extraction;Electroencephalography;Real-time systems;Physiology","","5","","22","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"EEG based stress recognition system based on Indian classical music","R. K. Nawasalkar; P. K. Butey; S. G. Deshpande; V. M. Thakare","Dept. of Comp. Science, Arts, Comm. and Science College, Amravati (M.S.), India; Dept. of Comp. Science, Arts, Comm. and Science College, Amravati (M.S.), India; Dept. of Computer Science, Kamla Nehru Mahavidyalaya, Nagpur, India; P.G.Dept. of Computer Science, S.G.B. Amravati University, Amravati, India",2015 International Conference on Advances in Computer Engineering and Applications,"23 Jul 2015","2015","","","936","939","Emotion and stress plays a significant role in day to day life. Stress arise many complicated medical situation. The aim of this study is to create a new fusion of EEG signals for emotional stress recognition and North Indian Classical Music. In this paper, proposed a method which extracts the EEG signals with the help of scalp of the brain in responding to various stimuli, and recognize the basic emotion like Happy, anger, sad and fear. The EEG signal feature is extracted by using the method of Kernel Density Estimation and emotions can be recognized by using the Multilayer Perceptron. This method visualizes the stress perception during the listening of Raga and neural network classifiers obtained an accuracy of emotion on the flow of valence of arousal model.","","978-1-4673-6911-4","10.1109/ICACEA.2015.7164840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164840","Kernel Density Estimation (KDE);Raga;Multilayer Perceptron(MLP);Electroencephalography (EEG)","Electroencephalography;Stress;Feature extraction;Accuracy;Brain modeling;Music;Multilayer perceptrons","","4","","14","IEEE","23 Jul 2015","","","IEEE","IEEE Conferences"
"EEG-based emotion recognition in Chinese emotional words","M. Cao; G. Fang; F. Ren","Center for Intelligence Science and Technology, Beijing University of Posts and Telecommunications, Beijing, China; Center for Intelligence Science and Technology, Beijing University of Posts and Telecommunications, Beijing, China; AIM, Hefei University of Technology, China",2011 IEEE International Conference on Cloud Computing and Intelligence Systems,"13 Oct 2011","2011","","","452","456","Emotion recognition is becoming a more and more hot topic nowadays. Many researches on facial recognition and speech recognition have been done. In this paper, we develop a method to facilitate emotion recognition with electroencephalographic (EEG) signals. The main objective of our study is to reveal the relation between emotional responses and EEG data gathered through giving seven subjects word-imagination stimuli. To induce emotions we choose 20 Chinese emotional words which have relatively high emotional intensities. We use support vector machine (SVM) and linear discriminant analysis (LDA) to recognize emotion, respectively, the average recognition rates of 48.78% and 57.04% could be achieved.","2376-595X","978-1-61284-204-2","10.1109/CCIS.2011.6045108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045108","emotion recognition;EEG;SVM;Chinese emotional words","Emotion recognition;Electroencephalography;Feature extraction;Support vector machines;Humans;Physiology;Biomedical monitoring","","3","","22","IEEE","13 Oct 2011","","","IEEE","IEEE Conferences"
"Evaluating the Benefits of Asymmetry Features for Emotion Recognition","C. E. Valderrama; F. Islam Mouri; S. G. Camorlinga","Dept. of Applied Computer Science, University of Winnipeg, Winnipeg, MB, Canada; Dept. of Applied Computer Science, University of Winnipeg, Winnipeg, MB, Canada; Dept. of Applied Computer Science, University of Winnipeg, Winnipeg, MB, Canada",2023 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),"26 Oct 2023","2023","","","124","129","Emotion recognition is a topic of interest in Affective Computing (AC). While deep learning architectures have gained popularity for classification tasks, their reliance on large datasets limits their applicability when data availability is scarce. An alternative approach is feature engineering, which involves extracting relevant features to train supervised machine learning models. Neuroscientific theories on emotion processing, such as the lateralization theory, have motivated the introduction of asymmetry features for emotion prediction. However, none of these studies have statistically evaluated whether including asymmetrical features could reduce classification error or computational time. To address that direction, the current work compared two approaches for emotion recognition. The first approach used features extracted from individual EEG channels, while the second used asymmetry features calculated by matching pairs of EEG nodes. The two approaches were compared in terms of performance and fitted computational time. The comparison indicated that the performances of both approaches were not statistically significant. Notably, the asymmetry approach required less computational time for the training stage. This finding implies that incorporating asymmetry features in emotion recognition models is viable when computational resources are limited, without significantly compromising performance.","2576-7046","979-8-3503-2397-9","10.1109/CCECE58730.2023.10288676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10288676","Emotion recognition;Electroencephalogram;Affective Computing;Brain hemisphere asymmetry;Supervised machine learning","Training;Emotion recognition;Computational modeling;Deep architecture;Machine learning;Feature extraction;Brain modeling","","2","","14","IEEE","26 Oct 2023","","","IEEE","IEEE Conferences"
"EEG Brainwave Emotion Detection Using Stacked Ensembling Method","V. Jain; K. Parab; S. Kalgutkar; R. Sonkusare","Department of Electronics and Telecommunications, Sardar Patel Institute of Technology, Mumbai-58, India; Department of Electronics and Telecommunications, Sardar Patel Institute of Technology, Mumbai-58, India; Department of Electronics and Telecommunications, Sardar Patel Institute of Technology, Mumbai-58, India; Department of Electronics and Telecommunications, Sardar Patel Institute of Technology, Mumbai-58, India",2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),"3 Nov 2021","2021","","","01","07","Emotion classification based on brain signals is popular in the Brain-machine interface. In BMI, machine learning techniques have proved to show better performance than traditional classification methods. The study examines a dataset collected using various signals that are recorded as a classification of BMI systems. The study implements stacking, an ensembling technique for emotion detection to achieve improved accuracy. Eight base models namely, Support Vector Classifier, Xgboost, Lightgbm, Random Forest, Deep neural networks, Logistic Regression, and K-Nearest neighbors are trained. Accuracies ranging from 91.36% to 96.06% were achieved using the base models. The base models and the dataset form the input of the proposed stacked meta-model. Accuracy as high as 97.0% was obtained using the proposed stacked meta-model.","","978-1-7281-8595-8","10.1109/ICCCNT51525.2021.9579818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9579818","EEG;Emotion detection;Stacked ensembling;Deep learning;Support Vector Classifier;Xgboost;LightGBM;Random Forest;Logistic Regression;K-Nearest neighbors","Training;Computational modeling;Support vector machine classification;Predictive models;Brain modeling;Electroencephalography;Data models","","3","","16","IEEE","3 Nov 2021","","","IEEE","IEEE Conferences"
"Emotion Recognition Using EEG-Based Brain Computer Interface","A. S. M. S. Reaj; M. Maniruzzaman; A. Al Jaid Jim","Electronics and Communication Engineering, Discipline Khulna University, Khulna, Bangladesh; Electronics and Communication Engineering, Discipline Khulna University, Khulna, Bangladesh; Electronics and Communication Engineering, Discipline Khulna University, Khulna, Bangladesh","2021 International Conference on Electronics, Communications and Information Technology (ICECIT)","21 Dec 2021","2021","","","1","4","EEG signals can assess human emotions that perform significantly in promoting robust brain-computer interface systems. This study has been conducted utilizing a public emotional EEG dataset called SEED. Before performing classification from the EEG signals, features have been extracted to acquire information. DWT has been introduced to disintegrate the preprocessed EEG signals into five separate frequency sub-bands (alpha, beta, gamma, delta, and theta), and the statistical features of these DWT coefficients have been computed in the frequency domain. We applied three different wavelet functions to extract multiple features from the preprocessed signals and considered three statistical ones to distinguish relevant information about EEG signals. The wavelet functions are “coif5”, “db4”, and “db8”, and the statistical features are entropy, power, and standard deviation. In this experiment, the efficiency of emotion identification has been investigated for a set of 62 EEG channels. A non-linear SVM classifier has been trained using the features to categorize the signals into three mental states: negative, neutral and positive by applying 5-fold cross-validation. In our model, we achieved a training accuracy of 70%, 83.75% and 82%on 62 channels for positive, neutral and negative, respectively.","","978-1-6654-2363-2","10.1109/ICECIT54077.2021.9641223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9641223","brain computer interface;discrete wavelet transform;EEG signal;human emotions;support vector machine","Training;Emotion recognition;Frequency-domain analysis;Feature extraction;Electroencephalography;Brain-computer interfaces;Entropy","","3","","17","IEEE","21 Dec 2021","","","IEEE","IEEE Conferences"
"Support vector machine for EEG signal classification during listening to emotional music","Y. -P. Lin; C. -H. Wang; T. -L. Wu; S. -K. Jeng; J. -H. Chen","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Cardinal Tien Hospital, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan",2008 IEEE 10th Workshop on Multimedia Signal Processing,"5 Nov 2008","2008","","","127","130","An approach to recognize the emotion responses during multimedia presentation using the electroencephalogram (EEG) signals is proposed. The association between EEG signals and music-induced emotion responses was investigated in three factors, including: 1) the types of features, 2) the temporal resolutions of features, and 3) the components of EEG. The results showed that the spectrum power asymmetry index of EEG signal was a sensitive marker to reflect the brain activation related to emotion responses, especially for the low frequency bands of delta, theta and alpha components. Besides, the maximum classification accuracy was obtained around 92.73% by using support vector machine (SVM) based on 60 features derived from all EEG components with the feature temporal resolution of one second. As such, it will be able to provide key clues to develop EEG-inspired multimedia applications, in which multimedia contents could be offered interactively according to the userspsila immediate feedback.","","978-1-4244-2294-4","10.1109/MMSP.2008.4665061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4665061","","Electroencephalography;Accuracy;Support vector machines;Feature extraction;Multimedia communication;Electrodes;Emotion recognition","","55","","13","IEEE","5 Nov 2008","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition Using Similarity Measure of Brain Rhythm Sequencing","J. W. Li; S. Barma; S. H. Pun; F. Chen; C. Li; M. T. Li; P. Ke Wang; M. I. Vai; P. Un Mak","Institute of Microelectronics, University of Macau, Macau, China; Department of Electronics and Communication Engineering, Indian Institute of Information Technology Guwahati (IIITG), Guwahati, India; Institute of Microelectronics, University of Macau, Macau, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Institute of Microelectronics, University of Macau, Macau, China; Institute of Microelectronics, University of Macau, Macau, China; Institute of Microelectronics, University of Macau, Macau, China; Institute of Microelectronics, University of Macau, Macau, China; Department of Electrical and Computer Engineering, University of Macau, Macau, China",2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"9 Dec 2021","2021","","","31","34","The similarity is a fundamental measure from the homology theory in bioinformatics, and the biological sequence can be classified based on it. However, such an approach has not been utilized for electroencephalography (EEG)-based emotion recognition. To this end, the sequence generated by choosing the dominant brain rhythm owning maximum instantaneous power at each 0.2 s timestamp of the EEG signal has been proposed. Then, to recognize emotional arousal and valence, the similarity measures between pairwise sequences have been performed by dynamic time warping (DTW). After evaluations, the sequence that provides the highest accuracy has been obtained. Thus, the representative channel has been found. Besides, the appropriate time segment for emotion recognition has been estimated. Those findings helpfully exclude redundant data for assessing emotion. Results from the DEAP dataset displayed that the classification accuracies between 72%–75% can be realized by applying the single-channel data with a 5 s length, which is impressive when considering fewer data sources as the primary concern. Hence, the proposed idea would open a new way that uses the similarity measures of sequences for EEG-based emotion recognition.","2694-0604","978-1-7281-1179-7","10.1109/EMBC46164.2021.9629520","Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629520","Electroencephalography (EEG);brain rhythm sequencing (BRS);similarity measure;sequence classification;emotion recognition","Emotion recognition;Sequential analysis;Soft sensors;Scalp;Rhythm;Electroencephalography;Biology","Arousal;Brain;Electroencephalography;Emotions;Information Storage and Retrieval","1","","14","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"Emotions Recognition Based on Physiological Signals Using Machine Learning Techniques","T. Kumar; R. C. Singh; R. Kumar","School of Basic Sciences & Research, Sharda University, Greater Noida, UP, India; School of Basic Sciences & Research, Sharda University Greater, Noida, UP, India; School of Engineering & Technology, Sharda University, Greater Noida, UP, India",2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS),"25 Jan 2024","2023","","","823","827","Emotion recognition through Electroence-phalogram (EEG) involves identifying and understanding emotions by analysing the brain's electrical activity. Electroencephalogram, electrocardiogram (ECG), respiration (RSP), Galvanic Skin Response (GSR), blood volume pulse (BVP), and electromyogram (EMG), are the example of physiological signals. In this paper, emotions are categorised using EEG signals and calculate the accuracy of the classifier with F1-score. The results show that the proposed method is more accurate, has better accuracy, a manageable F1 score, and has a lower standard deviation than existing methods for the two-class problem. The proposed method is compared to some recent other studies. Finally, the performance of our suggested technique on DEAP dataset (a public dataset) is evaluated. All of the results were implemented in MATLAB R2017a.","","979-8-3503-4233-8","10.1109/ICTACS59847.2023.10390266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10390266","EEG;physiological signals;Emotion recognition;Machine Learning;DEAP dataset","Emotion recognition;Machine learning;Electrocardiography;Electroencephalography;Physiology;Skin;Mathematical models","","","","29","IEEE","25 Jan 2024","","","IEEE","IEEE Conferences"
"Multi-modal Emotion Recognition for Determining Employee Satisfaction","F. U. Zaman; M. T. Zaman; M. A. Alam; M. G. R. Alam","Department of Computer Science and Engineering, BRAC University, Dhaka, Bangladesh; Department of Computer Science and Engineering, BRAC University, Dhaka, Bangladesh; Department of Computer Science and Engineering, BRAC University, Dhaka, Bangladesh; Department of Computer Science and Engineering, BRAC University, Dhaka, Bangladesh",2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE),"1 Mar 2022","2021","","","1","6","Emotion recognition has been popular in the field of research for quite a while now. In this paper bi-modal emotion detection has been used to find employee satisfaction in workplaces. Interviews were taken of employees from different workplaces and were recorded. The recorded interviews were then used to detect the emotions of the employees from which their satisfaction level was derived. From the interviews, six different entities of emotion were detected, which are: Happiness, Sadness, Neutral, Disgust, Anger and Surprise. Two separate independent neural networks have been utilised. In one the facial expressions were detected and in the other sentiment analysis was done on the speech of the interviews after converting it into text. The extracted emotions were then fed into a Support Vector Machine (SVM) for determining the satisfaction of the employees. The satisfactions were categorized into five different levels which are: Highly dissatisfied, Dissatisfied, Neutral, Satisfied and Highly satisfied.","","978-1-6654-9552-3","10.1109/CSDE53843.2021.9718373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718373","Deep learning;Emotion recognition;Neural Network;Support Vector Machine;Sentiment Analysis.","Support vector machines;Productivity;Deep learning;Emotion recognition;Sentiment analysis;Employment;Neural networks","","","","28","IEEE","1 Mar 2022","","","IEEE","IEEE Conferences"
"Cybersickness Analysis with EEG Using Deep Learning Algorithms","D. Jeong; S. Yoo; J. Yun",Sejeong University; Sejeong University; Sejeong University,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),"15 Aug 2019","2019","","","827","835","Cybersickness is a symptom of dizziness that occurs while experiencing Virtual Reality (VR) technology and it is presumed to occur mainly by crosstalk between the sensory and cognitive systems. However, since the sensory and cognitive systems cannot be measured objectively, it is difficult to measure cybersickness. Therefore, methodologies for measuring cybersickness have been studied in various ways. Traditional studies have collected answers to questionnaires or analyzed EEG data using machine learning algorithms. However, the system relying on the questionnaires lacks objectivity, and it is difficult to obtain highly accurate measurements with the machine learning algorithms in previous studies. In this work, we apply and compare Deep Neural Network (DNN) and Convolutional Neural Network (CNN) deep learning algorithms for objective cy-bersickness measurement from EEG data. We also propose a data preprocessing for learning and signal quality weights allowing us to achieve high performance while learning EEG data with the deep learning algorithms. Besides, we analyze video characteristics where cybersickness occurs by examining the 360 video stream segments causing cybersickness in the experiments. Finally, we draw common patterns that cause cybersickness.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798334","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798334","Human-centered computing—Virtual reality;Computing methodologies—Machine learning approaches","Electroencephalography;Deep learning;Motion measurement;Machine learning algorithms;Atmospheric measurements;Particle measurements;Emotion recognition","","52","","52","IEEE","15 Aug 2019","","","IEEE","IEEE Conferences"
"Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications","B. Abibullaev; A. Keutayeva; A. Zollanvari","Department of Robotics Engineering, Nazarbayev University, Astana, Kazakhstan; Department of Robotics Engineering, Nazarbayev University, Astana, Kazakhstan; Department of Electrical and Computer Engineering, Nazarbayev University, Astana, Kazakhstan",IEEE Access,"17 Nov 2023","2023","11","","127271","127301","Brain-computer interfaces (BCIs) have undergone significant advancements in recent years. The integration of deep learning techniques, specifically transformers, has shown promising development in research and application domains. Transformers, which were originally designed for natural language processing, have now made notable inroads into BCIs, offering a unique self-attention mechanism that adeptly handles the temporal dynamics of brain signals. This comprehensive survey delves into the application of transformers in BCIs, providing readers with a lucid understanding of their foundational principles, inherent advantages, potential challenges, and diverse applications. In addition to discussing the benefits of transformers, we also address their limitations, such as computational overhead, interpretability concerns, and the data-intensive nature of these models, providing a well-rounded analysis. Furthermore, the paper sheds light on the myriad of BCI applications that have benefited from the incorporation of transformers. These applications span from motor imagery decoding, emotion recognition, and sleep stage analysis to novel ventures such as speech reconstruction. This review serves as a holistic guide for researchers and practitioners, offering a panoramic view of the transformative potential of transformers in the BCI landscape. With the inclusion of examples and references, readers will gain a deeper understanding of the topic and its significance in the field.","2169-3536","","10.1109/ACCESS.2023.3329678","Nazarbayev University under the Faculty Development Competitive Research Grant Program (FDCRGP)(grant numbers:021220FD2051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305163","Deep learning;brain–computer interfaces;review;transformer architecture;EEG;emotion recognition;seizure detection;self-attention mechanism;neural networks;motor imagery;sleep stage analysis;transformer models;CNN;BCI","Transformers;Deep learning;Brain modeling;Task analysis;Feature extraction;Electroencephalography;Electrodes;Brain-computer interfaces;Emotion recognition;Neural networks","","24","","220","CCBYNCND","2 Nov 2023","","","IEEE","IEEE Journals"
"Deep Multimodal Emotion Recognition: Fusion of Facial Features and Neurophysiological Signals","F. Safavi; V. R. Venkannagari; R. K. Vinjamuri","Department of Computer Science and Electrical Engineering, University of Maryland Baltimore County, Baltimore, USA; Department of Computer Science and Electrical Engineering, University of Maryland Baltimore County, Baltimore, USA; Department of Computer Science and Electrical Engineering, University of Maryland Baltimore County, Baltimore, USA",2024 IEEE 20th International Conference on Body Sensor Networks (BSN),"11 Dec 2024","2024","","","1","4","Multimodal emotion recognition through the fusion of facial and neurophysiological features plays an important role in various applications, such as advertising, the automotive industry, wearable devices, and human-computer interactions. Fusing human facial expressions and neurophysiological signals traditionally requires domain-specific knowledge and complex preprocessing steps. However, with the advent of deep learning, we can fully leverage the end-to-end capabilities of these techniques for the intermediate integration of facial and neurophysiological signals in emotion recognition systems. As a result, we introduce a novel end-to-end deep network that leverages transformers to learn rich feature representations of neurophysiological signals, integrated with a transformer-inspired technique for facial expression recognition and emotion classification. By integrating transformers and deep neural networks, our approach successfully captures complex temporal and spatial patterns in the data. This combination allows for more robust analysis, enhancing the system's overall performance in recognizing and classifying emotions accurately. We validated our approach through experiments on the well-known DEAP dataset, achieving performance comparable to the state-of-the-art, with accuracy rates of 97.64% for valence and 97.78% for arousal.","2376-8894","979-8-3315-3014-3","10.1109/BSN63547.2024.10780554","National Science Foundation(grant numbers:HCC-2053498); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10780554","Multimodal Emotion Recognition;Affective Computing;Deep learning;Emotion Detection;Transformer","Human computer interaction;Emotion recognition;Accuracy;Face recognition;Artificial neural networks;Transformers;Electroencephalography;Reliability;Wearable devices;Usability","","1","","15","IEEE","11 Dec 2024","","","IEEE","IEEE Conferences"
"MTDN: Learning Multiple Temporal Dynamics Representation for Emotional Valence Classification with EEG.","C. Tong; Y. Ding; K. J. Lim; C. Guan","School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Wilmar International, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore",2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"11 Dec 2023","2023","","","1","4","Emotion recognition from electroencephalogram (EEG) requires computational models to capture the crucial features of the emotional response to external stimulation. Spatial, spectral, and temporal information are relevant features for emotion recognition. However, learning temporal dynamics is a challenging task, and there is a lack of efficient approaches to capture such information. In this work, we present a deep learning framework called MTDN that is designed to capture spectral features with a filterbank module and to learn spatial features with a spatial convolution block. Multiple temporal dynamics are jointly learned with parallel long short-term memory (LSTM) embedding and self-attention modules. The LSTM module is used to embed the time segments, and then the self-attention is utilized to learn the temporal dynamics by intercorrelating every embedded time segment. Multiple temporal dynamics representations are then aggregated to form the final extracted features for classification. We experiment on a publicly available dataset, DEAP, to evaluate the performance of our proposed framework and compare MTDN with existing published results. The results demonstrate improvement over the current state-of-the-art methods on the valence dimension of the DEAP dataset.","2694-0604","979-8-3503-2447-1","10.1109/EMBC40787.2023.10340760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340760","Deep learning;self-attention;electroencephalography;emotional valence","Deep learning;Emotion recognition;Filter banks;Information representation;Feature extraction;Brain modeling;Electroencephalography","Electroencephalography;Emotions;Recognition, Psychology;Neural Networks, Computer","","","15","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Movie Oriented Positive Negative Emotion Classification from EEG Signal using Wavelet transformation and Machine learning Approaches","A. S. M. Miah; J. Shin; M. A. M. Hasan; M. K. I. Molla; Y. Okuyama; Y. Tomioka","School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; Computer Science and Engineering, University of Rajshahi, Rajshahi, Bangladesh; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan",2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC),"16 Jan 2023","2022","","","26","31","Electroencephalography (EEG) sensor plays an important role in developing brain-computer interfaces (BCI) to enhance human-computer interaction (HCI). Nowadays, various types of research works are performed to develop EEG-based HCI systems for controlling and monitoring systems. However, researchers are still facing challenges in developing this system due to noise from the physiological and internal and external artefacts. This study proposed a method to find useful electrodes and extract potential information from the brain nerves for the classification of positive or negative emotions. The collected emotion's EEG signal is recorded using 14 electrodes from the 30-younger people. Two movies were used for positive and negative emotions. In the proposed method, we first extracted the five bands wavelet transform from the EEG and then calculated the standard deviation (SD), average power (AVP) and mean absolute value (MAV) of the five bands wavelet information. Finally, we applied an extra tree classifier (ETC), random forest (RF), and support vector machine (SVM) to classify the emotion based on the feature vector. Among three classifiers ETC achieved higher performance accuracy in F3, FC5, T8, FC6, F8, and AF4 electrodes. This indicates that the F3, FC5, T8, FC6, F8, and AF4 electrodes carry potential information in positive-negative emotion classification.","2771-3075","978-1-6654-6499-4","10.1109/MCSoC57363.2022.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10008424","brain-computer interfaces (BCI);human-computer interaction (HCI);mean absolute value (MAV);extra tree classifier (ETC);wavelet transform","Wavelet transforms;Electrodes;Human computer interaction;Support vector machines;Electric potential;Motion pictures;Feature extraction","","11","","35","IEEE","16 Jan 2023","","","IEEE","IEEE Conferences"
"EEG-Based Human Emotion Recognition Using Deep Learning","H. Ahmed; Z. Ali; S. Narejo; A. Irfan; D. Azeem","Computer Systems Engineering, Mehran University of Engineering and Technology, Jamshoro, Pakistan; Computer Systems Engineering, Mehran University of Engineering and Technology, Jamshoro, Pakistan; Computer Systems Engineering, Mehran University of Engineering and Technology, Jamshoro, Pakistan; Computer Systems Engineering, Mehran University of Engineering and Technology, Jamshoro, Pakistan; Computer Systems Engineering, Mehran University of Engineering and Technology, Jamshoro, Pakistan",2024 IEEE 1st Karachi Section Humanitarian Technology Conference (KHI-HTC),"2 Apr 2024","2024","","","1","7","To bridge the communication barrier and improve mobility for individuals who struggle to understand emotions, this paper present a novel solution. A deep learning algorithm capable of recognizing and interpreting emotions from EEG signals. This research addresses a crucial challenge in improving the quality of life for affected individuals. By harnessing the power of EEG data analysis, the aim is to enhance their ability to engage with the world and others. In this study, using the DEAP dataset, we trained and tested the algorithm, and found that the CNN model had the highest accuracy of 92%. These results demonstrate the feasibility of using EEG signals to accurately recognize human emotions.","","979-8-3503-7304-2","10.1109/KHI-HTC60760.2024.10481938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481938","Human Emotions;Electroencephalogram (EEG);Deep learning;Machine learning;Convolutional Neural Network;LSTM.","Deep learning;Training;Emotion recognition;Data analysis;Costs;Anxiety disorders;Electroencephalography","","1","","37","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"MGMS: A Modality-General and Modality-Specific Learning Model Using EEG and Facial Expression for Hearing-Impaired Emotion Recognition","Q. Wu; M. Zhu; W. Xu; J. Wang; Z. Mao; Q. Gao; Y. Song","School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Technical College for the Deaf, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, TUT Maritime College, Tianjin University of Technology, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China",IEEE Transactions on Instrumentation and Measurement,"29 May 2024","2024","73","","1","11","Currently, most research on emotion recognition primarily relies on single-modal physiological or nonphysiological methods, overlooking the complementarity of emotion representation across different modalities. Individuals with hearing impairments may experience emotional cognitive biases due to the loss of the emotional acquisition pathway associated with hearing. Therefore, this study introduces the modality-general and modality-specific (MGMS) learning model, which aims to examine the emotions of hearing-impaired individuals in four categories (fear, happy, neutral, and sad) through the fusion of electroencephalogram (EEG) and facial expression. Specifically, the differential entropy (DE) features are manually extracted from each EEG channel by different brain regions, and then the spatial information is captured by a long short-term memory (LSTM) network. In terms of facial expression, texture features and geometric features are combined which are extracted by the ResNet network and 68 facial key points, respectively. By constructing a general-specific discriminator, the MGMS features are separated from the two modes. Furthermore, a Transformer encoder is employed to classify the four features using a cross-entropy loss function. Experimental results demonstrate that the proposed methods achieve an average classification accuracy of 86.01% for subject-dependent classification, surpassing the respective accuracies of 65.12% for EEG and 59.86% for facial expressions.","1557-9662","","10.1109/TIM.2024.3400341","National Natural Science Foundation of China(grant numbers:62103299); Program of Graduate Education and Teaching Reform in Tianjin University of Technology(grant numbers:ZDXM2202); Tianjin University of Technology Education Foundation(grant numbers:TJ22-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530158","Electroencephalogram (EEG);emotion recognition;facial expression;hearing-impaired subjects;multimodal","Feature extraction;Electroencephalography;Emotion recognition;Brain modeling;Physiology;Face recognition;Long short term memory","","","","45","IEEE","13 May 2024","","","IEEE","IEEE Journals"
"High Order Frequency Features as Emotion Discriminators","A. B. R. Massaranduba; B. F. O. Coelho; L. R. Sampaio; R. P. Ramos","Health and Biological Sciences Postgraduate Program, Federal University of São Francisco Valley (UNIVASF), Petrolina, PE, Brazil; Health and Biological Sciences Postgraduate Program, Federal University of São Francisco Valley (UNIVASF), Petrolina, PE, Brazil; Health and Biological Sciences Postgraduate Program, Federal University of São Francisco Valley (UNIVASF), Petrolina, PE, Brazil; Health and Biological Sciences Postgraduate Program, Federal University of São Francisco Valley (UNIVASF), Petrolina, PE, Brazil",2021 International Conference on e-Health and Bioengineering (EHB),"30 Dec 2021","2021","","","1","4","Considering that emotions are crucial for the correct interpretation of an individual’s actions, knowledge about the user’s emotional state is fundamental to make human-machine interaction (HMI) more natural. Countless works have sought to automate the process of recognizing emotions, being the analysis of brain signals obtained through electroencephalograms (EEG) one of the most used techniques. Knowing that EEG signals are non-stationary and contain relevant information in various frequency bands, analysis in the spectral domain is a good alternative to extract relevant features from these signals. In this context, HOS (Higher-order spectra) is an effective method for analyzing EEG. Therefore, the objective of this work was to develop a methodology for recognizing emotions through EEG signals using the HOS tool in the feature extraction stage. In this study, the signals from a database collected at the Federal University of São Francisco Valley were used to obtain images from HOS, referring to two classes of emotions. Afterwards, in the classification stage, those images were used to train and test a pre-trained convolutional neural network (CNN) VGG-19. The developed model reached an accuracy of 80.66%.","2575-5145","978-1-6654-4000-4","10.1109/EHB52898.2021.9657606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9657606","EEG;Signal analysis;HOS;Emotions;CNN","Emotion recognition;Databases;Image synthesis;Frequency-domain analysis;Feature extraction;Electroencephalography;Convolutional neural networks","","1","","18","IEEE","30 Dec 2021","","","IEEE","IEEE Conferences"
"A Multi-Frequency Bands Residual Network with Multilevel Scales for EEG Emotion Recognition","W. Xu; Y. Song; X. Song; X. Zhao; Q. Gao","Tianjin University of Technology, School of Electrical Engineering and Automation, Tianjin, China; Tianjin University of Technology, School of Electrical Engineering and Automation, Tianjin, China; Tianjin University of Technology Engineering Training Center, Tianjin, China; Tianjin University of Technology, School of Electrical Engineering and Automation, Tianjin, China; Tianjin University of Technology, School of Maritime, Tianjin, China",2023 China Automation Congress (CAC),"19 Mar 2024","2023","","","7196","7201","In this paper, a multi-frequency bands residual framework with multilevel scales is proposed for EEG-based emotion recognition. Time-frequency information of EEG signals is captured by calculating the Hjorth mobility (HM) of the Hjorth parameter feature set across various frequency ranges. Additionally, feature matrix based on HM and differential entropy (DE) are constructed to represent the spatial domain information. By combining the DE and HM feature matrixes from each paralleled five frequency bands, the proposed multi-scale residual network can learn feature maps for each frequency band. A lightweight CNN with a full connectivity layer is used for cross-band learning to complete emotion classification. The performance of the emotion classification strategy is evaluated using the SEED and SEED-IV datasets, and achieves an outperforming classification accuracy. Furthermore, by analysing the normalized HM values, it was found that there is greater volatility and degree of fluctuation in the gamma and beta frequency bands, which are helpful for the EEG emotion classification task.","2688-0938","979-8-3503-0375-9","10.1109/CAC59555.2023.10450271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450271","multi-frequency bands;residual network;electroencephalography (EEG);emotion recognition","Emotion recognition;Time-frequency analysis;Fluctuations;Brain modeling;Electroencephalography;Real-time systems;Entropy","","","","18","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"Classification of human emotions from EEG signals using SVM and LDA Classifiers","A. Bhardwaj; A. Gupta; P. Jain; A. Rani; J. Yadav","Division of Instrumentation and Control Engineering, University of Delhi, Delhi, India; Division of Instrumentation and Control Engineering, University of Delhi, Delhi, India; Division of Instrumentation and Control Engineering, University of Delhi, Delhi, India; Division of Instrumentation and Control Engineering, University of Delhi, Delhi, India; Division of Instrumentation and Control Engineering, University of Delhi, Delhi, India",2015 2nd International Conference on Signal Processing and Integrated Networks (SPIN),"27 Apr 2015","2015","","","180","185","Emotion Detection has been a topic of great research in the last few decades. It plays a very important role in establishing human computer interface. We as humans are able to understand the emotions of other person but it is literally impossible for the computer to do so. The present work is to achieve the same as accurately as possible. Emotion detection can be done either through text, speech, facial expression or gesture. In the present work the emotions are detected using Electroencephalography (EEG) signals. EEG records the electrical activity within the neurons of the brain. The main advantage of using EEG signals is that it detects real emotions arising straight from our mind and ignores external features like facial expressions or gesture. Hence EEG can act as real indicator of the emotion depicted by the subject. We have employed Independent Component Analysis (ICA) and Machine Learning techniques such as Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) to classify EEG signals into seven different emotions. The accuracy achieved with both the algorithms is computed and compared. We are able to recognize seven emotions using the two algorithms, SVM and LDA with an average overall accuracy of 74.13% and 66.50% respectively. This accuracy was achieved after performing a 4-fold cross-validation. Future applications of emotion detection includes neuro-marketing, market survey, EEG based music therapy and music player.","","978-1-4799-5991-4","10.1109/SPIN.2015.7095376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7095376","EEG;Emotion Detection;IAPS;ICA;LDA;Machine Learning;Neuro-marketing;SVM","Electroencephalography;Accuracy;Support vector machines;Feature extraction;Brain modeling;Training;Electrodes","","83","","21","IEEE","27 Apr 2015","","","IEEE","IEEE Conferences"
"Relevance Vector Machine Based EEG Emotion Recognition","L. Xin; S. Xiao-Qi; Q. Xiao-Ying; S. Xiao-Feng","Institute of Biomedical Engineering, Yanshan University, Qinhuangdao, China; Institute of Biomedical Engineering, Yanshan University, Qinhuangdao, China; Institute of Biomedical Engineering, Yanshan University, Qinhuangdao, China; Institute of Biomedical Engineering, Yanshan University, Qinhuangdao, China","2016 Sixth International Conference on Instrumentation & Measurement, Computer, Communication and Control (IMCCC)","8 Dec 2016","2016","","","293","297","Personal emotions accompany us in our daily life, affecting our learning and work, therefore it is necessary to obtain better understanding of human behavior through emotional assessment. This paper proposes a method for recognizing emotions electroencephalography(EEG) based on relevance vector machine(RVM). Emotional states of two types as positive and negative were selected from a standard database of DEAP, with relevance vector machine and support vector machine(SVM) to apply classification and comparison. Experimental results show that RVM classification accuracy was 93.33% and the test run time was 0.0156s, while SVM classification accuracy was 78.67% and the test run time was 0.0211s. Compared with SVM, RVM's time complexity and test error rate are lower, and its classification performance is better.","","978-1-5090-1195-7","10.1109/IMCCC.2016.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774786","Relevance vector machine;Emotion recognition;EEG","Entropy;Electroencephalography;Feature extraction;Kernel;Emotion recognition;Support vector machine classification","","13","","12","IEEE","8 Dec 2016","","","IEEE","IEEE Conferences"
"Emotion classification from EEG signals","S. K. Roy; C. Ralekar; T. K. Gandhi","Department of Electrical Engineering, IIT Delhi, New Delhi, India; Department of Electrical Engineering, IIT Delhi, New Delhi, India; Department of Electrical Engineering, IIT Delhi, New Delhi, India",2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom),"31 Oct 2016","2016","","","2543","2546","Emotions are the state of feelings resulting from physical and psychological change which in turn influence our behaviour. Emotions play an important role in corporate activities like organizing, planning, motivating etc. The negative emotions may lead to chronic emotional disorders like depression, anxiety, stress. Therefore, there is a need to analyse and classify these emotional changes through brain signals. The proposed paper aims to classify the types of emotions by observing EEG signals. There is always a problem of correlated data with EEG signals. This correlation results in redundancy of the data and increases computational time. This paper proposes the use of common spatial pattern to address the problem of correlated data and high dimensionality. The classification is carried out using support vector machine and the performance of the classifier is being measured in terms of accuracy of classification and standard deviation.","","978-9-3805-4421-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724720","EEG;support vector machne;common spatial pattern;depression","Decision support systems;Handheld computers","","","","13","","31 Oct 2016","","","IEEE","IEEE Conferences"
"Automatic Emotion Recognition from EEG Signal Utilizing Wavelet Packet Node Reconstruction and a CNN Classifier","O. Saha; M. S. Mahmud; S. A. Fattah","Department of Electrical and Computer Engineering, The University of Maryland, College Park, Maryland, United States; Department of Electrical and Electronic Engineering, East Delta University, Chattogram, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh",2023 26th International Conference on Computer and Information Technology (ICCIT),"27 Feb 2024","2023","","","1","6","Emotion recognition using electroencephalogram (EEG) has encompassed a broad research area in the domain of affective computing since the change of emotional states in response to variations in brain activities. In this paper, a novel emotion recognition approach is proposed employing the wavelet decomposition and node reconstruction of EEG signals. Following the wavelet decomposition of the EEG signal into five different levels, a node reconstruction scheme is employed to reconstruct the signal at each level to retain the original signal length. Subsequently, a 2D matrix is formed by combining the reconstructed signal information at all decomposition levels. In view of incorporating the information of multi-channel EEG signal, a 3D frame is formed combining the 2D matrices acquired from all available channels. The frame is applied as an input to a 2D deep neural network where the signal information at different decomposition levels is passed to the depth dimension of the neural network. The proposed method offers substantial performance in the classification process for the valence and arousal domain considering the performance of all subjects. The average accuracies obtained for valence and arousal domains in binary class problems are 93.06% and 91.93% respectively.","","979-8-3503-5901-5","10.1109/ICCIT60459.2023.10440995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440995","Emotion Recognition;Electroencephalogram (EEG);Wavelet Decomposition;Signal Reconstruction;Convolutional Neural Network","Emotion recognition;Wavelet domain;Three-dimensional displays;Electroencephalography;Wavelet packets;Discrete wavelet transforms;Matrix decomposition","","1","","20","IEEE","27 Feb 2024","","","IEEE","IEEE Conferences"
"A Multi-Dimensional Graph Convolution Network for EEG Emotion Recognition","G. Du; J. Su; L. Zhang; K. Su; X. Wang; S. Teng; P. X. Liu","School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; School of Guangdong University of Technology, Guangzhou, China; Carleton University, Ottawa, ON, Canada",IEEE Transactions on Instrumentation and Measurement,"23 Sep 2022","2022","71","","1","11","Due to the changeable, high-dimensional, nonstationary, and other characteristics of electroencephalography (EEG) signals, the recognition of EEG signals is mostly limited to independent individuals. To deal with these issues, we propose a multidimensional graph convolution network (MD-GCN), which integrates EEG signals’ temporal and spatial characteristics and can classify emotions more accurately. First, we use that the asymmetry of neuron activity in the left and right hemispheres is very important for emotion prediction to initialize the adjacency matrix and perform preliminary edge prediction without considering node features. Then, we perform the feature fusion on the inception network and then input it into the GCN to learn the interrelationship between channels. Finally, we visually analyze the adjacency matrix. To evaluate the performance of the model, we conduct experiments on the SEED dataset and the SEED-IV dataset. The results show that the predefined adjacency matrix method can improve the accuracy of emotion recognition, and the graph convolution has a better performance than the same type of convolution. It also theoretically shows that the emotional state is mainly by the interaction of important brain regions.","1557-9662","","10.1109/TIM.2022.3204314","National Natural Science Foundation of China(grant numbers:61973126); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515010140); Innovation Team of the Modern Agriculture Industry Technology System in Guangdong Province(grant numbers:2019KJ139); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010166006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9877879","Deep neural network;electroencephalography (EEG);emotion recognition;graph convolution network (GCN);SEED","Electroencephalography;Feature extraction;Emotion recognition;Convolution;Brain modeling;Convolutional neural networks;Entropy","","35","","44","IEEE","5 Sep 2022","","","IEEE","IEEE Journals"
"A 10.13µJ/Classification 2-Channel Deep Neural Network Based SoC for Negative Emotion Outburst Detection of Autistic Children","A. R. Aslam; M. A. B. Altaf","Electrical Engineering Department, Lahore University of Management Sciences, Lahore, Pakistan; Electrical Engineering Department, Lahore University of Management Sciences, Lahore, Pakistan",IEEE Transactions on Biomedical Circuits and Systems,"9 Dec 2021","2021","15","5","1039","1052","An electroencephalogram (EEG)-based non-invasive 2-channel neuro-feedback SoC is presented to predict and report negative emotion outbursts (NEOB) of Autistic patients. The SoC incorporates area-and-power efficient dual-channel Analog Front-End (AFE), and a deep neural network (DNN) emotion classification processor. The classification processor utilizes only the two-feature vector per channel to minimize the area and overfitting problems. The 4-layers customized DNN classification processor is integrated on-sensor to predict the NEOB. The AFE comprises two entirely shared EEG channels using sampling capacitors to reduce the area by 30%. Moreover, it achieves an overall integrated input-referred noise, NEF, and crosstalk of 0.55 µVRMS, 2.71, and −79 dB, respectively. The 16 mm2 SoC is implemented in 0.18 um 1P6M, CMOS process and consumes 10.13 μJ/classification for 2 channel operation while achieving an average accuracy of >85% on multiple emotion databases and real-time testing.","1940-9990","","10.1109/TBCAS.2021.3113613","National Center of Artificial Intelligence Research Fund(grant numbers:RF-NCAI-030); Syed Babar Ali Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9541057","Autism;classification processor;deep neural network (DNN);electroencephalogram (EEG);emotion detection;neurological disorder","Feature extraction;Electroencephalography;System-on-chip;Classification algorithms;Deep learning;Real-time systems;Autism","Autistic Disorder;Child;Emotions;Equipment Design;Humans;Neural Networks, Computer;Signal Processing, Computer-Assisted","22","","53","IEEE","20 Sep 2021","","","IEEE","IEEE Journals"
"Performance Analysis of FTRL algorithm for Dense and Deep Model of Music Mood Classification","S. Sharma; N. Sharma; A. Sindgi; A. Malhan; P. T; S. E","CSE Department SOCSE, Presidency University, Bangalore, India; ECE Department SOE, Presidency University, Bangalore, India; ECE Department SOE, Presidency University, Bangalore, India; AI and DSDepartment, New Horizon Institue of Technolgy and Management, Thane, India; ECE Department SOE, Presidency University, Bangalore, India; ECE Department SOE, Nitte Meenakshi Institute of Technology, Bangalore, India",2024 International Conference on Recent Innovation in Smart and Sustainable Technology (ICRISST),"20 Mar 2025","2024","","","1","4","The music mood classification problem is finding a lot of utility in the development of recommender systems designed related to health, exercising, listening and many others. Music induces change in the emotion or mood of a person. Analysis of effect of music on human electroencephalogram EEG reveals the stimulus induced. However, classification of the mood of the music is the preliminary step towards analyzing the effect of music. The preferred choice for such problems in Machine learning algorithm is KNN (K -Nearest Neighbors) Music mood classification is generally solved using the ADAM optimizer, and experimentation using FTRL is not done extensively. However, due to the fast convergence of FTRL, it can be tested for dense and deep networks too. This paper explores the performance of FTRL algorithm in music mood classification problem for calculating accuracy and loss. The results obtained in the paper are encouraging and matches the efficiency of KNN the performance of other optimizers like ADAM with an additional advantage of faster convergence.","","979-8-3503-0914-0","10.1109/ICRISST59181.2024.10921949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10921949","FTRL;Emotion-Classification;optimization;music-classification;machine-learning","Technological innovation;Mood;Music;Nearest neighbor methods;Classification algorithms;Performance analysis;Multiple signal classification;Recommender systems;Optimization;Convergence","","","","22","IEEE","20 Mar 2025","","","IEEE","IEEE Conferences"
"An Novel Approach to Predict and Classify the Mental State of Person using EEG-based Brain-Computer Interface","Sanaullah; R. N. Ali; M. F. Shahid","Department of Computer Science, FAST-NUCES, Karachi, Pakistan; Department of Computer Science, FAST-NUCES, Karachi, Pakistan; Department of Computer Science, FAST-NUCES, Karachi, Pakistan","2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)","20 Mar 2023","2022","","","1","6","A person’s present state of mind is determined by a complex collection of brain activities that make up their mental state. It is influenced by several internal and external aspects of the brain. By examining an individual’s EEG patterns, one can ascertain their mental state. In order to recognise and alter harmful or troubling thinking patterns that have a detrimental impact on behaviour and emotions, we classified three different states as: relaxed, neutral, and focused. To classify and predict the behaviour of a person based on certain mental states, we deployed popular machine learning models like k-NN, RF, XGBOOST, and EL to classify different mental states. Moreover, to predict the mental states, we implemented deep learning models like CNN, RNN, and LSTM. XGBoost achieves the highest classification accuracy (97.29%) with 5-fold cross validation. For the prediction, RNN achieved the highest prediction accuracy of 97.84%.","","978-1-6654-9087-0","10.1109/ICETECC56662.2022.10069504","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10069504","EEG;Brain–Computer Interface;Mental State;CNN;RNN;LSTM;k-NN;Random Forest;XGBoost;Ensemble Learning","Radio frequency;Deep learning;Emotion recognition;Predictive models;Brain modeling;Electroencephalography;Brain-computer interfaces","","","","21","IEEE","20 Mar 2023","","","IEEE","IEEE Conferences"
"Adversarial Domain Adaptation-Based EEG Emotion Transfer Recognition","T. Li; Z. Wang; H. Liu","School of Computational Science and Computer Science, Xi’an Polytechnic University, Xi’an, China; School of Computational Science and Computer Science, Xi’an Polytechnic University, Xi’an, China; School of Computational Science and Computer Science, Xi’an Polytechnic University, Xi’an, China",IEEE Access,"24 Feb 2025","2025","13","","32706","32723","This paper introduces an Emotion Domain Adversarial Neural Network (EDANN) model for Electroencephalogram (EEG) emotion recognition, designed to accomplish EEG emotion classification across various periods and subjects. The model is composed of three essential components: an encoder, a label classifier, and a domain discriminator. Utilizing adversarial training, EDANN can extract features that are discriminative among different categories and invariant across domains. The study employed two public datasets: the DEAP dataset, which includes EEG signals from 32 participants for emotion analysis; and the SEED dataset, comprising emotional responses from 15 Chinese participants to 15 Chinese film clips. Both datasets utilized specific emotion models for labeling emotions, albeit with varying levels of precision. Extensive experiments on both Session-to-Session and Subject-to-Subject transfer tasks have demonstrated the proposed model’s superior performance in terms of accuracy, precision, recall, and F1 score for emotion recognition. The findings of this study not only illustrate the efficacy of EDANN in EEG-based emotion recognition but also underscore the importance of considering significant inter-individual differences when designing and evaluating machine learning models. These methodologies enable researchers to utilize limited data resources more efficiently, thus propelling the advancement of emotion recognition technology.","2169-3536","","10.1109/ACCESS.2025.3540436","National Natural Science Foundation of China(grant numbers:62207020); Shaanxi Provincial Department of Science and Technology General(grant numbers:S2022-JC-YB-0521); Shaanxi Provincial Department of Education(grant numbers:22JS020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10879258","EEG emotion recognition;deep learning;domain adaptation;adversarial training","Brain modeling;Electroencephalography;Emotion recognition;Adaptation models;Feature extraction;Computational modeling;Training;Deep learning;Manifolds;Costs","","","","44","CCBY","10 Feb 2025","","","IEEE","IEEE Journals"
"AdamGraph: Adaptive Attention-Modulated Graph Network for EEG Emotion Recognition","C. L. Philip Chen; B. Chen; T. Zhang","Guangdong Provincial Key Laboratory of Computational AI Models and Cognitive Intelligence, the School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Computational AI Models and Cognitive Intelligence, the School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Computational AI Models and Cognitive Intelligence, the School of Computer Science and Engineering, South China University of Technology, Guangzhou, China",IEEE Transactions on Cybernetics,"23 Apr 2025","2025","55","5","2038","2051","The underlying time-variant and subject-specific brain dynamics lead to inconsistent distributions in electroencephalogram (EEG) topology and representations within and between individuals. However, current works primarily align the distributions of EEG representations, overlooking the topology variability in capturing the dependencies between channels, which may limit the performance of EEG emotion recognition. To tackle this issue, this article proposes an adaptive attention-modulated graph network (AdamGraph) to enhance the subject adaptability of EEG emotion recognition against connection variability and representation variability. Specifically, an attention-modulated graph connection module is proposed to explicitly capture the individual important relationships among channels adaptively. Through modulating the attention matrix of individual functional connections using spatial connections based on prior knowledge, the attention-modulated weights can be learned to construct individual connections adaptively, thereby mitigating individual differences. Besides, a deep node-graph representation learning module is designed to extract long-range interaction characteristics among channels and alleviate the over-smoothing problem of representations. Furthermore, a graph domain co-regularized learning module is imposed to tackle the individual distribution discrepancies in connection and representations across different domains. Extensive experiments on three public EEG emotion datasets, i.e., SEED, DREAMER, and MPED, validate the superior performance of AdamGraph compared with state-of-the-art methods.","2168-2275","","10.1109/TCYB.2025.3550191","National Natural Science Foundation of China(grant numbers:62222603); STI2030-Major Projects grant from the Ministry of Science and Technology of the People’s Republic of China(grant numbers:2021ZD0200700); Key-Area Research and Development Program of Guangdong Province(grant numbers:2023B0303030001); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2019ZT08X214); Key-Area Research and Development Program of Guangdong Province(grant numbers:2023B0303030001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10943259","Adaptive attention-modulated graph network (AdamGraph);electroencephalogram (EEG) emotion recognition;graph connection;individual differences;subject adaptability","Electroencephalography;Emotion recognition;Feature extraction;Brain modeling;Topology;Adaptive systems;Wavelet transforms;Wavelet analysis;Stacking;Network topology","Humans;Electroencephalography;Emotions;Signal Processing, Computer-Assisted;Attention;Algorithms;Databases, Factual;Neural Networks, Computer;Brain","","","70","IEEE","27 Mar 2025","","","IEEE","IEEE Journals"
"Channel Selection Method for EEG Emotion Recognition Using Normalized Mutual Information","Z. -M. Wang; S. -Y. Hu; H. Song","Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing,, Xi’an University of Posts and Telecommunications, Xi’an, China; School of Computer Science and Technology, Xi’an University of Posts and Telecommunications, Xi’an, China; Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing,, Xi’an University of Posts and Telecommunications, Xi’an, China",IEEE Access,"14 Oct 2019","2019","7","","143303","143311","Electroencephalography (EEG) signals can reflect activities of the human brain and represent different emotional states. However, recognizing emotions based on full-channel EEG signals will lead to redundant data and hardware complexity, thus it is not suitable for designing wearable devices for daily-life emotion recognition. This paper proposes a channel selection method to select an optimal subset of EEG channels by using normalized mutual information (NMI). Compared with other methods, the proposed method solves the problem of obtaining a higher recognition rate while reducing EEG channels sharply. First, EEG signals are sliced into fixed-length pieces with a sliding window, and short-time Fourier transform is adopted to capture EEG spectrogram. Then inter-channel connection matrix is calculated based on NMI, and channel reduction is conducted by using thresholding and connection matrix analysis. The experiments are based on the widely-used emotion recognition database DEAP. It can be derived from the experimental results that the proposed method can select optimal EEG channel subsets to a certain number while maintaining high accuracy of 74.41% for valence and 73.64% for arousal with support vector machines. Further analysis also reveals that the distribution of the selected channels is consistent with cortical areas for general emotion tasks.","2169-3536","","10.1109/ACCESS.2019.2944273","National Natural Science Foundation of China(grant numbers:61373116); Education Department of Shaanxi Province(grant numbers:18JK0698); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851133","Channel selection;electroencephalography;emotion recognition;normalized mutual information;support vector machine","Electroencephalography;Emotion recognition;Mutual information;Spectrogram;Support vector machines;Classification algorithms;Brain modeling","","110","","40","CCBY","27 Sep 2019","","","IEEE","IEEE Journals"
"EEG Emotion Recognition Based on Graph Signal and Stable Learning","R. OuYang; S. Liu; L. Yang","School of Mathematics and Statistics, Henan University, Kaifeng, China; School of Mathematics and Statistics, Henan University, Kaifeng, China; Henan Engineering Research Center of Artificial Intelligence Theory and Algorithm, School of Mathematics and Statistics, Henan University, Kaifeng, China","2022 2nd International Conference on Computational Modeling, Simulation and Data Analysis (CMSDA)","4 Dec 2023","2022","","","213","216","In order to extract effective information from Electroencephalography (EEG) data, this paper proposes an EEG emotion recognition method based on graph signal and stable learning. To simulate the integration of information by the brain, the EEG data is endowed with spatial structure, that is, it is modeled as a graph signal, and the graph Fourier transform is used to transform the initial graph signal into a graph signal in the frequency domain, and then the graph convolution neural network is used to automatically extract the more discriminating features of the data with graph structure for emotion classification. In addition, in order to extract the causal features in EEG channels, we used the weight learning of decorrelation to remove confounding factors and exclude correlation features. The proposed method is tested on two public datasets for binary valence and arousal, and the classification accuracy is better than other similar methods.","","979-8-3503-4575-9","10.1109/CMSDA58069.2022.00045","National Natural Science Foundation of China(grant numbers:11701144); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10322827","EEG signal;graph signal;graph Fourier transform;stable learning;emotion recognition","Emotion recognition;Fourier transforms;Computational modeling;Frequency-domain analysis;Feature extraction;Brain modeling;Electroencephalography","","","","16","IEEE","4 Dec 2023","","","IEEE","IEEE Conferences"
"EEG Signal Processing for Action Recognition Using Machine Learning Paradigms","A. S P; M. C. B; K. G; E. E","Department of Computer Science and Engineering, Coimbatore Institute of Technology, Coimbatore, Tamil Nadu; School of Computer Science and Engineering, VIT-AP University, Andhra Pradesh, India; Department of Computer Science and Engineering, Coimbatore Institute of Technology, Coimbatore, Tamil Nadu; Department of Computer Science and Engineering, SRM University,AP, Andhra Pradesh, India",2024 OITS International Conference on Information Technology (OCIT),"29 Apr 2025","2024","","","246","252","Many intriguing applications, such as the ability to move prosthetic limbs and enable more fluid man-machine contact, may be made possible by automatic interpretation of brain readings. The problem of precisely categorizing EEG signals linked with memory categories is tackled first in the inquiry. With the restricted availability of pre-trained models for such signal classification, a Convolution-based Neural Network (CNN) is constructed from scratch. By using EEG recordings from UC Berkeley's Bio-Sense Lab, this study seeks to improve memory recall using machine learning and precise feature selection. Fifteen participants' EEG data are converted into the frequency domain, and the amplitudes are used as key characteristics. The selection of these qualities is improved by a self-attention mechanism, which maximizes the distinction among various memory categories. The primary focus is to evaluate the performance of the most advanced algorithms, with the secondary objective of outperforming previous methods in terms of classification accuracy. A fine-tuned subset of the frequency-based characteristics is evaluated using a Support Vector Machine (SVM) classifier. By showcasing the efficiency of self-attention in honing feature subsets, this study highlights the significance of feature engineering in EEG-based memory classification. This method is positioned as a promising advancement in the analysis of EEG data since it improves the separation between memory categories through the application of frequency-domain modifications and SVM classifiers. Furthermore, investigating time series features shows how well they may capture intricate patterns, pointing to fresh avenues for future neuro-informatics and cognitive study.","","979-8-3315-1040-4","10.1109/OCIT65031.2024.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10973668","EEG signal classification;Convolution-based Neural Network (CNN);memory categorization;Bio sense Lab;time series features;neuroscientific research","Support vector machines;Accuracy;Time series analysis;Pattern classification;Brain modeling;Electroencephalography;Data models;Real-time systems;Recording;Load modeling","","","","15","IEEE","29 Apr 2025","","","IEEE","IEEE Conferences"
"Machine Learning Approaches for Neuroscientific Behavioral Analysis: A Neuromarketing perspective","R. Gill; J. Singh","Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India; Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India","2021 4th International Conference on Recent Developments in Control, Automation & Power Engineering (RDCAPE)","10 Dec 2021","2021","","","576","580","Neuromarketing has evolved from the fusion of neuroscience and marketing techniques. It intends to measure the interest, attitude, and behavior of consumers towards either any product, service, design, or advertisement. The probable study of various neuroscience techniques with machine learning classification and prediction algorithms is have been studied. Early literature pertaining to neuromarketing is primarily focused on individual techniques, application areas, ethics, datasets, algorithms. This manuscript intends to extend a previous literature by studying essential machine learning techniques, prominent datasets, algorithms, and available equipment for designing neuroscientific experiments. The manuscript concluded with the probable pros and cons of using different machine learning techniques associated with contemporary use of neuroscientific methods. Towards machine learning techniques has many applications in the field of Brain computer interface (BCI), neuromarketing, neurophysiology and clinical neuroscience. In neuromarketing, these techniques help in improving product sales, return on investment, product design etc.","","978-1-6654-1429-6","10.1109/RDCAPE52977.2021.9633604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9633604","Neuroscience;Neuromarketing;electroencephalography (EEG);machine learning;classification techniques;Support vector machine (SVM);Artificial neural networks (ANN);Linear discriminant analyzer (LDA);K nearest neighbor(k-NN);Long short-term memory (LSTM);Probabilistic neural networks (PNN)","Support vector machines;Power engineering;Machine learning algorithms;Machine learning;Prediction algorithms;Brain-computer interfaces;Product design","","1","","37","IEEE","10 Dec 2021","","","IEEE","IEEE Conferences"
"Emotion Classification using Physiological Signals: A Recent Survey","A. Dessai; H. Virani","Department of Electronics & Telecommunication Engineering, Goa College of Engineering, Goa, India; Department of Electronics & Telecommunication Engineering, Goa College of Engineering, Goa, India","2022 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES)","16 May 2022","2022","1","","333","338","The diverse forms of emotions practiced by human beings such as happiness, sadness, anger, fear point to the state of mental, and social welfare of an individual. The sentiments can be noticed through imaging techniques, voice signals, and biosignals. Facial signs may not give a strong sign of the state of emotions since a person can fleece the emotions. The voice signals direct untrue emotions owing to the differences in the tone of the people. The biosignals like ECG, EEG, galvanic skin resistance(GSR), skin temperature, respiration rate are initiated from the autonomic nervous system. Hence the emotions sensed using the biosignals cannot be misread. We have conducted a recent survey on classifying the emotions detected using ECG & GSR signals, focusing on the various feature selection & classification methods.","","978-1-6654-4940-3","10.1109/SPICES52834.2022.9774240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774240","Emotion classification;Electrocardiography;Electrodermography;feature selection;classifiers","Temperature sensors;Focusing;Electrocardiography;Signal processing;SPICE;Feature extraction;Skin","","2","","24","IEEE","16 May 2022","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition with Combined Deep Neural Networks using Decomposed Feature Clustering Model","M. A. Asghar; Fawad; M. J. Khan; Y. Amin; A. Akram","Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan",2019 13th International Conference on Open Source Systems and Technologies (ICOSST),"23 Mar 2020","2019","","","1","6","Much attention has been paid to the recognition of human emotions with the help of EEG signals based on machine learning technology. Recognizing emotions is a difficult task due to the non-linear nature of the EEG signal. This paper presents an advanced signal processing method that uses the depth function to extract features from all channels related to emotion. A decomposed feature clustering model is presented in this paper to decrease the computational cost of recognizing emotions and achieve better results. In the proposed method, we convert the signal into a two-dimensional wavelet spectrogram and calculate the characteristics of each subject. An EEG-based emotion classification model using a deep convolutional neural network (DNN) is presented on the SJTU SEED dataset. Combined feature model using AlexNet, VGGNet and ResNet-50 machine learning models are used for feature extraction. SVM and k-NN are used to classify data into positive/negative/neutral dimensions for SEED dataset. The results showed that models with images are more accurate than traditional models for emotion recognition. The proposed model achieves 91.3% accuracy in the SEED dataset, which is more accurate as compared to the other state-of-the-art human emotions recognition methods.","","978-1-7281-4613-3","10.1109/ICOSST48232.2019.9043994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043994","Emotion recognition;Machine Learning;DNN;Feature Decomposition","","","4","","21","IEEE","23 Mar 2020","","","IEEE","IEEE Conferences"
"Revealing critical channels and frequency bands for emotion recognition from EEG with deep belief network","W. -L. Zheng; H. -T. Guo; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2015 7th International IEEE/EMBS Conference on Neural Engineering (NER),"2 Jul 2015","2015","","","154","157","For EEG-based emotion recognition tasks, there are many irrelevant channel signals contained in multichannel EEG data, which may cause noise and degrade the performance of emotion recognition systems. In order to tackle this problem, we propose a novel deep belief network (DBN) based method for examining critical channels and frequency bands in this paper. First, we design an emotion experiment and collect EEG data while subjects are watching emotional film clips. Then we train DBN for recognizing three emotions (positive, neutral, and negative) with extracted differential entropy features as input and compare DBN with other shallow models such as KNN, LR, and SVM. The experiment results show that DBN achieves the best average accuracy of 86.08%. We further explore critical channels and frequency bands by examining the weight distribution learned by DBN, which is different from the existing work. We identify four profiles with 4, 6, 9 and 12 channels, which achieve recognition accuracies of 82.88%, 85.03%, 84.02%, 86.65%, respectively, using SVM.","1948-3554","978-1-4673-6389-1","10.1109/NER.2015.7146583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7146583","","Emotion recognition;Electroencephalography;Accuracy;Electrodes;Support vector machines;Feature extraction;Standards","","42","","13","IEEE","2 Jul 2015","","","IEEE","IEEE Conferences"
"Multiple Convolutional Neural Networks in EEG Emotion Recognition","H. D. Khairunissa; E. Contessa Djamal; A. Wulandari","Department of Informatics, Univ. Jenderal Achmad Yani, Cimahi, Indonesia; Department of Informatics, Univ. Jenderal Achmad Yani, Cimahi, Indonesia; Faculty of Medicine, Univ. Jenderal Achmad Yani, Cimahi, Indonesia",2021 4th International Conference of Computer and Informatics Engineering (IC2IE),"27 Dec 2021","2021","","","30","35","Emotion is a psychological activity in controlling feelings, whether consciously or not. Emotions recognition can use neuropsychology signals obtained through the Electroencephalogram (EEG) device. EEG recording has a multi-channel that is taken from some area in the brain. Each channel provides information from a specific part of the brain, so the signals need to be processed in parallel. The multi-channel recording enriches emotional information so that the signal recognition of each channel is combined with the fusion function. In this way, multi-channel processing does not interfere with the signal sequencing within each channel. Convolutional Neural Networks (CNN) is one of the methods that can learn and recognize patterns in one area, such as the eye. This paper proposed multiple CNN to recognize emotion. First, the EEG is filtered using a wavelet to get a frequency component of 4-45 Hz that represents the characteristics of negative, neutral, or positive emotions. The frequency band contains Theta, Alpha, Beta, and Gamma waves. The experiment gave that Multiple CNN increased accuracy from 64.14% to 80.66% compared to Single CNN. In addition, a wavelet filter to maintain the signal sequence can obtain slightly better accuracy results than wavelet extraction.","","978-1-6654-4288-6","10.1109/IC2IE53219.2021.9649215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649215","EEG signal;wavelet filters;multiple convolutional neural networks;emotion recognition","Emotion recognition;Sequential analysis;Redundancy;Electroencephalography;Pattern recognition;Neuropsychology;Convolutional neural networks","","1","","25","IEEE","27 Dec 2021","","","IEEE","IEEE Conferences"
"Adaptive Optimization Multi-Layer Perceptron for Emotion Recognition from EEG Brain Maps","Q. Lan; B. Geng; Y. Deng; Y. Wang; H. Lin","School of Information and Communication Engineering, Communication University of China, Beijing, China; School of Information and Communication Engineering, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; Neuroscience and Intelligent Media Institute, Communication University of China, Beijing, China; The Media School, Indiana University Bloomington, Bloomington, USA",2024 International Conference on Cyber-Physical Social Intelligence (ICCSI),"18 Dec 2024","2024","","","1","6","Emotion recognition is an important issue in Affective Computing and Human Computer Interaction, and plays a crucial role in interpersonal communication and media effectiveness. In recent years, there has been a surge of interest in the field of emotion recognition, driven by advancements in machine learning models and computer vision techniques. These technologies have the potential to revolutionize the way people understand and interpret human emotions. This paper proposes a new electroencephalography (EEG) emotion recognition dataset based on daily media content that serves as emotion-inducing materials, including 16 video clips, 65 valid EEG signals and valence-arousal states self-assessment reports of subjects. And a novel EEG emotion recognition model based on multi-layer perceptron taking brain maps as spatial representations is proposed, using genetic algorithm to adaptively optimize the multi-layer perceptron structure in the algorithm that uses brain maps as input data and aims to predict emotional states of valence and arousal. The experimental results show that the method proposed in this paper has better performance than existing results in the literature.","","979-8-3503-7673-9","10.1109/ICCSI62669.2024.10799315","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10799315","EEG;Brain map;Emotion recognition;Multi-Layer Perceptron;Genetic Algorithm","Emotion recognition;Production;Media;Predictive models;Brain modeling;Feature extraction;Prediction algorithms;Electroencephalography;Social intelligence;Surges","","","","18","IEEE","18 Dec 2024","","","IEEE","IEEE Conferences"
"Comparative analysis of physiological signals and electroencephalogram (EEG) for multimodal emotion recognition using generative models","C. A. Torres-Valencia; H. F. García-Arias; M. A. Álvarez López; A. A. Orozco-Gutiérrez","Department of Electrical Engineering, Research Group of Automatic, Universidad Tecnológica de Pereira, Pereira, Colombia; Department of Electrical Engineering, Research Group of Automatic, Universidad Tecnológica de Pereira, Pereira, Colombia; Department of Electrical Engineering, Research Group of Automatic, Universidad Tecnologica de Pereira, Pereira, CO; Department of Electrical Engineering, Research Group of Automatic, Universidad Tecnológica de Pereira, Pereira, Colombia","2014 XIX Symposium on Image, Signal Processing and Artificial Vision","15 Jan 2015","2014","","","1","5","Multimodal Emotion recognition (MER) is an application of machine learning were different biological signals are used in order to automatically classify a determined affective state. MER systems has been developed for different type of applications from psychological evaluation, anxiety assessment, human-machine interfaces and marketing. There are several spaces of classification proposed in the state of art for the emotion recognition task, the most known are discrete and dimensional spaces were the emotions are described in terms of some basic emotions and latent dimensions respectively. The use of dimensional spaces of classification allows a higher range of emotional states to be analyzed. The most common dimensional space used for this purpose is the Arousal/Valence space were emotions are described in terms of the intensity of the emotion that goes from inactive to active in the arousal dimension, and from unpleasant to pleasant in the valence dimension. The use of physiological signals and the EEG is well suited for emotion recognition due to the fact that an emotional states generates responses from different biological systems of the human body. Since the expression of an emotion is a dynamic process, we propose the use of generative models as Hidden Markov Models (HMM) to capture de dynamics of the signals for further classification of emotional states in terms of arousal and valence. For the development of this work an international database for emotion classification known as Dataset for Emotion Analysis using Physiological signals (DEAP) is used. The objective of this work is to determine which of the physiological and EEG signals brings more relevant information in the emotion recognition task, several experiments using HMMs from different signals and combinations of them are performed, and the results shows that some of those signals brings more discrimination between arousal and valence levels as the EEG and the Galvanic Skin Response (GSR) and the Heart rate (HR).","2329-6259","978-1-4799-7666-9","10.1109/STSIVA.2014.7010181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010181","","Hidden Markov models;Emotion recognition;Physiology;Electroencephalography;Databases;Brain models","","38","","20","IEEE","15 Jan 2015","","","IEEE","IEEE Conferences"
"Human Emotion Classification Based on EEG Signals Using Naïve Bayes Method","N. Y. Oktavia; A. D. Wibawa; E. S. Pane; M. H. Purnomo","Dept. of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Dept. of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Dept. of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Dept. of Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",2019 International Seminar on Application for Technology of Information and Communication (iSemantic),"28 Oct 2019","2019","","","319","324","Currently, emotions recognition has been attracting a lot of interest for researchers in various field, so as in the study of human-computer interaction (HCI). One of an interesting issue in HCI emotions study is the use of physiological signals, such as electrocardiograph (ECG), blood vessel pressure (BVP), electroencephalograph (EEG), and any others signals to recognize emotions. Among all physiological signals, EEG is known to be the most reliable modality to understand emotions processing and perceptions. Therefore, this study observed emotions recognition through EEG signals by investigating emotions cue from time domain features extraction for differentiating two class of emotions, namely, happy and sad. We developed an EEG based emotion dataset from 12 participants with 4 recording channels of EEG cap, i.e., AF3, AF4, O1 and O2. The time domain features of mean, standard deviation and number of peaks were extracted from alpha and beta frequency bands. For the recognition, we train the features set into Naïve Bayes learning classifier. From the results, it was shown that feature of mean gives the highest contribution to the classification. Moreover, from the observation of frequency bands, the combination of alpha and beta bands tend to provide better accuracy in emotions recognition rather than using alpha or beta frequency alone. The highest classification result of Naïve Bayes reached 87.5% accuracy of emotions recognition with 66% split testing option.","","978-1-7281-3832-9","10.1109/ISEMANTIC.2019.8884224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8884224","Emotion from EEG;Statistical Features;Naïve Bayes Classifier","Electroencephalography;Feature extraction;Emotion recognition;Time-domain analysis;Electrocardiography;Biomedical signal processing;Bayes methods","","19","","26","IEEE","28 Oct 2019","","","IEEE","IEEE Conferences"
"F-CACNN: A framework integrating FNet and adaptive attention for end-to-end EEG emotion recognition","Z. Cao; X. Wang; Y. Liu","School of Computer Science and Engineering, Hunan University of Science and Technology, Xiangtan, China; School of Computer Science and Engineering, Hunan University of Science and Technology, Xiangtan, China; School of Computer Science and Engineering, Hunan University of Science and Technology, Xiangtan, China",2023 IEEE Smart World Congress (SWC),"1 Mar 2024","2023","","","1","8","An important area of study in human-computer interaction is emotional brain-computer interface based on electroencephalogram (EEG). A challenge in this area is how to make full use of channel correlation and provide timely feedback on effective emotion recognition results. In this study, we present a FNet-based and channel attention convolution neural network-based model (F-CACNN) for emotion recognition. We thoroughly explore the EEG channel information with the adaptive channel attention mechanism and employ the FNet mechanism for the first time to realize emotion recognition more efficiently. On the one hand, we include the adaptive channel attention technique into the convolutional neural network to adaptively assign the weights according to the importance of each channel and extract EEG spatial features. On the other hand, the FNet mechanism is integrated into long short-term memory. It takes the Fourier Transform to extract the EEG temporal feature without learnable parameterization. Experimental results show that our method can effectively realize emotion recognition without handcrafted features extraction, and significantly reduce time consumption, which meets the current demand for end-to-end emotion recognition.","","979-8-3503-1980-4","10.1109/SWC57546.2023.10448926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448926","Affective brain-computer interface;Emotion recognition;Fourier Transform;Channel attention mechanism;EEG","Emotion recognition;Fourier transforms;Feature extraction;Electroencephalography;Convolutional neural networks;Task analysis;Long short term memory","","1","","38","IEEE","1 Mar 2024","","","IEEE","IEEE Conferences"
"Deep Representation Learning for Multimodal Emotion Recognition Using Physiological Signals","M. Zubair; S. Woo; S. Lim; C. Yoon","Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea; Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea; Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea; Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea",IEEE Access,"9 Aug 2024","2024","12","","106605","106617","Physiological signal analysis has gained a lot of interest in recent years and has been used in a variety of fields including emotion recognition, activity recognition, and health monitoring. However, emotion recognition based on physiological signals is not yet explored entirely using deep learning, and there are still some exciting challenges to be handled. For example, deep representation learning for spatio-temporal feature extraction, the discrimination between adjacent emotions with entangled features, and the imbalanced distribution of data are the most prominent issues in emotion recognition. This work focuses on deep multimodal representation learning of physiological signals to alleviate the aforementioned challenges. We introduce a novel deep learning architecture for emotion classification that effectively extracts spatio-temporal information from physiological signals. We proposed a mutual attention mechanism to extract emotion-specific features for improved classification. To handle the issue of adjacent emotions and imbalanced data, we introduce a dense max-margin loss function based on Gaussian similarity measure. Our experiments on different datasets reveal that the proposed emotion classification methodology effectively learns a balanced deep representation of physiological signals, significantly maximizes the inter-class margin, and reduces intra-class variance to discriminate between different classes of emotions.","2169-3536","","10.1109/ACCESS.2024.3436556","National Research Foundation of Korea (NRF) grant; Korea Government (MSIT)(grant numbers:RS-2024-00423362); Institute of Information and Communications Technology Planning and Evaluation (IITP) grant; Korea Government (MSIT)(grant numbers:2022-0-01032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10620287","Emotion recognition;deep learning;attention mechanism;imbalanced data;EEG;ECG","Feature extraction;Emotion recognition;Brain modeling;Physiology;Electrocardiography;Deep learning;Biomedical monitoring","","","","54","CCBYNCND","1 Aug 2024","","","IEEE","IEEE Journals"
"Contrastive Learning of Subject-Invariant EEG Representations for Cross-Subject Emotion Recognition","X. Shen; X. Liu; X. Hu; D. Zhang; S. Song","Department of Biomedical Engineering and with the Tsinghua Laboratory of Brain and Intelligence, Tsinghua University, Beijing, China; College of Computer Science, Sichuan University, Chengdu, Sichuan, China; Department of Psychology and with the Tsinghua Laboratory of Brain and Intelligence, Tsinghua University, Beijing, China; Department of Psychology and with the Tsinghua Laboratory of Brain and Intelligence, Tsinghua University, Beijing, China; Department of Biomedical Engineering and with the Tsinghua Laboratory of Brain and Intelligence, Tsinghua University, Beijing, China",IEEE Transactions on Affective Computing,"18 Sep 2023","2023","14","3","2496","2511","EEG signals have been reported to be informative and reliable for emotion recognition in recent years. However, the inter-subject variability of emotion-related EEG signals still poses a great challenge for the practical applications of EEG-based emotion recognition. Inspired by recent neuroscience studies on inter-subject correlation, we proposed a Contrastive Learning method for Inter-Subject Alignment (CLISA) to tackle the cross-subject emotion recognition problem. Contrastive learning was employed to minimize the inter-subject differences by maximizing the similarity in EEG signkal representations across subjects when they received the same emotional stimuli in contrast to different ones. Specifically, a convolutional neural network was applied to learn inter-subject aligned spatiotemporal representations from EEG time series in contrastive learning. The aligned representations were subsequently used to extract differential entropy features for emotion classification. CLISA achieved state-of-the-art cross-subject emotion recognition performance on our THU-EP dataset with 80 subjects and the publicly available SEED dataset with 15 subjects. It could generalize to unseen subjects or unseen emotional stimuli in testing. Furthermore, the spatiotemporal representations learned by CLISA could provide insights into the neural mechanisms of human emotion processing.","1949-3045","","10.1109/TAFFC.2022.3164516","National Key Research and Development Program of China(grant numbers:2021ZD0200300); Spring Breeze Fund(grant numbers:2021Z99CFY037); National Natural Science Foundation of China(grant numbers:61977041,61836004); Tsinghua University Initiative Scientific Research Program(grant numbers:20197010009); Center for Brain-inspired Computing Research, Beijing Innovation Center for Future Chips; Beijing Academy of Artificial Intelligence; Guoqiang Institute; IDG/McGovern Institute for Brain Research; Tsinghua University; Key Scientific Technological Innovation Project by Ministry of Education; Anhui University(grant numbers:MMC202001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9748967","EEG;emotion recognition;brain-computer interface;cross-subject;contrastive learning","Emotion recognition;Electroencephalography;Training;Testing;Feature extraction;Brain modeling;Neuroscience","","82","","102","IEEE","4 Apr 2022","","","IEEE","IEEE Journals"
"Exploring CEEMDAN and LMD Domains Entropy Features for Decoding EEG-Based Emotion Patterns","N. Pusarla; A. Singh; S. Tripathi; A. Vujji; R. Bilas Pachori","Department of Electronics and Communication Engineering, International Institute of Information Technology, Naya Raipur, Naya Raipur, Chhattisgarh, India; Department of Electronics and Communication Engineering, International Institute of Information Technology, Naya Raipur, Naya Raipur, Chhattisgarh, India; Department of Electronics and Communication Engineering, International Institute of Information Technology, Naya Raipur, Naya Raipur, Chhattisgarh, India; Department of Electrical and Electronics Engineering, Vignan’s Institute of Engineering for Women, Visakhapatnam, Andhra Pradesh, India; Department of Electrical Engineering, Indian Institute of Technology Indore, Indore, India",IEEE Access,"5 Aug 2024","2024","12","","103606","103625","Electroencephalogram (EEG) signal-based emotion classification is vital in the ever-growing human-computer interface (HCI) applications. However, the chaotic, non-stationary, and person-dependent nature of EEG signals often limits such practical applications. These challenges reduce the ability of the state-of-the-art approaches to effectively distinguish between different emotional states from EEG data, resulting in sub-optimal emotion recognition performance. This work presented a time-frequency (T-F) analysis of EEG signals to localize different EEG frequency rhythms responsible for emotion-related information in the EEG signals. In particular, this work investigated two T-F analysis domains for multichannel EEG signals based on complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) and local mean decomposition (LMD) to extract entropy features that capture specific emotion-relevant traits from the specific EEG channels that are highly responsive to emotions. The CEEMDAN and LMD decompose the EEG signals into different EEG frequency rhythms called mode functions, intrinsic mode functions (IMFs), and product functions (PFs), respectively. Further, various types of entropy feature of these two categories of mode functions, such as approximate entropy (ApEn), sample entropy (SaEn), permutation entropy (PeEn), and bubble entropy (BuEn), are computed for extracting emotion-relevant distinguishing features. Entropy features help quantify EEG’s non-linear behavior and eventually help classify EEG-based emotions precisely. Emotion classification has been achieved using a grid search cross-validation (GSCV) optimized XGboost classifier. Thorough experimentations are conducted to validate the efficacy of the proposed approach on publically accessible datasets named Database for Emotion Analysis of Physiological Signals (DEAP), SJTU emotion EEG dataset (SEED), and SEED-IV. The efficacy of the proposed emotion recognition approach is measured in terms of widely used performance metrics such as accuracy, confusion metrics, receiver operating characteristics (RoC), and area under the curve (AuC). The highest average accuracy is attained using the proposed LMD-domain BuEn features, i.e., 97.8%, 98.6%, and 95.7% using SEED, SEED-IV, and DEAP databases, respectively, outperforming the recent state-of-the-art emotion recognition algorithms.","2169-3536","","10.1109/ACCESS.2024.3434675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10613917","CEEMDAN;LMD;XGboost;bubble entropy;grid search;cross-validation and emotion recognition","Electroencephalography;Emotion recognition;Entropy;Feature extraction;Brain modeling;Accuracy;Time-frequency analysis","","1","","64","CCBYNCND","29 Jul 2024","","","IEEE","IEEE Journals"
"ReGA based feature selection emotion recognition using EEG signals","Y. Kong; J. Yan; H. Xu","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China",2017 Chinese Automation Congress (CAC),"1 Jan 2018","2017","","","6588","6593","In recent years, with the development of computer technology, EEG emotion recognition has been paid much attention in the field of emotional computing and has become a hotspot. In the practical application of EEG emotion recognition, it is not only required to ensure the accuracy rate, but also the operation efficiency. Based on this, we propose a ReGA algorithm to select the EEG characteristics. In the ReGA algorithm, the ReliefF algorithm is used to calculate the feature weight, and the heuristic information is provided for the population initialization of the encapsulation stage genetic algorithm, so that the initial population contains a good starting point, so the genetic algorithm can adopt less evolution Algebra and small-scale populations to find a better subset of features. Therefore, the ReGA algorithm can avoid the danger of ReliefF to remove important features, but also reduce the probability of genetic algorithm to adapt and improve the efficiency of computing.","","978-1-5386-3524-7","10.1109/CAC.2017.8243964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8243964","Emotion recognition;ReliefF;Genetic algorithm;ReGA","Emotion recognition;Machine learning algorithms;Heuristic algorithms;Sociology;Signal processing algorithms;Signal processing;Feature extraction","","1","","12","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"Emotion classification based on forehead biosignals using support vector machines in music listening","M. Naji; M. Firoozabadi; P. Azadfallah","Department of Biomedical Engineering, Science and Research branch, Tehran, Iran; Department of Medical Physics, Tarbiat Modares University, Tehran, Iran; Department of Psychology, Tarbiat Modares University, Tehran, Iran",2012 IEEE 12th International Conference on Bioinformatics & Bioengineering (BIBE),"3 Jan 2013","2012","","","396","400","The purpose of this study was to investigate the feasibility of using forehead biosignals as informative channels for classification of music-induced emotions. Classification of four emotional states in Arousal-Valence space was performed by employing two parallel support vector machines as arousal and valence classifiers. Relative powers of EEG sub-bands, spectral entropy, mean power frequency, and higher order crossings were extracted from each of the three forehead data channels: left Temporalis, Frontalis, and right Temporalis. The inputs of the classifiers were obtained by a feature selection algorithm based on a fuzzy-rough model. The averaged subject-independent classification accuracy of 93.80%, 92.43%, and 86.67% for arousal classification, valence classification, and classification of four emotional states in Arousal-Valence space, respectively, is achieved.","","978-1-4673-4358-9","10.1109/BIBE.2012.6399657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6399657","forehead biosignals;emotion classification;arousal;valence","Accuracy;Emotion recognition;Feature extraction;Support vector machines;Electroencephalography;Electrodes;Forehead","","5","","11","IEEE","3 Jan 2013","","","IEEE","IEEE Conferences"
"Subject-General and Subject-Specific Emotion Recognition Across Video Stimuli Using EEG Signals","Z. Huang; A. Guo; J. Ma","Graduate School of Computer and Information Sciences, Hosei University; Graduate School of Informatics, Nagoya University; Graduate School of Computer and Information Sciences, Hosei University",2024 IEEE Conference on Pervasive and Intelligent Computing (PICom),"19 Dec 2024","2024","","","16","23","Recent studies on emotion recognition have mainly focused on an across-subject approach, where models are trained and tested on data collected from different users. To be more applicable in practice, the model needs to recognize emotion across stimuli (e.g., watching a movie or playing a game) in different environments and scenarios. This study explores emotion recognition across video stimuli using the DEAP dataset under subject-general and subject-specific approaches. The subject-general approach aims to recognize emotions across different subjects, while the subject-specific approach focuses on recognizing emotions specific to an individual subject. We used CNN-based models to analyze arousal and valence from EEG-based 2D brain maps. By using ranked-based and random labels for emotion recognition, we demonstrated the limitations of achieving across video stimuli in the condition of subject-general. In the subject-specific approach, we evaluated the model generalization performance across video stimuli in all subjects. Stronger consistency was observed in valence scores, while arousal scores showed weaker consistency among high-performing subjects.","","979-8-3315-2274-2","10.1109/PICom64201.2024.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795396","Emotion recognition;EEG;CNN;video stimuli;subject-general;subject-specific","Emotion recognition;Analytical models;Computational modeling;Games;Brain modeling;Motion pictures;Electroencephalography;Data models","","","","16","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"Real-Time Movie-Induced Discrete Emotion Recognition from EEG Signals","Y. -J. Liu; M. Yu; G. Zhao; J. Song; Y. Ge; Y. Shi","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China",IEEE Transactions on Affective Computing,"27 Nov 2018","2018","9","4","550","562","Recognition of a human's continuous emotional states in real time plays an important role in machine emotional intelligence and human-machine interaction. Existing real-time emotion recognition systems use stimuli with low ecological validity (e.g., picture, sound) to elicit emotions and to recognise only valence and arousal. To overcome these limitations, in this paper, we construct a standardised database of 16 emotional film clips that were selected from over one thousand film excerpts. Based on emotional categories that are induced by these film clips, we propose a real-time movie-induced emotion recognition system for identifying an individual's emotional states through the analysis of brain waves. Thirty participants took part in this study and watched 16 standardised film clips that characterise real-life emotional experiences and target seven discrete emotions and neutrality. Our system uses a 2-s window and a 50 percent overlap between two consecutive windows to segment the EEG signals. Emotional states, including not only the valence and arousal dimensions but also similar discrete emotions in the valence-arousal coordinate space, are predicted in each window. Our real-time system achieves an overall accuracy of 92.26 percent in recognising high-arousal and valenced emotions from neutrality and 86.63 percent in recognising positive from negative emotions. Moreover, our system classifies three positive emotions (joy, amusement, tenderness) with an average of 86.43 percent accuracy and four negative emotions (anger, disgust, fear, sadness) with an average of 65.09 percent accuracy. These results demonstrate the advantage over the existing state-of-the-art real-time emotion recognition systems from EEG signals in terms of classification accuracy and the ability to recognise similar discrete emotions that are close in the valence-arousal coordinate space.","1949-3045","","10.1109/TAFFC.2017.2660485","National Key Research and Development Plan(grant numbers:2016YFB1001200); National Natural Science Foundation of China(grant numbers:31300852,61661130156,61521002); Royal Society-Newton Advanced Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835688","Affective computing;EEG;emotion recognition;movie","Emotion recognition;Real-time systems;Electroencephalography;Brain models;Films;Support vector machines","","252","","59","IEEE","27 Jan 2017","","","IEEE","IEEE Journals"
"Multimodal Emotion Recognition Based on Hybrid Fusion","G. Yang; D. Yang; J. Li; G. Wang","School of Software, Shenyang University of Technology, Shenyang, China; School of Software, Shenyang University of Technology, Shenyang, China; School of Software, Shenyang University of Technology, Shenyang, China; Beijing Research Institute of Telemetry, Beijing, China","2023 5th International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)","2 Apr 2024","2023","","","85","88","Accurate emotion recognition can greatly improve the reliability of interpersonal communication and mental illness diagnosis. Aiming at the problem that the accuracy of single-modal emotion recognition is difficult to improve, a multimodal emotion recognition method is proposed by fusing speech, facial expression and EEG. The attention mechanism is introduced in the feature layer fusion, and the recognition accuracy is improved by designing the optimal weight allocation algorithm of the decision layer fusion. The results show that the recognition accuracy is 94.53% on the training set MAHNOB-HCI, 92.89% on the SEED dataset, and 91.54% on the self-built dataset, which indicates that the recognition accuracy is improved compared with the single-modal emotion recognition, and has good generalization ability.","2994-2977","979-8-3503-5993-0","10.1109/MLBDBI60823.2023.10481889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481889","Attention mechanism;Decision-layer;Multi-modal","Training;Emotion recognition;Visualization;Machine learning algorithms;Face recognition;Speech recognition;Machine learning","","","","10","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Emotion detection from EEG recordings","J. Liu; H. Meng; A. Nandi; M. Li","Dept of Electronic and Computer Engineering, Brunel University London, London, United Kingdom; Dept of Electronic and Computer Engineering, Brunel University London, London, United Kingdom; The Key Laboratory of Embedded Systems and Service, Tongji University, Shanghai, China; The Key Laboratory of Embedded Systems and Service, Tongji University, Shanghai, China","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","24 Oct 2016","2016","","","1722","1727","Human brain behavior is very complex and it is difficult to interpret. Human emotion might come from brain activities. However, the relationship between human emotion and brain activities is far from clear. In recent years, more and more researchers are trying to discover this relationship by recording brain signals such as electroencephalogram (EEG) signals with the associated emotion information extracted from other modalities such as facial expression. In this paper, machine learning based methods are used to model this relationship in the publicly available dataset DEAP (Database for Emotional Analysis using Physiological Signals). Different features are extracted from raw EEG recordings. Then Maximum Relevance Minimum Redundancy (mRMR) was used for feature selection. These features are fed into machine learning methods to build the prediction models to extract the emotion information from EEG signals. The models are evaluated on this dataset and satisfactory results are achieved.","","978-1-5090-4093-3","10.1109/FSKD.2016.7603437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603437","EEG;Emotion;Emotion Analysis;Affective Computing","Electroencephalography;Feature extraction;Electrodes;Frequency-domain analysis;Brain modeling;Learning systems;Discrete wavelet transforms","","50","","21","IEEE","24 Oct 2016","","","IEEE","IEEE Conferences"
"Generalizations of the subject-independent feature set for music-induced emotion recognition","Y. -P. Lin; J. -H. Chen; J. -R. Duann; C. -T. Lin; T. -P. Jung","Brain Research Center, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Biomedical Engineering Research and Development Center, China Medical University and Hospital, Taichung, Taiwan; Brain Research Center and Department of Electrical Engineering, National Chiao Tung University, Hsinchu, Taiwan; Swartz Center of Computational Neuroscience, Institute for Neural Computation, University of California, San Diego, USA",2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"1 Dec 2011","2011","","","6092","6095","Electroencephalogram (EEG)-based emotion recognition has been an intensely growing field. Yet, how to achieve acceptable accuracy on a practical system with as fewer electrodes as possible is less concerned. This study evaluates a set of subject-independent features, based on differential power asymmetry of symmetric electrode pairs [1], with emphasis on its applicability to subject variability in music-induced emotion classification problem. Results of this study have evidently validated the feasibility of using subject-independent EEG features to classify four emotional states with acceptable accuracy in second-scale temporal resolution. These features could be generalized across subjects to detect emotion induced by music excerpts not limited to the music database that was used to derive the emotion-specific features.","1558-4615","978-1-4577-1589-1","10.1109/IEMBS.2011.6091505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091505","","Electroencephalography;Emotion recognition;Feature extraction;Electrodes;Accuracy;Educational institutions;Brain modeling","Adult;Algorithms;Brain;Electroencephalography;Emotions;Female;Humans;Male;Music;Pattern Recognition, Automated","3","","10","IEEE","1 Dec 2011","","","IEEE","IEEE Conferences"
"Enhancing Emotion Recognition using Contrastive Learning Representation of Paired Peripheral Physiological and EEG Signal","I. K. P. B. Laksana; M. A. Yonatan; P. A. Parimartha; V. A. Hongastu; I. H. Parmonangan; Diana","Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia","2023 IEEE 9th International Conference on Computing, Engineering and Design (ICCED)","13 Feb 2024","2023","","","1","6","Human emotions play a fundamental role in human-computer interaction, and accurately recognizing emotions is essential for enhancing user-friendly experiences in HCI applications. Emotions are complex and cannot be simply identified by a single modality, making multimodal approaches crucial. In this study, we propose a cross-modal representation learning approach for emotion recognition using peripheral physiological signals (PPS) and electroencephalogram (EEG). We focus on leveraging PPS, which is easier to obtain in real-life scenarios due to wearable technologies’ availability. Through contrastive learning with paired EEG signals, we aim to enhance the capabilities of PPS representation and achieve comparable performance to EEG-based models in downstream unsegmented emotion recognition tasks. We utilized the DEAP dataset and low or high arousal category for the downstream task. The results of the study demonstrate the successful learning of PPS representations that closely resemble EEG signals representations, leading to comparable performance on downstream tasks.","2767-7826","979-8-3503-7012-6","10.1109/ICCED60214.2023.10425227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425227","emotion recognition;EEG;peripheral signal;contrastive learning;representation learning","Human computer interaction;Emotion recognition;Self-supervised learning;Electroencephalography;Physiology;Signal representation;Task analysis","","","","17","IEEE","13 Feb 2024","","","IEEE","IEEE Conferences"
"Deep Forest based EEG Signal Analysis and Classification","R. Geetha; D. J. Rani","Department of Biomedical Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Chennai, India; Department of Biomedical Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Chennai, India","2024 8th International Conference on Electronics, Communication and Aerospace Technology (ICECA)","24 Dec 2024","2024","","","1198","1203","This work focuses on the development of a deep forest-based EEG signal analysis and classification system for the detection of cognitive states. The dataset used in this study was self-acquired using a g. Nautilus EEG recording device with six student volunteers focusing on six cognitive tasks. The dataset was preprocessed and feature engineered using various signal processing techniques. The system utilises various features extracted from EEG signals, including Hjorth parameters, wavelet trans-form features, and autoregressive models. The feature dataset is then trained on a cascade forest classifier using hyper-parameter tuning with the XGBoost library to improve the accuracy of the classification model. The top 20 most important features which the classifier selected are identified, and its significance is discussed. The proposed system shows promising results, achieving an overall accuracy of ${9 9. 7 2 1 \%}$ in classifying cognitive states from EEG signals. The proposed method can be used in various fields, such as neuroscience, healthcare, and human-computer interaction, to analyse and monitor cognitive states in real-time","","979-8-3503-6790-4","10.1109/ICECA63461.2024.10801115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10801115","Deep learning;Electro- Encephalogram signal analysis;Feature extraction;Classification;Signal processing;Brain-computer interfaces;Electroencephalography","Training;Accuracy;Neuroscience;Medical services;Feature extraction;Brain modeling;Electroencephalography;Regression analysis;Signal analysis;Random forests","","1","","17","IEEE","24 Dec 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Partial Directed Coherence Dense Graph Propagation","Z. Wang; Y. Liu; R. Zhang; J. Zhang; X. Guo","School of Computer Science and Technology, Xi’an University of Posts and Telecommunications, Xi’an, China; School of Computer Science and Technology, Xi’an University of Posts and Telecommunications, Xi’an, China; School of Computer Science and Technology, Xi’an University of Posts and Telecommunications, Xi’an, China; School of Computer Science and Technology, Xi’an University of Posts and Telecommunications, Xi’an, China; Hospital of Neurology, Shaanxi Provincial People’s Hospital, Xi’an, China",2022 14th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA),"7 Mar 2022","2022","","","610","617","In order to preserve the EEG time-frequency domain features while fully uncovering the information flow and spatial information in the causal connectivity of relevant brain regions, this paper proposes a multichannel EEG signal emotion recognition method based on partial directed coherence dense graph propagation. The proposed method mainly uses partial directed coherence (PDC) to construct causal brain networks, which is used to describe the interactions and information flow relationships between different nodes or brain regions in the brain, and uses a modified model of dense graph propagation (DGP) in graph convolutional neural networks to train the graph data modeled by multichannel causal connectivity information of EEG signals and single-channel time-frequency domain features. Unlike traditional graph convolutional neural networks, the PDC-DGP model focuses on the directedness of the graph and does not dilute the feature information of the graph data by extensive smoothing. We investigate on two publicly available datasets, DEAP and SEED, and the results show that the method has good recognition performance on both datasets, and the average accurate recognition rate can be as high as 85.25% on SEED and 77.37% and 76.92% on the DEAP dataset for valence and arousal, respectively.","2157-1481","978-1-6654-9978-1","10.1109/ICMTMA54903.2022.00127","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723993","EEG signals;Emotion recognition;Partial directed coherence (PDC);Dense graph propagation (DGP)","Emotion recognition;Time-frequency analysis;Technological innovation;Coherence;Brain modeling;Feature extraction;Electroencephalography","","8","","29","IEEE","7 Mar 2022","","","IEEE","IEEE Conferences"
"Simultaneous Detection And Analysis Of Multi-Source Data In Prediction Of Human Emotion","K. Saranya; S. Jayanthy","Department of Information Science and Engineering, Kumaraguru College of Technology, Coimbatore, India; Department of Electronics and Communication Engineering, Sri Ramakrishna Engineering College, Coimbatore, Tamil Nadu, India","2021 International Conference on Advancements in Electrical, Electronics, Communication, Computing and Automation (ICAECA)","18 Jan 2022","2021","","","1","6","Massive advancements in technology has made it possible to enable human and computer interaction which was once unimaginable. As the new modalities of the Human computer interaction through voice, gesture, brain signals have reached the market, rapid migration to those have been noticed from the regular keyboard and mouse interaction. Despite of various technical advancements, complete understanding of emotions as that of natural interactions remain missing leaving a huge space for research in that area. Beyond the verbal communication, emotions are key aspect of expressing oneself in human-human interaction and the need of understanding that emotions play a vital role in many computer applications. This paper explores all the existing ways of human-computer interactions that are currently in practice to enable the computer to understand the user’s emotions. In this paper, emotional data in various format from different sources were taken and their results were fused, and analyses has been carried out. The sources used for emotional data were the emotions in recorded speech, emotion in textual data and emotions through EEG signals. In recorded speech, acoustic features of the audio data were analysed by extracting the features with the help of LIBROSA library in Python. Fusion of the analysis result from the three sources helps in improving the accuracy of the emotion detection. Applications with emotion detection technology have wide range of applications in that would benefit companies in gain better understanding of their customers.","","978-1-6654-2829-3","10.1109/ICAECA52838.2021.9675654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9675654","Emotion-speech-textual-EEGsignals-Feature;extraction-fusion","Human computer interaction;Emotion recognition;Keyboards;Predictive models;Feature extraction;Brain modeling;Electroencephalography","","","","11","IEEE","18 Jan 2022","","","IEEE","IEEE Conferences"
"Emotion recognition through integrating EEG and peripheral signals","Y. Shu; S. Wang","Key Lab of Computing and Communication Software of Anhui Province, University of Science and Technology of China, Hefei, Anhui, P.R. China; Key Lab of Computing and Communication Software of Anhui Province, University of Science and Technology of China, Hefei, Anhui, P.R. China","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","2871","2875","The inherent dependencies among multiple physiological signals are crucial for multimodal emotion recognition, but have not been thoroughly exploited yet. This paper propose to use restricted Boltzmann machine (RBM) to model such dependencies.Specifically, the visible nodes of RBM represent EEG and peripheral physiological signals, and thus the connections between visible nodes and hidden nodes capture the intrinsic relations among multiple physiological signals. The RBM generates new representation from multiple physiological signals. Then, a support vector machine is adopted to recognize users' emotion states from the generated features. Furthermore, we extend the proposed fusion method for incomplete datas, since physiological signals are often corrupted due to artifacts. Specifically, we pre-train the RBM using all the complete data, then we update missing values and RBM parameters to minimize free energy of visible vectors using both complete and incomplete data. Experiments on two benchmark databases demonstrate the effectiveness of the proposed methods.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7952681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952681","multimodal emotion recognition;RBM;EEG;peripheral physiological signal;missing data","Physiology;Electroencephalography;Emotion recognition;Brain modeling;Databases;Feature extraction;Data models","","29","","10","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Emotion Recognition based on Double Filtration Signals Learning Network on Different Hemispheres","W. Guo; G. Xu; Y. Wang","College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, Shandong, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, Shandong, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, Shandong, China",2022 16th IEEE International Conference on Signal Processing (ICSP),"2 Dec 2022","2022","1","","139","144","Recently, emotion recognition has been diffusely applied in many fields and achieved excellent results, which is a promising research direction. However, existing researches fail to combine brain structure commendably to study brain activity signals. In this article, to capture human brain activity in the different hemispheres, we construct a double filtration signals learning network on different hemispheres (DFSLN-DH) to explore the influence of different cerebral hemispheres on subject independent emotion recognition. The proposed DFSLNDH model can not only extract the effective spatiotemporal information of Electroencephalogram (EEG) signals but also explores the effects of EEG signals in the left and right hemispheres on emotion recognition. Specifically, the EEG signals are firstly divided into three groups: left hemisphere signals, middle signals, and right hemisphere signals. Then, the double filtration signals learning module (DFSLM) is designed to mine the time-series features of each group, which could doubly screen the EEG signal to enrich the EEG feature information. Next, spatiotemporal information is extracted from the EEG features through depthwise separable convolutions. Finally, the significant emotional features learned from different groups are interacted through the fusion operator. Extensive experiments are performed on emotion datasets (SEED and SEED-IV). The experimental results suggest that the DFSLN-DH model is more competitive and can achieve better recognition performance.","2164-5221","978-1-6654-6056-9","10.1109/ICSP56322.2022.9965342","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965342","EEG emotion recognition;left and right hemispheres;double filtration feature learning;deep learning","Emotion recognition;Filtration;Filtering;Convolution;Feature extraction;Brain modeling;Electroencephalography","","","","27","IEEE","2 Dec 2022","","","IEEE","IEEE Conferences"
"Adversarial Adaptation Neural Networks With Class-Informed Discriminator for EEG Emotion Recognition","M. Meng; H. Ye; Y. Ma; Y. Gao; Z. Luo","International Joint Research Laboratory for Autonomous Robotic Systems and the School of Automation, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; School of Automation, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; International Joint Research Laboratory for Autonomous Robotic Systems, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; International Joint Research Laboratory for Autonomous Robotic Systems, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; School of Automation, Hangzhou Dianzi University, Hangzhou, Zhejiang, China",IEEE Sensors Journal,"15 May 2025","2025","25","10","17541","17549","Individual differences and nonstationary characteristics are prominent in electroencephalography (EEG) signals. Therefore, aligning the source and target domain data becomes essential in cross-subject and cross-session classification tasks. Although many adversarial adaptation networks can achieve distribution alignment through domain-level adaptation, they tend to disregard the multimodal structure inherent in the data. In this article, we present a concise and effective adversarial paradigm for EEG emotion recognition. This approach fully utilizes the label structure information of source domain data to reuse the binary discriminator as a class-informed discriminator instead of introducing additional modules, which not only realizes domain confusion but also ensures that mode information is retained in the process of confusion to avoid mode collapse. To evaluate our method, a systematic experimental study was conducted on the public datasets SEED and SEED-IV. The average accuracy of cross-subject and cross-session scenarios achieved 90.21%, 95.47% on SEED, and 77.50%, 77.54% on SEED-IV, respectively. Compared to the existing domain adaptation methods, the evident improvements of classification performance demonstrate the feasibility and effectiveness of our method.","1558-1748","","10.1109/JSEN.2025.3553489","National Natural Science Foundation of China(grant numbers:62271181,62371171,62071161,62171171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10944283","Adversarial network;class-informed discriminator;domain adaptation;electroencephalography (EEG) signal;emotion recognition;multimodal structure","Feature extraction;Electroencephalography;Emotion recognition;Brain modeling;Adaptation models;Training;Data models;Intelligent sensors;Data mining;Deep learning","","","","31","IEEE","27 Mar 2025","","","IEEE","IEEE Journals"
"Self-supervised Contrastive Pre-training for Dry Electrode EEG Emotion Recognition via Cross Device Representation Consistency","M. Zhang; S. Zhao; Z. Luo; L. Xie; T. Liu; D. Yao; Y. Yan; E. Yin","School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Defense Innovation Institute, Chinese Academy of Military Sciences, Beijing, China; Defense Innovation Institute, Chinese Academy of Military Sciences, Beijing, China; Defense Innovation Institute, Chinese Academy of Military Sciences, Beijing, China; School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Defense Innovation Institute, Chinese Academy of Military Sciences, Beijing, China; Defense Innovation Institute, Chinese Academy of Military Sciences, Beijing, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","The use of dry electrode electroencephalography (EEG) systems holds significant importance in advancing the everyday application of emotion recognition. However, adapting it to real-world applications faces unique challenges due to low signal-to-noise ratios and unreliable emotion labels. To address these challenges, we propose a Cross-Device Representation Consistency (CDRC) pre-training paradigm for dry EEG emotion recognition, where the self-supervised signal is provided by the distance between representations embedded in wet and dry EEG components and trained via contrastive estimation. Specifically, we employ a dual-branch embedding prediction task coupled with contrastive feature alignment module to extract robust and distinctive features from dry electrode EEG signals. We evaluate our model on an available emotional dataset PaDWEED, extensive experiments demonstrate that CDRC performs comparably to fully supervised training and achieves state-of-the-art results compared to several self-supervised approaches. Moreover, the remarkable performance on subject-independent tasks highlights its effectiveness in addressing and mitigating subject variability.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888833","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888833","Self-supervised learning;dry electrode system;emotion recognition;EEG","Electrodes;Performance evaluation;Training;Emotion recognition;Speech recognition;Feature extraction;Brain modeling;Electroencephalography;Speech processing;Signal to noise ratio","","","","31","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Long-Short Term Memory Analysis of EEG Data Using Python for Emotion Classification","A. R. Singh; G. Singh; N. Saluja","Chitkara Uiversity Institute of Engineering and Technology, Chitkara University, Punjab, India; Chitkara Uiversity Institute of Engineering and Technology, Chitkara University, Punjab, India; Chitkara Uiversity Institute of Engineering and Technology, Chitkara University, Punjab, India","2023 3rd International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)","28 Feb 2024","2023","","","1","5","Python has made significant progress and its adaptability allows it to be utilized in nearly all fields recognized by humanity. The utilization of this technology in the field of research and development has left a distinctive impact. Advancements in brain research have occurred at a rapid pace, resulting in a continuous stream of novel results being unveiled on a daily basis. Electroencephalogram (EEG) signals refer to the electrical impulses generated by the brain in reaction to specific visual stimuli, such as positive, negative, neutral, or soothing stimuli. The utilization of Python, along with its robust modules and libraries, facilitates the efficient identification and subsequent study of human emotions, hence enabling emotion recognition. In this discussion, we will explore Long Short Term memory a machine learning (ML) models that demonstrate efficacy in extracting and conveying significant insights pertaining to the human brain and EEG signals.","","979-8-3503-1912-5","10.1109/SMARTGENCON60755.2023.10442499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442499","EEG signals;Machine learning;Deep Analysis;Python;Exploratory Data Analysis (EDA);Long-Short Term Memory (LSTM)","Training;Emotion recognition;Machine learning;Brain modeling;Electroencephalography;Long short term memory;Python","","2","","19","IEEE","28 Feb 2024","","","IEEE","IEEE Conferences"
"Fuzzy Logic for Video Game Engagement Analysis using Facial Emotion Recognition","T. Killedar; G. Suriya; P. Sharma; M. Rathor; A. Gupta","Department of Electronics and Telecommunication, College of Engineering Pune, Pune, India; Department of Electronics and Telecommunication, College of Engineering Pune, Pune, India; Department of Electronics and Telecommunication, College of Engineering Pune, Pune, India; Department of Electronics and Telecommunication, College of Engineering Pune, Pune, India; Department of Electronics and Telecommunication, College of Engineering Pune, Pune, India",2021 8th International Conference on Signal Processing and Integrated Networks (SPIN),"19 Oct 2021","2021","","","481","485","Computer games have become popular recently, yet evaluating the user’s emotional state to understand their engagement lags far behind. There are few methods of assessing emotional responses and even fewer methods of quantifying emotions during play. This paper proposes a method for classifying a player’s emotional state and quantifying the engagement of a player in a game. The Emotions are recognized using a facial emotion recognizer built using convolutional neural networks. A fuzzy logic model transformed the seven universal emotions into low, medium or high engagement. This approach offers a method for quantifying emotional responses during a play experience.","2688-769X","978-1-6654-3564-2","10.1109/SPIN52536.2021.9566124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566124","Emotion detection;artificial intelligence;facial emotion recognition;convolutional neural networks;deep learning;fuzzy logic","Fuzzy logic;Emotion recognition;Analytical models;Target recognition;Face recognition;Games;Signal processing","","1","","15","IEEE","19 Oct 2021","","","IEEE","IEEE Conferences"
"The role of data balancing for emotion classification using EEG signals","E. T. Pereira; H. Martins Gomes","Systems and Computing Department - DSC/CEEI, Federal University of Campina Grande; Systems and Computing Department - DSC/CEEI, Federal University of Campina Grande",2016 IEEE International Conference on Digital Signal Processing (DSP),"2 Mar 2017","2016","","","555","559","In this paper, we demonstrate the role of data balancing in experimental evaluation of emotion classification systems based on electroencephalogram (EEG) signals. ADASYN method was employed to create a balanced version of the DEAP EEG dataset. Experiments considered Support Vector Machine classifiers trained with HOC and PSD features to predict valence and arousal affective dimensions. Using signals from only four channels (Fp1, Fp2, F3 and F4) we obtained, after balancing, accuracies of 98% (valence) and 99% (arousal) for subject dependent experiments with three classes, and 85% (valence) and 87% (arousal) for two-class classification. However, accuracies for subject independent experiments were lower than the ones obtained using imbalanced datasets. We obtained accuracies of 52% (valence) and of 49% (arousal) for two classes, and accuracies of 36% (valence) and of 31% (arousal) for three classes. To explain the low accuracies in subject independent experiments, we present arguments and empirical evidence using correlations between the percentage of samples for each class and the accuracies obtained by approaches which did not use balanced datasets.","2165-3577","978-1-5090-4165-7","10.1109/ICDSP.2016.7868619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868619","EEG Signal;Imbalanced Datasets;Affective Computing;Pattern Recognition","Electroencephalography","","5","","15","IEEE","2 Mar 2017","","","IEEE","IEEE Conferences"
"A Mutual Information Based Adaptive Windowing of Informative EEG for Emotion Recognition","L. Piho; T. Tjahjadi","School of Engineering, University of Warwick, Coventry, United Kingdom; School of Engineering, University of Warwick, Coventry, United Kingdom",IEEE Transactions on Affective Computing,"13 Nov 2020","2020","11","4","722","735","Emotion recognition using brain wave signals involves using high dimensional electroencephalogram (EEG) data. In this paper, a window selection method based on mutual information is introduced to select an appropriate signal window to reduce the length of the signals. The motivation of the windowing method comes from EEG emotion recognition being computationally costly and the data having low signal-to-noise ratio. The aim of the windowing method is to find a reduced signal where the emotions are strongest. In this paper, it is suggested, that using only the signal section which best describes emotions improves the classification of emotions. This is achieved by iteratively comparing different-length EEG signals at different time locations using the mutual information between the reduced signal and emotion labels as criterion. The reduced signal with the highest mutual information is used for extracting the features for emotion classification. In addition, a viable framework for emotion recognition is introduced. Experimental results on publicly available datasets, DEAP and MAHNOB-HCI, show significant improvement in emotion recognition accuracy.","1949-3045","","10.1109/TAFFC.2018.2840973","University of Warwick(grant numbers:School of Engineering Scholarship); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367876","EEG;human emotions;mutual information;entropy;data reduction","Electroencephalography;Feature extraction;Emotion recognition;Electrooculography;Mutual information;Entropy coding","","74","","64","IEEE","28 May 2018","","","IEEE","IEEE Journals"
"Spectral Graph Wavelet Transform-Based Feature Representation for Automated Classification of Emotions From EEG Signal","R. Krishna; K. Das; H. K. Meena; R. B. Pachori","Department of Electrical Engineering, Indian Institute of Technology Indore, Indore, India; Department of Electrical Engineering, Indian Institute of Technology Indore, Indore, India; Department of Electrical Engineering, Malaviya National Institute of Technology Jaipur, Jaipur, India; Department of Electrical Engineering, Indian Institute of Technology Indore, Indore, India",IEEE Sensors Journal,"14 Dec 2023","2023","23","24","31229","31236","Electroencephalogram (EEG) monitors the brain’s electrical activity and carries useful information regarding the subject’s emotional states. Due to the nonstationary and being complex in nature, proper signal-processing techniques are necessary to get meaningful interpretations. The EEG signal has been represented using a graph by incorporating the temporal dependency. In this article, a novel feature based on spectral graph wavelet transform (SGWT) for representing EEG signals has been proposed by considering the interdependency among different samples of EEG signals. SGWT is effective in finding multiscale information at the local level as well as the global level. These multiscale representations allow for the extraction of information about the EEG signal at different scales. The SGWT coefficients are used to develop machine-learning classifiers for emotion identification. Principal component analysis (PCA) is also used for feature reduction. The proposed framework is evaluated based on a publicly available SEED dataset with the help of extensive experiments. The  ${k}$ -nearest neighbor (KNN) classifier provides 97.3% accuracy with a standard deviation of 1.2%. The SGWT-based representation has achieved 12.7% higher accuracy compared to the raw EEG signal, which shows the usefulness of the proposed approach. Our model for emotion recognition attains superior classification performance compared to state-of-the-art methods. Finally, the investigation of interdependency among the samples of EEG signals reveals that the SGWT-based representation of EEG signals is a useful tool for analyzing EEG signals.","1558-1748","","10.1109/JSEN.2023.3330090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10320290","Electroencephalogram (EEG);emotion recognition;graph signal representation;k-nearest neighbor (KNN);naive Bayes (NB) classifier;random forest (RF);spectral graph wavelet transform (SGWT)","Electroencephalography;Emotion recognition;Feature extraction;Sensors;Electrodes;Brain modeling;Wavelet transforms","","13","","49","IEEE","16 Nov 2023","","","IEEE","IEEE Journals"
"Spiking Spatiotemporal Neural Architecture Search for EEG-Based Emotion Recognition","W. Li; Z. Zhu; S. Shao; Y. Lu; A. Song","School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Computer Science and Technology, Harbin institute of Technology (Shenzhen), and the Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies, Shenzhen, Guangdong, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China",IEEE Transactions on Instrumentation and Measurement,"14 Jan 2025","2025","74","","1","14","Spiking neural network (SNN) has the promising ability to take advantage of the spatiotemporal information from electroencephalogram (EEG) for emotion recognition. However, manually designing suitable SNN architectures needs considerable effort. In this article, we propose a novel and effective method, spiking spatiotemporal neural architecture search (SSTNAS), for EEG-based emotion recognition. SSTNAS exploits the discriminative spatial and temporal EEG features via spiking convolution neural network (SCNN) and spiking long short-term memory (SLSTM), respectively. Then, SSTNAS explores a proper SNN architecture for each task by investigating the spike activation patterns of pretrained networks based on genetic search, which is free of training. Experimental results on three public benchmark datasets, namely, FACED, DEAP, and DREAMER, demonstrate the superiority of the proposed method over the related state-of-the-art approaches.","1557-9662","","10.1109/TIM.2024.3472838","Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies(grant numbers:2022B1212010005); Open Projects Program of State Key Laboratory of Multimodal Artificial Intelligence Systems(grant numbers:MAIS2024108); National Key Laboratory of Avionics Integration and Aviation System-of-Systems Synthesis(grant numbers:2024AIASS0302); Big Data Computing Center of Southeast University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10741587","Electroencephalogram (EEG);emotion recognition;spiking neural network (SNN);spiking spatiotemporal neural architecture search (SSTNAS);training free","Feature extraction;Electroencephalography;Emotion recognition;Neurons;Convolution;Brain modeling;Computer architecture;Membrane potentials;Genetics;Training","","","","62","IEEE","4 Nov 2024","","","IEEE","IEEE Journals"
"Exploring the Use of Spatial Information in Emotion Classification Using Functional Connectivity and Channel Distribution","Y. Xu; S. Otsuka; S. Nakagawa","Graduate School of Science and Engineering, Chiba University, Chiba, Japan; Center for Frontier Medical Engineering, Chiba University, Chiba, Japan; Center for Frontier Medical Engineering, Chiba University, Chiba, Japan",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","5","Advancements in neuroscience, especially in estimation technique of functional connectivity and brain networks, have been instrumental in emotion classification. However, conventional methods that utilize feature vectors or connectivity matrices do not entirely harness the spatial information of brain activity. We propose the Sliding Functional Connectivity Convolution (SFCC), which constructs brain networks by leveraging the spatial information of channel distribution of electroencephalogram (EEG). Similar to image convolution, SFCC begins by defining a matrix to be processed and an operation kernel, both derived from a channel projection. We then calculate the functional connectivity between the matrix elements and elements of the kernel while sliding the kernel across the matrix. The resultant brain networks provide additional spatial information for a classifier compared to conventional methods. Comparison between brain networks constructed using conventional feature vectors and connectivity matrices with those created by the proposed method showed a significant enhancement in classification performance.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10781684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781684","EEG based emotion classification;functional connectivity;spatial brain networks;machine learning","Brain;Visualization;Neuroscience;Convolution;Scalp;Network analyzers;Motors;Vectors;Electroencephalography;Kernel","Humans;Electroencephalography;Emotions;Brain;Algorithms;Signal Processing, Computer-Assisted","","","8","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"EEG signal analysis using deep learning: A systematic literature review","G. Amrani; A. Adadi; M. Berrada; Z. Souirti; S. Boujraf","National School of Applied Sciences, Laboratory of Artificial Intelligence, Data Science and Emerging Systems, Fez, Morocco; ISIC Research Team of High School of Technology, LMMI Laboratory, Moulay Ismail University, Meknes, Morocco; National School of Applied Sciences, Laboratory of Artificial Intelligence, Data Science and Emerging Systems, Fez, Morocco; Faculty of Medicine and Pharmacy Neurology Department, Clinical Neurosciences Laboratory, Sleep Center Hassan II University Hospital, Fez, Morocco; Clinical Neurosciences Laboratory Department of Biophysics and Clinical MRI Methods, Faculty of Medicine and Pharmacy Department of Radiology and Clinical Imaging, Hassan II University Hospital, Fez, Morocco",2021 Fifth International Conference On Intelligent Computing in Data Sciences (ICDS),"1 Dec 2021","2021","","","1","8","Objective: Electroencephalography (EEG) is very crucial for understanding the dynamic healthy and pathological complex processes in the brain. However, the manual analysis of the EEG signal is very complex, time-consuming, and depends on the expertise and experience of the users. Hence, nowadays research is conducted on automated EEG signal analysis using artificial intelligence and computer-aided technologies. This would allow fast and highly accurate results. The goal of this paper is to provide an extensive review of the EEG signal analysis using deep learning (DL).Methods: This systematic literature review of EEG processing using Deep Learning (DL) was achieved on Web of Science, PubMed, and Science Direct databases, resulting in 403 identified papers. All collected studies were analyzed based on main disorders studied, type of tasks performed, data source, stages of analysis, and DL architecture.Results: DL in EEG processing is promising in various research applications. It covered the common neurological disorders diagnosis such as epilepsy, movement disorder, depression, schizophrenia, autism, alcohol use, attention, memory, sleep, pain, etc. The main tasks covered by the included studies are detection and classification. The average range of data sources utilized by the included studies is 127 subjects with an EEG recording a total duration of 458 hours. In fact, we identified the use of a plethora of DL architecture for EEG analysis. 57% of papers used Convolutional Neural Networks (CNNs), whereas Recurrent Neural Networks (RNNs) were the architecture choice of about 12% of papers. Combinations of CNNs and Long Short-Term Memory (LSTM) were used in 13% of studies. Generative Adversarial Networks (GAN) and Autoencoder (AEs) were used in 5% and 4% of papers respectively. Restricted Boltzmann Machine (RBMs), Deep Belief Networks (DBNs), and other hybrid architectures appeared in 1% of studies.Conclusion: This systematic review showed that DL is a powerful tool to process, analyze, and interpret EEG data without requiring extraction steps. These intelligent models can allow self-learning from the training data. On the other hand, DL models need a lot of data to learn, while suffering from a lack of confidence due to their black-box nature. Hence, studies on transfer learning and Explainable Artificial Intelligence (XAI) could help in solving such issues. Big Data, Cloud Computing, the Internet of Things (IoT), and closed-loop technology can also help DL-based systems in achieving fast, and accurate processing of EEG recordings.","","978-1-6654-4238-1","10.1109/ICDS53782.2021.9626707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626707","Deep learning;Electroencephalography;EEG;Machine Learning;Systematic Literature Review","Deep learning;Systematics;Computational modeling;Bibliographies;Computer architecture;Brain modeling;Electroencephalography","","21","","48","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"A Comprehensive Study on Emotion Recognition in Healthcare by Applying Machine Learning and Deep Learning Techniques","T. Ahmed; C. Gopala Krishnan","Department of CSE, GITAM School of Technology, Bengaluru, INDIA; Department of CSE, GITAM School of Technology, Bengaluru, INDIA",2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS),"7 Aug 2024","2024","1","","1","6","The capacity of exactly infer human feelings from enlarger modalities and sources called emotion recognition with the help of physiological signals, physical signals and questionnaires. The diverse application fields such as healthcare, market research, human-robot interactions and affective computing to make huge attention in emotion recognition. Nowadays, human emotions are analyzed and recognized based on the growth of information technology and sensors machines. Different manifestations present in human emotions. The examination of physiological signals, behavior, speech and facial expressions realizes the emotion recognition. The growth of affective computing promoted with accurate recognition human emotions in which different sensors collect the human emotions. This work reviewed various kinds of techniques for emotion recognition in healthcare system (ERHS). More than 35 papers selected and reviewed literature research methodologies. According to various innovations, this survey is classified into machine and deep learning (ML and DL) models. For recognizing emotions, the dataset details and various methods with its merits and demerits are focused in this article. The existing literature with its potential challenges and also the future research direction is reviewed.","","979-8-3503-5968-8","10.1109/ICKECS61492.2024.10617024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10617024","Emotion Recognition;Healthcare;AI techniques;Facial Images and Emotion signal processing","Deep learning;Emotion recognition;Affective computing;Technological innovation;Accuracy;Human-robot interaction;Medical services","","","","30","IEEE","7 Aug 2024","","","IEEE","IEEE Conferences"
"Emotion recognition based on wavelet analysis of Empirical Mode Decomposed EEG signals responsive to music videos","C. Shahnaz; Shoaib-Bin-Masud; S. M. S. Hasan","Bangladesh University of Engineering and Technology, Dhaka, Dhaka District, BD; Bangladesh University of Engineering and Technology, Dhaka, Dhaka District, BD; Bangladesh University of Engineering and Technology, Dhaka, Dhaka District, BD",2016 IEEE Region 10 Conference (TENCON),"9 Feb 2017","2016","","","424","427","Human Computer Interface and implicit emotion tagging of multimedia contents require determination of various emotional states. This paper presents a new emotion recognition method based on wavelet analysis of Empirical Mode Decomposed (EMD) Electroencephalogram (EEG) signals responsive to music videos. Discrete Wavelet Transform (DWT) is performed on the selected Intrinsic Mode Functions (IMFs) obtained from EMD operation. Then Higher Order Statistics (HOS), namely, variance, kurtosis, skewness of suitably chosen DWT coefficients are exploited to form feature vector. Furthermore, Principal Component Analysis (PCA) is applied on the feature vector to reduce the feature dimension. The reduced feature set thus obtained is then fed to Support Vector Machine (SVM) to perform two class classification of emotions. Extensive simulations are carried out to test the efficacy of the proposed method using DEAP, an affective computing database. It is found that the proposed method outperforms some state of the art methods of similar emotion recognition using the same database.","2159-3450","978-1-5090-2597-8","10.1109/TENCON.2016.7848034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848034","Emotion Recognition;EEG;EMD;IMF;DWT;PCA;HOS;SVM","Electroencephalography;Discrete wavelet transforms;Principal component analysis;Feature extraction;Emotion recognition;Support vector machines;Videos","","29","","13","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Multimodal Motor Imagery BCI Based on EEG and NIRS","I. Ivaylov; M. Lazarova; A. Manolova","Department. Computer Systems, Faculty of Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria; Department. Computer Systems, Faculty of Computer Systems and Technologies, Technical University of Sofia, Sofia, Bulgaria; Dept. Radio Communications and Video Technology, Faculty of Telecommunications, Technical University of Sofia, Sofia, Bulgaria","2021 56th International Scientific Conference on Information, Communication and Energy Systems and Technologies (ICEST)","28 Jul 2021","2021","","","73","76","Brain-computer interface comprises technologies for brain activity identification used in many application fields such as motor imagery, disease or mental state detection. Multimodal approach that utilizes hybrid data can be improve motor imagery classification. The paper explores utilization of several classification techniques for multimodal electroencephalography (EEG) and near-infrared spectroscopy (NIRS) data classification in motor imagery BCI. Five classifiers used in the evaluation are Logistic Regression, K-Nearest Neighbours, Support Vector Machines, Linear Regression, SVC Radial Basis Regression and their performance is compared on EEG and EEG+NIRS datasets for motor imagery tasks classification.","","978-1-6654-2887-3","10.1109/ICEST52640.2021.9483551","TU-Sofia(grant numbers:2020001-09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483551","Brain computer interface;EEG;Multimodal BCI;NIRS","Support vector machines;Spectroscopy;Linear regression;Static VAr compensators;Feature extraction;Electroencephalography;Brain-computer interfaces","","1","","27","IEEE","28 Jul 2021","","","IEEE","IEEE Conferences"
"Emotion Recognition from EEG Signal using XGBoost Algorithm","S. Parui; A. K. Roshan Bajiya; D. Samanta; N. Chakravorty","Advanced Technology Development Center, Indian Institute of Technology, Kharagpur, India; Dept. of Mining Engineering, Indian Institute of Technology, Kharagpur, India; Dept. of Computer Sc. and Engineering, Indian Institute of Technology, Kharagpur, India; School of Medical Sc. and Technology, Indian Institute of Technology, Kharagpur, India",2019 IEEE 16th India Council International Conference (INDICON),"12 Mar 2020","2019","","","1","4","Of late, emotion detection from brain signal has become a topic of research. Various machine learning algorithms have been applied to classify the emotion as a psychological measurement. However, good accuracy in measurements is yet to be achieved. This work proposes an approach to classify the different type of estimation in human being with improved accuracy. This work extracts several features from EEG brain signals and then optimize the set of features using the correlation matrix, information gain calculation, and recursive feature elimination method. The optimized feature set is then used in classification. For the classification technique, the Extreme Gradient Boosting (XGBoost) algorithm has been followed. The XGBoost algorithm works on the Gradient Boosting framework and follows linear model solver and tree learning concept. The results show that feature optimization followed by the XGBoost algorithm gives better classification accuracy. The proposed approach has been tested with DEAP data set. The proposed approach also has been compared with different classification algorithms, namely Naive Bayes, KNN, C4.5, Decision Tree, Random Forest algorithms. The results substantiate the efficacy in favor of the proposed approach.","2325-9418","978-1-7281-2327-1","10.1109/INDICON47234.2019.9028978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9028978","","Feature extraction;Electroencephalography;Forestry;Emotion recognition;Brain modeling;Support vector machines;Correlation","","39","","19","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"Efficient Sample and Feature Importance Mining in Semi-Supervised EEG Emotion Recognition","X. Li; F. Shen; Y. Peng; W. Kong; B. -L. Lu","School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Circuits and Systems II: Express Briefs,"28 Jun 2022","2022","69","7","3349","3353","Recently, electroencephalogram (EEG)-based emotion recognition has attracted increasing interests in research community. The weak, non-stationary, multi-rhythm and multi-channel properties of EEG data easily cause the extracted EEG samples and features contribute differently in recognizing emotional states. However, existing studies either failed to consider both the issues of sample and feature importance or only considered one of them. In this brief, we propose a new model termed sJSFE (semi-supervised Joint Sample and Feature importance Evaluation) to quantitatively measure the sample and feature importance by self-paced learning and feature self-weighting respectively. Experimental results on the SEED-IV data set show that the emotion recognition performance is greatly improved by mining both the sample and feature importance. Specifically, the average accuracy obtained by sJSFE across the three cross-session recognition tasks is 82.45%, which is respectively 3.72% and 7.21% and 10.47% and 18.82% higher than the results of traditional models. Besides, the feature importance vector depicts that the Gamma frequency band contributes the most, and the brain regions of prefrontal, left/right temporal and (central) parietal lobes correlate more to emotion recognition. The sample importance descriptor shows that continual transitions of video types in consecutive trials might weaken the feature-label consistency of the collected EEG data.","1558-3791","","10.1109/TCSII.2022.3163141","National Key Research and Development Program of China(grant numbers:2017YFE0116800); National Natural Science Foundation of China(grant numbers:61971173); Fundamental Research Funds for the Provincial Universities of Zhejiang(grant numbers:GK209907299001-008); CAAC Key Laboratory of Flight Techniques and Flight Safety(grant numbers:FZ2021KF16); Guangxi Key Laboratory of Optoelectronic Information Processing (Guilin University of Electronic Technology)(grant numbers:GD21202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744018","EEG;emotion recognition;feature importance;sample importance;semi-supervised learning","Electroencephalography;Emotion recognition;Brain modeling;Task analysis;Feature extraction;Data models;Data mining","","9","","15","IEEE","29 Mar 2022","","","IEEE","IEEE Journals"
"Emotion Detection Mastery: EEG Signal Analysis with Balanced vs. Unbalanced Data","J. Menaka; S. I. G; A. Poulose","School of Data Science, Indian Institute of Science Education and Research, Thiruvananthapuram, Kerala, India; Department of Computational Biology and Bioinformatics, University of Kerala, Thiruvananthapuram, Kerala, India; School of Data Science, Indian Institute of Science Education and Research, Thiruvananthapuram, Kerala, India",2024 International Conference on Brain Computer Interface & Healthcare Technologies (iCon-BCIHT),"19 Feb 2025","2024","","","17","22","Emotion is crucial in daily life, influencing stress levels, decision-making, relationships, and overall mental health. Recognizing emotions accurately is valuable across sectors like healthcare, education, and beyond. This paper presents a comparative study on emotion recognition, applying various deep learning models with the DEAP dataset, which includes EEG recordings from 32 participants who watched 40 short music video clips. We extracted time-domain, frequency-domain, and wavelet features from the preprocessed EEG data to capture nuanced patterns within the signals. The models assessed in this study include convolutional neural networks (CNN), long short-term memory networks (LSTM), gated recurrent units (GRU), and hybrid architectures, each trained on both balanced and unbalanced datasets. To address class imbalance, data was augmented using the Synthetic Minority Oversampling Technique (SMOTE). Results indicate that the Attention GRU model achieved slightly higher performance by utilizing its attention mechanism to capture extended dependencies within the data. Additionally, model accuracy improved by 5-9% when trained on balanced datasets, underscoring the importance of data balancing in enhancing classification outcomes.","","979-8-3315-4006-7","10.1109/iCon-BCIHT63907.2024.10882443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10882443","Emotion Recognition;Electroencephalogram (EEG);DEAP Dataset;Synthetic Minority Oversampling Technique (SMOTE);Deep Learning Models","Deep learning;Emotion recognition;Attention mechanisms;Medical services;Brain modeling;Feature extraction;Transformers;Electroencephalography;Data models;Convolutional neural networks","","","","7","IEEE","19 Feb 2025","","","IEEE","IEEE Conferences"
"UNiFY-ESSL: UNIfied Framework for Yielding EEG-Based Emotion Recognition Model with Self-Supervised Learning","C. Ahuja; D. Sethia","Department of Computer Engineering, Delhi Technology University, Delhi, India; Department of Software Engineering, Delhi Technology University, Delhi, India",2024 International Conference on Brain Computer Interface & Healthcare Technologies (iCon-BCIHT),"19 Feb 2025","2024","","","249","256","Electroencephalography (EEG) based emotion recognition shows promise in human-computer interaction and mental health monitoring, but faces challenges in cross-dataset generalization. This study introduces the Unified Framework for EEG-based Emotion Recognition with Self-Supervised Learning (UNiFY-ESSL), combining self-supervised learning (SSL) and advanced sampling techniques to integrate multiple EEG datasets. UNiFY-ESSL utilizes a unified 14-channel setup across all datasets (SEED, DEAP, and DREAMER), allowing consistent preprocessing and feature extraction. This approach adapts datasets with varying initial channel counts (62 for SEED, 32 for DEAP, and 14 for DREAMER) to a common configuration based on DREAMER's channels. The framework explores contrastive learning methods, specifically Simple Contrastive Learning (Sim-CLR) and Contrastive Predictive Coding (CPC), enhanced by a novel stratified sampling scheme. Experiments yield impressive results: SimCLR achieves F1-scores of 82.62%, 87.83%, and 89.05% for SEED, DEAP, and DREAMER respectively, while CPC attains 81.35%, 82.27%, and 91.23%. It demonstrates improved cross-dataset generalization, with a 1–2% performance gain on DREAMER and maintained performance on DEAP despite channel reduction. However, SEED experiences a 3% F1-score drop due to significant channel reduction. These findings highlight the potential of our sampling method in advancing EEG-based emotion recognition while underscoring the need for future work on efficient channel combination strategies, particularly for datasets with higher initial channel counts.","","979-8-3315-4006-7","10.1109/iCon-BCIHT63907.2024.10882427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10882427","EEG Emotion Recognition;Self-Supervised Learning (SSL);Cross-Dataset Learning;Contrastive Learning;Unified Model Architecture","Emotion recognition;Contrastive learning;Mental health;Medical services;Predictive coding;Performance gain;Brain modeling;Sampling methods;Electroencephalography;Monitoring","","","","23","IEEE","19 Feb 2025","","","IEEE","IEEE Conferences"
"An Efficient Spatial-Temporal Representation Method for EEG Emotion Recognition","W. Weng; Y. Gu; Y. Chen; G. Wang; N. Shi","Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer and Information Engineering, Luoyang Institute of Science and Technology, Luoyang, China; School of Computer and Information Engineering, Luoyang Institute of Science and Technology, Luoyang, China","2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)","27 Jul 2023","2022","","","458","467","Affective computing based on electroencephalogram (EEG) has been widely studied due to the high correlation between the brain and emotions. Neurological research shows that emotions are generated by the interaction of neural circuits separated in different brain regions. Recently, many deep learning methods have been demonstrated to recognize different emotions. However, these methods extract features at the data level, ignoring the brain’s neural mechanisms of emotion generation, which may result in the features’ weak ability to express emotion. To solve this problem and extract more discriminant features according to the neural mechanism of emotion generation, this article proposes an attention based spatial-temporal representation method (ASTR) for emotion recognition. The ASTR method constructs spatial-temporal representations of emotion from neural circuits’ activity in different brain regions. In the model, the cascade attention quantifies the multi-granularity correlation and information exchange pattern between brain regions as the spatial features, which measure neural circuits’ activation and information interaction in different brain regions. The one-way transformer introduces temporal order in spatial features, calculates temporal variation features from the long-term activity of neural circuits and generates the emotion representation. The representation incorporates spatial-temporal features of neural circuits’ activity during emotion generation, which is highly related to emotion. We conducted experiments on SEED and DEAP datasets. The experimental results show that the emotion recognition accuracy of our model is about 3.0% higher than that of the SOTA algorithms, reaching more than 98%. The results also visualize the emotion generation process, revealing how the brain works under different emotions.","","979-8-3503-4655-8","10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00084","Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10189574","Emotion Recognition;Electroencephalogram (EEG);Transformer;Cascade Attention","Emotion recognition;Visualization;Correlation;Neural circuits;Feature extraction;Brain modeling;Transformers","","","","36","IEEE","27 Jul 2023","","","IEEE","IEEE Conferences"
"Adaptive Hierarchical Graph Convolutional Network for EEG Emotion Recognition","Y. Xue; W. Zheng; Y. Zong; H. Chang; X. Jiang","School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; School of Biological Science and Medical Engineering, Southeast University, Nanjing, China",2022 International Joint Conference on Neural Networks (IJCNN),"30 Sep 2022","2022","","","1","8","Human emotion is closely related to multiple distributed brain regions, and functional connections exist between the regions. However, how to abstract the region-level information to improve electroencephalograph (EEG) emotion recognition performance has not been well considered. To address this problem, we proposed a novel Adaptive Hierarchical Graph Convolutional Network (AHGCN), which includes the basic channel-level graph of EEG channels and the region-level graph of brain regions. Different from previous methods, we propose an adaptive pooling operation to automatically partition brain regions rather than manually define them. To capture the intrinsic functional connections between the brain regions or EEG channels, we design a gated adaptive graph convolution operation. Besides, we develop a graph unpooling operation to integrate the region-level graph and channel-level graph to extract more discrimination features for classification. Experiments on two widely-used datasets show that our proposed method is superior to many state-of-the-art methods on EEG emotion recognition and could find some interesting combinations of EEG channels.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892411","EEG emotion recognition;graph convolutional neural network (GCNN);graph pooling","Emotion recognition;Visualization;Adaptive systems;Convolution;Color;Logic gates;Feature extraction","","5","","33","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Emotion Recognition in Gaming Dataset to Reduce Artifacts in the Self-Assessed Labeling Using Semi-Supervised Clustering","O. Almanza-Conejo; J. G. Avina-Cervantes; A. Garcia-Perez; M. A. Ibarra-Manzano","Department of Electronics Engineering, Engineering Division of the Campus Irapuato-Salamanca, University of Guanajuato, Salamanca, Mexico; Department of Electronics Engineering, Engineering Division of the Campus Irapuato-Salamanca, University of Guanajuato, Salamanca, Mexico; Department of Electronics Engineering, Engineering Division of the Campus Irapuato-Salamanca, University of Guanajuato, Salamanca, Mexico; Department of Electronics Engineering, Engineering Division of the Campus Irapuato-Salamanca, University of Guanajuato, Salamanca, Mexico",IEEE Access,"17 Apr 2024","2024","12","","52659","52668","Popular comments suggest that continuous exposure of children and adolescents to video games yields a non-benefit behavior in the players’ mental health. Contrarily, several studies have proven that commercial and serious games improve mental activity; some are used in treating psychological and physical disorders. This paper presents a method based on electroencephalogram signals analysis to classify multiple emotions recorded from subjects’ gameplay seasons. In the core of this study, a self-assessed labeling method is evaluated using the Force, EEG, and Emotion Labelled Dataset (FEEL) for emotion recognition tasks. Besides, a 1-D Local Binary Pattern (LBP) method transforms the EEG temporal behavior to extract time-frequency features. Complementarily, the database artifacts were removed using a novel Conflict Learning approach for machine learning models, associating the extracted samples with the subjects’ emotion labeling. A semi-supervised clustering method was employed to show the similarity between self-assessed subjects’ labels. Finally, numerical results suggested a conflict between 23 original labels, improving the classification by over 92% in accuracy for 19 self-assessed classes.","2169-3536","","10.1109/ACCESS.2024.3387357","Doctoral Scholarship; Mexican National Council for Humanities, Sciences, and Technology (CONAHCyT)(grant numbers:815485); University of Guanajuato through the Institutional Call for Scientific Research (CIIC)(grant numbers:243/2024); Annual Operational Program (POA) 2024; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496111","Emotion recognition;gaming;conflict learning;clustering;machine learning","Video games;Electroencephalography;Games;Feature extraction;Emotion recognition;Brain modeling;Measurement;Machine learning;Clustering algorithms;User experience","","1","","46","CCBYNCND","10 Apr 2024","","","IEEE","IEEE Journals"
"A two-stream channel reconstruction and feature attention network for EEG emotion recognition","Q. Wang; L. Yang; H. Sun; Q. Zhang; C. Tang","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China",2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Jan 2024","2023","","","4840","4847","Research on human emotions based on EEG during multimedia stimuli is an emerging field that has made significant progress in EEG-based emotion classification. However, current studies often neglect the extraction of dynamic information from EEG signals and lack the exploration of local information. Moreover, many existing models are overly complex, demanding an excessive investment in training resources and time. In this paper, we propose a novel, simple two-stream channel reconstruction and feature attention network, named CRFAE-motionNet, for EEG emotion recognition. The main advantage of CRFAEmotionNet is its ability to simultaneously integrate static and dynamic information from EEG signals within a unified network. Additionally, it can extract continuous EEG temporal information through channel reconstruction and utilize feature attention to further explore local information. The proposed network was evaluated using the publicly available DEAP dataset. The experimental results indicate that the proposed CRFAEmotionNet outperforms the state-of-the-art baselines, achieving the accuracy of 98.7% for valence and 98.6% for arousal.","2156-1133","979-8-3503-3748-8","10.1109/BIBM58861.2023.10385459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385459","EEG;Emotion recognition;Two-stream;Attention mechanism","Training;Emotion recognition;Sleep;Feature extraction;Brain modeling;Fatigue;Electroencephalography","","","","32","IEEE","18 Jan 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition from Electroencephalogram using Variational Mode Decomposition and optimized Fuzzy K-Nearest Neighbor Algorithm","T. Kaur; T. K. Gandhi","Department of Electrical Engineering, Indian Institute of Technology, Delhi, New Delhi, India; Department of Electrical Engineering, Indian Institute of Technology, Delhi, New Delhi, India",2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT),"23 Nov 2023","2023","","","1","5","Emotion recognition via Electroencephalogram (EEG) has gained considerable attention in the recent years due to its objective and naturalistic response to the emotional stimulus. However, the non-stationarity of the EEG signal makes it challenging to decipher the information hidden in the raw EEG using the conventional time or frequency domain methods. To obtain the pattern information from the EEG data useful for recognition, a framework using Variational Mode Decomposition (VMD) is proposed. This framework firstly uses VMD, an adaptive approach to decompose EEG into specific Intrinsic mode function representations (IMF’s). Thereafter, power in different rhythms and absolute value of wavelet coefficients is extracted from each IMF. Moreover, a BAT optimized Fuzzy K-Nearest Neighbor (FKNN) algorithm is presented to finally recognize the different emotional states. The BAT algorithm optimizes the FKNN hyper-parameters to boost the classification performance. The proposed framework is tested on EEG signals from DEAP dataset, in which the Arousal and the Valence conditions are classified as High or Low. The results indicate that the proposed framework outperforms the existing state-of-the-art works by attaining a classification accuracy of 65% for High/Low Arousal and 62% for High/Low Valence conditions.","2473-7674","979-8-3503-3509-5","10.1109/ICCCNT56998.2023.10308188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308188","Electroencephalogram;Variational Mode Decomposition;BAT optimization;Fuzzy K-Nearest Neighbor;Emotion Classification","Emotion recognition;Frequency-domain analysis;Feature extraction;Brain modeling;Electroencephalography;Real-time systems;Classification algorithms","","1","","41","IEEE","23 Nov 2023","","","IEEE","IEEE Conferences"
"Enhancing EEG Signals Classification through Spatio-Temporal Feature Fusion in Dense Connection Neural Networks","M. Liu; W. Guo; J. Chen; B. Zhang; Y. Wang","College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China",2024 IEEE 17th International Conference on Signal Processing (ICSP),"23 Jan 2025","2024","","","577","581","Brain-computer interface (BCI) technology based on electroencephalogram (EEG) has attracted significant interest, particularly in EEG signal interpretation, pattern recognition, and brain activity classification, which are considered promising research avenues. Nonetheless, the classification of target signals using EEG continues to pose substantial challenges regarding the performance and interpretability of human brain signals. To address these issues, this study proposes a novel dense connection convolutional neural network with spatio-temporal feature fusion to classify brain visual images. Drawing inspiration from visual attention and brain memory mechanisms, a densely connected module is incorporated to mitigate the vanishing gradient issue and facilitate effective training of deep networks. EEG signals are subsequently encoded and stored along both temporal and spatial dimensions. To leveraging the unique properties of EEG signals, a bidirectional gated recurrent unit network is employed to derive temporal features, while spatial features are extracted using a two-dimensional mixed dilated convolution module. Ultimately, the extracted spatio-temporal features are concatenated and classified. The findings confirm the feasibility and effectiveness of the proposed model.","2164-5221","979-8-3503-8738-4","10.1109/ICSP62129.2024.10846701","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10846701","electroencephalogram signal classification;convolutional neural networks;dense connection;gated recurrent unit;dilation residual shrinkage module","Training;Visualization;Convolution;Time series analysis;Pattern classification;Signal processing algorithms;Feature extraction;Brain modeling;Electroencephalography;Biological neural networks","","","","19","IEEE","23 Jan 2025","","","IEEE","IEEE Conferences"
"Multimodal Emotion Recognition from Eye Image, Eye Movement and EEG Using Deep Neural Networks","J. -J. Guo; R. Zhou; L. -M. Zhao; B. -L. Lu","Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, China",2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"7 Oct 2019","2019","","","3071","3074","In consideration of the complexity of recording electroencephalography(EEG), some researchers are trying to find new features of emotion recognition. In order to investigate the potential of eye tracking glasses for multimodal emotion recognition, we collect and use eye images to classify five emotions along with eye movements and EEG. We compare four combinations of the three different types of data and two kinds of fusion methods, feature level fusion and Bimodal Deep AutoEncoder (BDAE). According to the three-modality fusion features generated by BDAE, the best mean accuracy of 79.63% is achieved. By analyzing the confusion matrices, we find that the three modalities can provide complementary information for recognizing five emotions. Meanwhile, the experimental results indicate that the classifiers with eye image and eye movement fusion features can achieve a comparable classification accuracy of 71.99%.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8856563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8856563","","Feature extraction;Electroencephalography;Emotion recognition;Brain modeling;Training;Support vector machines;Glass","Electroencephalography;Emotions;Eye Movements;Humans;Neural Networks, Computer","43","","17","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"EEG-based emotion recognition using wavelet features","Z. Zhou; H. Jiang; X. Song","MinZu University of China, Beijing, China; MinZu University of China, Beijing, China; MinZu University of China, Beijing, China",2014 IEEE 5th International Conference on Software Engineering and Service Science,"23 Oct 2014","2014","","","585","588","This paper described a research project conducted to recognize to finding the relationship between EEG signals and Human emotions. EEG signals are used to classify three kinds of emotions, positive, neuter and negative. Firstly, literature research has been performed to establish a suitable approach for emotion recognition. Secondly, we extracted features from original EEG data using 4-order wavelet and put them in SVM classifier with different kernel functions. The result shows that an SVM with linear kernel has higher average test accuracy than other kernel function.","2327-0594","978-1-4799-3279-5","10.1109/ICSESS.2014.6933636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933636","Brain-computer interaction;electroencephalogram;emotion recognition;wavelet","Emotion recognition;Electroencephalography;Kernel;Feature extraction;Support vector machines;Accuracy;Speech recognition","","5","","8","IEEE","23 Oct 2014","","","IEEE","IEEE Conferences"
"Novel approach for classification of stress EEG data using statistical techniques","M. J. Patil; A. Shaikh; M. G. Dhopeshwarkar","Department of CS and IT, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad, MS, India; Department of CS and IT, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad, MS, India; Department of CS and IT, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad, MS, India",2016 International Conference System Modeling & Advancement in Research Trends (SMART),"12 Apr 2017","2016","","","14","16","Many people undergo from the stress in their everyday life. Due to there close relationship between the stress, mental health and psychological aspect. Stress management using EEG data is very challenging task. In this paper, classification of EEG data is done by using statistical techniques like, mean, standard deviation, variance and correlation. This algorithm got the 0.0056 correlation which is positive association.","","978-1-5090-3543-4","10.1109/SYSMART.2016.7894481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894481","EEG;Mean;Standard Deviation;Variance;Correlation","Electroencephalography;Correlation;Stress;Feature extraction;Classification algorithms;Standards;Databases","","1","","8","IEEE","12 Apr 2017","","","IEEE","IEEE Conferences"
"Facial expression and EEG signal based classification of emotion","A. Basu; A. Halder","ECE Department, Heritage Institute of Technology; ECE Department, Heritage Institute of Technology","International Conference on Electronics, Communication and Instrumentation (ICECI)","17 Mar 2014","2014","","","1","4","The paper provides a novel approach to emotion recognition from facial expression and Electro Encephalograph (EEG) signal of subjects. Five subjects are requested to watch particular videos for arousing five different emotions in their mind. The facial expressions and EEG signal of subjects are recorded by a good quality camera and EEG machine respectively while watching the movie clips. Facial features include mouth-opening, eye-opening, eyebrow-constriction, and EEG features include, 132 number of Wavelet coefficients, 16 numbers of Kalman Filter coefficients and power spectral density, are then extracted from the facial expression and EEG signal frames. Then these huge numbers of features are reduced by Principle Component Analysis (PCA) and feature vector is constructed for 5 different emotions. A linear Support Vector Machine classifier is used to classify the extracted feature vectors into different emotion classes. Experimental results confirm that the recognition accuracy of emotion up to a level of 97% is maintained, even when the mean and standard deviation of noise are as high as 5% and 20% respectively over the individual features.","","978-1-4799-3983-1","10.1109/ICECI.2014.6767365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6767365","Linear Support Vector Machine;Facial expression;EEG;Linear Classification","Face recognition;Feature extraction;Accuracy;Standards;Principal component analysis;Noise","","4","","6","IEEE","17 Mar 2014","","","IEEE","IEEE Conferences"
"FPGA-Based Automated EEG Analysis for Efficient Emotion Detection","P. Vinayagam; P. Kishore Kumar; M. Kandhan","Department of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, India; Department of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, India; Department of Electronics and Communication Engineering, Saveetha Engineering College, Chennai, India",2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),"12 Jun 2024","2024","","","1","6","The recognition of emotions has significant importance in the domains of human-computer interaction and affective computing. The integration of facial expressions, voice analysis, and EEG data in multimodal techniques has shown potential in improving the accuracy of emotion identification. In this study, we provide a novel approach for emotion identification by incorporating facial expression analysis, voice processing, and EEG data analysis into a multimodal system. Machine learning methods are used by the system to classify emotions. The findings of our study indicate that the suggested multimodal technique yields a notable level of classification accuracy across many emotion categories, namely Happy (85%), Sad (75%), Angry (80%), and Neutral (70%). In conclusion, the incorporation of many modalities has been shown to improve the accuracy of emotion identification in comparison to unimodal methodologies. This study highlights the efficacy of multimodal emotion detection systems in capturing a wide range of emotional states, hence facilitating advancements in human-computer interaction and affective computing applications.","","979-8-3503-1860-9","10.1109/ICDCECE60827.2024.10548113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548113","field-programmable gate array;positron emission tomography;electroencephalography;analog-to-digital conversion;support vector machines","Emotion recognition;Affective computing;System performance;Soft sensors;Scalability;Electroencephalography;Real-time systems","","","","16","IEEE","12 Jun 2024","","","IEEE","IEEE Conferences"
"Towards Bi-Hemispheric Emotion Mapping Through EEG: A Dual-Stream Neural Network Approach","D. Freire-Obregón; D. Hernández-Sosa; O. J. Santana; J. Lorenzo-Navarro; M. Castrillón-Santana","SIANI, Universidad de Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain; SIANI, Universidad de Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain; SIANI, Universidad de Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain; SIANI, Universidad de Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain; SIANI, Universidad de Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain",2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG),"11 Jul 2024","2024","","","1","5","Emotion classification through EEG signals plays a significant role in psychology, neuroscience, and human-computer interaction. This paper addresses the challenge of mapping human emotions using EEG data in the Mapping Human Emotions through EEG Signals FG24 competition. Subjects mimic the facial expressions of an avatar, displaying fear, joy, anger, sadness, disgust, and surprise in a VR setting. EEG data is captured using a multi-channel sensor system to discern brain activity patterns. We propose a novel two-stream neural network employing a Bi-Hemispheric approach for emotion inference, surpassing baseline methods and enhancing emotion recognition accuracy. Additionally, we conduct a temporal analysis revealing that specific signal intervals at the beginning and end of the emotion stimulus sequence contribute significantly to improve accuracy. Leveraging insights gained from this temporal analysis, our approach offers enhanced performance in capturing subtle variations in the states of emotions. Code is available at https://github.com/davidfreire/FG24-EmoNeuroDB/","2770-8330","979-8-3503-9494-8","10.1109/FG59268.2024.10581965","Spanish Ministry of Science and Innovation(grant numbers:PID2021-122402OB-C22); ACIISI-Gobierno de Canarias and European FEDER(grant numbers:EIS 2021 04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581965","","Human computer interaction;Emotion recognition;Accuracy;Neuroscience;Psychology;Gesture recognition;Electroencephalography","","","","18","IEEE","11 Jul 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition of EEG Based on Dual-Input Multi-Network Fusion Features","W. Sun; Y. Su","School of Digital Art, Xi'an University of Posts and Telecommunications, Xian, China; School of Computer Science, Shaanxi Normal University, Xian, China",2024 7th International Conference on Information Communication and Signal Processing (ICICSP),"31 Dec 2024","2024","","","809","817","Aiming at the problems of insufficient feature extraction and low emotion recognition accuracy caused by the existing EEG emotion recognition network structure, a multi-branch network EEG emotion recognition method is proposed, and a dual-input and three-branch network (DTNET) model is constructed. The model takes EEG time series samples and 3D spatiotemporal matrix as inputs, and features of EEG signals are extracted by combining convolution neural network, bidirectional gated loop unit and echo state network. Experimental results show that the model has high performance.","2770-792X","979-8-3503-5589-5","10.1109/ICICSP62589.2024.10809300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10809300","Electroencephalography (EEG);Emotional EEG Signal Recognition;Multi-feature fusion;Multi-branch network","Emotion recognition;Solid modeling;Three-dimensional displays;Time series analysis;Neural networks;Logic gates;Feature extraction;Brain modeling;Electroencephalography;Spatiotemporal phenomena","","","","25","IEEE","31 Dec 2024","","","IEEE","IEEE Conferences"
"Negative emotion recognition from stimulated EEG signals","G. Singh; A. Jati; A. Khasnobish; S. Bhattacharyya; A. Konar; D. N. Tibarewala; R. Janarthanan","Dept. of Electronics &Telecommunication Engg.; Dept. of Electronics &Telecommunication Engg.; School of Bioscience & Engg.; School of Bioscience & Engg.; Dept. of Electronics &Telecommunication Engg.; School of Bioscience & Engg., India; Dept. of IT, Jaya Engg. College, Chennai","2012 Third International Conference on Computing, Communication and Networking Technologies (ICCCNT'12)","31 Dec 2012","2012","","","1","8","This paper proposes a scheme of emotion recognition from audio-visually stimulated EEG (electroencephalography) signals, the choice of signal being influenced by the fact that these signals are the direct unaltered outcome of one's brain activity and hence cannot be voluntarily suppressed. The EEG signals have been recorded using the NEUROWIN EEG amplifier of Nasan Medicals with a sampling rate of 250Hz from electrodes positioned at F3, F4, Fp1 and Fp2, since they lie over the frontal and pre-frontal lobe. The Raw EEG signals obtained need to be processed and classified into different emotional categories, using various features and intelligent classification algorithms. Features from these signals have been extracted using wavelet transform, statistical parameters and Hjorth parameter estimation, which are then classified using linear support vector machine (LSVM) and k-nearest neighbour (kNN). These extracted features are classified into the two different negative emotion classes of sad and disgust, with an average classification accuracy of the sad emotion being 78.04% and disgust being 76.31%. With our objective of development of emotionally challenged machines and devices that could become compatible with the emotional state of the user and nullify the effects of negative emotions on their work performance; the proposed scheme takes us a step closer to realisation of the same.","","","10.1109/ICCCNT.2012.6395891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395891","emotion recognition;electroencephalogram (EEG);wavelet transform (WT);statistical parameters;Hjorth parameter;linear support vector machine (LSVM);k-nearest neighbour (kNN)","Electroencephalography;Feature extraction;Complexity theory","","3","","31","IEEE","31 Dec 2012","","","IEEE","IEEE Conferences"
"Inter-Brain EEG Feature Extraction and Analysis for Continuous Implicit Emotion Tagging During Video Watching","Y. Ding; X. Hu; Z. Xia; Y. -J. Liu; D. Zhang","Department of Biomedical Engineering, Tsinghua University, Beijing, P. R. China; Department of Psychology, Tsinghua University, Beijing, P. R. China; Department of Psychology, Tsinghua University, Beijing, P. R. China; Department of Computer Science and Technology, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, P. R. China; Department of Psychology, Tsinghua University, Beijing, P. R. China",IEEE Transactions on Affective Computing,"1 Mar 2021","2021","12","1","92","102","How to efficiently tag the emotional experience of multimedia contents is an important and challenging problem in the field of affective computing. This paper presents an EEG-based real-time emotion tagging approach, by extracting inter-brain features from a group of participants when they watch the same emotional video clips. First, the continuous subjective reports on both the arousal and valence dimensions of emotion were obtained by employing a three-round behavioral rating paradigm. Second, the inter-brain features were systematically explored in both spectral and temporal domain. Finally, regression analyses were performed to evaluate the effectiveness of inter-brain amplitude and phase features. The inter-brain amplitude feature showed significantly better prediction performance than the inter-brain phase feature, as well as another two conventional features (spectral power and inter-subject correlation). By combining the four types of features, regression values (R2) were obtained for the prediction of arousal (0.61 + 0.01) and valence (0.70 + 0.01), corresponding to prediction errors of 1.01 + 0.02 and 0.78 + 0.02 (unit on 9-point scales), respectively. The contributions of different electrodes and frequency bands were also analyzed. Our results show promising potentials of inter-brain EEG features in real-time emotion tagging applications.","1949-3045","","10.1109/TAFFC.2018.2849758","National Key Research and Development Program of China(grant numbers:2016YFB1001200); National Natural Science Foundation of China(grant numbers:U1736220,61725204); National Social Science Fund of China(grant numbers:17ZDA323); Ministry of Education, China(grant numbers:17YJA190017); Tsinghua University Initiative Scientific Research Program(grant numbers:2014z21043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392701","Emotion;inter-brain;EEG;implicit tagging","Tagging;Electroencephalography;Streaming media;Multimedia communication;Feature extraction;Real-time systems;Correlation","","40","","76","IEEE","22 Jun 2018","","","IEEE","IEEE Journals"
"EEG-based emotion recognition with manifold regularized extreme learning machine","Y. Peng; J. -Y. Zhu; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University and Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University and Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University and Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University and Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai, China",2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"6 Nov 2014","2014","","","974","977","EEG signals, which can record the electrical activity along the scalp, provide researchers a reliable channel for investigating human emotional states. In this paper, a new algorithm, manifold regularized extreme learning machine (MRELM), is proposed for recognizing human emotional states (positive, neutral and negative) from EEG data, which were previously evoked by watching different types of movie clips. The MRELM can simultaneously consider the geometrical structure and discriminative information in EEG data. Using differential entropy features across whole five frequency bands, the average accuracy of MRELM is 81.01%, which is better than those obtained by GELM (80.25%) and SVM (76.62%). The accuracies obtained from high frequency band features (β, γ) are obviously superior to those of low frequency band features, which shows β and γ bands are more relevant to emotional states transition. Moreover, experiments are conducted to further evaluate the efficacy of MRELM, where the training and test sets are from different sessions. The results demonstrate that the proposed MRELM is a competitive model for EEG-based emotion recognition.","1558-4615","978-1-4244-7929-0","10.1109/EMBC.2014.6943755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6943755","","Electroencephalography;Emotion recognition;Support vector machines;Training;Accuracy;Motion pictures;Brain modeling","Adult;Algorithms;Electroencephalography;Emotions;Female;Humans;Machine Learning;Male;Pattern Recognition, Physiological;Young Adult","9","","11","IEEE","6 Nov 2014","","","IEEE","IEEE Conferences"
"Cross-Subject Emotion Recognition From Multichannel EEG Signals Using Multivariate Decomposition and Ensemble Learning","R. Vempati; L. D. Sharma; R. K. Tripathy","School of Electronics Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India; School of Electronics Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India; Birla Institute of Technology and Science (BITS) Pilani, Hyderabad, India",IEEE Transactions on Cognitive and Developmental Systems,"6 Feb 2025","2025","17","1","77","88","Emotions are mental states that determine the behavior of a person in society. Automated identification of a person's emotion is vital in different applications such as brain–computer interfaces (BCIs), recommender systems (RSs), and cognitive neuroscience. This article proposes an automated approach based on multivariate fast iterative filtering (MvFIF) and an ensemble machine learning model to recognize cross-subject emotions from electroencephalogram (EEG) signals. The multichannel EEG signals are initially decomposed into multichannel intrinsic mode functions (MIMFs) using the MvFIF. The features, such as differential entropy (DE), dispersion entropy (DispEn), permutation entropy (PE), spectral entropy (SE), and distribution entropy (DistEn), are extracted from MIMFs. The binary atom search optimization (BASO) technique is employed to reduce the dimension of the feature space. The light gradient boosting machine (LGBM), extreme learning machine (ELM), and ensemble bagged tree (EBT) classifiers are used to recognize different human emotions using the features of EEG signals. The results demonstrate that the LGBM classifier has achieved the highest average accuracy of 99.50% and 98.79%, respectively, using multichannel EEG signals from the GAMEEMO and DREAMER databases for cross-subject emotion recognition (ER). Compared to other multivariate signal decomposition algorithms, the MvFIF-based method has demonstrated higher accuracy in recognizing emotions using multichannel EEG signals. The proposed (MvFIF+DE+BASO+LGBM) technique outperforms the existing state-of-the-art methods in ER using EEG signals.","2379-8939","","10.1109/TCDS.2024.3417534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10589482","Binary atom search optimization (BASO);electroencephalogram (EEG) signal;emotion recognition (ER);leave-one-subject-out cross validation (LOSOCV);light gradient boosting machine (LGBM);multivariate fast iterative filtering (MvFIF)","Electroencephalography;Emotion recognition;Feature extraction;Databases;Filters;Entropy;Iterative methods","","2","","52","IEEE","8 Jul 2024","","","IEEE","IEEE Journals"
"Attention-based convolution-recurrent neural network for cross-subject EEG emotion recognition","Z. Han; J. Su; T. Qin","School of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; Shanghai Second Light Industry School, Shanghai, China","2023 16th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","2 Jan 2024","2023","","","1","6","In contemporary applications of emotional brain-computer interfaces, the experimental data frequently originate from distinct individuals, leading to considerable inter-individual variance. This variance poses challenges in generalizing models to novel, unseen individuals. This study introduces a novel approach to cross-subject EEG emotion recognition, leveraging a spatial-temporal neural network integrated with an attention mechanism. This approach sequentially extracts spatial and temporal domain features, effectively mitigating inter-subject variability by discerning emotionally salient features through the attention module. This refinement significantly enhances classification performance. The network operates end-to-end, initially eliminating the baseline signal from the input raw EEG data and subsequently segmenting it into windows for preprocessing. Specifically, the spatial attributes of diverse subjects’ EEG data are discerned through a spatial attention-based CNN network, while temporal dynamics are captured via a self-attention-based LSTM network. Finally, the two sets of features are amalgamated for cross-subject EEG emotion recognition. Empirical findings demonstrate that this methodology can derive more distinctive features from the original EEG signals, achieving an average classification accuracy of 89.29% on the DEAP dataset. This constitutes a noteworthy enhancement in classification efficacy compared to alternative methods. These results offer methodological insights pertinent to emotional brain-computer interface systems in authentic scenarios.","","979-8-3503-3075-5","10.1109/CISP-BMEI60920.2023.10373236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10373236","Electroencephaligraph(EEG);Emotion Recognition;Convolution-recurrent Neural Network;Cross-subject;Attention Mechanism","Emotion recognition;Image recognition;Convolution;Feature extraction;Brain modeling;Electroencephalography;Brain-computer interfaces","","","","25","IEEE","2 Jan 2024","","","IEEE","IEEE Conferences"
"An RSB-GNN-Based EEG Approach for Exploring Students' Affective States in E-Learning","T. Li; C. Yin; Y. Chai; H. Chen; W. Rong; Y. Ouyang","Sino-French Engineer School, Beihang University, Beijing, China; Sino-French Engineer School, Beihang University, Beijing, China; School of Information, Central University of Finance and Economics, Beijing, China; Department of Planning and Finance, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China",2024 IEEE Frontiers in Education Conference (FIE),"26 Feb 2025","2024","","","1","9","This research paper describes an original method of affective state recognition in the e-learning field. Emotion plays a key role in both knowledge-building and mental health. In recent years, more and more emotion researchers have broken through traditional questionnaires to utilize physiological data to monitor students' cognitive and affective states. Among these methods, electroencephalography (EEG) can directly reflect the physiological activities of the brain and has unique potentials and advantages over others. For analyzing this multi-channel noised signal, current emotion recognition methods mainly use deep learning methods to learn the spatial or temporal representation of each channel, and then process the classification through a multimodal fusion strategy, while emotional expression highly relies on brain functional connectivity. In this research work, for the EEG-based learning-centered affective state recognition, we adopted a novel residual shrinkage block (RSB) to construct the graph neural network (GNN). During the feature extraction, the RSB is designed to obtain the features of interest and reduce the influence of artifact noises for recognition. GNN considers the biological topology among different brain regions to capture relations among different EEG channels. Extensive experiments on the CAL dataset prove that the performance of the proposed model is superior to current deep learning methods. Prior research may use the findings of this study to empower adaptive self-regulated learning environments through the automated recommendation of learning strategies, learning contents, and emotion regulation strategies according to students' learning-centered affective states, to further improve their learning performance as well as mental health. On the other hand, teachers or online course designers can use emotional feedback to adjust the learning materials and the pace of the instruction according to students' needs and preferences.","2377-634X","979-8-3503-5150-7","10.1109/FIE61694.2024.10893168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893168","emotional learning;adaptive computer learning;confusion;electroencephalogram (EEG);graph neural network (GNN)","Deep learning;Emotion recognition;Electronic learning;Mental health;Feature extraction;Graph neural networks;Electroencephalography;Regulation;Topology;Biomedical monitoring","","","","49","IEEE","26 Feb 2025","","","IEEE","IEEE Conferences"
"Intelligent Classification for Emotional Issues by Deep Learning Network on EEG Signal Processing","S. Yin; F. Zhu; X. Wei; G. Han; R. Zhang; X. Liu; B. Hu; Q. Wang","Key Laboratory of Spectral Imaging Technology, Xi'an Institute of Optics and Precision Mechanics (XIOPM), Chinese Academy of Sciences, Xi'an, China; Key Laboratory of Spectral Imaging Technology, Xi'an Institute of Optics and Precision Mechanics (XIOPM), Chinese Academy of Sciences, Xi'an, China; Key Laboratory of Spectral Imaging Technology, Xi'an Institute of Optics and Precision Mechanics (XIOPM), Chinese Academy of Sciences, Xi'an, China; Xi'an Gaoxin No.1 High School, Xi'an, China; Xi'an Gaoxin No.1 High School, Xi'an, China; Key Laboratory of Spectral Imaging Technology, Xi'an Institute of Optics and Precision Mechanics (XIOPM), Chinese Academy of Sciences, Xi'an, China; Key Laboratory of Spectral Imaging Technology, Xi'an Institute of Optics and Precision Mechanics (XIOPM), Chinese Academy of Sciences, Xi'an, China; Key Laboratory of Spectral Imaging Technology, Xi'an Institute of Optics and Precision Mechanics (XIOPM), Chinese Academy of Sciences, Xi'an, China","2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)","6 Apr 2022","2022","","","742","747","Identifying the risk of emotional issues is of great significance to the development of adolescents. Electroencephalography (EEG) signals can reflect brain activities, and imply the emotional status, therefore can be used to identify emotional issues. In this study, we designed an experimental paradigm for high school adolescents by displaying emotion-inducing pictures as stimuli and recorded their EEG signals simultaneously. The EEG signals was preprocessed and analyzed. In this paper we applied a convolutional network EEGNet to classify four emotional issues in adolescents: depression symptom, manic symptom, anxiety symptom and the control group. The results showed that the classification accuracy of the four groups can reach 94.24%. In addition, this paper explored using different types of picture stimuli on the classification and reached a result above chance level. This work extended the previous work on the classification of emotional issues to four categories, and achieved a good classification accuracy.","","978-1-6654-1606-1","10.1109/EEBDA53927.2022.9744774","Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744774","EEG Classification;Emotional issues;Adolescents;EEGNet","Training;Electrical engineering;Deep learning;Convolution;Conferences;Anxiety disorders;Signal processing algorithms","","1","","17","IEEE","6 Apr 2022","","","IEEE","IEEE Conferences"
"Deep Learning-Based Wearable Ear-EEG Emotion Recognition System With Superlets-Based Signal-to-Image Conversion Framework","N. -D. Mai; H. -T. Nguyen; W. -Y. Chung","Department of Artificial Intelligence Convergence, Pukyong National University, Busan, South Korea; Department of Artificial Intelligence Convergence, Pukyong National University, Busan, South Korea; Department of Artificial Intelligence Convergence and the Department of Electronic Engineering, Pukyong National University, Busan, South Korea",IEEE Sensors Journal,"1 Apr 2024","2024","24","7","11946","11958","This study presents a novel approach for affective computing through deep learning-assisted electroencephalography (EEG) analysis aimed at enhancing the detection, processing, interpretation, and emulation of human emotions. The proposed methodology introduces an Ear-EEG emotion recognition (EEER) system, leveraging a deep neural network and integrating the Internet of Things (IoT) capabilities. The key contributions of this study include 1) a comprehensive design framework encompassing both hardware and software aspects of the Ear-EEG system, as well as 2) a signals-to-three-channel image conversion method employing time-frequency super-resolution with superlets as inputs for deep learning models. Moreover, 3) a modified vision transformer (ViT) architecture is introduced, incorporating shifted patch tokenization (SPT) and locality self-attention (LSA) techniques, addressing challenges associated with limited and imbalanced small datasets, as well as the lack of locality inductive bias for emotion recognition. Additionally, 4) an IoT-assisted EEER platform is proposed, enabling remote monitoring and management. Experimental results indicate that the trained ViT model with SPT and LSA surpasses recent models in terms of performance on untrained datasets, achieving an average accuracy of 92.39%. The findings highlight the efficacy of the proposed EEER system in accurately detecting emotional states, encompassing positive and negative affective states. The integration of artificial intelligence and IoT-based healthcare platforms represents a significant advancement in the development of medical assistant tools with broad implications for future applications.","1558-1748","","10.1109/JSEN.2024.3369062","National Research Foundation of Korea (NRF); Korea Government through Ministry of Science and Information and Communication Technology (MSIT)(grant numbers:NRF-2019R1A2C1089139); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456557","Brain–computer interface;deep learning;electroencephalography (EEG);emotion recognition;Internet of Things (IoT);small-size dataset","Electroencephalography;Emotion recognition;Brain modeling;Scalp;Time-frequency analysis;Sensors;Deep learning;Brain-computer interfaces;Internet of Things","","6","","54","IEEE","29 Feb 2024","","","IEEE","IEEE Journals"
"Mental state identification based on the classification of EEG signals","Y. Liu; Z. Huang","College of Computer and Data Science/College of Software, Fuzhou University, Fuzhou, China; College of Computer and Data Science/College of Software, Fuzhou University, Fuzhou, China","2022 15th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","21 Dec 2022","2022","","","1","6","EEG state classification is used in many fields, and decision-making, as a higher cognitive function of the brain, has high research significance, and this paper is mainly to study the state of decision-making. Therefore, we study the classification of three decision states, before-decision, in-decision, post-decision, and two resting states, eye-opened, eye-closed. In this study, three methods are used to compare the classification effects, namely DE+SVM, DE+DGCNN, EEGNet, among which differential entropy (DE) is a frequency domain feature, which can extract effective features in EEG emotion recognition; DGCNN is a dynamic graph convolutional neural network which uses DE as the node feature and dynamically learns the adjacency matrix for classification; EEGNet is an end-to-end neural network, which is designed to be used in multiple experimental paradigms. The above 3 methods achieved 62.80%±9.67%, 78.70%±8.27%, and 88.83±6.03% accuracy in within-subject classification respectively. Finally, we visualize the adjacency matrix learned by DGCNN and the spatial filter learned by EEGNet to see the knowledge learned by the model.","","978-1-6654-8887-7","10.1109/CISP-BMEI56279.2022.9980282","Natural Science Foundation of Fujian Province(grant numbers:2019J01242); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9980282","EEG;decision-making;mental state identification;Ultimatum game;DGCNN;EEGNet","Visualization;Frequency-domain analysis;Decision making;Signal processing;Feature extraction;Spatial filters;Electroencephalography","","1","","21","IEEE","21 Dec 2022","","","IEEE","IEEE Conferences"
"Design of Intelligent EEG System for Human Emotion Recognition with Convolutional Neural Network","K. -Y. Wang; Y. -L. Ho; Y. -D. Huang; W. -C. Fang","Pervasive Artificial Intelligence Research (PAIR) Labs, Taiwan; Pervasive Artificial Intelligence Research (PAIR) Labs, Taiwan; Pervasive Artificial Intelligence Research (PAIR) Labs, Taiwan; Pervasive Artificial Intelligence Research (PAIR) Labs, Taiwan",2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS),"25 Jul 2019","2019","","","142","145","Emotions play a significant role in the field of affective computing and Human-Computer Interfaces(HCI). In this paper, we propose an intelligent human emotion detection system based on EEG features with a multi-channel fused processing. We also proposed an advanced convolutional neural network that was implemented in VLSI hardware design. This hardware design can accelerate both the training and classification processes and meet real-time system requirements for fast emotion detection. The performance of this design was validated using DEAP [1] database with datasets from 32 subjects, the mean classification accuracy achieved is 83.88%.","","978-1-5386-7884-8","10.1109/AICAS.2019.8771581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8771581","Emotion Recognition;Convolutional Neural Network;Deep Learning;Hardware Machine Learning;Real-time EEG System;Affective Computing","Electroencephalography;Emotion recognition;Brain modeling;Hardware;Training;Mathematical model;Very large scale integration","","25","","11","IEEE","25 Jul 2019","","","IEEE","IEEE Conferences"
"MEERNet: Multi-source EEG-based Emotion Recognition Network for Generalization Across Subjects and Sessions","H. Chen; Z. Li; M. Jin; J. Li","HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, China",2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"9 Dec 2021","2021","","","6094","6097","As an important element in the human-machine interaction, the electroencephalogram (EEG)-based emotion recognition has achieved significant progress. However, one obstacle to practicality lies in the variability between subjects and sessions. Although several studies have adopted domain adaptation (DA) approaches to tackle this problem, most of them treat multiple data from different subjects and different sessions together as a single source for transfer. Since different EEG data have different marginal distributions, these approaches fail to satisfy the assumption of DA that the source has a certain marginal distribution. We therefore propose the multi-source EEG-based emotion recognition network (MEERNet), which takes both domain-invariant and domain-specific features into consideration. Firstly we assume that different EEG data share the same low-level features, and then we construct multiple branches corresponding to multiple sources to extract domain-specific features, and then DA is conducted between the target and each source. Finally, the inference is made by multiple branches. We evaluate our method on SEED and SEED-IV for recognizing three and four emotions, respectively. Experimental results show that the MEERNet outperforms the single-source methods in cross-session and cross-subject transfer scenarios with an accuracy of 86.7% and 67.1% on average, respectively.","2694-0604","978-1-7281-1179-7","10.1109/EMBC46164.2021.9630277","Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9630277","","Emotion recognition;Transfer learning;Feature extraction;Brain modeling;Electroencephalography;Biology;Data mining","Electroencephalography;Emotions;Generalization, Psychological;Humans","13","","20","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"GSCNN: Gender-Sensitive EEG Emotion Recognition Using Convolutional Neural Network","D. Duan; Q. Li; W. Zhong; L. Ye; Q. Zhang","Key Laboratory of Media Audio & Video, (Communication University of China), Ministry of Education, Beijing, China; Key Laboratory of Media Audio & Video, (Communication University of China), Ministry of Education, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China",2023 29th International Conference on Mechatronics and Machine Vision in Practice (M2VIP),"2 Feb 2024","2023","","","1","6","Emotion recognition plays an important role in the field of human-computer interaction. The previous studies have demonstrated the existence of gender differences in EEG signals, which leads to undesirable EEG emotion recognition in cross-subject tasks. However, there is no EEG emotion recognition model that involves gender factors. In this paper, a gender-sensitive convolutional neural network is proposed to enhance the performance of EEG emotion recognition. Specifically, we design a gender information extractor to help the network learn the individual difference during classification. Meanwhile, we also design an emotion information extractor to assign different weights for key frequency bands according to their classification performance. The performance of the proposed method is evaluated on two publicly available EEG emotion recognition datasets, SEED and SEED-IV. The experimental results show that our model can achieve higher recognition accuracies of 85.6% and 63.3% on the two datasets, respectively.","","979-8-3503-2562-1","10.1109/M2VIP58386.2023.10413377","National Natural Science Foundation of China(grant numbers:62271455); Fundamental Research Funds for the Central Universities(grant numbers:CUC18LG024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10413377","EEG emotion recognition;gender difference;deep learning;cross-subject","Performance evaluation;Emotion recognition;Brain modeling;Electroencephalography;Data mining;Convolutional neural networks;Task analysis","","1","","23","IEEE","2 Feb 2024","","","IEEE","IEEE Conferences"
"EEG emotion detection review","L. R. Christensen; M. A. Abdullah",IT University of Copenhagen; Sudan University of Science and Technology,2018 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB),"9 Jul 2018","2018","","","1","7","EEG (Electroencephalography) allows to elicit the mental state of the user, which in turn reveals the user emotion, which is an important factor in HMI (Human Machine Interaction). Researchers across the globe are developing new techniques to increase the EEG accuracy by using different signal processing, statistics, and machine learning techniques in this work we will discuss the most common techniques that can yield to better results, along with discussing the common experiment steps to classify the emotion, starting from collecting the signal, and extracting the features and select the best feature to classify the emotions. Along with highlighting some standing problems in field and potential growth areas.","","978-1-5386-1399-3","10.1109/CIBCB.2018.8404976","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404976","EEG;Emotion Recognition;Emotion Detection;HMI;BCI","Feature extraction;Support vector machines;Electroencephalography;Discrete wavelet transforms;Wavelet analysis;Videos","","10","","31","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"EEG-MLP: An all-MLP Architecture for EEG Emotion Recognition","D. Liu; L. Yang; P. Ni; Q. Wang; H. Sun; Q. Zhang; C. Tang","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China",2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Jan 2024","2023","","","2655","2662","Emotion recognition based on EEG has attracted widespread research interest in the field of brain-computer interfaces. To extract EEG intra- and inter-channel features and find discriminative representations for EEG emotion recognition, we propose EEG-Multilayer Perceptron (EEG-MLP) architecture. EEG-MLP is completely composed of MLPs and mainly consists of two modules, one is a temporal mixer that captures intra-channel (temporal) information, and the other is a channel mixer that captures inter-channel information. The two modules learn knowledge in a parallel manner, and then their outputs are fused to extract global information and classify EEG emotions. We conduct extensive experiments on DEAP dataset. EEG-MLP is first compared with five inter-channel interaction models (related to CNN or GCN) to verify its effectiveness. Then, five other models with similar architecture to EEG-MLP were also contrasted. Experimental results show that EEG-MLP achieves the best performance among the above methods, with accuracies of 94.87% and 95.32% in the valence and arousal dimensions, respectively. In addition, it has a strong discrimination ability for complex categories, and has low requirements for storage resources.","2156-1133","979-8-3503-3748-8","10.1109/BIBM58861.2023.10385434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385434","EEG;Affective Computing;Multilayer Perceptron;EEG-MLP;MLP-Mixer","Training;Emotion recognition;Fuses;Computer architecture;Feature extraction;Brain modeling;Electroencephalography","","2","","19","IEEE","18 Jan 2024","","","IEEE","IEEE Conferences"
"Transformer-Based Emotion Recognition with EEG","K. Patel; F. Safavi; R. Chandramouli; R. Vinjamuri","Vinjamuri Lab, University of Maryland, Baltimore, MD, USA; Vinjamuri Lab, University of Maryland, Baltimore, MD, USA; Vinjamuri Lab, University of Maryland, Baltimore, MD, USA; Vinjamuri Lab, University of Maryland, Baltimore, MD, USA",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","Emotion recognition via electroencephalography (EEG) has emerged as a pivotal domain in biomedical signal processing, offering valuable insights into affective states. This paper presents a novel approach utilizing a tailored Transformer-based model to predict valence and arousal levels from EEG signals. Diverging from traditional Transformers handling singular sequential data, our model adeptly accommodates multiple EEG channels concurrently, enhancing its ability to discern intricate temporal patterns across the brain. The modified Transformer architecture enables comprehensive exploration of spatiotemporal dynamics linked with emotional states. Demonstrating robust performance, the model achieves mean accuracies of 92.66% for valence and 91.17% for arousal prediction, validated through 10-fold cross-validation across subjects on the DEAP dataset. Trained for subject-specific analysis, our methodology offers promising avenues for enhancing understanding and applications in emotion recognition through EEG. This research contributes to a broader discourse in biomedical signal processing, paving the way for refined methodologies in decoding neural correlates of emotions with implications across various domains including brain-computer interfaces, and human-robot interaction.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10781700","University of Maryland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781700","EEG;Transformers;emotion recognition;brain-computer interfaces;human-robot interaction","Emotion recognition;Biological system modeling;Human-robot interaction;Signal processing;Predictive models;Transformers;Brain modeling;Electroencephalography;Spatiotemporal phenomena;Biomedical communication","Electroencephalography;Humans;Emotions;Signal Processing, Computer-Assisted;Algorithms;Brain-Computer Interfaces;Male;Adult;Female;Arousal","1","","12","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Multi-Modal Cross-Subject Emotion Feature Alignment and Recognition with EEG and Eye Movements","Q. Zhu; T. Zhu; L. Fei; C. Zheng; W. Shao; D. Zhang; D. Zhang","College of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, China; College of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Data Science, Chinese University of Hong Kong, (Shenzhen), Shenzhen, China; College of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics, Nanjing, China",IEEE Transactions on Affective Computing,"","2025","PP","99","1","15","Multi-modal emotion recognition has attracted much attention in human-computer interaction, because it provides complementary information for the recognition model. However, the distribution drift among subjects and the heterogeneity of different modalities pose challenges to multi-modal emotion recognition, thereby limiting its practical application. Most of the current multi-modal emotion recognition methods are difficult to suppress above uncertainties in fusion. In this paper, we propose a cross-subject multi-modal emotion recognition framework, which jointly learns subject-independent representation and common feature between EEG and eye movements. Firstly, we design the dynamic adversarial domain adaptation for cross-subject distribution alignment, dynamically selecting source domains in training. Secondly, we simultaneously capture intra-modal and inter-modal emotion-related features by both self-attention and cross-attention mechanisms, thus obtaining the robust and complementary representation of emotional information. Then, two contrastive loss functions are imposed on above network to further reduce inter-modal heterogeneity, and mine higher-order semantic similarity between synchronously collected multi-modal data. Finally, we used the output of the softmax layer as the predicted value. The experimental results on several multi-modal emotion datasets with EEG and eye movements demonstrate that our method is significantly superior to the state-of-the-art emotion recognition approaches. Our code is available at: https://github.com/xbrainnet/CSMM.","1949-3045","","10.1109/TAFFC.2025.3554399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938180","EEG;cross-subject;multi-modal fusion;emotion recognition;contrastive learning;adversarial domain adaptation","Emotion recognition;Electroencephalography;Feature extraction;Data mining;Training;Contrastive learning;Brain modeling;Physiology;Long short term memory;Correlation","","","","","IEEE","24 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Spectral-Spatial Attention Alignment for Multi-Source Domain Adaptation in EEG-Based Emotion Recognition","Y. Yang; Z. Wang; W. Tao; X. Liu; Z. Jia; B. Wang; F. Wan","Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau, China; Macao Centre for Mathematical Sciences, and the Respiratory Disease AI Laboratory on Epidemic Intelligence and Medical Big Data Instrument Applications, Faculty of Innovation Engineering, Macau University of Science and Technology, Macau, China; Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau, China; Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau, China; Brainnetome Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, and the Brain Mind Institute, Western University, London, ON, Canada; Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau, China",IEEE Transactions on Affective Computing,"26 Nov 2024","2024","15","4","2012","2024","In electroencephalographic-based (EEG-based) emotion recognition, high non-stationarity and individual differences in EEG signals could lead to significant discrepancies between sessions/subjects, making generalization to a new session/subject very difficult. Most existing domain adaptation (DA) and multi-source domain adaptation (MSDA) techniques aim to mitigate this discrepancy by aligning feature distributions. However, when confronted with many diverse domain distributions, learning domain-invariant features via aligning pairwise feature distributions between domains can be hard or even counterproductive. To address this issue, this article proposes an attention alignment approach to learning abundant domain-invariant features. The motivation is simple: despite individual differences causing significant differences in feature distributions in EEG-based emotion recognition, shared affective cognitive attributes (attention) of spectral and spatial domains can be observed within the same emotion categories. The proposed spectral-spatial attention alignment multi-source domain adaptation (S2A2-MSDA) constructs domain attention to represent affective cognition attributes in spatial and spectral domains and utilizes domain consistent loss to align them between domains. Furthermore, to facilitate discriminative feature learning on the target classes, S2A2-MSDA learns the conditional semantic information of the target domain using a pseudo-labeling method. This algorithm has been validated on the SEED and SEED-IV datasets in cross-session and cross-subject scenarios, respectively. Experimental results demonstrate that S2A2-MSDA outperforms existing representative DA and MSDA methods, achieving state-of-the-art performance.","1949-3045","","10.1109/TAFFC.2024.3394436","Science and Technology Development Fund, Macau SAR(grant numbers:0045/2019/AFJ,0022/2021/APD,0024/2023/ITP1,0024/2023/RIA1); University of Macau Research Committee(grant numbers:2017-00207-FST,2022-00197-FST); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515010844); Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10509712","Brain-computer interface;domain adaptation;EEG-based emotion recognition","Feature extraction;Emotion recognition;Electroencephalography;Cognition;Frequency-domain analysis;Brain modeling;Semantics","","3","","46","IEEE","29 Apr 2024","","","IEEE","IEEE Journals"
"Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches","D. Ayata; Y. Yaslan; M. Kamaşak","Computer Engineering Faculty, Istanbul Technical University, Istanbul, Turkey; Computer Engineering Faculty, Istanbul Technical University, Istanbul, Turkey; Computer Engineering Faculty, Istanbul Technical University, Istanbul, Turkey",2016 Medical Technologies National Congress (TIPTEKNO),"28 Feb 2017","2016","","","1","4","Emotions play a significant and powerful role in everyday life of human beings. Developing algorithms for computers to recognize emotional expression is a widely studied area. In this study, emotion recognition from Galvanic signals was performed using time domain and wavelet based features. Feature extraction has been done with various feature set attributes. Various length windows have been used for feature extraction. Various feature attribute sets have been implemented. Valence and arousal have been categorized and relationship between physiological signals and arousal and valence has been studied using Random Forest machine learning algorithm. We have achieved 71.53% and 71.04% accuracy rate for arousal and valence respectively by using only galvanic skin response signal. We have also showed that using convolution has positive affect on accuracy rate compared to non-overlapping window based feature extraction.","","978-1-5090-2386-8","10.1109/TIPTEKNO.2016.7863130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863130","Biomedical Signal Processing;Emotion Recognition;Pattern Recognition;Machine Learning;Physiological Signal;Galvanic Skin Response;Random Forest","Skin;Feature extraction;Physiology;Emotion recognition;Convolution;Videos;Time-domain analysis","","35","","8","IEEE","28 Feb 2017","","","IEEE","IEEE Conferences"
"Classification of Human Emotions Using Multiclass Support Vector Machine","A. Patil; K. Behele","Dept. of E&TC, CCEW, Pune, India; Dept. of E&TC, CCEW, Pune, India","2017 International Conference on Computing, Communication, Control and Automation (ICCUBEA)","13 Sep 2018","2017","","","1","4","Emotion recognition plays a very important role in establishing brain computer interface. Emotion recognition can be done by analyzing speech signal or facial expressions. But these methods cannot be considered as reliable indicators of emotion, because it is possible to generate fake data in these methods. In this paper, Electroencephalography (EEG) is used for detection and classification of different emotions. EEG proves to be more reliable method as it is not possible for the subject to alter the data. The proposed method consists of four steps, viz., data acquisition, pre-processing, feature extraction and classification. Emotions are invoked by using audio visual stimuli. EEG signal is captured for four emotions viz. happy, sad, angry and neutral using power lab instrument by ADInsruments. The recorded EEG signal is then filtered using band pass filter with cutoff frequencies of 3Hz and 30Hz. Discrete Wavelet Transform is applied to the filtered data and then statistical features are extracted. Multiclass Support Vector Machine is incorporated to classify EEG signals into different emotion classes.","","978-1-5386-4008-1","10.1109/ICCUBEA.2017.8463656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463656","Electroencephalography;Discrete Wavelet Transform;Multiclass Support Vector Machine","Electroencephalography;Feature extraction;Support vector machines;Emotion recognition;Discrete wavelet transforms;Data acquisition;Band-pass filters","","3","","13","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Emotion Recognition From Multi-Channel EEG Signals by Exploiting the Deep Belief-Conditional Random Field Framework","H. Chao; Y. Liu","School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo, China; School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo, China",IEEE Access,"24 Feb 2020","2020","8","","33002","33012","Recently, much attention has been attracted to automatic emotion recognition based on multi-channel electroencephalogram (EEG) signals, with the rapid development of machine learning methods. However, traditional methods ignore the correlation information between different channels, and cannot fully capture the long-term dependencies and contextual information of EEG signals. To address the problems, this paper proposes a deep belief-conditional random field (DBN-CRF) framework which integrates the improved deep belief networks with glia chains (DBN-GC) and conditional random field. In the framework, the raw feature vector sequence is firstly extracted from the multi-channel EEG signals by a sliding window. Then, parallel DBN-GC models are utilized to obtain the high-level feature sequence of the multi-channel EEG signals. And the conditional random field (CRF) model generates the predicted emotion label sequence according to the high-level feature sequence. Finally, the decision merge layer based on K-nearest neighbor algorithm is employed to estimate the emotion state. According to our best knowledge, this is the first attempt that applies the conditional random field methodology to deep belief networks for emotion recognition. Experiments are conducted on three publicly available emotional datasets which include AMIGOS, SEED and DEAP. The results demonstrate that the proposed framework can mine inter correlation information of multiple-channel by the glia chains and catch inter channel correlation information and contextual information of EEG signals for emotion recognition. In addition, the classification accuracy of the proposed method is compared with several classical techniques. The results indicate that the proposed method outperforms most of the other deep classifiers. Thus, potential of the proposed framework is demonstrated.","2169-3536","","10.1109/ACCESS.2020.2974009","National Natural Science Foundation of China(grant numbers:61502150); Fundamental Research Funds for the Universities of Henan Province(grant numbers:NSFRF1616); Foundation for Scientific and Technological Project of Henan Province(grant numbers:172102210279); Key Scientific Research Projects of Universities in Henan(grant numbers:19A520004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999626","Human-computer interaction;emotion recognition;multi-channel EEG signal;DBN-GC;conditional random field","Electroencephalography;Brain modeling;Emotion recognition;Feature extraction;Correlation;Hidden Markov models;Machine learning","","48","","38","CCBY","14 Feb 2020","","","IEEE","IEEE Journals"
"A novel effective feature selection algorithm based on S-PCA and wavelet transform features in EEG signal classification","S. Nasehi; H. Pourghassem","Department of Electrical Engineering, Islamic Azad University, Isfahan, Iran; Department of Electrical Engineering, Islamic Azad University, Isfahan, Iran",2011 IEEE 3rd International Conference on Communication Software and Networks,"8 Sep 2011","2011","","","114","117","There are various methods to extract feature from EEG signals but the effective feature selection is an issue. In this paper, a novel effective feature selection based on Statistical-Principal Component Analysis (S-PCA) and wavelet transform (WT) features in medical and BCI application is proposed. In this method, we decompose the signals to six sub-bands by four mother wavelet (sym6, db5, bior1.5 and robio2.8). Then five features (such as the number of zero coefficients, the smallest and largest coefficients, the mean and standard deviation of coefficients) extract from each sub-band as feature vector. In this algorithm, S-PCA is used to select ten effective features from among WT features. Finally, we use KNN classifier and seven different signals of brain activities to evaluate the proposed method. The results indicate the improvement of the classification performance in comparison with current methods.","","978-1-61284-486-2","10.1109/ICCSN.2011.6014686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014686","Statistical-Principal Component Analysis;EEG signal;wavelet transform;feature extraction;KNN classifier","Electroencephalography;Feature extraction;Transforms;Principal component analysis;Classification algorithms","","9","","18","IEEE","8 Sep 2011","","","IEEE","IEEE Conferences"
"Functional Emotion Transformer for EEG-Assisted Cross-Modal Emotion Recognition","W. -B. Jiang; Z. Li; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","1841","1845","Multimodal emotion recognition based on electroencephalography (EEG) and eye movements has attracted increasing attention due to their high performance and complementary properties. However, there are two challenges that hinder its practical applications: the inconvenient EEG data collection and high-cost data annotation. In contrast, eye movements are convenient to obtain and process in real scenarios. To combine high performance of EEG and easy setups of eye tracking, we propose a novel EEG-assisted Contrastive Learning Framework with a Functional Emotion Transformer (ECO-FET) for cross-modal emotion recognition. ECO-FET leverages both the functional brain connectivity and the spectral-spatial-temporal domain of EEG signals simultaneously, which dramatically benefit the learning of eye movements. The whole process consists of three phases: pre-training, test, and fine-tuning. ECO-FET exploits the complementary information provided by multiple modalities during pre-training in order to improve the performance of unimodal models. In the pre-training phase, unlabeled EEG and eye movement data are fed into the model to contrastively learn the emotional latent representations between the two modalities, while in the test phase, eye movements and few labeled EEG samples are used to predict different emotions. Experimental results on three public datasets demonstrate that ECO-FET surpasses the state-of-the-art dramatically.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10446937","National Natural Science Foundation of China; Shanghai Jiao Tong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446937","EEG;eye movement;cross-modal emotion recognition","Emotion recognition;Biological system modeling;Self-supervised learning;Speech recognition;Signal processing;Predictive models;Brain modeling","","5","","19","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"EEG Based Emotion Recognition Using Variational Mode Decomposition and Convolutional Neural Network for Affective Computing Interfaces","T. Dondup; M. S. Manikandan; L. R. Cenkeramaddi","Department of Electrical Engineering, Indian Institute of Technology, Palakkad, Kerala, India; Department of Electrical Engineering, Indian Institute of Technology, Palakkad, Kerala, India; Information and Communication Technology, University of Agder, Grimstad, Norway","2023 11th International Conference on Control, Mechatronics and Automation (ICCMA)","1 Jan 2024","2023","","","37","42","Human emotion recognition plays a vital role in brain-to-brain communication, human-machine interactions and affective computing interfaces. This paper presents electroencephalogram (EEG) based emotion recognition using variational mode decomposition (VMD) and convolutional neural network (CNN) by finding optimal hyperparameters for recognizing three emotional classes: positive, neutral and negative. The two-stage VMD based EEG processing is proposed for effectively removing artifacts and noises from the EEG signal and also for decomposing EEG signal into five brain waves such as delta, theta, alpha,beta and gamma. The CNN based emotion recognition is presented based on the differential entropy feature extracted from 1 second brain waves instead of using brain waves directly in order to reduce size of CNN model. In this study, we created twelve CNN models using three number of layers (2, 5, and 7) and four activation functions with major objective of finding best CNN model(s). The standard SEED database is used to obtain trained CNN models and test their performance. Evaluation results show that the CNN architecture with rectified linear unit (ReLU) yielded higher accuracy of 90.33% among four activation functions. For predicting emotions of positive, negative and neutral, the CNN model with 2-layer and ReLU achieves an accuracy of 100%, 94.44% and 78.2%, respectively whereas the CNN model with 7-layer results in accuracy of 100%, 94.40% and 89.13%, respectively. This study also demonstrates significance of selecting optimal hyperparameters and best activation function.","2837-5149","979-8-3503-1568-4","10.1109/ICCMA59762.2023.10374647","Research Council of Norway; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10374647","Electroencephalogram;Emotion classification;Variational mode decomposition;Convolutional neural network;brain-to-brain communication and affective computing","Emotion recognition;Affective computing;Mechatronics;Predictive models;Brain modeling;Feature extraction;Electroencephalography","","","","24","IEEE","1 Jan 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Spatial-Temporal-Connective Features via Multi-Scale CNN","T. Li; B. Fu; Z. Wu; Y. Liu","Institute of Future, Qingdao University, Qingdao, China; Institute of Future, Qingdao University, Qingdao, China; Institute of Future, Qingdao University, Qingdao, China; Institute of Future, Qingdao University, Qingdao, China",IEEE Access,"4 May 2023","2023","11","","41859","41867","Electroencephalography (EEG) signals from each channel mainly reflect activities of the brain region close to the channel position, and the activities cooperated by various brain regions are response to the emotion-induced stimuli. In this paper, temporal, spatial and connective features are extracted from EEG signals gotten around the head, and used for emotion recognition via a proposed model, spatial-temporal-connective muti-scale convolutional neural network (STC-CNN). The channel-to-channel connectivity is gotten to describe brain region-to-region cooperation under emotion stimuli. The model obtained an average accuracy of 96.79% and 96.89% in classifying the two emotional dimensions of valence and arousal.","2169-3536","","10.1109/ACCESS.2023.3270317","National Key Research and Development Program of China(grant numbers:2020YFB1313600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10108503","Emotion recognition;EEG;connective features;STC-CNN","Feature extraction;Electroencephalography;Emotion recognition;Convolutional neural networks;Brain modeling","","16","","32","CCBYNCND","25 Apr 2023","","","IEEE","IEEE Journals"
"Emotion classification of EEG brain signal using SVM and KNN","R. M. Mehmood; H. J. Lee",Division of Computer Science and Engineering; Center for Advanced Image and Information Technology,2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),"30 Jul 2015","2015","","","1","5","Affective computing research field is growing in large scale with the frequent development of human-computer applications. These applications use information of mental or affective conditions of desired subjects to train their brain responses. Facial impressions, text physiology, vocal and other information about the subjects are used in the classification algorithms. However, the classification frameworks for EEG brain signals have been used infrequently due to the lack of a complete theoretical framework. Therefore, we present here an analysis of two different classification methods which are SVM and KNN. Four different types of emotional stimulus were presented to each subject. After preprocessing of raw EEG data, we employed Hjorth parameters for feature extraction of all EEG channels at each epoch. Five male subjects were selected in this experiment. Our results show that the emotion recognition from EEG brain signals might be possible.","","978-1-4799-7079-7","10.1109/ICMEW.2015.7169786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169786","EEG;emotion;Hjorth;SVM;KNN","Electroencephalography;Indexes;Biomedical imaging;Pain;Computers;Data mining;Training","","58","","20","IEEE","30 Jul 2015","","","IEEE","IEEE Conferences"
"An Efficient Analysis of EEG Signals to Perform Emotion Analysis","A. A. Rahman; M. R. Kabir; R. H. Ratul; F. A. Shamns; M. M. Nishat; F. Faisal","Department of Electrical and Electronic Engineering (EEE), Islamic University of Technology (IUT), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Islamic University of Technology (IUT), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Islamic University of Technology (IUT), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Islamic University of Technology (IUT), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Islamic University of Technology (IUT), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Islamic University of Technology (IUT), Dhaka, Bangladesh","2023 4th International Conference on Artificial Intelligence, Robotics and Control (AIRC)","2 Nov 2023","2023","","","1","7","The analysis of human emotional features is a significant hurdle to surmount on the path to understanding the human mind. Human emotions are convoluted thus making its analysis even more daunting. In this paper, a meticulous and thorough analysis of EEG Brainwave Dataset: Feeling Emotions is performed in order to classify three basic sentiments experienced by people. A Machine Learning (ML) based framework is proposed to execute a multi-class classification process to identify positive, neutral and negative emotional experiences in people. The dataset is analysed in two distinct ways. The first method employs chi-square algorithm to select 500 of the best features from each sample in the dataset which are then employed in classifying multiple emotions utilizing several machine learning models. The second method utilizes the sparsePCA algorithm for feature extraction before conducting a multi-class classification with the help of machine learning models. It is supplemented with a binary classification process of each of the individual sentiments available in the entire dataset to analyze the efficacy of these ML models. The ML algorithms-Support Vector Machines (SVM), Random Forest (RF), Light Gradient Boosting Machines (LGBM) and Multi-Layer Perceptron (MLP) are employed in this investigative study. Maximum accuracy of 99.25% and a precision of 99.25% is obtained from the application of the LGBM model after the optimization of hyper-parameters after sparsePCA is used in feature extraction.","","979-8-3503-4824-8","10.1109/AIRC57904.2023.10303179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10303179","Chi-square;SparsePCA;SVM;Random Forest;Light Gradient Boosting Machine (LGBM);Multi-Layer Perceptron (MLP)","Analytical models;Machine learning algorithms;Support vector machine classification;Medical services;Brain modeling;Feature extraction;Boosting","","13","","50","IEEE","2 Nov 2023","","","IEEE","IEEE Conferences"
"Investigation of Human Emotion Pattern Based on EEG Signal Using Wavelet Families and Correlation Feature Selection","D. U. Surya; P. Siwindarto; E. Yudaningtyas","Department of Electrical Engineering, Brawijaya University, Malang, Indonesia; Department of Electrical Engineering, Brawijaya University, Malang, Indonesia; Department of Electrical Engineering, Brawijaya University, Malang, Indonesia",2019 International Conference on Information and Communications Technology (ICOIACT),"23 Dec 2019","2019","","","310","315","Human emotion is transcendence for human beings given by God compared to other living creatures. Emotion has a significant role in human life. There were many studies done to recognize human emotion using physiological measurement, Electroencephalograph is one of the methods. Yet, the previous studies have not been discussed the wavelet families which has the best performance and optimal channel in human emotion recognition. This research used the power feature from some wavelet family, i.e. Daubechies, symlet, and coiflet with CFS feature as the selection method to choose the superlative feature among alpha, beta, gamma, and tetha frequencies. Based on this research results, the coiflet method has the most accurate value on recognizing the emotion among the wavelet families. The use of CFS feature selection method could increase the accuracy from 81% to 93%, also obtained five most dominant channel, i.e. T8, T7, C5, CP5, and TP7 on power feature of alpha and gamma frequencies so the temporal part of the left brain is more dominant in recognizing human emotion.","","978-1-7281-1655-6","10.1109/ICOIACT46704.2019.8938440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938440","Human Emotion;Electroencephalograph;Wavelet;Correlation Feature Selection (CFS)","Emotion recognition;Correlation;Transforms;Feature extraction;Physiology;Discrete wavelet transforms;Information and communication technology","","1","","17","IEEE","23 Dec 2019","","","IEEE","IEEE Conferences"
"Multi-Modal Domain Adaptation Variational Autoencoder for EEG-Based Emotion Recognition","Y. Wang; S. Qiu; D. Li; C. Du; B. -L. Lu; H. He","Research Center for Brain-inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Science, Beijing; Research Center for Brain-inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Science, Beijing; Research Center for Brain-inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Science, Beijing; Research Center for Brain-inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Science, Beijing; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Research Center for Brain-inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Science, Beijing",IEEE/CAA Journal of Automatica Sinica,"23 Aug 2022","2022","9","9","1612","1626","Traditional electroencephalograph (EEG)-based emotion recognition requires a large number of calibration samples to build a model for a specific subject, which restricts the application of the affective brain computer interface (BCI) in practice. We attempt to use the multi-modal data from the past session to realize emotion recognition in the case of a small amount of calibration samples. To solve this problem, we propose a multi-modal domain adaptive variational autoencoder (MMDA-VAE) method, which learns shared cross-domain latent representations of the multi-modal data. Our method builds a multi-modal variational autoencoder (MVAE) to project the data of multiple modalities into a common space. Through adversarial learning and cycle-consistency regularization, our method can reduce the distribution difference of each domain on the shared latent representation layer and realize the transfer of knowledge. Extensive experiments are conducted on two public datasets, SEED and SEED-IV, and the results show the superiority of our proposed method. Our work can effectively improve the performance of emotion recognition with a small amount of labelled multi-modal data.","2329-9274","","10.1109/JAS.2022.105515","National Natural Science Foundation of China(grant numbers:61976209,62020106015,U21A20388); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754329","Cycle-consistency;domain adaptation;electroencephalograph (EEG);multi modality;variational autoencoder","Emotion recognition;Adaptation models;Brain modeling;Data models;Electroencephalography;Image reconstruction;Calibration","","46","","53","","8 Apr 2022","","","IEEE","IEEE Journals"
"Classification of human emotions from EEG signals using filtering and ANFIS classifier","R. M. Ravindran","P.G Department of Computer Applications, N.G.M College, Pollachi, Coimbatore",Second International Conference on Current Trends In Engineering and Technology - ICCTET 2014,"1 Dec 2014","2014","","","113","119","Human emotion classification has been an attractive research area in the field of data mining. Several research works have been carried out for investigating the classification system for human emotions. This research article is intended to deal with the classification of human emotions through Electroencephalogram (EEG). The essential aspects of this system are feature extraction and accurate classification of the emotion-related EEG-characteristics to attain a significant emotion recognition system. In this research work, the emotions and the patterns of EEG signals of human brain are studied. The aim of this work is to investigate the changes in the brain signals in the domain of different emotions. The study can be analyzed for its utility in the diagnosis of psychosomatic disorders like anxiety and depression in economical way with higher precision. The modified adaptive filtering for signal preprocessing is proposed in this system for removing the noise and artifacts in EEG signal. The adaptive neuro fuzzy inference system is also proposed for classifying and analyzing the emotions based on the features selected. The proposed system is compared with the existing system and the performance of the proposed system is evaluated using the metrics such as specificity and sensitivity.","","978-1-4799-7987-5","10.1109/ICCTET.2014.6966272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6966272","Brain signal;Adaptive neuro fuzzy inference system;adaptive filtering","Electroencephalography;Feature extraction;Classification algorithms;Adaptive filters;Conferences;Brain modeling;Emotion recognition","","6","","24","IEEE","1 Dec 2014","","","IEEE","IEEE Conferences"
"Emotion Recognition from Multi-channel EEG Data through A Dual-pipeline Graph Attention Network","X. Li; J. Li; Y. Zhang; P. Tiwari","Shandong Computer Science Center (National Supercomputing Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), China; Jiuquan Satellite Launch Center, China; Software Engineering College, Zhengzhou University of Light Industry, China; Department of Computer Science, Aalto University, Finland",2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"14 Jan 2022","2021","","","3642","3647","EEG based emotion recognition technology is currently an important concept in artificial intelligence, and also holds great potential in emotional health care. Nevertheless, one major limitation of the prior approaches is they do not capture the relationships between different time-series and channels explicitly, resulting in inevitable low performance, especially in subject-independent recognition settings. In this paper, we propose a novel graph attention network based model to address this issue. Our framework includes dual-pipeline Graph Attention Network layers in parallel to learn the complex dependencies of multi-channel EEG in both temporal and spatial dimensions. The proposed method outperforms other state-of-the-art models on benchmark SEED dataset. Further analysis shows that our method also has good interpretability. As far as we know, it is the first work that introduce graph attention network into EEG based emotion detection research.","","978-1-6654-0126-5","10.1109/BIBM52615.2021.9669544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669544","EEG;emotion recognition;mental health;graph attention network","Learning systems;Emotion recognition;Neuroscience;Correlation;Conferences;Medical services;Benchmark testing","","9","","27","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"Examining the Relationship between EEG Dynamics and Emotion Ratings during Video Watching using Adaptive Mixture Independent Component Analysis","S. Ran; S. -H. Hsu; T. -P. Jung","Swartz Center for Computational Neuroscience, University of California, San Diego (UCSD), La Jolla, CA; Swartz Center for Computational Neuroscience, University of California, San Diego (UCSD), La Jolla, CA; Swartz Center for Computational Neuroscience, University of California, San Diego (UCSD), La Jolla, CA","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","1491","1497","Electroencephalography (EEG)-based emotion recognition has advanced the field in affective computing and has enabled applications in human-computer interactions. Despite significant progress has been made in decoding emotion using supervised machine-learning methods, few studies applied data-driven, unsupervised approaches to explore the underlying EEG dynamics during an emotion experiment and examine how such dynamics correlate with subjective reports of emotion. This study employs the adaptive mixture independent component analysis (AMICA), an unsupervised approach, to EEG data from the DEAP dataset where 32 subjects watched emotional videos. Empirical results showed that AMICA could learn distinct models that separated EEG date collected in the emotion experiment. The identified changes in EEG patterns were weakly-correlated with the four reported emotion scales, indicating the underlying EEG dynamics partially reflected the emotional activities as well as the emotion-irrelevant brain dynamics. Further, the correlations between EEG dynamics and individuals' subjective emotional ratings were significantly higher than those between the EEG and the average ratings from online raters. Finally, building an emotion-decoding model based on the EEG dynamics revealed a significantly better classification performance for valence ratings compared to arousal. This study demonstrated the use of AMICA in characterizing the EEG dynamics in emotion experiments and provided insight into the relationship between EEG and the reported emotional experiences. The unsupervised learning approach can be applied to studying emotion and other confounding factors such as emotion irrelevant EEG artifacts, thereby improving the performance of emotion decoding for EEG-based affective computing.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283175","","Correlation;Independent component analysis;Predictive models;Brain modeling;Electroencephalography;Decoding;Videos","","1","","17","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"Investigating the Effect of EEG Channel Location on Emotion Classification using EEG Signal","M. S. Z. Azalan; N. A. W. N. N. Hisham; A. F. A. Zaidi; Y. Jusman","Faculty of Electrical Engineering & Technology, University Malaysia Perlis, Arau Perlis, Malaysia; Faculty of Electrical Engineering & Technology, University Malaysia Perlis, Arau Perlis, Malaysia; Faculty of Electrical Engineering & Technology, University Malaysia Perlis, Arau Perlis, Malaysia; Department of Electrical Engineering Faculty of Engineering, Universitas Muhammadiyah Yogyakarta, Yogyakarta, Indonesia",2023 International Workshop on Artificial Intelligence and Image Processing (IWAIIP),"19 Mar 2024","2023","","","166","171","EEG channel location on emotion classification and the choice of classifier are the factor that significantly impact the emotion recognition accuracy. Currently, there is a gap in the literature concerning the identification of optimal channel locations and classifier choice for effective emotion recognition. This study aims to investigate the influence of EEG channel electrode locations and classifiers on the accuracy of emotion recognition. Emotion classification was conducted using three classifiers namely Neural Network (NN), Support Vector Machine (SVM), and K-Nearest Neighbor (KNN) - on two EEG channels configuration with 62 channels and 32 channels, as well as five separate brain regions: Frontal, Temporal, Central, Parietal, and Occipital. The analysis was performed on the SEED-IV dataset. Features were extracted using Differential Entropy. Result indicates that SVM outperformed NN and KNN across all configurations. When assessing EEG channel location, the 32-channel setup yield higher average accuracies for both SVM (91.07%) and NN (86.63%), whereas the 62-channel setup was optimal for KNN (88.28%). The most significant results were identified in the Parietal and Occipital regions. SVM achieved the highest accuracies in these regions (96.55% in the Parietal and 97.18% in the Occipital), with NN (90.17% in Parietal and 92.65% in Occipital) and KNN (82.76% in Parietal and 92.81% in Occipital). These findings emphasize the crucial roles of the Parietal and Occipital regions, associated with sensory integration and visual processing, in emotion recognition. The study highlights the importance of EEG channel location and classifier selection in enhancing the reliability of EEG-based emotion recognition systems.","","979-8-3503-8291-4","10.1109/IWAIIP58158.2023.10462897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10462897","Electroencephalogram (EEG);brain regions;NN;SVM;KNN","Support vector machines;Emotion recognition;Visualization;Artificial neural networks;Feature extraction;Electroencephalography;Entropy","","","","18","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition with Similarity Learning Network","Y. Wang; S. Qiu; J. Li; X. Ma; Z. Liang; H. Li; H. He","University of Chinese Academy of Sciences, Beijing, China; Research Center for Brain-inspired Intelligence, Institute of Automation, Chinese Academy of Science, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Educational technology, Capital Normal University, Beijing, China; Department of Educational technology, Capital Normal University, Beijing, China; Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Science, Beijing, China",2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"7 Oct 2019","2019","","","1209","1212","Emotion recognition is an important field of research in Affective Computing (AC), and the EEG signal is one of useful signals in detecting and evaluating emotion. With the development of the deep learning, the neural network is widely used in constructing the EEG-based emotion recognition model. In this paper, we propose an effective similarity learning network, on the basis of a bidirectional long short term memory (BLSTM) network. The pairwise constrain loss will help to learn a more discriminative embedding feature space, combined with the traditional supervised classification loss function. The experiment result demonstrates that the pairwise constrain loss can significantly improve the emotion classification performance. In addition, our method outperforms the state-of-the- art emotion classification approaches in the benchmark EEG emotion dataset-SEED dataset, which get a mean accuracy of 94.62%.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8857499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8857499","","Electroencephalography;Emotion recognition;Feature extraction;Support vector machines;Brain modeling;Affective computing;Recurrent neural networks","Algorithms;Brain;Electroencephalography;Emotions;Humans;Neural Networks, Computer","12","","22","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"Harnessing Gated Recurrent Units for Enhanced Emotion Recognition in Deep Learning","B. Sachdeva; K. S. Gill; M. Kumar; R. S. Rawat","Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India; Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India; Computer Science & Engineering, Graphic Era Hill University, Dehradun, Uttarakhand, India; Computer Science & Engineering, Graphic Era Deemed to be University, Dehradun, Uttarakhand, India",2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit),"9 Apr 2025","2024","","","71","76","This study explores emotion analysis in natural language processing (NLP) utilizing bidirectional gated recurrent units (GRUs). Unlike traditional models that process data in a single direction, bidirectional GRUs are more effective as they capture dependencies in sequential data from both past and future contexts. This bidirectional capability is crucial for tasks like emotion analysis, where context plays a significant role. The proposed model is built using a sequential framework. The initial step involves an embedding layer that converts word indices into dense vectors, essential for managing textual data. To prevent overfitting, a dropout layer with a rate of 0.5 is included. Two bidirectional GRU layers follow, each returning sequences to capture detailed temporal dependencies. A batch normalization layer is applied to stabilize and accelerate the training process. An additional bidirectional GRU layer further enhances the model's ability to capture complex dependencies from both directions. This layer has six units and uses a softmax activation function to classify emotions into six categories. The final layer is dense. The Adam optimizer is employed, using sparse categorical cross-entropy as the loss function, and accuracy is the metric used to evaluate performance. This design showcases the effectiveness of bidirectional GRUs for emotion analysis, achieving a 94% accuracy rate. The model excels in predicting emotions due to its ability to identify intricate relationships in sequential data, as evidenced by the results. The model summary includes the components and parameter choices contributing to these impressive results.","","979-8-3503-7971-6","10.1109/GlobalAISummit62156.2024.10947820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10947820","Emotion Analysis;Textual Data;Bidirectional GRU;Natural Language Processing (NLP);Sequential Data;Bidirectional Dependencies;Model Accuracy","Training;Measurement;Analytical models;Emotion recognition;Accuracy;Data models;Vectors;Tokenization;Artificial intelligence;Overfitting","","","","20","IEEE","9 Apr 2025","","","IEEE","IEEE Conferences"
"Emotion Recognition From EEG Signal Focusing on Deep Learning and Shallow Learning Techniques","M. R. Islam; M. A. Moni; M. M. Islam; M. Rashed-Al-Mahfuz; M. S. Islam; M. K. Hasan; M. S. Hossain; M. Ahmad; S. Uddin; A. Azad; S. A. Alyami; M. A. R. Ahad; P. Lió","Department of Electrical and Electronic Engineering, Bangladesh Army University of Engineering & Technology, Natore, Bangladesh; UNSW Digital Health, WHO Center for eHealth, Faculty of Medicine, The University of New South Wales, Sydney, NSW, Australia; Department of Computer Science and Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Department of Computer Science and Engineering, University of Rajshahi, Rajshahi, Bangladesh; School of Information and Communication Technology, Griffith University, Gold Coast, Southport, QLD, Australia; Department of Electrical and Electronic Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Department of Computer Science and Engineering, Chittagong University of Engineering & Technology, Chittagong, Bangladesh; Department of Electrical and Electronic Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Complex Systems Research Group, Faculty of Engineering, The University of Sydney, Darlington, NSW, Australia; IThree Institute, Faculty of Science, University of Technology Sydney, Ultimo NSW, Australia; Department of Mathematics and Statistics, Faculty of Science, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia; Department of Media Intelligent, Osaka University, Ibaraki, Japan; Computer Laboratory, University of Cambridge, Cambridge, U.K",IEEE Access,"8 Jul 2021","2021","9","","94601","94624","Recently, electroencephalogram-based emotion recognition has become crucial in enabling the Human-Computer Interaction (HCI) system to become more intelligent. Due to the outstanding applications of emotion recognition, e.g., person-based decision making, mind-machine interfacing, cognitive interaction, affect detection, feeling detection, etc., emotion recognition has become successful in attracting the recent hype of AI-empowered research. Therefore, numerous studies have been conducted driven by a range of approaches, which demand a systematic review of methodologies used for this task with their feature sets and techniques. It will facilitate the beginners as guidance towards composing an effective emotion recognition system. In this article, we have conducted a rigorous review on the state-of-the-art emotion recognition systems, published in recent literature, and summarized some of the common emotion recognition steps with relevant definitions, theories, and analyses to provide key knowledge to develop a proper framework. Moreover, studies included here were dichotomized based on two categories: i) deep learning-based, and ii) shallow machine learning-based emotion recognition systems. The reviewed systems were compared based on methods, classifier, the number of classified emotions, accuracy, and dataset used. An informative comparison, recent research trends, and some recommendations are also provided for future research directions.","2169-3536","","10.1109/ACCESS.2021.3091487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462089","Emotion;electroencephalogram;human-computer interaction;deep learning;shallow learning","Emotion recognition;Electroencephalography;Brain modeling;Support vector machines;Solid modeling;Human computer interaction;Three-dimensional displays","","106","","112","CCBY","22 Jun 2021","","","IEEE","IEEE Journals"
"Emotion Recognition by Discriminating EEG Segments With High Affective Content From Automatically Selected Relevant Channels","P. Sarma; S. Barma","Department of Electronics and Communication, Indian Institute of Information Technology Guwahati (IIITG), Guwahati, India; Department of Electronics and Communication, Indian Institute of Information Technology Guwahati (IIITG), Guwahati, India",IEEE Transactions on Instrumentation and Measurement,"24 Feb 2022","2022","71","","1","12","In EEG-based emotion recognition, subject’s affective responses are captured by several channels (scalp level) by presenting target stimuli. However, the emotional responses are inconsistent throughout the acquired signal, and rather, it arises at certain duration with high prominence on certain channels. However, existing studies ignored this vital issue and considered the entire acquired signal for processing leading to inaccurate results. Therefore, this work proposes emotion recognition by identifying highly affective EEG segments from automatically chosen relevant channels based on instantaneous phase synchronization measurement among all channels involving Hilbert transform and phase locking values followed by a majority voting algorithm. Next, a random matrix theory discriminates the appropriate segments whose power spectral density has been used in the  $k$ -NN-based classification. Experimental validation has been performed using the EEG dataset from SEED (with positive, neutral, and negative emotions). Five EEG subbands ( $\delta $ ,  $\theta $ ,  $\alpha $ ,  $\beta $ , and  $\gamma$ ) are exclusively examined for their high correlation with emotions. The analyses include distinguishing proper segment lengths and their locations, minimum dominant channels while achieving the best classification for individual subband. The proposed method achieved the highest classification accuracy of up to 95% for higher frequency subbands ( $\beta $  and  $\gamma$ ) with 15 channels. Furthermore, the segment locations for positive and neutral emotions lie from start till 75% of the entire experiment time, whereas, for negative emotion, it is 25%–75%. Moreover, channels from the left frontal, central, and temporal regions are found very active, which is steady. In a comparative study, the proposed idea demonstrates its superiority by displaying the highest efficiency considering less data from minimum channels.","1557-9662","","10.1109/TIM.2022.3147876","Technical Education Quality Improvement Programme-III (TEQIP-III), Government of India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698073","EEG channel selection;EEG segmentation;emotion recognition;phase locking value (PLV)","Electroencephalography;Channel estimation;Synchronization;Emotional responses;Emotion recognition;Scalp;Phase measurement","","17","","54","IEEE","31 Jan 2022","","","IEEE","IEEE Journals"
"MEEG-Transformer: Transformer Network based on Multi-domain EEG for Emotion Recognition","H. Sun; L. Yang; Q. Wang; D. Liu; P. Ni","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China",2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Jan 2024","2023","","","3332","3339","Emotion recognition is a trending topic for research in the area of the brain computer interface (BCI). As an effective signal source, EEG(Electroencephalogram) is widely used in emotion recognition tasks, from which multiple features can be extracted in different domains, such as time domain and frequency domain. However, how to make full use of multiple domain features has become a challenge. In this study, we propose a transformer network for emotion recognition based on Multi-domain EEG features, named MEEG-Transformer. MEEG-Transformer can effectively capture the spatial information with the convolution layer, mine unique information within each domain, and explore the complementary information between features from different domains using self-attention mechanism. Specifically, we extract the features of time domain, frequency domain and wavelet domain respectively, construct the two-dimensional feature matrix of three domains based on the 10-20 system, and merge the three matrices into multi-domain EEG features. Using the DEAP dataset to perform experiments, the proposed model achieves 96.8% and 96.0% recognition accuracy in the arousal and valence dimensions respectively. It is indicated that the proposed method has a strong inspiration for emotion recognition tasks.","2156-1133","979-8-3503-3748-8","10.1109/BIBM58861.2023.10385996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385996","EEG;Emotion recognition;Multi-domain;Transformer","Emotion recognition;Wavelet domain;Convolution;Frequency-domain analysis;Feature extraction;Transformers;Brain modeling","","3","","29","IEEE","18 Jan 2024","","","IEEE","IEEE Conferences"
"Electroencephalogram Emotion Recognition Based on Empirical Mode Decomposition and Optimal Feature Selection","Z. -T. Liu; Q. Xie; M. Wu; W. -H. Cao; D. -Y. Li; S. -H. Li","School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China",IEEE Transactions on Cognitive and Developmental Systems,"10 Dec 2019","2019","11","4","517","526","Electroencephalogram (EEG) emotion recognition based on a hybrid feature extraction method in empirical mode decomposition domain combining with optimal feature selection based on sequence backward selection is proposed, which can reflect subtle information of multiscale components of unstable and nonlinear EEG signals and remove the reductant features to improve the performance of emotion recognition. The proposal is tested on DEAP dataset, in which the emotional states in the Valance dimension and Arousal dimension are classified by both K-nearest neighbor and support vector machine, respectively. In the experiments, temporal windows of different length and three kinds of rhythms of EEG signal are taken into account for comparison, from which the results show that EEG signal with 1s temporal window achieves highest recognition accuracy of 86.46% in Valence dimension and 84.90% in Arousal dimension, respectively, which is superior to some state-of-the-art works. The proposed method would be applied to real-time emotion recognition in multimodal emotional communication-based humans-robots interaction system.","2379-8939","","10.1109/TCDS.2018.2868121","National Natural Science Foundation of China(grant numbers:61403422,61703375,61603356); Natural Science Foundation of Hubei Province(grant numbers:2018CFB447,2015CFA010); Wuhan Science and Technology Project(grant numbers:2017010201010133); Higher Education Discipline Innovation Project(grant numbers:B17040); Fundamental Research Funds for National University, China University of Geosciences (Wuhan)(grant numbers:1810491T07); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452989","Electroencephalogram (EEG);emotion recognition;empirical mode decomposition (EMD);hybrid feature;sequential backward selection (SBS)","Electroencephalography;Feature extraction;Emotion recognition;Frequency-domain analysis;Empirical mode decomposition;Support vector machines","","58","","51","IEEE","31 Aug 2018","","","IEEE","IEEE Journals"
"Processing of Human Motions using Cost Effective EEG Sensor and Machine Learning Approach","S. A. Waheed; S. Revathi; M. A. Matheen; A. Khan Lodhi; M. Ashrafuddin; G. S. Maboobatcha","Dept. of Computer Science and Engineering, B.S. Abdur Rahman Crescent Institute of Science & Technology, Chennai, India; Dept. of Computer Science and Engineering, B.S. Abdur Rahman Crescent Institute of Science & Technology, Chennai, India; Dept. of SDS, Common First Year, King Saud University, Riyadh, Saudi Arabia; Dept. of Electronics and Communication Engineering, Shadan College of Engineering and Technology, Hyderabad, India; Dept. of SDS, Common First Year, King Saud University, Riyadh, Saudi Arabia; Dept. of SDS, Common First Year, King Saud University, Riyadh, Saudi Arabia",2021 1st International Conference on Artificial Intelligence and Data Analytics (CAIDA),"11 May 2021","2021","","","138","143","Emotions are different biological states brought on by neurophysiological changes associated with the nervous system which are affected and results in thoughts, feelings, behavioral responses, and a degree of pleasure or displeasure. These emotions play a vital role in understanding the human response towards any means of action they experience. The system uses an EEG headband device made out of IoT and various machine learning algorithms to understand the human’s emotion of any incident they undergo. In this paper, we propose a method to detect and recognize the emotional changes in a human who is exposed to various images.","","978-1-6654-1511-8","10.1109/CAIDA51941.2021.9425088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425088","EEG;sensor applications;emotion detection","Emotion recognition;Machine learning algorithms;Image recognition;Data analysis;Machine learning;Nervous system;Electroencephalography","","5","","10","IEEE","11 May 2021","","","IEEE","IEEE Conferences"
"Decision fusion for EEG-based emotion recognition","S. Wang; J. Du; R. Xu","Shenzhen Engineering Laboratory of Performance Robots at Digital Stage, Graduate School, Shenzhen, China; Shenzhen Engineering Laboratory of Performance Robots at Digital Stage, Graduate School, Shenzhen, China; Shenzhen Engineering Laboratory of Performance Robots at Digital Stage, Graduate School, Shenzhen, China",2015 International Conference on Machine Learning and Cybernetics (ICMLC),"3 Dec 2015","2015","2","","883","889","The emotion recognition using the electroencephalogram (EEG) receives a lot of attentions in recent years. Various features extracted from different angles are proposed. In this paper, we propose an EEG-based emotion recognition framework based on the weighted fusion of outputs from base classifiers. Threebase classifiers based on the SVM with RBF kernel using Power Spectral, Higuchi Fractal Dimension and Lempel-Ziv Complexity features are developed, respectively. The outputs of base classifiers are integrated by the weighted fusion strategy which is based on the confidence estimation on each class by each base classifier. The evaluation on the DEAP dataset shows that our proposed decision fusion based method outperforms individual base classifiers and the feature fusion based classifier integration for EEG-based emotion recognition.","","978-1-4673-7221-3","10.1109/ICMLC.2015.7340670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340670","EEG;Emotion recognition;Decision fusion","","","8","","23","IEEE","3 Dec 2015","","","IEEE","IEEE Conferences"
"Machine Learning Models for Brain Signal Classification: A Focus on EEG Analysis in Epilepsy Cases","A. M. Ali; S. Nashwan; A. Al-Qerem; A. Almomani; M. A. Sakhnini; A. Aldweesh","Communications and Computer Engineering Department, Faculty of Engineering, AlAhliyya Amman University, Amman, Jordan; Cybersecurity Department Faculty of Information Technology, Middle East University (MEU), Amman, Jordan; Computer Science Department Faculty of Information Technology, Zarqa University, Zarqa, Jordan; School of Computing, Skyline University College, Sharjah, UAE; School of Computing, Skyline University College, Sharjah, UAE; College of Computing and IT, Shaqra University, Shaqra, Saudi Arabia",2024 2nd International Conference on Cyber Resilience (ICCR),"22 May 2024","2024","","","1","8","Electroencephalography (EEG) provides critical insights into brain function and neurological disorders. However, analyzing and classifying EEG signals remains challenging. Manual review is time-consuming, prone to bias, and lacks scalability. Hence, developing automated EEG classification has become vital for clinical assessment and treatment of epilepsy. This study presents a machine learning framework for classifying normal and epileptic EEG recordings. The approach involves feature extraction using discrete wavelet transform (DWT) and statistical methods, feature selection to identify the most discriminative attributes, followed by classification across multiple algorithms. Specifically, DWT decomposes non-stationary signals for time-frequency representation. Principal component analysis and cosine similarity assist in selecting robust features. Supervised classifiers including naive Bayes, decision trees, neural networks, k-nearest neighbors, random forest, and support vector machines categorize the signals. Results demonstrate 100% accuracy with neural networks, indicating highly reliable automated classification is achievable. By comparing multiple techniques, the optimal machine learning pipeline emerges. This epilepsy EEG classification framework demonstrates the potential for AI to significantly improve screening, diagnosis, treatment, and management of neurological disorders. Ongoing research aims to further enhance efficiency, scalability, and real-time capabilities.","","979-8-3503-9496-2","10.1109/ICCR61006.2024.10532919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10532919","electroencephalography signals;epileptic seizure;cosine similarity measure;machine learning algorithms;probability density function","Neurological diseases;Machine learning algorithms;Scalability;Epilepsy;Transforms;Feature extraction;Electroencephalography","","","","20","IEEE","22 May 2024","","","IEEE","IEEE Conferences"
"A Novel Bi-Hemispheric Discrepancy Model for EEG Emotion Recognition","Y. Li; L. Wang; W. Zheng; Y. Zong; L. Qi; Z. Cui; T. Zhang; T. Song","Department of Information Science and Engineering, Key Laboratory of Child Development and Learning Science (Ministry of Education), Southeast University, Nanjing, China; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; Key Laboratory of Child Development and Learning Science (Ministry of Education), School of Biological Sciences and Medical Engineering, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science (Ministry of Education), School of Biological Sciences and Medical Engineering, Southeast University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Information Science and Engineering, Key Laboratory of Child Development and Learning Science (Ministry of Education), Southeast University, Nanjing, China",IEEE Transactions on Cognitive and Developmental Systems,"8 Jun 2021","2021","13","2","354","367","Neuroscience study has revealed the discrepancy of emotion expression between the left and right hemispheres of human brain. Inspired by this study, in this article, we propose a novel bi-hemispheric discrepancy model (BiHDM) to learn this discrepancy information between the two hemispheres to improve electroencephalograph (EEG) emotion recognition. Concretely, we first employ four directed recurrent neural networks (RNNs) based on two spatial orientations to traverse electrode signals on two separate brain regions. This enables the proposed model to obtain the deep representations of all the EEG electrodes’ signals that keep their intrinsic spatial dependence. Upon this representation, a pairwise subnetwork is designed to explicitly capture the discrepancy information between the two hemispheres and extract higher level features for final classification. Furthermore, considering the presence of the domain shift between training and testing data, we incorporate a domain discriminator that adversarially induces the overall feature learning module to generate emotion related but domain-invariant feature representation so as to further promote EEG emotion recognition. Experiments are conducted on three public EEG emotional data sets, in which we evaluate the performance of the proposed BiHDM as well as investigated the important brain areas in emotion expression and explore to use less electrodes to achieve comparable results. These experimental results jointly demonstrate the effectiveness and advantage of the proposed BiHDM model in solving the EEG emotion recognition problem.","2379-8939","","10.1109/TCDS.2020.2999337","National Key Research and Development Program of China(grant numbers:2018YFB1305200); National Natural Science Foundation of China(grant numbers:61921004,61906094,61772276,61902064,81971282); Natural Science Foundation of Jiangsu Province(grant numbers:BK20190452,BK20192004); Fundamental Research Funds for the Central Universities(grant numbers:2242018K3DN01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105104","Bi-hemispheric discrepancy model (BiHDM);electroencephalograph (EEG);EEG emotion recognition","Electroencephalography;Emotion recognition;Feature extraction;Brain modeling;Electrodes;Data mining","","200","","49","IEEE","1 Jun 2020","","","IEEE","IEEE Journals"
"A Feature-Fused Convolutional Neural Network for Emotion Recognition From Multichannel EEG Signals","Q. Yao; H. Gu; S. Wang; X. Li","State Key Laboratory of Cognitive Neuroscience and Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, China; State Key Laboratory of Cognitive Neuroscience and Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, China; State Key Laboratory of Cognitive Neuroscience and Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, China; State Key Laboratory of Cognitive Neuroscience and Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, China",IEEE Sensors Journal,"14 Jun 2022","2022","22","12","11954","11964","Automatic emotion recognition based on multichannel electroencephalogram (EEG) data is a fundamental but challenging problem. Some previous researches ignore the correlation information of brain activity among the inter-channel and inter-frequency bands, which may provide potential information related to emotional states. In this work, we propose a 3-D feature construction method based on spatial-spectral information. First, power values per channel are arranged into a 2-D spatial feature representation according to the position of electrodes. Then, features from different frequency bands are arranged into a 3-D integration feature tensor to capture their complementary information. Simultaneously, we propose a novel framework based on feature fusion modules and dilated bottleneck-based convolutional neural networks (DBCN) which builds a more discriminative model to process the 3-D features for EEG emotion recognition. Both participant-dependent and participant-independent protocols are conducted to evaluate the performance of the proposed DBCN on the DEAP benchmark datasets. Mean 2-class classification accuracies of 89.67% / 90.93% (for participant-dependent) and 79.45% / 83.98% (for participant-independent) were respectively achieved for arousal / valence. These results suggest the proposed method based on the integration of spatial and spectral information could be extended to the assessment of mood disorder and human-computer interaction (HCI) applications.","1558-1748","","10.1109/JSEN.2022.3172133","National Defense Basic Scientific Research Program of China(grant numbers:JCKY2019208C025,JCKY2018110B011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9766346","Emotion recognition;electroencephalogram (EEG);feature fusion;affective computing;deep learning","Feature extraction;Electroencephalography;Emotion recognition;Brain modeling;Convolutional neural networks;Electrodes;Computational modeling","","22","","54","IEEE","3 May 2022","","","IEEE","IEEE Journals"
"Online music emotion prediction on multiple sessions of EEG data using SVM","K. Kawintiranon; Y. Buatong; P. Vateekul","Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand",2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE),"21 Nov 2016","2016","","","1","6","Electroencephalogram (EEG) has been used in the domain of emotion recognition, especially during the experience from music stimulus. A number of works have been submitted with promising results in emotion prediction tasks. Unfortunately, the majority of literature did not sufficiently take into account a non-stationary characteristic of EEG signals which could differ in each recording session, and this issue might be underlying reason why such research could not be transferred into real-world application. In this paper, we are proposing a novel solution by introducing a method of normalization across session. In particular, we performed a comparison of several normalization techniques to explore various techniques to address the issue of non-stationary in EEG data. The three proposed techniques in this study are rescaling, z-score standardization, and frequency band percentage. In our experiment, we collected EEG data from ten subjects in two scenarios: consecutive session and time varied session. Our emotion prediction results suggested that z-score technique was superior to other normalization techniques based on using support vector machine (SVM). To encourage other researchers to test the efficiency of their own approach with multiple session data, our dataset is publicly provided.","","978-1-5090-2033-1","10.1109/JCSSE.2016.7748921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748921","emotion prediction;multiple sessions;electroencephalogram;support vector machine","Electroencephalography;Support vector machines;Brain modeling;Standardization;Feature extraction;Headphones;Data models","","2","","17","IEEE","21 Nov 2016","","","IEEE","IEEE Conferences"
"Multi-Source Domain Adaptation with Transformer-Based Feature Generation for Subject-Independent EEG-Based Emotion Recognition","S. Sartipi; M. Cetin","Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA; Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","2086","2090","Although deep learning-based algorithms have demonstrated excellent performance in automated emotion recognition via electroencephalogram (EEG) signals, variations across brain signal patterns of individuals can diminish the model’s effectiveness when applied across different subjects. While transfer learning techniques have exhibited promising outcomes, they still encounter challenges related to inadequate feature representations and may overlook the fact that source subjects themselves can possess distinct characteristics. In this work, we propose a multi-source domain adaptation approach with a transformer-based feature generator (MSDA-TF) designed to leverage information from multiple sources. The proposed feature generator retains convolutional layers to capture shallow spatial, temporal, and spectral EEG data representations, while self-attention mechanisms extract global dependencies within these features. During the adaptation process, we group the source subjects based on correlation values and aim to align the moments of the target subject with each source as well as within the sources. MSDA-TF is validated on the SEED dataset and is shown to yield promising results.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10445959","National Science Foundation; Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10445959","Brain-computer interface;Domain adaptation;Emotion recognition;Moment matching;Transformer","Emotion recognition;Signal processing algorithms;Speech recognition;Feature extraction;Transformers;Electroencephalography;Generators","","","","21","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Food Image-Induced Discrete Emotion Recognition Using a Single-Channel Scalp-EEG Recording","K. Zhao; D. Xu","Yunnan University, Kunming, China; Yunnan University, Kunming, China","2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","23 Jan 2020","2019","","","1","6","We construct a food image data set as visual stimulus cues for the field of affective computing. After 40 volunteers rating each of 500 food images in terms of the levels of arousal, valence, liking, and familiarity, 188 food images with high consistency scores are selected as the experimental stimuli that evoked three emotional tags: positive, neutral, and negative. The electroencephalogram (EEG) signals of 20 participants were recorded as each watched food images, comprising a food emotion dataset which can be an effective stimulus material. The results show that the food data sets can induce different emotions and the features of each EEG sub-band of different emotional labels have significant differences. This study proposes an innovative theme of emotional stimuli selection based on the food visual cues and expect to construct the optimal emotional feature space as input of deep learning model to achieve better classification accuracy on the real-time emotional recognition system.","","978-1-7281-4852-6","10.1109/CISP-BMEI48845.2019.8966064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966064","food emotiondataset;EEG signal;stimuli selection;emotion feature space","Electroencephalography;Emotion recognition;Visualization;Feature extraction;Real-time systems;Brain modeling;Psychology","","6","","27","IEEE","23 Jan 2020","","","IEEE","IEEE Conferences"
"Towards Superior EEG-Based Emotion Recognition: Integrating CNN Outputs with Machine Learning Classifiers for Enhanced Performance","T. Das; S. M. Mahedy Hasan; A. H. Efat; M. F. Faruk; A. Y. Srizon; M. Al Mamun; M. R. Hossain","Dept.of CSE, RUET, Rajshahi, Bangladesh; Dept.of CSE, RUET, Rajshahi, Bangladesh; Dept.of CSE, RUET, Rajshahi, Bangladesh; Dept.of CSE, RUET, Rajshahi, Bangladesh; Dept.of CSE, RUET, Rajshahi, Bangladesh; Dept.of CSE, RUET, Rajshahi, Bangladesh; Dept.of CSE, RUET, Rajshahi, Bangladesh",2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT),"23 May 2024","2024","","","699","704","Emotions exert a foundational influence on human experiences, influencing daily interactions, decision-making processes, and overall well-being. The integration of electroen-cephalography (EEG) into emotion recognition has emerged as a crucial component in advancing affective computing and enhancing Human-Computer Interaction (HCI). This study makes a noteworthy contribution to existing methods for EEG-based emotion identification, leveraging the SEED dataset and introducing distinctive features, particularly Differential Entropy (DE). Initiating with the training of a 2D Convolutional Neural Network (CNN) using the DE feature, a commendable accuracy rate of 89.0% on the SEED dataset was achieved. Subsequently, the prediction probabilities generated by the 2D CNN were employed to construct a new feature vector for each sample. Utilizing this vector, a novel feature map was created, and a range of machine learning (ML) classifiers including decision tree (DT), random forest (RF), K-nearest neighbors (KNN), and support vector machine (SVM) were independently trained. These diverse classifiers were then amalgamated through a soft voting-based approach to augment the overall classification performance, resulting in an impressive 93.33 % accuracy. The experimental outcomes underscore the effectiveness of the proposed methodology, showcasing superior performance when compared to various approaches in the realm of EEG-based emotion analysis. This research contributes substantially to the ongoing evolution of emotion recognition technology, fostering a deeper understanding of human-computer interaction.","2769-5700","979-8-3503-8577-9","10.1109/ICEEICT62016.2024.10534372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534372","Emotion Recognition;Convolutional Neural Network;Ensemble Classifier;Differential Entropy","Support vector machines;Radio frequency;Emotion recognition;Brain modeling;Electroencephalography;Vectors;Entropy","","","","17","IEEE","23 May 2024","","","IEEE","IEEE Conferences"
"EEG Signals and Spectrogram with Deep Learning Approaches Emotion Analysis with Images","A. G. Eker; N. Duru; K. Eker","Bilgisayar Mühendisliği Bölümü, Kocaeli Universitesi Kocaeli, Türkiye; Mühendislik ve Doğa Bilimleri Fakültesi Kocaeli Sağlik ve, Teknoloji Universitesi Kocaeli, Türkiye; Bilgisayar Mühendisliği Bölümü, Kocaeli Universitesi Kocaeli, Türkiye",2022 7th International Conference on Computer Science and Engineering (UBMK),"28 Oct 2022","2022","","","1","5","EEG signals are one of the most basic methods used in identifying and analyzing brain activities. Visual representation of EEG signals can be achieved with spectrograms. Spectrograms represent a visual representation of a signal's signal strength over time. In this study, the signals in an EEG dataset containing ‘positive’, ‘negative’ and ‘neutral’ emotion classes were classified with a deep learning model, and then these signals were transformed into a spectrogram image in the dataset with convolutional network model and also with transfer learning (EfficientNet and XceptionNet). Multiple classification was performed with pre-trained models. The success value obtained by the classification of the EEG signals and the success of the visualization in this classification were measured and presented by comparison. While higher accuracy values were achieved in the classification of signals with the deep network model, in metrics such as precision and F1-score, the classification of images with the proposed convolutional network model achieved much higher performance.","2521-1641","978-1-6654-7010-0","10.1109/UBMK55850.2022.9919468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919468","eeg signals;sentiment analysis;transfer learning;emotional recognition","Electroencephalography;Brain modeling;Spectrogram;Deep learning;Computational modeling;6G mobile communication;Visualization","","1","","","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Features Extraction of Different Emotional based on Brain Rhythm","Y. Liu; C. Han; W. Tong; J. Liu; Y. Qin; Y. Che","Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; Tianjin Key Laboratory of Information Sensing & Intelligent Control, School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China",2023 Asia Symposium on Image Processing (ASIP),"28 Dec 2023","2023","","","50","55","Emotions are an essential part of human physiological performance. Therefore, the study of emotion recognition is extremely relevant in both practical applications and theoretical research. Based on the electroencephalogram (EEG) data we collected, this paper explored the differences from five different characteristics among the coefficient of variation (CV) , power spectral density (PSD) , differential entropy (DE) , approximate entropy (AE) , and permutation entropy (PE) of three emotions at different rhythms, analyzed the activity of each brain region under different emotions. Finally, this paper also compares the classification accuracy of single features and combined features. The experimental results reveal that emotions are more strongly correlated with Beta and Gamma bands; temporal lobe regions are most active in emotion-related brain activity; and the accuracy of combined features is higher than the accuracy of single feature classification. The experiment conclusively demonstrates that effective feature extraction and selection have far-reaching implications for emotion recognition.","","979-8-3503-2342-9","10.1109/ASIP58895.2023.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10371513","EEG;brain rhythm;feature extraction;emotion recognition","Emotion recognition;Temporal lobe;Asia;Feature extraction;Entropy;Electroencephalography;Physiology","","","","17","IEEE","28 Dec 2023","","","IEEE","IEEE Conferences"
"Performance comparison of classical neural network algorithms in emotion recognition","J. Dong; K. Yu","Information Engineering, Hebei University of Environmental Engineering, Chinwangtao, China; Biomedical Engineering, Case Western Reserve University, Cleveland, United States",2022 2nd International Conference on Electronic Information Engineering and Computer Technology (EIECT),"21 Mar 2023","2022","","","364","368","Emotion recognition with computer-assisted methods has been an important tool for psychological analysis. Traditional approaches based on designing and extracting various features of multi-channel neurophysiological signals may require extensive knowledge of this field. This can be a sophisticated task for any expert. In this paper, we examined a hybrid model that utilizes ‘Convolutional Neural Network (CNN)’ for feature extraction and ‘Recurrent Neural Network (RNN)’ that incorporates contextual information. Through the method proposed in this paper, the information that needs to be processed is optimized through multi-channel processing. At the same time, the signal data are packaged into different grid frames by the method of scale transformation, and finally the preprocessing is realized. The whole experiment process is carried out using the publicly available DEAP dataset. The final comparison result based on Valence and Arousal classification accuracy demonstrates that our method is effective.","","979-8-3503-9956-1","10.1109/EIECT58010.2022.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10066707","CNN;RNN;deep learning;electroencephalogram;emotion recognition","Support vector machines;Radio frequency;Training;Emotion recognition;Predictive models;Brain modeling;Feature extraction","","1","","10","IEEE","21 Mar 2023","","","IEEE","IEEE Conferences"
"Fusion of Time and Frequency Domain Attributes for the Automated Recognition of Emotion using EEG Recordings","M. Maithri; U. Raghavendra; A. Gudigar; Sambrama; S. K. Praharaj","Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, India; Department of Instrumentation and Control Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, India; Department of Instrumentation and Control Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, India; Department of Electrical and Computer Engineering, North Carolina State University, United States; Department of Psychiatry, Kasturba Medical College, Manipal, Manipal Academy of Higher Education, Manipal, India",2023 OITS International Conference on Information Technology (OCIT),"19 Feb 2024","2023","","","249","253","Emotions have a significant impact on a person's behavior. Expression reflects how people perceive events, interactions, and judgment. It is possible to identify emotions and categorize them using several techniques, including electroencephalography (EEG) signals, gestures, facial expressions, speech patterns, etc. However, emotion recognition using physiological signals has gained popularity owing to its authenticity. In this study, 54 attributes were extracted from each EEG channel to gather emotional information. These attributes were then fed to multiple classifiers and the results were compared. The study attained a maximum accuracy of 89.52% for channel T7 - 24 using ensemble bagged tree classifiers.","","979-8-3503-5823-0","10.1109/OCIT59427.2023.10430706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430706","Emotion recognition;Electroencephalograph (EEG);Attributes","Emotion recognition;Speech recognition;Feature extraction;Electroencephalography;Real-time systems;Physiology;Recording","","","","14","IEEE","19 Feb 2024","","","IEEE","IEEE Conferences"
"Deep Neural Network for Emotion Recognition Based on Meta-Transfer Learning","H. Tang; G. Jiang; Q. Wang","Computer School, Huanggang Normal University, Huanggang, China; Computer School, Huanggang Normal University, Huanggang, China; Computer School, Huanggang Normal University, Huanggang, China",IEEE Access,"29 Jul 2022","2022","10","","78114","78122","In recent years, many EEG-based emotion recognition methods have been proposed, which can achieve good performance on single-subject data. However, when the models are applied to cross-subject scenarios, due to the existence of subject differences, these models are often difficult to accurately identify the emotions of new subjects, which is not conducive to the practical application of the models. Many transfer learning methods have been applied to cross-subject EEG emotion recognition tasks to reduce the effect of subject differences. Most of them need to be trained with source data of many subjects and calibrated with more data of target subjects to obtain better classification performance on target subjects. However, this process relies on a large amount of training data to guarantee the final effect. This paper proposed a meta-transfer learning model for emotion recognition. The model can reduce the amount of data required by the meta-learning optimization algorithm. Even if only a small amount of data is used for training, it can achieve good performance, thereby reducing the cost of EEG acquisition and labeling, and it is also conducive to the model for new subjects. Finally, this paper conducts cross-subject emotion recognition experiments based on two public datasets SEED and SEED-IV. The experimental results show that the performance of the proposed meta-transfer learning method is better than the baseline method, and can rapid adaptation to unknown subjects while reducing training data.","2169-3536","","10.1109/ACCESS.2022.3193768","Natural Science Foundation of Hubei Province, China(grant numbers:2019CFC868); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9839571","EEG signal;emotion recognition;transfer learning;meta-learning","Brain modeling;Emotion recognition;Electroencephalography;Adaptation models;Feature extraction;Task analysis;Physiology","","11","","43","CCBY","25 Jul 2022","","","IEEE","IEEE Journals"
"Entropy-Based Emotion Recognition Using EEG Signals","Y. Alidoost; B. Mohammadzadeh Asl","Department of Biomedical Engineering, Tarbiat Modares University, Tehran, Iran; Department of Biomedical Engineering, Tarbiat Modares University, Tehran, Iran",IEEE Access,"27 Mar 2025","2025","13","","51242","51254","Automatic detection of emotional states using electroencephalogram (EEG) signals has emerged as an attractive research topic due to advancements in brain-computer interfaces (BCIs). Considering the non-stationary nature of EEG signals, extracting emotion-related features remains a challenging task. Additionally, practical applications demand a balance between reducing computational complexity and maintaining high classification accuracy. To address these challenges, this study pioneers the application of multiscale fluctuation-based dispersion entropy (MFDE) and refined composite MFDE (RCMFDE) for emotion recognition, marking the first exploration of these features in this context. By leveraging these advanced entropy-based methods, the proposed approach significantly enhances classification accuracy while maintaining computational efficiency. The method employs optimal channel selection and data balancing strategies to provide both superior EEG characterization and reduced computational times compared to other entropy-based methods. Comprehensive evaluations on the DEAP dataset achieved binary classification accuracies of 93.51% (HA/LA) and 92.91% (HV/LV), with a multi-class accuracy of 96.67% across four classes. These results demonstrate the effectiveness of MFDE and RCMFDE in addressing critical challenges in emotion recognition and showcase their potential for real-world BCI applications. The proposed method outperforms other state-of-the-art approaches.","2169-3536","","10.1109/ACCESS.2025.3553809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10937156","EEG signal;emotion recognition;multiscale fluctuation-based dispersion entropy;refined composite MFDE;random over-sampling","Electroencephalography;Entropy;Emotion recognition;Feature extraction;Time series analysis;Dispersion;Physiology;Accuracy;Noise;Time measurement","","","","55","CCBY","21 Mar 2025","","","IEEE","IEEE Journals"
"Emotion Recognition System based on EEG signal: A Comparative Study of Different Features and Classifiers","M. K. Ahirwal; M. R. Kose","Department of Computer Applications, National Institute of Technology Raipur, Raipur, CG, India; Department of Computer Applications, National Institute of Technology Raipur, Raipur, CG, India",2018 Second International Conference on Computing Methodologies and Communication (ICCMC),"11 Oct 2018","2018","","","472","476","In this paper, emotion recognition system based on electroencephalogram (EEG) signals has been implemented. The new technology of emotion recognition using physiological signal is based on pattern recognition and classification problem that is well described in this paper. A very famous dataset of EEG signals for emotion recognition known as DEAP dataset is used and three categories of features, time domain features, frequency domain features and entropy based features has been extracted. Classification through support vector machine (SVM), artificial neural networks (ANN) and Naïve bayes (NB) has been done. Performance of system is observed by the parameters like, classification accuracy, precision and recall. After performance observation, it is found that ANN gives the best performance with all types of features. Highest classification accuracy achieved by ANN for said dataset, entropy based features and implementation is 93.75 percent.","","978-1-5386-3452-3","10.1109/ICCMC.2018.8488044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488044","EEG signal processing;Emotion recognition;classification","Feature extraction;Emotion recognition;Electroencephalography;Support vector machines;Frequency-domain analysis;Entropy;Artificial neural networks","","13","","23","IEEE","11 Oct 2018","","","IEEE","IEEE Conferences"
"Emotion Recognition Based on Baseline-corrected Differential Entropy Features and Sparse Graph Regularized Extreme Learning Machine","H. Liu; X. Yan; Y. Guo","Faculty of Information Techonology, Beijing University of Technology, Beijing, China; Sino-French Engineer School, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China",2023 4th International Conference on Computers and Artificial Intelligence Technology (CAIT),"21 Mar 2024","2023","","","175","180","In recent years, with the rapid development of brain science and information science, affective Brain Computer Interfaces (aBCI) have become a hot topic. EEG signals which record the electrical activity of neurons in the brain can provide direct evidence for emotional states. Compared with end-to-end deep learning methods, expert knowledge based emotion recognition methods, using hand-crafted features, are computationally cheaper and can have better generalization ability and interpretability. Due to the effort of balancing the energy spectrum of EEG signals, differential entropy features have been proved a powerful tool for emotion recognition. However, differential entropy of EEG signals still exhibits a decrease trend with increasing frequency, resulting in a low representation of the effect of high-frequency components in emotion recognition and do not perfectly solve the problem caused by the 1/f power law of EEG signals. In this paper, a decibel (dB) feature is extracted by introducing baseline normalization of EEG power spectral density (PSD) , which balances the effect of different rhythmic components better. On this basis, for the features are smoothed and a sparse graph regularized extreme learning machine (SGELM) is used to compare the dB feature with DE in emotion recognition. The proposed method is validated on DEAP data set, and the average classification accuracy increased by 11.33% over those of DE features with wild feature smoothing, and by 2.96% with high feature smoothness. The baseline normalized DE feature not only provide a new option for low-cost affective brain-computer interfaces can but also be used as an alternative input feature for deep learning models.","","979-8-3503-2671-0","10.1109/CAIT59945.2023.10469445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10469445","affective computing;affective brain-computer interfaces;emotion recognition;EEG;machine learning","Deep learning;Emotion recognition;Smoothing methods;Extreme learning machines;Computational modeling;Feature extraction;Brain modeling","","","","19","IEEE","21 Mar 2024","","","IEEE","IEEE Conferences"
"Emotion classification in movie clips based on 3D fuzzy GIST and EEG signal analysis","M. Kwon; J. -S. Kang; M. Lee","School of Electrical Engineering and Computer Science, Kyungpook National University, South Korea; School of Electrical Engineering and Computer Science, Kyungpook National University, South Korea; School of Electrical Engineering and Computer Science, Kyungpook National University, South Korea",2013 International Winter Workshop on Brain-Computer Interface (BCI),"22 Apr 2013","2013","","","67","68","In this paper, we propose an emotion classification model, which can differentiate human-like emotions by using visual and electroencephalography (EEG) dynamic features. To understand human emotions in a more natural situation, we use dynamic stimuli such as movies for the analysis. We incorporate the 3D fuzzy GIST to effectively describe both dynamic visual features and EEG signals. The extracted features are used as inputs to an adaptive neuro-fuzzy inference system (ANFIS). The classifier is provided with the mean opinion scores as the teaching signals. Experimental results show that the system using both low-level visual feature and semantic level EEG feature not only discriminates the positive emotional features from the negative ones but also can get the more stable result than the model using only visual or EEG information.","","978-1-4673-5974-0","10.1109/IWW-BCI.2013.6506633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6506633","emotion classification;ANFIS;3D fuzzy GIST","Electroencephalography;Visualization;Motion pictures;Brain modeling;Tensile stress;Image color analysis;Emotion recognition","","10","","4","IEEE","22 Apr 2013","","","IEEE","IEEE Conferences"
"An Emotion Recognition Method for Game Evaluation Based on Electroencephalogram","G. Du; W. Zhou; C. Li; D. Li; P. X. Liu","School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Information Engineering, Nanchang University, Nanchang, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada",IEEE Transactions on Affective Computing,"28 Feb 2023","2023","14","1","591","602","Players-based emotion recognition can help the understanding game players’ emotional states, contributing to the improvement of the game's quality and value. This article develops a hybrid neural network learning framework called convolutional smooth feedback fuzzy network (CSFFN) to detect a player's emotional states in real-time during a gaming process based on electroencephalogram (EEG) signals. Specifically, CSFFN rationally combines a convolutional neural network (CNN), a fuzzy neural network (FNN), and a recurrent neural network (RNN). CNN not only captures spatial characteristics between EEG signals from different channels but also eliminates noise from EEG signals, improving the accuracy and anti-noise performance in game emotion recognition. FNN extracts the membership degree of a player's different emotional states, further improving the emotion recognition accuracy. Since a player's current emotional state is influenced by the previous emotional states during the game process, RNN is employed to capture the temporal characteristics of EEG signals, better improving the emotion recognition accuracy. Experimental results show that CSFFN has higher recognition accuracy and noise resistance in identifying four emotional states (happiness, sadness, superiority, and anger) compared to support vector machine (SVM) with different kernels, linear discrimination analysis (LDA), AlexNet, and VGG16 methods.","1949-3045","","10.1109/TAFFC.2020.3023966","National Natural Science Foundation of China(grant numbers:61973126,61863028,81660299,61503177); Guangdong Natural Science Funds for Distinguished Young Scholar(grant numbers:2017A030306015); Guangdong special projects(grant numbers:2016TQ03X824); Fundamental Research Funds for the Central Universities(grant numbers:2019ZD27); Science and Technology Planning Project of Guangdong Province(grant numbers:2017B090914002); Innovation Team of the Modern Agriculture Industry Technology System in Guangdong Province(grant numbers:2019KJ139); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010166006); National Key R&D Program of China(grant numbers:2018YFB1700500); Science and Technology Department of Jiangxi Province of China(grant numbers:20161ACB21007,20171BBE50071,20171BAB202033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9198145","Electroencephalogram (EEG);game emotion recognition;games evaluation;feature extraction;classification","Electroencephalography;Games;Emotion recognition;Convolution;Brain modeling;Fuzzy neural networks;Neural networks","","20","","39","IEEE","15 Sep 2020","","","IEEE","IEEE Journals"
"An Effective Deep Neural Network Architecture for EEG-Based Recognition of Emotions","K. Henni; N. Mezghani; A. Mitiche; L. Abou-Abbas; A. Benazza-Ben Yahia","Applied Intelligence Artificial Institute, TÉLUQ University, Montreal, QC, Canada; Applied Intelligence Artificial Institute, TÉLUQ University, Montreal, QC, Canada; INRS—Centre Énergie, Matériaux et Télécommunications, Montréal, QC, Lebanon; Electrical and Computer Engineering Department, Lebanese American University, Byblos, Canada; COSIM Laboratory, SUP’COM, University of Carthage, Tunis, Tunisia",IEEE Access,"9 Jan 2025","2025","13","","4487","4498","Emotions are caused by a human brain reaction to objective events. The purpose of this study is to investigate emotion identification by machine learning using electroencephalography (EEG) data. Current research in EEG-based emotion recognition faces significant challenges due to the high-dimensionality and variability of EEG signals, which complicate accurate classification. Traditional methods often struggle to extract relevant features from noisy and high-dimensional data, and they typically fail to capture the complex temporal dependencies within EEG signals. Recent progress in machine learning by deep neural networks has opened up opportunities to develop methods highly efficient and practicable as to serve useful real-world applications. The purpose of this study is to investigate a novel end-to-end deep learning method of emotion recognition using EEG data, which prefaces a combination of two-dimensional (2D) convolutional network (CNN) and Long short-term memory network (LSTM) by an autoencoder. The autoencoder layers seek a lower dimensionality encoding for optimal input signal reconstruction, and the 2D CNN/LSTM combination layers capture both spatial and temporal features that best describe the emotion classes present in the data. Experiments in four-category classification of emotions, using the public and freely available DEAP dataset, revealed that the method reached superior performance: 90.04% for the “arousal” category, 89.97% for “valence”, 87.73% for “dominance,” and 90.84% for liking”, as measured by the accuracy metric.","2169-3536","","10.1109/ACCESS.2025.3525996","Canada Research Chair on Biomedical Data Mining(grant numbers:950-231214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10824765","EEG signal;emotion recognition;auto-encoder;LSTM;CNN","Electroencephalography;Emotion recognition;Feature extraction;Long short term memory;Electrodes;Convolutional neural networks;Support vector machines;Accuracy;Physiology;Deep learning","","","","53","CCBY","3 Jan 2025","","","IEEE","IEEE Journals"
"Affective Brain-Computer Interfaces: The Significance of P300 Components in Emotion Detection and Classification","C. Cheng; Y. Liu; C. Liao","Xi'an Shiyou University, Xi'an, China; Xi'an Shiyou University, Xi'an, China; School of Computer Science, Xi'an Shiyou University, Xi'an, China",2023 2nd International Conference on Artificial Intelligence and Intelligent Information Processing (AIIIP),"6 Feb 2024","2023","","","357","361","This paper provides an in-depth analysis around the application of P300 EEG signals in EEG emotion recognition. The P300 components in the SEED dataset are extracted by peak detection method and the 4D-CRNN model is improved, which confirms the effectiveness of the P300 components in emotion recognition. The improved model has high accuracy and good generalization ability in the emotion classification task. The model not only adapts to the characteristics of P300 signals, but also shows good performance in the emotion classification task. This study reinforces the importance of the P300 component in emotion recognition, contributes new value to emotion learning datasets in the field of emotional brain-computer interfaces and provides new perspectives and clues for subsequent research in related fields.","","979-8-3503-7145-1","10.1109/AIIIP61647.2023.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10417638","emotion recognition;affective brain-computer interface;P300 component","Training;Emotion recognition;Adaptation models;Brain modeling;Electroencephalography;Brain-computer interfaces;Task analysis","","","","10","IEEE","6 Feb 2024","","","IEEE","IEEE Conferences"
"Attention evaluation with eye tracking glasses for EEG-based emotion recognition","Z. -F. Shi; C. Zhou; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2017 8th International IEEE/EMBS Conference on Neural Engineering (NER),"14 Aug 2017","2017","","","86","89","Attention of subjects in EEG-based emotion recognition experiments determines the quality of EEG data. Traditionally, self-assessment with questionnaires is used to evaluate the attention degree of subjects in experiments. However, this kind of self-assessment approach is subjective and inaccurate. Low quality EEG data from subjects without attention might influence the experiment evaluation and degrade the performance of affective models. In this paper, we extract scanpaths of subjects while watching emotion clips with eye tracking glasses and propose an attention evaluation method with spacial-temporal scanpath analysis. Based on the assumption that subjects with attention have similar scanpath patterns under the same clips, our approach clusters these similar scanpath patterns and evaluate the attention degree. Experimental results demonstrate that our proposed approach can cluster EEG features under attentive conditions effectively and significantly improve the classification performance. The mean accuracy of emotion recognition based on clustered high quality data is 81.70%, whereas the mean accuracy of using the whole dataset is 68.54%.","1948-3554","978-1-5090-4603-4","10.1109/NER.2017.8008298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008298","","Electroencephalography;Gaze tracking;Feature extraction;Clustering algorithms;Emotion recognition;Reliability;Glass","","7","","13","IEEE","14 Aug 2017","","","IEEE","IEEE Conferences"
"Enhanced Techniques for Multi-Emotion Detection from EEG Signals: Focus on Channel and Feature Selection","S. Parui; A. Podder; A. N. F. Shuvo; M. Gourisaria; A. Diwan","School of Computer Engineering, Kalinga Institute of Industrial Technology Deemed to be University, India; School of Computer Engineering, Kalinga Institute of Industrial Technology Deemed to be University, India; School of Computer Engineering, Kalinga Institute of Industrial Technology Deemed to be University, India; School of Computer Engineering, Kalinga Institute of Industrial Technology Deemed to be University, India; Computer Engineering Department, Marwadi University, India","2024 IEEE International Conference on Biomedical Engineering, Computer and Information Technology for Health (BECITHCON)","17 Apr 2025","2024","","","264","269","This paper presents a comprehensive framework for multi-emotion detection using electroencephalogram (EEG) signals, focusing on feature extraction, channel selection, feature selection, and classification. The proposed methodology leverages advanced techniques such as Asymmetrical Variance Ratio, Common Spatial Pattern (CSP), Ant Colony Optimization (ACO), and a Convolutional Neural Network (CNN) with a Policy Network for effective channel selection. For feature selection, we employ the BorutaPy algorithm, Lasso Regularization, Feature Importance, and Mutual Information to optimize the feature space. Multiple classifiers, including Random Forest, AdaBoost, Gradient Boosting, Support Vector Machine (SVM), and a Stacking classifier, are used to enhance classification performance. The DEAP dataset, which includes emotional responses to various stimuli, is utilized to validate the framework across binary, multi-label, and 3-class classification tasks. Experimental results demonstrate that the proposed channel selection methods significantly improve emotion detection accuracy compared to baseline models, making this approach highly promising for EEG-based emotion recognition applications.","","979-8-3315-3435-6","10.1109/BECITHCON64160.2024.10962735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962735","Emotion;EEG;Asymmetrical Variance Ratio;Common Spatial Pattern (CSP);Ant Colony Optimization (ACO);Convolutional Neural Network (CNN);Policy Network","Support vector machines;Emotion recognition;Ant colony optimization;Stacking;Focusing;Feature extraction;Electroencephalography;Convolutional neural networks;Information technology;Mutual information","","","","21","IEEE","17 Apr 2025","","","IEEE","IEEE Conferences"
"Haptic Vibrations in Emotion Induction and Regulation: Insights From Subjective Ratings and EEG Signals","X. Wang; B. Xu; J. Wang; Z. Gao; J. Ping; H. Li; A. Song","State Key Laboratory of Digital Medical Engineering, Jiangsu Key Laboratory of Robot Perception and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, Jiangsu Key Laboratory of Robot Perception and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, Jiangsu Key Laboratory of Robot Perception and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, Jiangsu Key Laboratory of Robot Perception and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, Jiangsu Key Laboratory of Robot Perception and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, Jiangsu Key Laboratory of Robot Perception and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, Jiangsu Key Laboratory of Robot Perception and Control, School of Instrument Science and Engineering, Southeast University, Nanjing, China",IEEE Transactions on Instrumentation and Measurement,"6 Mar 2025","2025","74","","1","13","Investigating the role of haptics in emotional communication, specifically in emotion induction and regulation, is critical in the field of affective haptics. Although haptic vibrations are widely used for emotion display due to their simplicity and portability, existing studies have not fully explored vibration characteristics and lack objective methods to verify the efficiency of haptic vibrations in emotion induction. Moreover, the potential of haptic vibrations in emotion regulation remains unexplored. This article aims to fill these gaps. First, we examine participants’ emotional perceptions through subjective ratings of vibrotactile patterns generated by a wearable haptic vest, combining four comprehensive vibration characteristics: frequency, intensity, duty cycle, and location. Three patterns representing happiness, fear, and relaxation are preliminarily identified. Next, the effectiveness of these patterns in emotion induction is validated objectively using electroencephalogram (EEG)-based emotion recognition. Finally, we explore the potential of haptic vibrations in emotion regulation by combining haptic stimuli with audio-visual stimuli that have mismatched emotional labels. Experimental results indicate that all four vibration characteristics significantly impact emotion induction, with location and intensity greatly affecting valence and arousal, respectively. EEG validation confirms that the selected vibrotactile patterns significantly induce specific emotions, showing a strong correlation with brain activity in the beta and gamma bands of temporal lobe. In addition, a gradual approach is found to be necessary for emotion regulation through haptic vibrations, with relaxed vibrotactile patterns proving more effective in regulating negative emotions than those representing happiness. These findings provide a scientific basis for designing wearable haptic instruments for emotional communication and pave the way for advancing emotion regulation technologies.","1557-9662","","10.1109/TIM.2025.3544326","National Key Research and Development Program of China(grant numbers:2022YFC2405602); Natural Science Foundation of Jiangsu Province(grant numbers:BK20221464); Key Research and Development Program of Jiangsu Province(grant numbers:BE2022363); Basic Research Project of Leading Technology of Jiangsu Province(grant numbers:BK20192004); National Natural Science Foundation of China(grant numbers:92148205); Joint Fund Project(grant numbers:8091B042206); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10898046","Affective haptics;electroencephalogram (EEG);emotion induction;emotion regulation;haptic vibrations","Haptic interfaces;Vibrations;Regulation;Emotion recognition;Electroencephalography;Motors;Hands;Costs;Correlation;Temporal lobe","","","","48","CCBYNCND","21 Feb 2025","","","IEEE","IEEE Journals"
"Multiscale Feature Extraction Techniques for Emotion Recognition from Physiological Signal","S. G. Taley; M. A. Pund","Department of Computer Science & Engineering, Prof. Ram Meghe Institute of Technology & Research, Amravati, India; Department of Computer Science & Engineering, Prof. Ram Meghe Institute of Technology & Research, Amravati, India","2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)","28 Feb 2025","2024","","","1867","1872","Emotion detection from physiological data is a growing field with important uses in affective computing, HCI, and medical imaging. To achieve significant progress in human-computer interaction, ability to recognise emotions is essential, particularly when using non-intrusive physiological sensors. This work investigates multiscale feature extraction methodologies used in emotion detection systems, with a focus on methodology and benefits. We examine physiological signals like electrocardiogram (ECG), galvanic skin response (GSR) & electroencephalogram (EEG) and we discuss state-of-the-art multiscale feature-based deep learning and machine learning methods. The aim of the paper is to present thorough assessment of state-of-the-art multi-scale machine learning techniques.","","979-8-3503-5681-6","10.1109/ICAC2N63387.2024.10895032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10895032","emotion;human–computer interaction;physiological sensors;affective computing;multiscale feature extraction","Deep learning;Wavelet transforms;Emotion recognition;Affective computing;Computational modeling;Feature extraction;Brain modeling;Physiology;Skin;Sensors","","","","33","IEEE","28 Feb 2025","","","IEEE","IEEE Conferences"
"Short-time-span EEG-based personalized emotion recognition with deep convolutional neural network","K. H. Cheah; H. Nisar; V. V. Yap; C. -Y. Lee","Department of Electronic Engineering, Universiti Tunku Abdul Rahman Kampar, Perak, Malaysia; Department of Electronic Engineering, Universiti Tunku Abdul Rahman Kampar, Perak, Malaysia; Department of Electronic Engineering, Universiti Tunku Abdul Rahman Kampar, Perak, Malaysia; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan",2019 IEEE International Conference on Signal and Image Processing Applications (ICSIPA),"3 Feb 2020","2019","","","78","83","Emotion recognition can be useful in various applications such as in neurofeedback training for functional enhancement. A practically realizable emotion recognition system should rely on as little physiological signals/modalities as possible. Also, emotion-related neurological activities may be vastly different from person to person. Hence, this paper presents the single-modal EEG-based personalized emotion recognition convolutional neural network (CNN) models working on the DEAP dataset. The valence and arousal level classification performance of our presented CNN classifiers have surpassed the other emotion classifiers working on the DEAP dataset based on our scope of literature reviewed. The models, which are deep CNN, rely on only plain EEG data and require no pre-extracted EEG features. The design and application of the CNN models is aimed at possible future work of identification of new emotion-related EEG features, relying on the automated feature extraction capability of the CNN. The two CNN models presented have achieved the 3-class valence classification test accuracy of 97.59% and 98.75% respectively, and the 3-class arousal classification test accuracy of 98.48% and 97.58%.","2642-6471","978-1-7281-3377-5","10.1109/ICSIPA45851.2019.8977786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8977786","Emotion classification;valence;arousal;EEG;neural network;dilated convolution;DEAP;transferred learning","","","6","","26","IEEE","3 Feb 2020","","","IEEE","IEEE Conferences"
"Graph Convolutional Network With Connectivity Uncertainty for EEG-Based Emotion Recognition","H. Gao; X. Wang; Z. Chen; M. Wu; Z. Cai; L. Zhao; J. Li; C. Liu","State Key Laboratory of Digital Medical Engineering, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Instrument Science and Engineering, Southeast University, Nanjing, China; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; State Key Laboratory of Digital Medical Engineering, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Instrument Science and Engineering, Southeast University, Nanjing, China; State Key Laboratory of Digital Medical Engineering, School of Instrument Science and Engineering, Southeast University, Nanjing, China",IEEE Journal of Biomedical and Health Informatics,"3 Oct 2024","2024","28","10","5917","5928","Automatic emotion recognition based on multichannel Electroencephalography (EEG) holds great potential in advancing human-computer interaction. However, several significant challenges persist in existing research on algorithmic emotion recognition. These challenges include the need for a robust model to effectively learn discriminative node attributes over long paths, the exploration of ambiguous topological information in EEG channels and effective frequency bands, and the mapping between intrinsic data qualities and provided labels. To address these challenges, this study introduces the distribution-based uncertainty method to represent spatial dependencies and temporal-spectral relativeness in EEG signals based on Graph Convolutional Network (GCN) architecture that adaptively assigns weights to functional aggregate node features, enabling effective long-path capturing while mitigating over-smoothing phenomena. Moreover, the graph mixup technique is employed to enhance latent connected edges and mitigate noisy label issues. Furthermore, we integrate the uncertainty learning method with deep GCN weights in a one-way learning fashion, termed Connectivity Uncertainty GCN (CU-GCN). We evaluate our approach on two widely used datasets, namely SEED and SEEDIV, for emotion recognition tasks. The experimental results demonstrate the superiority of our methodology over previous methods, yielding positive and significant improvements. Ablation studies confirm the substantial contributions of each component to the overall performance.","2168-2208","","10.1109/JBHI.2024.3416944","National Key Research and Development Program of China(grant numbers:2023YFC3603600,2022YFC2405600); National Natural Science Foundation of China(grant numbers:62171123); China Postdoctoral Science Foundation(grant numbers:2023M730585); Jiangsu Funding Program for Excellent Postdoctoral Talent(grant numbers:2023ZB812,2024ZB701); Nation Research Foundation, Singapore(grant numbers:AISG2-RP-2021-027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10566023","Emotion recognition;EEG;connectivity uncertainty;graph neural network","Electroencephalography;Uncertainty;Emotion recognition;Convolution;Brain modeling;Feature extraction;Noise measurement","Humans;Electroencephalography;Emotions;Signal Processing, Computer-Assisted;Neural Networks, Computer;Algorithms;Brain","2","","63","IEEE","20 Jun 2024","","","IEEE","IEEE Journals"
"Minimizing EEG Human Interference: A Study of an Adaptive EEG Spatial Feature Extraction With Deep Convolutional Neural Networks","H. Deng; S. Wang; Y. Yang; W. G. W. Zhao; H. Zhang; R. Wei; Q. M. J. Wu; B. -L. Lu","Department of Electrical and Computer Engineering, Western University, London, ON, Canada; Department of Computer Science, Lakehead University, Thunder Bay, ON, Canada; Department of Electrical and Computer Engineering, Western University, London, ON, Canada; Stratford School of Interaction Design and Business, University of Waterloo, Stratford, ON, Canada; College of Electrical and Information Engineering, Hunan University, Changsha, China; Department of Computer Science, Lakehead University, Thunder Bay, ON, Canada; Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada; Department of Computer Science and Engineering, Shanghai JiaoTong University, Shanghai, China",IEEE Transactions on Cognitive and Developmental Systems,"3 Dec 2024","2024","16","6","1915","1928","Emotion is one of the main psychological factors that affects human behavior. Using a neural network model trained with electroencephalography (EEG)-based frequency features has been widely used to accurately recognize human emotions. However, utilizing EEG-based spatial information with popular 2-D kernels of convolutional neural networks (CNNs) has rarely been explored in the extant literature. This article addresses these challenges by proposing an EEG-based spatial-frequency-based framework for recognizing human emotion, resulting in fewer human interference parameters with better generalization performance. Specifically, we propose a two-stream hierarchical network framework that learns features from two networks, one trained from the frequency domain while another trained from the spatial domain. Our approach is extensively validated on the SEED, SEED-V, and DREAMER datasets. Our proposed method achieved an accuracy of 94.84% on the SEED dataset and 68.61% on the SEED-V dataset with EEG data only. The average accuracy of the Dreamer dataset is 93.01%, 92.04%, and 91.74% in valence, arousal, and dominance dimensions, respectively. The experiments directly support that our motivation of utilizing the two-stream domain features significantly improves the final recognition performance. The experimental results show that the proposed framework obtains improvements over state-of-the-art methods over these three varied scaled datasets. Furthermore, it also indicates the potential of the proposed framework in conjunction with current ImageNet pretrained models for improving performance on 1-D psychological signals.","2379-8939","","10.1109/TCDS.2024.3391131","Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant Program(grant numbers:RGPIN-2020-04757); NSERC CREATE TrustCAV program; Western Research Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10505033","Deep learning;electroencephalography (EEG);emotion recognition;feature combination","Electroencephalography;Feature extraction;Brain modeling;Emotion recognition;Electrodes;Task analysis;Sensor fusion","","2","","66","IEEE","18 Apr 2024","","","IEEE","IEEE Journals"
"From EEG to Eye Movements: Cross-modal Emotion Recognition Using Constrained Adversarial Network with Dual Attention","Y. Wang; J. -W. Liu; B. -L. Lu; W. -L. Zheng","Department of Clinical Neuroscience, Karolinska Institutet, Stockholm, Sweden; Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, and the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, and the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, and the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Affective Computing,"","2024","PP","99","1","15","Emotion recognition is a fundamental part of affective computing, obtaining performance gain from multimodal methods. Electroencephalography (EEG) and eye movements are extensively used as they contain complementary information. However, the inconvenient acquisition of EEG is hindering the extensive adoption of multimodal emotion recognition in daily applications while eye movements are more convenient to collect but with lower performance. To tackle this issue, we propose a Constrained Adversarial Network with Dual Attention (CANDA), exploiting the complementary information from multiple modalities during training to improve the test-time performance of single easily acquired modality, i.e., transferring knowledge from a stronger modality to a weaker modality. During training, a common joint space is learned to diminish the distribution discrepancy among different modalities and incorporate the multimodal representations. During test, single modality is converted to the common space achieving comparable performance to multiple modalities. Extensive experiments demonstrate that our model achieves the state-of-the-art performance for cross-modal emotion recognition. Specifically, the mean accuracy increases around 15% on SEED, 15% on SEED-IV, and 2% on SEED-V compared to the latest baseline for emotion recognition. Visualization of features in the joint space illustrates that the distribution of different modalities aligns together with the discriminative ability regarding various emotions.","1949-3045","","10.1109/TAFFC.2024.3524418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10818647","Cross-modal Emotion Recognition;Adversarial Learning;Brain-Computer Interface;Eye Movements;EEG","Electroencephalography;Emotion recognition;Brain modeling;Training;Physiology;Feature extraction;Hands;Gaze tracking;Affective computing;Accuracy","","","","","IEEE","31 Dec 2024","","","IEEE","IEEE Early Access Articles"
"Domain Adaptation for EEG Emotion Recognition Based on Latent Representation Similarity","J. Li; S. Qiu; C. Du; Y. Wang; H. He","Research Center for Brain-Inspired Intelligence and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-Inspired Intelligence and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-Inspired Intelligence and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-Inspired Intelligence and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-Inspired Intelligence and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Cognitive and Developmental Systems,"10 Jun 2020","2020","12","2","344","353","Emotion recognition has many potential applications in the real world. Among the many emotion recognition methods, electroencephalogram (EEG) shows advantage in reliability and accuracy. However, the individual differences of EEG limit the generalization of emotion classifiers across subjects. Moreover, due to the nonstationary characteristic of EEG, the signals of one subject change over time, which is a challenge to acquire models that could work across sessions. In this article, we propose a novel domain adaptation method to generalize the emotion recognition models across subjects and sessions. We use neural networks to implement the emotion recognition models, which are optimized by minimizing the classification error on the source while making the source and the target similar in their latent representations. Considering the functional differences of the network layers, we use adversarial training to adapt the marginal distributions in the early layers and perform association reinforcement to adapt the conditional distributions in the last layers. In this way, we approximately adapt the joint distributions by simultaneously adapting marginal distributions and conditional distributions. The method is compared with multiple representatives and recent domain adaptation algorithms on benchmark SEED and DEAP for recognizing three and four affective states, respectively. The experimental results show that the proposed method reaches and outperforms the state of the arts.","2379-8939","","10.1109/TCDS.2019.2949306","National Natural Science Foundation of China(grant numbers:61976209,81701785); Chinese Academy of Sciences (CAS) International Collaboration Key Project; Strategic Priority Research Program of CAS(grant numbers:XDB32040200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8882370","Domain adaptation;electroencephalogram (EEG);emotion recognition;neural network;transfer learning","Electroencephalography;Brain modeling;Emotion recognition;Adaptation models;Training;Feature extraction;Neural networks","","189","","40","IEEE","24 Oct 2019","","","IEEE","IEEE Journals"
"Using Bayesian Networks with Human Personality and Situation Information to Detect Emotion States from EEG","X. -a. Fan; L. Bi; H. Ding","School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China",2013 Fourth Global Congress on Intelligent Systems,"1 May 2014","2013","","","284","288","Emotional interaction is an important aspect of the interaction between humans and robots. Further, emotion affects a variety of cognitive processes and thus might leads to accidents. Finding ways to recognize emotion of humans has received a great deal of research attention. In this paper, the recognition model of multi-emotion states from electroencephalogram (EEG) is proposed based on Bayesian Networks with human personality and situation information as causes. Several kinds of emotion states were elicited with videos and EEG signals from fourteen channels were acquired. Experimental results from six subjects suggest that the proposed model have good performance, indicating the feasibility of using EEG to detect multi-emotion states.","2155-6091","978-1-4799-2886-6","10.1109/GCIS.2013.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805949","Bayesian Networks;emotion recognition;EEG;human personality","Electroencephalography;Brain modeling;Emotion recognition;Bayes methods;Robots;Accuracy;Feature extraction","","","","29","IEEE","1 May 2014","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition via Neural Architecture Search","C. Li; Z. Zhang; R. Song; J. Cheng; Y. Liu; X. Chen","Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Neurosurgery, The First Affiliated Hospital of USTC, Division of Life Sciences and Medicine, and the Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, Anhui, China",IEEE Transactions on Affective Computing,"30 May 2023","2023","14","2","957","968","With the flourishing development of deep learning (DL) and the convolution neural network (CNN), electroencephalogram-based (EEG) emotion recognition is occupying an increasingly crucial part in the field of brain-computer interface (BCI). However, currently employed architectures have mostly been designed manually by human experts, which is a time-consuming and labor-intensive process. In this paper, we proposed a novel neural architecture search (NAS) framework based on reinforcement learning (RL) for EEG-based emotion recognition, which can automatically design network architectures. The proposed NAS mainly contains three parts: search strategy, search space, and evaluation strategy. During the search process, a recurrent network (RNN) controller is used to select the optimal network structure in the search space. We trained the controller with RL to maximize the expected reward of the generated models on a validation set and force parameter sharing among the models. We evaluated the performance of NAS on the DEAP and DREAMER dataset. On the DEAP dataset, the average accuracies reached 97.94%, 97.74%, and 97.82% on arousal, valence, and dominance respectively. On the DREAMER dataset, average accuracies reached 96.62%, 96.29% and 96.61% on arousal, valence, and dominance, respectively. The experimental results demonstrated that the proposed NAS outperforms the state-of-the-art CNN-based methods.","1949-3045","","10.1109/TAFFC.2021.3130387","National Key Research and Development Program of China(grant numbers:2019YFA0706203); National Natural Science Foundation of China(grant numbers:61922075,41901350,62176081,62171176); National Defense Basic Scientific Research Program of China(grant numbers:JCKY2019548B001); Anhui Key Project of Research and Development Plan(grant numbers:202104d07020005); Fundamental Research Funds for the Central Universities(grant numbers:JZ2021HGTB0078,PA2021KCPY0051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626645","Electroencephalogram (EEG);emotion recognition;neural architecture search (NAS);reinforcement learning (RL)","Brain modeling;Emotion recognition;Electroencephalography;Computer architecture;Feature extraction;Computational modeling;Convolutional neural networks","","32","","66","IEEE","24 Nov 2021","","","IEEE","IEEE Journals"
"A Survey on EEG-Based Solutions for Emotion Recognition With a Low Number of Channels","A. Apicella; P. Arpaia; F. Isgrò; G. Mastrati; N. Moccaldi","Department of Electrical Engineering and Information Technology (DIETI), University of Naples Federico II, Naples, Italy; Department of Electrical Engineering and Information Technology (DIETI), University of Naples Federico II, Naples, Italy; Department of Electrical Engineering and Information Technology (DIETI), University of Naples Federico II, Naples, Italy; Department of Electrical Engineering and Information Technology (DIETI), University of Naples Federico II, Naples, Italy; Department of Electrical Engineering and Information Technology (DIETI), University of Naples Federico II, Naples, Italy",IEEE Access,"15 Nov 2022","2022","10","","117411","117428","The market uptake of Brain-Computer Interface technologies for clinical and non-clinical applications is attracting the scientific world towards the development of daily-life wearable systems. Beyond the use of dry electrodes and wireless technology, reducing the number of channels is crucial to enhance the ergonomics of devices. This paper presents a review of the studies exploiting a number of channels less than 16 for electroencephalographic (EEG) based-emotion recognition. The main findings of this review concern: (i) the criteria to select the most promising scalp areas for EEG acquisitions; (ii) the attention to prior neurophysiological knowledge; and (iii) the convergences among different studies with respect to preferable areas of the scalp for signal acquisition. Three main approaches emerge for channel selection: data-driven, prior knowledge-based, and based on commercially-available wearable solutions. The most spread is the data-driven, but the neurophysiology of emotions is rarely taken into account. Furthermore, commercial EEG devices usually do not provide electrodes purposefully chosen to assess emotions. Considerable convergences emerge for some electrodes: Fp1, Fp2, F3 and F4 resulted the most informative channels for the valence dimension, according to both data-driven and neurophysiological prior knowledge approaches. The P3 and P4 resulted in being significant for the arousal dimension.","2169-3536","","10.1109/ACCESS.2022.3219844","Italian Ministry of Education, University and Research (MIUR)(grant numbers:232/2016); Department of Information Technology and Electrical Engineering of the University of Naples Federico II, Naples, Italy; European Union–Fondo Sociale Europeo (FSE)-REcovery Assistance for Cohesion and the Territories of Europe (REACT-EU), Programma Operativo Nazionale (PON) Research and Innovation 2014–2020 DM1062/2021(grant numbers:18-I-15350-2); Progetti di Ricerca di Interesse Nazionale (PRIN) Research Project Bias, RIsk and Opacity in AI (BRIO)–BIAS, RISK, and OPACITY in AI: design, verification and development of trustworthy AI(grant numbers:2020SSKZ7R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9940267","Emotion;EEG;channel selection;machine learning;neurophysiology of emotions;wearable devices","Electroencephalography;Emotion recognition;Electrodes;Biomedical monitoring;Machine learning;Reproducibility of results;Physiology;Wearable computing;Neurophysiology","","17","","193","CCBY","4 Nov 2022","","","IEEE","IEEE Journals"
"DEEPSATTNET: An End-to-End Deep Neural Network with Self-Attention for EEG-Based Emotion Recognition","O. Saha; M. Sultan Mahmud; S. Anowarul Fattah","Department of Electrical and Electronic Engineering (EEE), Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering (EEE), Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh",2022 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE),"19 Jun 2023","2022","","","275","279","Emotion analysis by electroencephalogram (EEG) provides vital information about our overall well-being. Analyzing the change in emotional states in response to changes in brain activity has the potential to anticipate the synchronization of attentional, cognitive, and behavioral reactions to emotionally expressive events and hence detect severe mental disorders. This paper proposes an efficient deep learning architecture, DEEPSATTNET, to classify emotions. In contrast to the traditional method, an additional stage for feature extraction is skipped to reduce computational costs. The CNN block of the DEEPSATTNET network captures spatial characteristics from raw EEG data, whereas the LSTM block efficiently encodes significant features over time steps. To acquire the intra-relationship of the extracted feature, the proposed method employs an efficient self-attention mechanism that effectively assigns greater weights to the most relevant information. Finally, the collected feature vector is fed into the dense classifier, which classifies distinct types of emotions. To validate the performance of the proposed architecture, thorough and extensive experimentations are carried out on a publicly available EEG emotion (DEAP) dataset considering mixed subject analysis. For binary class problems, the mean accuracies found for valence and arousal dimensions are 88.65% and 89.24%, respectively.","","979-8-3503-1156-3","10.1109/WIECON-ECE57977.2022.10150514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10150514","Convolution Neural Network (CNN);Long-Short Term Memory (LSTM);Self Attention;Emotion Recognition;Electroencephalogram (EEG)","Deep learning;Emotion recognition;Electric potential;Mental disorders;Neural networks;Computer architecture;Feature extraction","","1","","30","IEEE","19 Jun 2023","","","IEEE","IEEE Conferences"
"Dynamic Domain Adaptation for Class-Aware Cross-Subject and Cross-Session EEG Emotion Recognition","Z. Li; E. Zhu; M. Jin; C. Fan; H. He; T. Cai; J. Li","HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, Zhejiang, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, Zhejiang, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, Zhejiang, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, Anhui, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, Zhejiang, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, Zhejiang, China",IEEE Journal of Biomedical and Health Informatics,"7 Dec 2022","2022","26","12","5964","5973","It is vital to develop general models that can be shared across subjects and sessions in the real-world deployment of electroencephalogram (EEG) emotion recognition systems. Many prior studies have exploited domain adaptation algorithms to alleviate the inter-subject and inter-session discrepancies of EEG distributions. However, these methods only aligned the global domain divergence, but overlooked the local domain divergence with respect to each emotion category. This degenerates the emotion-discriminating ability of the domain invariant features. In this paper, we argue that aligning the EEG data within the same emotion categories is important for generalizable and discriminative features. Hence, we propose the dynamic domain adaptation (DDA) algorithm where the global and local divergences are disposed by minimizing the global domain discrepancy and local subdomain discrepancy, respectively. To tackle the absence of emotion labels in the target domain, we introduce a dynamic training strategy where the model focuses on optimizing the global domain discrepancy in the early training steps, and then gradually switches to the local subdomain discrepancy. The DDA algorithm is formally implemented as an unsupervised version and a semi-supervised version for different experimental settings. Based on the coarse-to-fine alignment, our model achieves the average peak accuracy of 91.08%, 92.89% on SEED, and 81.58%, 80.82% on SEED-IV in the cross-subject and cross-session scenarios, respectively.","2168-2208","","10.1109/JBHI.2022.3210158","National Natural Science Foundation of China(grant numbers:62106248); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LQ20F030013); Ningbo Public Service Technology Foundation, China(grant numbers:202002N3181,2021S152); Ningbo Science and Technology Service Industry Demonstration Project, China(grant numbers:2020F041); Medical Scientific Research Foundation of Zhejiang Province, China(grant numbers:2021KY1028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9904850","Brain-computer interface;emotion recognition;transfer learning;domain adaptation","Emotion recognition;Electroencephalography;Brain-computer interfaces;Adaptation models;Deep learning;Transfer learning;Feature extraction","Humans;Emotions;Electroencephalography;Algorithms","58","","42","IEEE","28 Sep 2022","","","IEEE","IEEE Journals"
"Emotion detection using brain and peripheral signals","Z. Khalili; M. H. Moradi","Biomedical Engineering Faculty, Amirkabir University of Technology슠, Tehran, Iran; Biomedical Engineering Faculty, Amirkabir University of Technology슠, Tehran, Iran",2008 Cairo International Biomedical Engineering Conference,"20 Feb 2009","2008","","","1","4","An emotion recognition system on the basis of physiological signals is proposed in this paper. The aim is to perform a multimodal fusion between electroencephalographic signals of the brain (EEG) and peripheral physiological signals. our acquisition protocol is based on a subset of pictures which correspond to three specific areas of valance-arousal emotional space (positively excited, negatively excited, and calm). Preprocessing and feature extraction methods have been set up in such away that emotion-specific characteristics can be extracted from input signals. The performance of two classifiers has been evaluated on different feature sets: peripheral signals, EEG's, and both. A comparison among the results of different feature sets confirms the interest of using brain signals as peripherals in emotion assessment.","2156-6100","978-1-4244-2694-2","10.1109/CIBEC.2008.4786096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4786096","Emotion;EEG;peripheralsignals;feature;extraction;classification","Electroencephalography;Human computer interaction;Computer interfaces;Biomedical engineering;Emotion recognition;Protocols;Data preprocessing;Feature extraction;Decision making;Human robot interaction","","35","1","12","IEEE","20 Feb 2009","","","IEEE","IEEE Conferences"
"Robust Supervised Graph Embedding Method For EEG-Based Brain Network Emotion Recognition","P. Zhu; C. Li; P. Li; F. Li; D. Yao; P. Xu","School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Life Health Information Science and Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Emotion recognition based on brain networks has attracted increasing research attention due to its ability to reveal the information interactions between brain regions under different emotional states. However, there are still two challenges in practical applications: 1) The high dimensionality of brain networks can lead to issues of feature redundancy, overfitting, and high computational costs; 2) Electroencephalography (EEG) signals are susceptible to outlier noise, and label noise caused by mismatches between the stimuli labels used to induce emotions and the individuals’ actual emotional responses can significantly impact emotion recognition. To address these challenges, we propose a supervised graph embedding algorithm based on the L1-norm space (L1-SGE). This method leverages the local structure and class information of the original data for discriminative subspace learning, achieving a low-dimensional representation of high-dimensional networks. Additionally, the constraints of the L1-norm space enable the method to effectively suppress outliers and label noise. The performance on publicly available emotional EEG databases has successfully validated the effectiveness of the proposed method in low-dimensional feature representation and noise suppression. Furthermore, this method not only offers a powerful tool for research in affective brain-computer interfaces but also provides a potential solution for pattern recognition tasks facing similar challenges in the field of artificial intelligence.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890349","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890349","emotion recognition;EEG;graph embedding;L1-norm;supervised","Emotion recognition;Noise;Redundancy;Noise reduction;Signal processing algorithms;Speech recognition;Electroencephalography;Emotional responses;Speech processing;Overfitting","","","","30","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Comparative Evaluation of EEG Feature Fusion Techniques for Emotion Classification","D. Ameta; A. Gupta; H. D. Singh; D. Uzir; R. Kothari; R. S. C. Bose; H. Singh; L. Behera","IKSMHA Center Indian Institute of Technology Mandi, Mandi, India; Department of Electrical Engineering, Indian Institute of Technology Kanpur, Mandi, India; Department of Computer Science, Indian Institute of Information Technology, Una, HP, India; Department of Computer Science, Indian Institute of Information Technology, Una, HP, India; Department of Computer Science, Indian Institute of Information Technology, Una, HP, India; CAIR Center Indian Institute of Technology Mandi, Mandi, India; Department of Electronics, Indian Institute of Information Technology, Una, HP, India; Indian Institute of Technology Mandi, Mandi, India",2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"4 Nov 2024","2024","","","1","6","Brain-computer interface (BCI) system-based emotion classification is an intriguing topic for research. Feature extraction is an essential step in these systems, usually carried out by a deep neural network or hand-crafted features. Studies in this area frequently highlight the significance of a small set of features; a precise comparison of different proposed features is still necessary. To classify emotions from EEG data, we attempt an exploratory evaluation of a number of EEG features, including those in the time domain, frequency domain, auto-encoded, and deep features, using the DENS dataset. Baseline techniques like Random Forest (RF), XgBoost (XGB), and KNN are used to compare feature performance. The results of this study shed light on the possibility of using these EEG features to identify emotional states. Compared to the previous results on the same dataset, our results show accuracies of 82.10% and 81.33% for valence and arousal, an increase of 29.63% and 18.82%, respectively.","2473-7674","979-8-3503-7024-9","10.1109/ICCCNT61001.2024.10724400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10724400","","Accuracy;Frequency-domain analysis;Artificial neural networks;Nearest neighbor methods;Feature extraction;Electroencephalography;Brain-computer interfaces;Time-domain analysis;Classification tree analysis","","","","24","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"Classification of EEG signals using MLP based on categorical and dimensional perceptions of emotions","H. Yaacob; W. Abdul; N. Kamaruddin","Kulliyyah of Information & Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Kulliyyah of Information & Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Faculty of Computer and Mathematical Sciences, MARA, Universiti Teknologi Malaysia, Shah Alam, Selangor, Malaysia",2013 5th International Conference on Information and Communication Technology for the Muslim World (ICT4M),"23 May 2013","2013","","","1","6","Emotions are frequently studied based on two approaches; categorical and dimensional. In this study, Multi-Layer Perceptron (MLP) was employed to classify four affective states as posited from these approaches. It was observed that emotional states viewed from the dimensional perspective are well discriminated using memory test. In addition to that, the dynamic for each of the four emotions were also presented, in which it was also indicated that an emotional state does not occur abruptly.","","978-1-4799-0136-4","10.1109/ICT4M.2013.6518914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6518914","Emotion recognition;EEG;discrete emotion","Accuracy;Electroencephalography;Brain models;Neurons;Feature extraction;Training","","5","","34","IEEE","23 May 2013","","","IEEE","IEEE Conferences"
"EEG Based Emotion Recognition with Convolutional Neural Networks","C. Özcan; H. Çızmecı","Bilgisayar Mühendisliği, Karabük Üniversitesi, Karabük, Türkiye; Bilgisayar Teknolojileri, Hitit Üniversitesi, Çorum, Türkiye",2020 28th Signal Processing and Communications Applications Conference (SIU),"7 Jan 2021","2020","","","1","4","The use of multichannel electroencephalography (EEG) signals has become increasingly common in emotion recognition. However, studies have shown that due to the complexity of EEG signals, even the signals recorded from the same person may be disturbed. Therefore, EEG signals from the human brain need to be accurately and consistently analyzed and processed. With the method based on the Welch power spectral density estimation and a convolutional neural network, a high degree of classification accuracy was obtained on the SEED EEG dataset.","2165-0608","978-1-7281-7206-4","10.1109/SIU49456.2020.9302498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302498","convolutional neural network;emotion analysis;Electroencephalography;feature extraction","Electroencephalography;Brain modeling;Convolutional neural networks;Emotion recognition;Biological neural networks;Neural engineering;Feature extraction","","2","","","IEEE","7 Jan 2021","","","IEEE","IEEE Conferences"
"Emotion Classification in Response to Tactile Enhanced Multimedia using Frequency Domain Features of Brain Signals","A. Raheel; M. Majid; S. M. Anwar; U. Bagci","Signal, Image, Multimedia Processing and LEarning (SIMPLE) Research Group, University of Engineering and Technology, Taxila, Pakistan; Signal, Image, Multimedia Processing and LEarning (SIMPLE) Research Group, University of Engineering and Technology, Taxila, Pakistan; Center for Research in Computer Vision (CRCV), University of Central Florida, Orlando, Florida, USA; Center for Research in Computer Vision (CRCV), University of Central Florida, Orlando, Florida, USA",2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"7 Oct 2019","2019","","","1201","1204","Tactile enhanced multimedia is generated by synchronizing traditional multimedia clips, to generate hot and cold air effect, with an electric heater and a fan. This objective is to give viewers a more realistic and immersing feel of the multimedia content. The response to this enhanced multimedia content (mulsemedia) is evaluated in terms of the appreciation/emotion by using human brain signals. We observe and record electroencephalography (EEG) data using a commercially available four channel MUSE headband. A total of 21 participants voluntarily participated in this study for EEG recordings. We extract frequency domain features from five different bands of each EEG channel. Four emotions namely: happy, relaxed, sad, and angry are classified using a support vector machine in response to the tactile enhanced multimedia. An increased accuracy of 76.19% is achieved when compared to 63.41% by using the time domain features. Our results show that the selected frequency domain features could be better suited for emotion classification in mulsemedia studies.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8857632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8857632","","Electroencephalography;Feature extraction;Frequency-domain analysis;Electrodes;Support vector machines;Emotion recognition;Time-domain analysis","Brain;Electroencephalography;Emotions;Humans;Multimedia;Support Vector Machine","10","","18","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"EEG-based emotion recognition using domain adaptation network","Y. -M. Jin; Y. -D. Luo; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2017 International Conference on Orange Technologies (ICOT),"12 Apr 2018","2017","","","222","225","This paper explores a fundamental problem of eliminating the differences between source subject and target subject in EEG-based emotion recognition. The major limitation of traditional classification methods is that the lack of domain adaptation and subspace alignment will degrade the performance of cross-subject emotion recognition. To address this problem, we adopt Domain Adaptation Network (DAN) for knowledge transfer, which maintains both feature discriminativeness and domain-invariance during training stage. A feed-forward neural network is constructed by augmenting a few standard layers and a gradient reversal layer. Compared with five traditional methods, DAN outperforms its counterparts and achieves the mean accuracy of 79.19%. Moreover, a visualization of the features learned by DAN is represented in this paper, which intuitively describes the transfer virtue of domain adaptation network.","","978-1-5386-3276-5","10.1109/ICOT.2017.8336126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8336126","emotion recognition;domain adaptation network;transfer learning","Feature extraction;Electroencephalography;Support vector machines;Emotion recognition;Artificial neural networks;Training;Standards","","44","","14","IEEE","12 Apr 2018","","","IEEE","IEEE Conferences"
"Data-Driven Dynamic Graph Convolution Transformer Network Model for EEG Emotion Recognition Under IoMT Environment","X. Jin; F. Zhu; Y. Shen; G. Jeon; D. Camacho","College of Information Science and Technology & Artificial Intelligence, Nanjing, China; College of Information Science and Technology & Artificial Intelligence, Nanjing, China; College of Information Science and Technology & Artificial Intelligence, Nanjing, China; Department of Embedded Systems Engineering, Incheon National University, Incheon, Republic of Korea; Computer Systems Engineering Department, Universidad Politécnica de Madrid, Madrid, Spain",Big Data Mining and Analytics,"4 Apr 2025","2025","8","3","712","725","With the rapid progress in data-driven approaches, artificial intelligence, and big data analytics technologies, utilizing electroencephalogram (EEG) signals for emotion analysis in the field of the Internet of Medical Things can effectively assist in the diagnosis of specific diseases. While existing emotion analysis methods focus on the utilization of effective deep models for data-driven and big data analytics technology, they often struggle to extract long-range dependencies and accurately model local relationships within multi-channel EEG signals. In addition, the subjective scores of the subjects may not match the predefined emotional labels. To overcome these limitations, this paper proposes a new data-driven dynamic graph-embedded Transformer network (DGETN) that has emerged in different tasks of graph data mining for emotion analysis of EEG signals in the scene of IoMT. Firstly, we extract the frequency features differential entropy (DE) and use the linear dynamic system (LDS) method to alleviate the redundancy and noise information. Secondly, to effectively explore the long-range information and local modeling ability, a novel feature extraction module is designed by embedding the dynamic graph convolution operations in the Transformer encoder for mining the discriminant features of data. Moreover, the graph convolution operations can effectively exploit the spatial information between different channels. At last, we introduce the minimum category confusion (MCC) loss to alleviate the fuzziness of classification. We take two commonly used EEG sentiment analysis datasets as a study. The DGETN has achieved state-of-the-art accuracies of 99.38% on the SEED dataset, and accuracies of 99.24 % and 98.85% for valence and arousal prediction on the DEAP dataset, respectively.","2097-406X","","10.26599/BDMA.2024.9020071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10949840","Internet of Medical Things (IoMT) emotion analysis;electroencephalogram (EEG) signals;data-driven big data analytics;graph convolution operation;minimum category confusion (MCC) loss;graph data mining","Emotion recognition;Accuracy;Convolution;Big Data;Feature extraction;Brain modeling;Transformers;Electroencephalography;Data models;Data mining","","1","","62","","4 Apr 2025","","","TUP","TUP Journals"
"Mild Cognitive Impairment Detection with Machine Learning and Topological Data Analysis Applied to EEG Time-series in Facial Emotion Oddball Paradigm","T. M. Rutkowski; M. S. Abe; H. Sugimoto; M. Otake-Matsuura","Cognitive Behavioral Assistive Technology Team, RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan; Doshisha University, Kyoto, Japan; Cognitive Behavioral Assistive Technology Team, RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan; Cognitive Behavioral Assistive Technology Team, RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan",2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"11 Dec 2023","2023","","","1","4","We report a novel approach to dementia neurobiomarker development from EEG time series using topological data analysis (TDA) methodology and machine learning (ML) tools in the ‘AI for social good’ application domain, with possible following application to home-based point of care diagnostics and cognitive intervention monitoring. We propose a new approach to a digital dementia neurobiomarker for early-onset mild cognitive impairment (MCI) prognosis. We report the best median accuracies in a range of upper 85% linear discriminant analysis (LDA), as well above 90% for linear SVM and deep fully connected neural network classifier models in leave-one-out-subject cross-validation, which presents very encouraging results in a binary healthy cognitive aging versus MCI stages using TDA features applied to brainwave time series patterns captured from a four-channel EEG wearable.Clinical relevance— The reported study offers an objective dementia early onset neurobiomarker prospect to replace traditional subjective paper and pencil tests with an application of EEG-wearable-based and topological data analysis machine learning tools in a possibly successive home-based point-of-care environment.","2694-0604","979-8-3503-2447-1","10.1109/EMBC40787.2023.10340508","Japan Society for the Promotion of Science; Japan Science and Technology Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340508","","Data analysis;Point of care;Time series analysis;Aging;Brain modeling;Electroencephalography;Data models","Humans;Time Factors;Cognitive Dysfunction;Machine Learning;Emotions;Dementia;Electroencephalography","","","22","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using DWT and Artificial Neural Network: A Case Study on Autism Spectrum Disorder","I. L. Salim; O. A. Awad; A. S. A. Jalal","Department of Information Engineering, Al-Nahrain University, Baghdad, Iraq; Department of Information Engineering, Al-Nahrain University, Baghdad, Iraq; Department of Information Engineering, Al-Nahrain University, Baghdad, Iraq","2023 IEEE 20th International Conference on Smart Communities: Improving Quality of Life using AI, Robotics and IoT (HONET)","1 Jan 2024","2023","","","1","6","Autism Spectrum Disorder (ASD) impacts brain development, leading to social communication challenges and interaction. Researchers are increasingly exploring using Artificial Intelligence (AI) to diagnose ASD, interpret their emotions, and search for effective change interventions. This study investigates computer-aided ASD emotion recognition using electroencephalography (EEG) signals. The proposed method implements a four-level Discrete Wavelet Transform (DWT) for feature extraction and an Artificial Neural Network (ANN) to classify three dimensions of emotions: valence, arousal, and dominance. The model achieved 83% accuracy for valence and 96% for arousal and dominance. These findings hold potential for developing an adaptable closed-loop ASD intervention system. In conclusion, EEG-based emotion recognition using DWT and ANN appears promising for identifying emotional challenges in autism. However, further research is needed, considering limitations like sample size and static stimuli.","1949-4106","979-8-3503-3111-0","10.1109/HONET59747.2023.10374719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10374719","Autism Spectrum Disorder (ASD);Emotion Recognition;Electroencephalography (EEG);Artificial Neural Network (ANN)","Emotion recognition;Autism;Smart cities;Artificial neural networks;Transforms;Feature extraction;Electroencephalography","","1","","24","IEEE","1 Jan 2024","","","IEEE","IEEE Conferences"
"Feature extraction of EEG for emotion recognition using Hjorth features and higher order crossings","A. Patil; C. Deshmukh; A. R. Panat","Presidency College of Engineering, Kota, Rajasthan, IN; Presidency College of Engineering, Kota, Rajasthan, IN; Presidency College of Engineering, Kota, Rajasthan, IN",2016 Conference on Advances in Signal Processing (CASP),"17 Nov 2016","2016","","","429","434","The Human Machine Interface (HMI) is the technology that enables direct communication between the human brain and the other external devices. Emotion recognition, thus, plays an important role in the design of HMI. Electroencephalogram (EEG) shows the internal emotional state changes of a person very effectively as compared to other traditional methods such as face recognition, gesture recognition, speech recognition, etc. Thus, EEG has gained the attention of researchers in recent years for recording brain activity in HMI design. Emotion recognition can be regarded as a pattern recognition task, hence it includes basic steps likes preprocessing, feature extraction and feature classification. Many different techniques exist for extracting important features from the EEG signal such as Principal Component Analysis (PCA), Statistical features, Discrete Wavelet Transform (DWT), Frequency domain feature extraction using Fourier Transform (FT), Short time Fourier Transform (STFT), etc. The techniques implemented in this paper are Higher Order Crossings (HOC), Discrete Wavelet Transform (DWT) and Hjorth features. A comparative study of these methods is undertaken.","","978-1-5090-0849-0","10.1109/CASP.2016.7746209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746209","Encephalography;Higher Order Crossings (HOC);Discrete Wavelet Transform (DWT);Wavelet Packet Transform (WPT);Hjorth features","Feature extraction;Electroencephalography;Discrete wavelet transforms;Wavelet packets;Emotion recognition","","33","","","IEEE","17 Nov 2016","","","IEEE","IEEE Conferences"
"Emotion Recognition Based on High-Resolution EEG Recordings and Reconstructed Brain Sources","H. Becker; J. Fleureau; P. Guillotel; F. Wendling; I. Merlet; L. Albera","Technicolor R&D France, Cesson-Sévigné, France; Technicolor R&D France, Cesson-Sévigné, France; Technicolor R&D France, Cesson-Sévigné, France; INSERM, U1099, Rennes, France; INSERM, U1099, Rennes, France; INSERM, U1099, Rennes, France",IEEE Transactions on Affective Computing,"29 May 2020","2020","11","2","244","257","Electroencephalography (EEG)-based emotion recognition is currently a hot issue in the affective computing community. Numerous studies have been published on this topic, following generally the same schema: 1) presentation of emotional stimuli to a number of subjects during the recording of their EEG, 2) application of machine learning techniques to classify the subjects' emotions. The proposed approaches vary mainly in the type of features extracted from the EEG and in the employed classifiers, but it is difficult to compare the reported results due to the use of different datasets. In this paper, we present a new database for the analysis of valence (positive or negative emotions), which is made publicly available. The database comprises physiological recordings and 257-channel EEG data, contrary to all previously published datasets, which include at most 62 EEG channels. Furthermore, we reconstruct the brain activity on the cortical surface by applying source localization techniques. We then compare the performances of valence classification that can be achieved with various features extracted from all source regions (source space features) and from all EEG channels (sensor space features), showing that the source reconstruction improves the classification results. Finally, we discuss the influence of several parameters on the classification scores.","1949-3045","","10.1109/TAFFC.2017.2768030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8089737","EEG;emotion recognition;source localization;functional connectivity","Electroencephalography;Feature extraction;Videos;Databases;Emotion recognition;Physiology;Brain modeling","","83","","50","IEEE","30 Oct 2017","","","IEEE","IEEE Journals"
"Emotion stress detection using EEG signal and deep learning technologies","C. -Y. Liao; R. -C. Chen; S. -K. Tai","Department of Information Management, Chaoyang University of Technology, Taichung, Taiwan; Department of Information Management, Chaoyang University of Technology, Taichung, Taiwan; Department of Information Management, Chaoyang University of Technology, Taichung, Taiwan",2018 IEEE International Conference on Applied System Invention (ICASI),"25 Jun 2018","2018","","","90","93","Brainwave reflects the change in electrical potential resulting from the conjunction between the thousands of brain neurons. A neuron can receive signals from other neurons and starts off cyclic discharge reaction when sufficient energy is accumulated. That is also the reason why people persistently emit brainwaves. According to experts from Laboratory of Brain Recognition and Behavior, Michigan University, long-term multitask operation results in the lack of efficiency and in filtering out irrelevant signals leads to the distraction of paying attention of the irrelevant message rather than work-related information. As a result, one would have problems in the transition from one job to another. However, for some people rely on their brain to deal with many things and it may lead to fatigue. Therefore, we did this experiment and tried to figure out the most efficient way to soothe the spiritual pressure and calm the mind down. We utilize deep learning as learning method to predict user's stress feeling through listening to the music. Through above research, by listening to music or create the atmosphere of a music background also with an artistic performance could provide not only psychological treatment effect but also improve the ability of the person to focus.","","978-1-5386-4342-6","10.1109/ICASI.2018.8394414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8394414","Brainwaves;Emotion Stress Detection;EEG;Deep Learning","Electroencephalography;Machine learning;Stress;Mathematical model;Neurons;Music;Psychology","","49","","13","IEEE","25 Jun 2018","","","IEEE","IEEE Conferences"
"The Impact of Transient and Stable Patterns of Functional Connectivity in Emotion Recognition","Y. Huang; X. Liu; Y. Li; C. -I. Ieong; Y. Hu; F. Wan","Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau, China; Department of Electrical and Computer Engineering, Faculty of Science and Technology, Centre for Cognitive and Brain Sciences, Institute of Collaborative Innovation, University of Macau, Macau, China; Shenzhen Institute of Advanced Technology, Chinese Academy Sciences, Shenzhen, China; Guangdong Institute of Intelligence Science and Technology, Zhuhai, China; Department of Orthopaedics and Traumatology, The University of Hong Kong, Hong Kong, China; Department of Electrical and Computer Engineering, Faculty of Science and Technology, Centre for Cognitive and Brain Sciences, Institute of Collaborative Innovation, University of Macau, Macau, China",2024 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA),"10 Jul 2024","2024","","","1","6","The achievement of emotional functions relies on the interactions of various functional systems of the human brain. Numerous studies tried to explore the mechanism of the emotions based on the functional connectivity. However, the contribution of the transient and stable patterns of brain communication in brain emotions was still unclear. The recently proposed activation network framework assumed the activity of functional connectivity (AFC) and background of functional connectivity (BFC) to respectively represent transient and stable patterns of functional connectivity. In this paper, we employed the activation network framework to SEED-IV dataset to achieve the emotion recognition and evaluate the performance of the transient and stable patterns in emotional activities. The top 100 critical connections of each subject were extracted by a data-driven feature selection strategy. The critical connections across all subjects of both AFC and BFC suggested the importance of Gamma band in emotion recognition. Especially, the AFC and BFC showed the different communication modes during the emotions. Finally, the subject-independent classification was employed on each subject’s critical connections to achieve the emotion recognition. The BFC showed the best classification accuracy of 78.71% ± 1.73% (mean ± std). The findings demonstrated that human emotions were mostly influenced by the consistent brain communication patterns. The findings of this investigation offer a new insight on the studies of human emotion.","2377-9322","979-8-3503-2299-6","10.1109/CIVEMSA58715.2024.10586628","Technology Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10586628","Emotion recognition;Functional connectivity;EEG","Emotion recognition;Accuracy;Virtual environments;Feature extraction;Electroencephalography;Transient analysis;Computational intelligence","","","","25","IEEE","10 Jul 2024","","","IEEE","IEEE Conferences"
"Generalized Contrastive Partial Label Learning for Cross-Subject EEG-Based Emotion Recognition","W. Li; L. Fan; S. Shao; A. Song","School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China",IEEE Transactions on Instrumentation and Measurement,"3 Jun 2024","2024","73","","1","11","Electroencephalogram (EEG)-based emotion recognition has become a hot topic in affective computing. However, due to the challenges of intersubject variability and label ambiguity of EEG data, existing research often suffers from poor performance. This limitation significantly hampers the practical application of cross-subject EEG-based emotion recognition. To overcome these challenges, we propose a novel and effective partial label learning (PLL) method, named generalized contrastive PLL (GCPL). By performing label disambiguation, GCPL can uncover the authentic emotion label from the multiple ambiguous emotions reported in the self-assessment of each subject. By integrating contrastive learning with domain generalization seamlessly, GCPL can extract the class-discriminative and domain-invariant features in spite of intersubject variability. Besides, by employing self-distillation, GCPL can mitigate the overfitting problem caused by the limited data size. Experimental results on the SEED, SEED-IV, MPED, and FACED datasets demonstrate the effectiveness of GCPL in cross-subject EEG-based emotion recognition.","1557-9662","","10.1109/TIM.2024.3398103","Basic Research Project of Leading Technology of Jiangsu Province(grant numbers:BK20192004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522819","Contrastive learning;domain generalization;electroencephalogram (EEG);emotion recognition;partial label learning (PLL);self-distillation","Brain modeling;Electroencephalography;Emotion recognition;Phase locked loops;Data models;Feature extraction;Training","","3","","61","IEEE","8 May 2024","","","IEEE","IEEE Journals"
"EEG Emotion Recognition of Different Brain Regions Based on 2DCNN-DGRU","G. Qu; S. Wen; J. Bi; J. Liu; Q. Wu; L. Han; F. Wang","College of Information Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; LiaoNing Disabled Persons' Service Center, Shenyang, China; LiaoNing Disabled Persons' Service Center, Shenyang, China; LiaoNing Disabled Persons' Service Center, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China","2023 IEEE 13th International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","28 Sep 2023","2023","","","692","697","In order to explore the differences in the activation of brain regions under emotional stimuli, and the best effect of emotion recognition under the combination of different brain regions, this paper divides the electrode channels into four different regions: Frontal lobe (Fron), Temporal lobe (Temp), Parietal lobe (Pari) and Occipital lobe (Occi), and Combine them into 9 schemes for further exploration. The differential entropy feature (DE) extracted on the emotion datasets SEED and DEAP is used as the input of the model, and the emotion categories are defined as positive, neutral, and negative. Secondly, in order to evaluate the effect of emotion recognition in brain regions, we propose a hybrid deep model— 2DCNN-DGRU. In the end, an average accuracy rate of 97.74% and 92.18% was obtained for the Whole brain region, respectively. In terms of single brain region, the Temp was the most critical, with an average accuracy rate of 94.99% and 80.16%. In the combination of the three brain regions Fron-Temp-Occi, the highest accuracy rate of 98.40% was achieved, surpassing that of the Whole brain region.","2642-6633","979-8-3503-1519-6","10.1109/CYBER59472.2023.10256529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256529","","Emotion recognition;Temporal lobe;Frontal lobe;Feature extraction;Brain modeling;Parietal lobe;Entropy","","1","","15","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"TDD:Auxiliary framework for recognizing people with depression based on physiological and emotional characteristics of EEG signals","B. Hu; Y. Wang; Z. Chen; X. Jiang; W. Liu; P. Wang","Laboratory of Computing, University of Jinan, JiNan, China; Laboratory of Computing, University of Jinan, JiNan, China; Laboratory of Computing, University of Jinan, JiNan, China; Laboratory of Computing, University of Jinan, JiNan, China; Laboratory of Computing, University of Jinan, JiNan, China; Laboratory of Computing, University of Jinan, JiNan, China",2022 Euro-Asia Conference on Frontiers of Computer Science and Information Technology (FCSIT),"14 Apr 2023","2022","","","164","169","Due to the severe imbalance of the doctor-patient ratio in the world, many patients cannot get mental health diagnoses in time, and there is a lack of objective diagnostic methods for depression at present. To address these issues rapidly and automatically, this paper proposes a new framework, Two-tracks depression diagnosis(TDD), which only requires subjects to watch 60 seconds of emotion-induced material. The framework can extract linear and nonlinear features from Electro-EncephaloGram(EEG) signals for identifying depression. At the same time, it effectively extracts the subjects' emotional features as cues for identifying depression under the guidance of the emotion-induced paradigm. The accuracy of this method in a normal environment can reach 89.03%. The robustness of TDD is obviously better when using cross-individual physiological signals to train the model. Under the condition of using all subjects, the accuracy of TDD is 11.71% higher than that of the model using only physiological signal features. TDD is completely based on objective physiological signals for depression detection, which are difficult to be manipulated subjectively. Therefore, this method can eliminate subjective interference and make the results more realistic and objective than scale detection.","","978-1-6654-6353-9","10.1109/FCSIT57414.2022.00041","National Natural Science Foundation of China(grant numbers:61672262); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097845","TDD;Emotion-induced;Depression;EEG;physical signals","Mental health;Medical services;Interference;Watches;Depression;Feature extraction;Brain modeling","","","","14","IEEE","14 Apr 2023","","","IEEE","IEEE Conferences"
"Research on Emotion Recognition Based on Parameter Transfer and Res2Net","Y. Wu; W. Liu; Q. Li","The School of Science and Technology, Changchun University of Science and Technology 7089 Weixing Road, Changchun, Jilin, China; The School of Science and Technology, Changchun University of Science and Technology 7089 Weixing Road, Changchun, Jilin, China; The School of Science and Technology, Changchun University of Science and Technology 7089 Weixing Road, Changchun, Jilin, China",2023 5th International Conference on Frontiers Technology of Information and Computer (ICFTIC),"13 Mar 2024","2023","","","515","518","In the field of emotion recognition, physiological signals have gradually become a hot object of research because they can objectively reflect real emotions. However, it is difficult to describe emotions completely and accurately with a single signal. Multiple physiological signal fusion models establish a unified classification model through the consistency and complementarity of different physiological signals to improve recognition performance. However, the current multi-modal physiological signal emotion recognition has insufficient information exchange. Aiming at the above problems, this paper proposes a multi-modal physiological signal emotion classification model. Feature extraction of four modes of physiological signals of EEG, EOG, EMG and Skin Electricity is carried out by the method of parameter migration, which saves network training time and improves learning performance. Emotion recognition of multi-modal physiological signals is realized by using Res2Net to more fully mine emotional feature information and fuse it effectively. The experimental results show that the proposed model achieves 95.27% and 94.71% accuracy in the binary classification task of arousal and potency of the DEAP dataset, respectively.","","979-8-3503-0903-4","10.1109/ICFTIC59930.2023.10455873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10455873","Physiological signals;Emotion recognition;Multimodal fusion;Deep learning","Emotion recognition;Electrooculography;Fuses;Transfer learning;Brain modeling;Feature extraction;Physiology","","","","10","IEEE","13 Mar 2024","","","IEEE","IEEE Conferences"
"EEG Based Emotion Recognition by Combining Functional Connectivity Network and Local Activations","P. Li; H. Liu; Y. Si; C. Li; F. Li; X. Zhu; X. Huang; Y. Zeng; D. Yao; Y. Zhang; P. Xu","School of Bioinformatics, Chongqing University of Posts and Telecommunications; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Laboratory for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Laboratory for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Laboratory for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Laboratory for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Laboratory for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Laboratory for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Laboratory for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Laboratory for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China; School of Computer Science and Technology, Southwest University of Science and Technology, Mianyang, China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Laboratory for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China",IEEE Transactions on Biomedical Engineering,"18 Sep 2019","2019","66","10","2869","2881","Objective: Spectral power analysis plays a predominant role in electroencephalogram-based emotional recognition. It can reflect activity differences among multiple brain regions. In addition to activation difference, different emotions also involve different large-scale network during related information processing. In this paper, both information propagation patterns and activation difference in the brain were fused to improve the performance of emotional recognition. Methods: We constructed emotion-related brain networks with phase locking value and adopted a multiple feature fusion approach to combine the compensative activation and connection information for emotion recognition. Results: Recognition results on three public emotional databases demonstrated that the combined features are superior to either single feature based on power distribution or network character. Furthermore, the conducted feature fusion analysis revealed the common characters between activation and connection patterns involved in the positive, neutral, and negative emotions for information processing. Significance: The proposed feasible combination of both information propagation patterns and activation difference in the brain is meaningful for developing the effective human-computer interaction systems by adapting to human emotions in the real world applications.","1558-2531","","10.1109/TBME.2019.2897651","National Key Research and Development Plan of China(grant numbers:# 2017YFB1002501); National Natural Science Foundation of China(grant numbers:#61522105,#61603344,#81401484,#81330032,#61701089); Open Foundation of Henan Key Laboratory of Brain Science and Brain–Computer Interface Technology(grant numbers:HNBBL17001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8634938","Emotion recognition;multiple-feature fusion;activation patterns;network patterns;Electroencephalogram (EEG)","Electroencephalography;Emotion recognition;Feature extraction;Synchronization;Brain;Databases","Brain Mapping;Datasets as Topic;Electroencephalography;Emotions;Humans;Machine Learning;Recognition, Psychology","281","","76","IEEE","5 Feb 2019","","","IEEE","IEEE Journals"
"EEG-based emotion classification using innovative features and combined SVM and HMM classifier","K. Guo; H. Candra; H. Yu; H. Li; H. T. Nguyen; S. W. Su","Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; School of Information and Electronics, Beijing Institute of Technology; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technology, University of Technology, Sydney, New South Wales, Australia",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"14 Sep 2017","2017","","","489","492","Emotion classification is one of the state-of-the-art topics in biomedical signal research, and yet a significant portion remains unknown. This paper offers a novel approach with a combined classifier to recognise human emotion states based on electroencephalogram (EEG) signal. The objective is to achieve high accuracy using the combined classifier designed, which categorises the extracted features calculated from time domain features and Discrete Wavelet Transform (DWT). Two innovative designs are involved in this project: a novel variable is established as a new feature and a combined SVM and HMM classifier is developed. The result shows that the joined features raise the accuracy by 5% on valence axis and 1.5% on arousal axis. The combined classifier can improve the accuracy by 3% comparing with SVM classifier. One of the important applications for high accuracy emotion classification system is offering a powerful tool for psychologists to diagnose emotion related mental diseases and the system developed in this project has the potential to serve such purpose.","1558-4615","978-1-5090-2809-2","10.1109/EMBC.2017.8036868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036868","","Support vector machines;Hidden Markov models;Electroencephalography;Feature extraction;Discrete wavelet transforms;Wavelet analysis;Brain modeling","Arousal;Electroencephalography;Emotions;Humans;Support Vector Machine;Wavelet Analysis","29","","16","IEEE","14 Sep 2017","","","IEEE","IEEE Conferences"
"Contrastive Learning of EEG Representation of Brain Area for Emotion Recognition","S. Dai; M. Li; X. Wu; X. Ju; X. Li; J. Yang; D. Hu","College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China",IEEE Transactions on Instrumentation and Measurement,"21 Feb 2025","2025","74","","1","13","Emotion recognition based on electroencephalography (EEG) has demonstrated promising effectiveness in recent years. However, challenges have been experienced, such as limited dataset availability, experimental protocol inconsistencies, and inherent spatiotemporal redundancies in the EEG data. In this work, we introduce a novel method of contrastive learning of EEG representation of brain area (CLRA). Our method is based on the fact that the EEG signals are of high similarity within brain regions and show significant differences between brain regions. The model is designed to obtain the representation capable of distinguishing signals from different brain areas. Specifically, a 1-D convolutional neural network (CNN) and a recurrent network were applied to learn temporal representations from channelwise EEG in contrastive learning. The representations were recombined and fused to extract features for emotion classification. Experimental evaluations performed on public database for emotion analysis using physiological signals (DEAP) and Shanghai Jiao Tong University emotion EEG dataset (SEED) demonstrate the efficacy of our proposed framework, yielding state-of-the-art results in EEG-based emotion recognition tasks. In our cross-subject experiment, our method achieved an accuracy of 95.23% and 96.31% in valence and arousal on the DEAP, and an accuracy of 95.16% on SEED. Additionally, our experiments involving reduced channel configurations demonstrated an improvement in classification accuracy even with fewer electrodes. Furthermore, CLRA exhibits strong generalization performance and robustness, facilitated by its ability to extract informative single-channel features, thus enabling seamless cross-dataset integration and training.","1557-9662","","10.1109/TIM.2025.3533618","Natural Science Foundation of China(grant numbers:62076248,62036013); Science and Technology Innovation Program of Hunan Province(grant numbers:2024QK2006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10855685","Brain area;contrastive learning;electroencephalography (EEG);emotion recognition;self-supervised learning","Electroencephalography;Feature extraction;Contrastive learning;Electrodes;Emotion recognition;Brain modeling;Brain;Data mining;Deep learning;Training","","1","","52","IEEE","27 Jan 2025","","","IEEE","IEEE Journals"
"MASTF-net: An EEG Emotion Recognition Network Based on Multi-Source Domain Adaptive Method Based on Spatio-Temporal Image and Frequency Domain Information","H. Xu; Z. Pei; Q. Han; M. Hou; X. Qian; T. Weng; Y. Tian; Z. Qiu; B. Zhou","School of Intelligent Technology and Engineering, Chongqing University of Science and Technology, Chongqing, China; School of Materials Science and Engineering, Chongqing University of Arts and Sciences, Chongqing, China; School of Intelligent Technology and Engineering, Chongqing University of Science and Technology, Chongqing, China; School of Intelligent Technology and Engineering, Chongqing University of Science and Technology, Chongqing, China; School of Intelligent Technology and Engineering, Chongqing University of Science and Technology, Chongqing, China; School of Electrical Engineering, Chongqing University of Science and Technology, Chongqing, China; School of Intelligent Technology and Engineering, Chongqing University of Science and Technology, Chongqing, China; School of Intelligent Technology and Engineering, Chongqing University of Science and Technology, Chongqing, China; College of Information Engineering, Tarim University, Alar, China",IEEE Access,"19 Jan 2024","2024","12","","8485","8501","In the field of neuroscience, the electroencephalogram (EEG) is a crucial indicator of emotion. The EEG emotion recognition method based on domain adaptation (DA) has good objectivity and high time resolution and is the preferred method to study the brain’s response to emotional stimuli. However, due to the obvious instability of EEG emotion characteristics, it is difficult to predict the emotion corresponding to EEG signals of cross-subjects by a model that combines all source domains into a single source. In order to solve the problem of cross-subject emotion analysis, we propose an EEG emotion recognition net with a cross-subject multi-source adaptive method (MASTF-net), where EEG features of different subjects are regarded as different domains. Through analyzing the invariance of the target domain and the uniqueness of the source domain, this method realizes the emotional analysis of different objects according to the spatio-temporal images and frequency domain information. First, features of EEG image are extracted from frequency and time dimensions. Secondly, combined with the serialized EEG frequency characteristics of local brain regions, independent classification module are established for different domains to recognize the emotion feature distribution of different subjects. In addition, a feature extraction method of differential entropy(DE) data of EEG is proposed based on frequency band division, which can provide stable feature input for our network structure. Finally, experiments are conducted on the SEED dataset. The experimental results show that our method has better classification accuracy in the experiment on the problem of cross multiple subjects. MASTF-net is superior to other relevant methods and models in multi-source domain. On the issue of cross subject emotion analysis, the highest accuracy of our method can reach  $88.19\%$ .","2169-3536","","10.1109/ACCESS.2024.3349552","West Light Foundation of the Chinese Academy of Sciences; Research Foundation of the Natural Foundation of Chongqing City(grant numbers:cstc2021jcyj-msxmX0146,cstc2021jcyj-msxmX1212); Scientific and Technological Research Program of the Chongqing Municipal Education Commission(grant numbers:KJQN202301517,HZ2021015,KJZD-K202100104,KJQN202301543); Chongqing Science and Technology Military-Civilian Integration Innovation Project, in 2022; Bingtuan Science and Technology Program in China(grant numbers:2021AB026); Shanxi Province Applied Basic Research Program, China(grant numbers:202203021211116); Oil and Gas Production Safety and Risk Control Key Laboratory of Chongqing Open Fund(grant numbers:cqsrc202110); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10380564","Cross-subject;domain adaptation;electroencephalogram;emotional stimuli;spatiotemporal image","Electroencephalography;Feature extraction;Emotion recognition;Brain modeling;Adaptive systems;Time-frequency analysis;Motion pictures","","1","","44","CCBY","4 Jan 2024","","","IEEE","IEEE Journals"
"Classification of Five Emotions from EEG and Eye Movement Signals: Complementary Representation Properties","L. -M. Zhao; R. Li; W. -L. Zheng; B. -L. Lu","Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, China; Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, China; Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, China; Center for Brain-Like Computing and Machine Intelligence, the Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering and the Brain Science and Technology Research Center, Shanghai Jiao Tong University, China",2019 9th International IEEE/EMBS Conference on Neural Engineering (NER),"20 May 2019","2019","","","611","614","Recently, various multimodal approaches to enhancing the performance of affective models have been developed. In this paper, we investigate the complementary representation properties of EEG and eye movement signals on classification for five human emotions: happy, sad, fear, disgust, and neutral. We compare the performance of single modality and two different modality fusion approaches. The results indicate that EEG is superior to eye movements in classifying happy, sad and disgust emotions, whereas eye movements outperform EEG in recognizing fear and neutral emotions. Compared with eye movements, EEG has the advantage of classifying the five emotions, with the mean accuracies of 69.50% and 59.81%, respectively. Due to the complementary representation properties, the modality fusion with bimodal deep auto-encoder significantly improves the classification accuracy to 79.71%. Furthermore, we study the neural patterns of five emotion states and the recognition performance of different eye movement features. The results reveal that five emotions have distinguishable neural patterns and pupil diameter has a relatively high discrimination ability than the other eye movement features.","1948-3554","978-1-5386-7921-0","10.1109/NER.2019.8717055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8717055","","Electroencephalography;Emotion recognition;Feature extraction;Support vector machines;Encoding;Brain modeling;Functional magnetic resonance imaging","","30","","14","IEEE","20 May 2019","","","IEEE","IEEE Conferences"
"Optimized preprocessing and Tiny ML for Attention State Classification","Y. Wang; R. Nahon; E. Tartaglione; P. Mozharovskyi; V. -T. Nguyen","LTCI, Télécom Paris, Institut Polytechnique de Paris; LTCI, Télécom Paris, Institut Polytechnique de Paris; LTCI, Télécom Paris, Institut Polytechnique de Paris; LTCI, Télécom Paris, Institut Polytechnique de Paris; LTCI, Télécom Paris, Institut Polytechnique de Paris",2023 IEEE Statistical Signal Processing Workshop (SSP),"9 Aug 2023","2023","","","695","699","Electroencephalography has been widely used to study mental processes such as attention, perception, and emotion. This is because mental state classification has important applications in many fields, including healthcare, human-computer interaction, and education.In this paper, we present a new approach to mental state classification from EEG signals by combining signal processing techniques and machine learning (ML) algorithms. We evaluate the performance of the proposed method on a dataset of EEG recordings collected during a cognitive load task. The results show that the proposed method achieves high accuracy in classifying mental states and outperforms state-of-the-art methods in terms of classification accuracy and computational efficiency.","2693-3551","978-1-6654-5245-8","10.1109/SSP53291.2023.10207930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10207930","Tiny ML;Attention level detection;EEG;Brain-computer interface","Machine learning algorithms;Signal processing algorithms;Medical services;Machine learning;Feature extraction;Electroencephalography;Classification algorithms","","4","","21","IEEE","9 Aug 2023","","","IEEE","IEEE Conferences"
"Classification of the Stress Levels Based on Heartbeat Evoked Potentials – a Pilot Study","S. Lu; X. Fan; C. A. Zhan","School of Software Engineering, Dalian University of Technology, Dalian, China; School of Software Engineering, Dalian University of Technology, Dalian, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China",2022 12th International Conference on Information Technology in Medicine and Education (ITME),"4 Apr 2023","2022","","","263","267","It is known that emotion affects the brain signals (e.g., EEG) and heart rate variability (HRV). However, few studies have explored how the heartbeat-evoked potentials (HEPs) may relate to stress, a type of emotion. Here we show that the features extracted from the HEPs can be used to classify the low and high stress levels induced using a mental arithmetic (MA) paradigm. The HEPs were derived from 22 channels of scalp EEGs and one channel of ECGs simultaneously recorded when the subjects were solving the simple or complex MA problems. The feature matrix was constructed from the HEP epochs differing significantly between the two types of MA problems. Among three machine learning models, the best average classification accuracy reaches 99.89%.","2474-3828","979-8-3503-1015-3","10.1109/ITME56794.2022.00065","Guangzhou Municipal Science and Technology Project; Bureau of Education of Guangzhou Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086286","stress;heart-beat evoked potentials;EEG;ECG;stress levels classification","Scalp;Education;Human factors;Machine learning;Electrocardiography;Feature extraction;Brain modeling","","2","","10","IEEE","4 Apr 2023","","","IEEE","IEEE Conferences"
"Recognition and analyses of EEG & ERP signals related to emotion: from the perspective of psychology","Yang Yuankui; Zhou Jianzhong","Research Center for Learning Science, South-East University, Nanjing, China; Research Center for Learning Science, South-East University, Nanjing, China","Proceedings. 2005 First International Conference on Neural Interface and Control, 2005.","29 Aug 2005","2005","","","96","99","Electroencephalography (EEG) is widely used to record activities of human brain in the area of psychology for many years. With the development of technology, neural basis of functional areas of emotion processing is revealed gradually. In order to extract the useful information of emotion from the background of EEG signals and noise, we propose to combine methods of psychology and the technology of signal processing such as pattern recognition, etc. In this paper, we first review the psychological methods and signal processing technology in the field of emotion research, and point out the junctions of these two approaches. Secondly, we introduce a method to evaluate emotion competence objectively, which involves the analyses of frequency fluctuations of EEG signals and frontal EEG asymmetry. Then, we take an example of event-related potentials (ERP) study about the face recognition task and the discrimination of sad/happy/neutral emotional facial expressions task. Finally, we indicate the present difficulties in this research area, and advance the possible solution to resolve these problems.","","0-7803-8902-6","10.1109/ICNIC.2005.1499851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1499851","","Emotion recognition;Signal analysis;Electroencephalography;Enterprise resource planning;Psychology;Signal processing;Humans;Data mining;Background noise;Pattern recognition","","6","","5","IEEE","29 Aug 2005","","","IEEE","IEEE Conferences"
"GANSER: A Self-Supervised Data Augmentation Framework for EEG-Based Emotion Recognition","Z. Zhang; Y. Liu; S. -h. Zhong","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China",IEEE Transactions on Affective Computing,"18 Sep 2023","2023","14","3","2048","2063","Electroencephalography (EEG)-based affective computing has a scarcity problem. As a result, it is difficult to build effective, highly accurate and stable models using machine learning algorithms, especially deep learning models. Data augmentation has recently shown performance improvements in deep learning models with increased accuracy, stability and reduced overfitting. In this paper, we propose a novel data augmentation framework, named the generative adversarial network-based self-supervised data augmentation (GANSER). As the first to combine adversarial training with self-supervised learning for EEG-based emotion recognition, the proposed framework generates high-quality and high-diversity simulated EEG samples. In particular, we utilize adversarial training to learn an EEG generator and force the generated EEG signals to approximate the distribution of real samples, ensuring the quality of the augmented samples. A transformation operation is employed to mask parts of the EEG signals and force the generator to synthesize potential EEG signals based on the unmasked parts to produce a wide variety of samples. A masking possibility during transformation is introduced as prior knowledge to generalize the classifier for the augmented sample space. Finally, numerous experiments demonstrate that our proposed method can improve emotion recognition with an increase in performance and achieve state-of-the-art results.","1949-3045","","10.1109/TAFFC.2022.3170369","Natural Science Foundation of Guangdong Province(grant numbers:2019A1515011181); Science and Technology Innovation Commission of Shenzhen(grant numbers:JCYJ20190808162613130); Shenzhen High-Level Talents Program; Open Research Fund from Guangdong Laboratory of Artificial Intelligence & Digital Economy(grant numbers:GML-KF-22-28); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763358","EEG-based emotion recognition;data augmentation;generative adversarial networks","Electroencephalography;Emotion recognition;Brain modeling;Training;Generative adversarial networks;Deep learning;Electrodes","","49","","68","IEEE","26 Apr 2022","","","IEEE","IEEE Journals"
"Classifier comparison using EEG features for emotion recognition process","L. A. Martínez-Tejada; N. Yoshimura; Y. Koike","Laboratory for Future Interdisciplinary Research of Science and Technology (FIRST), Tokyo Institute of Technology, Yokohama, Japan; Laboratory for Future Interdisciplinary Research of Science and Technology (FIRST), Tokyo Institute of Technology, Yokohama, Japan; Laboratory for Future Interdisciplinary Research of Science and Technology (FIRST), Tokyo Institute of Technology, Yokohama, Japan",2020 IEEE 18th World Symposium on Applied Machine Intelligence and Informatics (SAMI),"5 Jun 2020","2020","","","225","230","In this work we compare the contribution of EEG traits for arousal and valence classification using information from the Dataset AMIGOS and different machine learning classifiers. We use support vector machines, Naïve Bayes, Random Forest and an Artificial Neural Network to recognize arousal and valence labels and evaluated them obtaining the accuracy, F1 and AUC scores. We calculated different EEG traits from the ones proposed in the AMIGOS work, as Fractal Dimension, Differential Entropy, Difference and Ratio from differential entropy, and compared the results against the original set of features. We got better performance for the arousal label when the new set of features was used, however for valence label the original set of features perform better in the classification process.","","978-1-7281-3149-8","10.1109/SAMI48414.2020.9108746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108746","Emotion recognition;electroencephalography;machine learning.","Support vector machines;Emotion recognition;Face recognition;Electroencephalography;Entropy;Physiology;Relays","","5","","18","IEEE","5 Jun 2020","","","IEEE","IEEE Conferences"
"GLADA: Global and Local Associative Domain Adaptation for EEG-Based Emotion Recognition","T. Pan; N. Su; J. Shan; Y. Tang; G. Zhong; T. Jiang; N. Zuo","University of Chinese Academy of Sciences, Beijing, China; Ocean University of China, Qingdao, China; Ocean University of China, Qingdao, China; University of Chinese Academy of Sciences, Beijing, China; Ocean University of China, Qingdao, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Cognitive and Developmental Systems,"6 Feb 2025","2025","17","1","167","178","Emotion recognition based on electroencephalography (EEG) has significant advantages in terms of reliability and accuracy. However, individual differences in EEG limit the ability of sentiment classifiers to generalize across subjects. Furthermore, due to the nonstationarity of EEG, subject signals can vary with time, an important challenge for temporal emotion recognition. Several emotion recognition methods have been developed that consider the alignment of conditional distributions, but do not balance the weights of conditional and marginal distributions. In this article, we propose a novel approach to generalize emotion recognition models across individuals and time, i.e., global and local associative domain adaptation (GLADA). The proposed method consists of three parts: 1) deep neural networks are used to extract deep features from emotional EEG data; 2) considering that marginal and conditional distributions between domains can contribute to adaptation differently, a method that combines coarse-grained adversarial adaptation and fine-grained adversarial adaptation is used to narrow the domain distance of the joint distribution in the EEG data between subjects (i.e., reduce intersubject variability), and the weights of the marginal and conditional distributions are automatically balanced using dynamic balancing factors; and 3) domain adaptation is used to accelerate model convergence. Using GLADA, subject-independent EEG emotion recognition is improved by reducing the influence of the subject’s personal information on EEG emotion. Experimental results demonstrate that the GLADA model effectively addresses the domain transfer problem, resulting in improved performance across multiple EEG emotion recognition tasks.","2379-8939","","10.1109/TCDS.2024.3432752","National Natural Science Foundation of China(grant numbers:61971420); Science Frontier Program of the Chinese Academy of Sciences(grant numbers:QYZDJ-SSW-SMC019); Science and Technology Innovation 2030—Brain Science and Brain-Inspired Intelligence Project(grant numbers:2021ZD0200200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10607912","Deep learning;domain adaptation;electroencephalography (EEG);emotion recognition;subject-independent","Electroencephalography;Emotion recognition;Feature extraction;Brain modeling;Adaptation models;Transfer learning;Data models","","","","58","IEEE","23 Jul 2024","","","IEEE","IEEE Journals"
"Automatic EEG Based Emotion Recognition Using Extreme Learning Machine","N. Pusarla; A. Singh; S. Tripathi","Department of Electronics and Communication Engineering, IIIT Naya Raipur, INDIA; Department of Electronics and Communication Engineering, IIIT Naya Raipur, INDIA; Department of Electronics and Communication Engineering, IIIT Naya Raipur, INDIA","2022 IEEE 9th Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)","28 Dec 2022","2022","","","1","6","Emotion is very essential natural feeling of humans. Emotion recognition is often used in brain-computer interface devices to assist impaired people. Electroencephalogram (EEG) signal is essential for identifying emotional states since it reacts instantly to every variation in the individual's brain. In this work, the usefulness of the tunable-Q wavelet transform (TQWT) for classifying various emotions in EEG signals is studied. TQWT breaks the EEG signals into sub-bands and extracts statistical momemts from the sub-bands. The extracted moments are features which are fed to classifier named extreme learning machine, which classifies different emotions. In comparison to other existing approaches, the experimental results of the proposed technique acheived improved emotion recognition performance on open-source datasets, SEED, SEED-IV, and DEAP. The maximum accuracy obtained with the proposed emotion recognition system is 95.2%, 95%, and 93.8% using SEED, SEED-IV, and DEAP databases, respectively, which is higher compared to the state-of-art methods.","2687-7767","979-8-3503-3250-6","10.1109/UPCON56432.2022.9986366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9986366","EEG;TQWT;Extreme learning machine;Emotion Recognition","Human computer interaction;Wavelet transforms;Emotion recognition;Extreme learning machines;Databases;Anxiety disorders;Feature extraction","","1","","30","IEEE","28 Dec 2022","","","IEEE","IEEE Conferences"
"Deep Fusion of Neurophysiological and Facial Features for Enhanced Emotion Detection","F. Safavi; V. R. Venkannagari; D. Parikh; R. K. Vinjamuri","Department of Computer Science and Electrical Engineering, Vinjamuri Laboratory, University of Maryland at Baltimore County, Baltimore, MD, USA; Department of Computer Science and Electrical Engineering, Vinjamuri Laboratory, University of Maryland at Baltimore County, Baltimore, MD, USA; Department of Computer Science and Electrical Engineering, Vinjamuri Laboratory, University of Maryland at Baltimore County, Baltimore, MD, USA; Department of Computer Science and Electrical Engineering, Vinjamuri Laboratory, University of Maryland at Baltimore County, Baltimore, MD, USA",IEEE Access,"23 Apr 2025","2025","13","","67434","67445","The fusion of facial and neurophysiological features for multimodal emotion detection is vital for applications in healthcare, wearable devices, and human-computer interaction, as it enables a more comprehensive understanding of human emotions. Traditionally, the integration of facial expressions and neurophysiological signals has required specialized knowledge and complex preprocessing. With the rise of deep learning and artificial intelligence (AI), new methodologies in affective computing allow for the seamless fusion of multimodal signals, advancing emotion recognition systems. In this paper, we present a novel multimodal deep network that leverages transformers to extract comprehensive features from neurophysiological data, which are then fused with facial expression features for emotion classification. Our transformer-based model analyzes neurophysiological time-series data, while transformer-inspired methods extract facial expression features, enabling the classification of complex emotional states. We compare single modality with multimodal systems, testing our model on Electroencephalography (EEG) signals using the DEAP and Lie Detection datasets. Our hybrid approach effectively captures intricate temporal and spatial patterns in the data, significantly enhancing the system’s emotion recognition accuracy. Validated on the DEAP dataset, our method achieves near state-of-the-art performance, with accuracy rates of 97.78%, 97.64%, 97.91%, and 97.62% for arousal, valence, liking, and dominance, respectively. Furthermore, we achieved a precision of 97.9%, a ROC AUC score of 97.6%, an F1-score of 98.1%, and a recall of 98.2%, demonstrating the model’s robust performance. We demonstrated the effectiveness of this method, specifically for EEG caps with a limited number of electrodes, in emotion detection for wearable devices.","2169-3536","","10.1109/ACCESS.2025.3555934","National Science Foundation Faculty Early Career (CAREER) Development(grant numbers:HCC-2053498); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10945364","Affective computing;emotion detection;deep learning;multimodal emotion recognition;transformer","Emotion recognition;Transformers;Feature extraction;Accuracy;Electroencephalography;Brain modeling;Computer architecture;Visualization;Videos;Facial features","","","","41","CCBY","28 Mar 2025","","","IEEE","IEEE Journals"
"FLDNet: Frame-Level Distilling Neural Network for EEG Emotion Recognition","Z. Wang; T. Gu; Y. Zhu; D. Li; H. Yang; W. Du","Key Laboratory of Advanced Control and Optimization for Chemical Processes, Ministry of Education, East China University of Science and Technology, Shanghai, P.R. China; Key Laboratory of Advanced Control and Optimization for Chemical Processes, Ministry of Education, East China University of Science and Technology, Shanghai, P.R. China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, P.R. China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, P.R. China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, P.R. China; Department of Computer Science and Engineering, East China University of Science and Technology, Shanghai, P.R. China",IEEE Journal of Biomedical and Health Informatics,"27 Jul 2021","2021","25","7","2533","2544","Based on the current research on EEG emotion recognition, there are some limitations, such as hand-engineered features, redundant and meaningless signal frames and the loss of frame-to-frame correlation. In this paper, a novel deep learning framework is proposed, named the frame-level distilling neural network (FLDNet), for learning distilled features from the correlations of different frames. A layer named the frame gate is designed to integrate weighted semantic information on multiple frames to remove redundant and meaningless signal frames. A triple-net structure is introduced to distill the learned features net by net to replace the hand-engineered features with professional knowledge. Specifically, one neural network is normally trained for several epochs. Then, a second network of the same structure will be initialized again to learn the extracted features from the frame gate of the first neural network based on the output of the first net. Similarly, the third net improves the features based on the frame gate of the second network. To utilize the representation ability of the triple neural network, an ensemble layer is conducted to integrate the discriminative ability of the proposed framework for final decisions. Consequently, the proposed FLDNet provides an effective method for capturing the correlation between different frames and automatically learn distilled high-level features for emotion recognition. The experiments are carried out in a subject-independent emotion recognition task on public emotion datasets of DEAP and DREAMER benchmarks, which have demonstrated the effectiveness and robustness of the proposed FLDNet.","2168-2208","","10.1109/JBHI.2021.3049119","Shanghai Science and Technology Program(grant numbers:20511100600); National Natural Science Foundation of China(grant numbers:62076094); Key Lab of Information Network Security of Ministry of Public Security; The Third Research Institute of Ministry of Public Security(grant numbers:C20603); National Key Research and Development Project of Ministry of Science and Technology of China(grant numbers:2018AAA0101302); Natural Science Foundations of China(grant numbers:61806078); National Major Scientific and Technological Special Project; Significant New Drugs Development(grant numbers:2019ZX09201004); Zhejiang Lab(grant numbers:2019ND0AB01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314183","Knowledge distillation;self-attention;teacher-student structure;neural network;EEG emotion recognition","Electroencephalography;Emotion recognition;Feature extraction;Logic gates;Brain modeling;Training;Correlation","Electroencephalography;Emotions;Humans;Neural Networks, Computer","31","","49","IEEE","5 Jan 2021","","","IEEE","IEEE Journals"
"Exploiting Multiple EEG Data Domains with Adversarial Learning","D. Bethge; P. Hallgarten; O. Özdenizci; R. Mikut; A. Schmidt; T. Grosse-Puppendahl","Dr. Ing. h.c. F. Porsche AG, Stuttgartn, Germany; Dr. Ing. h.c. F. Porsche AG, Stuttgartn, Germany; Institute of Theoretical Computer Science, TU Graz, Austria; Karlsruhe Institute of Technology, Karlsruhe, Germany; Ludwig-Maximilians University, Munich, Germany; Dr. Ing. h.c. F. Porsche AG, Stuttgartn, Germany",2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"8 Sep 2022","2022","","","3154","3158","Electroencephalography (EEG) is shown to be a valuable data source for evaluating subjects' mental states. However, the interpretation of multi-modal EEG signals is challenging, as they suffer from poor signal-to-noise-ratio, are highly subject-dependent, and are bound to the equipment and experimental setup used, (i.e. domain). This leads to machine learning models often suffer from poor generalization ability, where they perform significantly worse on real-world data than on the exploited training data. Recent research heavily focuses on cross-subject and cross-session transfer learning frameworks to reduce domain calibration efforts for EEG signals. We argue that multi-source learning via learning domain-invariant representations from multiple data-sources is a viable alternative, as the available data from different EEG data-source domains (e.g., subjects, sessions, experimental setups) grow massively. We propose an adversarial inference approach to learn data-source invariant representations in this context, enabling multi-source learning for EEG-based brain- computer interfaces. We unify EEG recordings from different source domains (i.e., emotion recognition datasets SEED, SEED-IV, DEAP, DREAMER), and demonstrate the feasibility of our invariant representation learning approach in suppressing data- source-relevant information leakage by 35% while still achieving stable EEG-based emotion classification performance.","2694-0604","978-1-7281-2782-8","10.1109/EMBC48229.2022.9871743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9871743","adversariallearning;domain invariance;EEG","Training;Emotion recognition;Soft sensors;Transfer learning;Training data;Brain modeling;Electroencephalography","Brain-Computer Interfaces;Electroencephalography;Emotions;Humans;Machine Learning;Signal-To-Noise Ratio","6","","22","IEEE","8 Sep 2022","","","IEEE","IEEE Conferences"
"Attention-Based Temporal Graph Representation Learning for EEG-Based Emotion Recognition","C. Li; F. Wang; Z. Zhao; H. Wang; B. W. Schuller","College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; Department of Computer Science, Zhejiang University College, Hangzhou, China; Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany",IEEE Journal of Biomedical and Health Informatics,"3 Oct 2024","2024","28","10","5755","5767","Due to the objectivity of emotional expression in the central nervous system, EEG-based emotion recognition can effectively reflect humans' internal emotional states. In recent years, convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have made significant strides in extracting local features and temporal dependencies from EEG signals. However, CNNs ignore spatial distribution information from EEG electrodes; moreover, RNNs may encounter issues such as exploding/vanishing gradients and high time consumption. To address these limitations, we propose an attention-based temporal graph representation network (ATGRNet) for EEG-based emotion recognition. Firstly, a hierarchical attention mechanism is introduced to integrate feature representations from both frequency bands and channels ordered by priority in EEG signals. Second, a graph convolutional neural network with top-k operation is utilized to capture internal relationships between EEG electrodes under different emotion patterns. Next, a residual-based graph readout mechanism is applied to accumulate the EEG feature node-level representations into graph-level representations. Finally, the obtained graph-level representations are fed into a temporal convolutional network (TCN) to extract the temporal dependencies between EEG frames. We evaluated our proposed ATGRNet on the SEED, DEAP and FACED datasets. The experimental findings show that the proposed ATGRNet surpasses the state-of-the-art graph-based mehtods for EEG-based emotion recognition.","2168-2208","","10.1109/JBHI.2024.3395622","National Natural Science Foundation of China(grant numbers:62071330,61702370,61902282,62322120,U21B2010); Tianjin Postgraduate Scientific Research Innovation Project(grant numbers:2022SKYZ267); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10517398","Affective computing;attention mechan- ism;EEG;emotion recognition;graph convolution network","Electroencephalography;Feature extraction;Emotion recognition;Convolution;Brain modeling;Electrodes;Graph neural networks","Humans;Electroencephalography;Emotions;Signal Processing, Computer-Assisted;Neural Networks, Computer;Algorithms","2","","68","IEEE","2 May 2024","","","IEEE","IEEE Journals"
"Rethinking Saliency Map: A Context-Aware Perturbation Method to Explain EEG-Based Deep Learning Model","H. Wang; X. Zhu; T. Chen; C. Li; L. Song","Academy for Engineering and Technology, Fudan University, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, China; School of Information Science and Technology, Fudan University, China; Academy for Engineering and Technology, Fudan University, China; Academy for Engineering and Technology, Fudan University, Shanghai, China",IEEE Transactions on Biomedical Engineering,"20 Apr 2023","2023","70","5","1462","1472","Deep learning is widely used to decode the electroencephalogram (EEG) signal. However, there are few attempts to specifically study how to explain EEG-based deep learning models. In this paper, we review the related works that attempt to explain EEG-based models. And we find that the existing methods are not perfect enough to explain the EEG-based model due to the non-stationary nature, high inter-subject variability and dependency of EEG data. The characteristics of the EEG data require the explanation to incorporate the instance-level saliency identification and the context information of EEG data. Recently, mask perturbation is proposed to explain deep learning model. Inspired by the mask perturbation, we propose a new context-aware perturbation method to address these concerns. Our method not only extends the scope to the instance level but can capture the representative context information when estimating the saliency map. In addition, we point out another role of context information in explaining the EEG-based model. The context information can also help suppress the artifacts in the EEG-based deep learning model. In practice, some users might want a simple version of the explanation, which only indicates a few features as salient points. To further improve the usability of our method, we propose an optional area limitation strategy to restrict the highlighted region. In the experiment section, we select three representative EEG-based models and implement them on the emotional EEG dataset DEAP. The results of the experiments support the advantages of our method.","1558-2531","","10.1109/TBME.2022.3218116","Shanghai Key Research Lab of NSAI; Joint Lab on Networked AI Edge Computing Fudan University-Changan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9932670","Explaining deep learning;EEG-based deep learning model;perturbation-based method;saliency map","Brain modeling;Deep learning;Electroencephalography;Context modeling;Perturbation methods;Reliability;Data models","Deep Learning;Electroencephalography;Emotions;Artifacts","3","","49","IEEE","31 Oct 2022","","","IEEE","IEEE Journals"
"Time and Frequency Domain Feature Selection Using Mutual Information for EEG-based Emotion Recognition","A. D. Wibawa; N. Fatih; Y. Pamungkas; M. Pratiwi; P. A. Ramadhani; Suwadi","Department of Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia; Department of Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia; Department of Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia; Department of Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia; Department of Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia; Department of Medical Technology, Institut Teknologi Sepuluh Nopember, Surabaya, East Java, Indonesia","2022 9th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)","16 Nov 2022","2022","","","19","24","Emotion Recognition using EEG signals remains a challenging task. Usually, feature extraction and channel selection are determined based on neuro-scientific assumptions. Too many features during the EEG-based human emotion recognition will lead to reduced classification accuracy and consume high computational costs. This study analyzes time and frequency domain features such as Mean, Mean Absolute Value, Standard Deviation, and Power Spectral Density. In this study, an EEG Recording session involved 25 subjects consisting of 12 males and 13 females. Video with two emotions, happy and sad, were stimulated to the subjects. The electrodes were placed in channels F7, F8, FP1, and FP2 based on the 10/20 EEG system. The EEG pre-processing, such as signal filtering, Automatic Artifact Removal EOG, Artifact Subspace Reconstruction, and Independent Component Analysis, were done using MATLAB Toolbox, followed by Infinite Impulse Response with Butterworth was applied to separate the EEG signal into alpha, beta, and gamma sub-band. Therefore, 48 numbers of features were extracted to perform emotion recognition. Mutual Information is used for calculating the degree of importance of each feature. Then, the features were ranked to eliminate features with a minimal contribution. We implemented a Random Forest algorithm to classify human emotions based on the EEG signal. The experimental results show that reducing the number of utilized features from 48 to 12 can increase the accuracy score from 82.61 % to 95.65 %.","","978-623-92135-6-5","10.23919/EECSI56542.2022.9946522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946522","EEG;Emotion Recognition;Feature Selection;Mutual Information;Random Forest","Emotion recognition;Frequency-domain analysis;Feature extraction;Electroencephalography;Recording;Time-domain analysis;Task analysis","","2","","25","","16 Nov 2022","","","IEEE","IEEE Conferences"
"Parallel Sequence-Channel Projection Convolutional Neural Network for EEG-Based Emotion Recognition","L. Shen; W. Zhao; Y. Shi; T. Qin; B. Liu","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China",IEEE Access,"21 Dec 2020","2020","8","","222966","222976","One of the challenges in emotion recognition is finding an effective way to represent spatial-temporal features from EEG. To fully utilize the features on multiple dimensions of EEG signals, we propose a parallel sequence-channel projection convolutional neural network, including temporal stream sub-network, spatial stream sub-network, and fusion classification block. Temporal stream extracts temporal continuity via sequence-projection layer while spatial stream captures spatial correlation via channel-projection layer. Both sequence-projection and channel-projection adopt length-synchronized convolutional kernel to decode whole time and space information. The size of length-synchronized convolutional kernel is equal to the length of transmitted EEG sequence. The fusion classification block combines the extracted temporal and spatial features into a joint spatial-temporal feature vector for emotion prediction. In addition, we present a baseline noise filtering module to amplify input signals and a random channels exchange strategy to enrich the baseline-removed emotional signals. Experimental evaluation on DEAP dataset reveals that the proposed method achieves state-of-the-art classification performance for the binary classification task. The recognition accuracies reach to 96.16% and 95.89% for valence and arousal. The proposed method can improve 3% to 6% than other latest advanced works.","2169-3536","","10.1109/ACCESS.2020.3039542","National Natural Science Foundation of China(grant numbers:61671404,61520106002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9265254","Emotion recognition;multi-channel EEG;data augmentation;length-synchronized convolutional kernel;spatial-temporal feature","Feature extraction;Electroencephalography;Convolution;Emotion recognition;Kernel;Correlation;Task analysis","","12","","59","CCBYNCND","20 Nov 2020","","","IEEE","IEEE Journals"
"Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks","W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Transactions on Autonomous Mental Development,"19 May 2017","2015","7","3","162","175","To investigate critical frequency bands and channels, this paper introduces deep belief networks (DBNs) to constructing EEG-based emotion recognition models for three emotions: positive, neutral and negative. We develop an EEG dataset acquired from 15 subjects. Each subject performs the experiments twice at the interval of a few days. DBNs are trained with differential entropy features extracted from multichannel EEG data. We examine the weights of the trained DBNs and investigate the critical frequency bands and channels. Four different profiles of 4, 6, 9, and 12 channels are selected. The recognition accuracies of these four profiles are relatively stable with the best accuracy of 86.65%, which is even better than that of the original 62 channels. The critical frequency bands and channels determined by using the weights of trained DBNs are consistent with the existing observations. In addition, our experiment results show that neural signatures associated with different emotions do exist and they share commonality across sessions and individuals. We compare the performance of deep models with shallow models. The average accuracies of DBN, SVM, LR, and KNN are 86.08%, 83.99%, 82.70%, and 72.60%, respectively.","1943-0612","","10.1109/TAMD.2015.2431497","National Basic Research Program of China(grant numbers:2013CB329401); Science and Technology Commission of Shanghai Municipality(grant numbers:13511500200); European Union Seventh Framework Program(grant numbers:247619); National Natural Science Foundation of China(grant numbers:61272248); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7104132","Affective computing;deep belief networks;EEG;emotion recognition","Electroencephalography;Feature extraction;Brain modeling;Electrodes;Emotion recognition;Entropy","","1503","1","61","IEEE","8 May 2015","","","IEEE","IEEE Journals"
"MS-FRAN: A Novel Multi-Source Domain Adaptation Method for EEG-Based Emotion Recognition","W. Li; W. Huan; S. Shao; B. Hou; A. Song","School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu, China",IEEE Journal of Biomedical and Health Informatics,"7 Nov 2023","2023","27","11","5302","5313","Electroencephalogram (EEG)-based emotion recognition has gradually become a research hotspot. However, the large distribution differences of EEG signals across subjects make the current research stuck in a dilemma. To resolve this problem, in this article, we propose a novel and effective method, Multi-Source Feature Representation and Alignment Network (MS-FRAN). The effectiveness of proposed method mainly comes from three new modules: Wide Feature Extractor (WFE) for feature learning, Random Matching Operation (RMO) for model training, and Top-$\mathit{h}$ ranked domain classifier selection (TOP) for emotion classification. MS-FRAN is not only effective in aligning the distributions of each pair of source and target domains, but also capable of reducing the distributional differences among the multiple source domains. Experimental results on the public benchmark datasets SEED and DEAP have demonstrated the advantage of our method over the related competitive approaches for cross-subject EEG-based emotion recognition.","2168-2208","","10.1109/JBHI.2023.3311338","Leading Technology of Jiangsu Province(grant numbers:BK20192004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10238715","Electroencephalogram;emotion recognition;multi-source feature representation and alignment network;domain adaptation","Feature extraction;Emotion recognition;Electroencephalography;Brain modeling;Adaptation models;Transfer learning;Representation learning","Humans;Electroencephalography;Benchmarking;Emotions","7","","48","IEEE","4 Sep 2023","","","IEEE","IEEE Journals"
"Robust Emotion Recognition in EEG Signals Based on a Combination of Multiple Domain Adaptation Techniques","A. Mirzaee; M. Kordestani; L. Rueda; M. Saif","Electrical Engineering Islamic Azad University Dariun Branch, Shiraz, Iran; Electrical Engineering University of Windsor, Windsor, ON, Canada; School of Computer Science University of Windsor, Windsor, ON, Canada; Electrical Engineering University of Windsor, Windsor, ON, Canada","2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","29 Jan 2024","2023","","","3265","3270","Conventional classification approaches for EEG- based emotion recognition cannot often adapt to different domains, such as cross-subject or cross-dataset scenarios, leading to poor performance. To handle this challenge, we introduce a novel fusion method using a combination of multiple domain adaptation techniques to improve the emotional states in EEG datasets via classification accuracy. For this aim, Our proposed approach exploits domain adaptation approaches such as Transfer Component Analysis (TCA), Correlation Alignment (CORAL), Transfer Joint Matching (TJM), Geodesic Flow Kernel (GFK), and Joint Distribution Adaptation (JDA), to enhance the overall classification performance. Later, a new fusion approach called Multiple Domain Adaptation based on a Neuro-Fuzzy Inference System (MDA-NF) is applied to combine the classifiers using proper fuzzy membership functions and deliver maximum separation between classes. The main contribution is by applying the fusion approach using MDA- NF technique, adaptability is sufficiently enhanced. Another advantage is to employ multiple adaptation techniques that improve separation between classes. In experimental test results conducted with cross-subject and cross-dataset scenarios, the MDA-NF approach demonstrates superior performance in terms of accuracy for both the valence and arousal aspects, as observed in two public DEAP and DREAMER datasets.","2577-1655","979-8-3503-3702-0","10.1109/SMC53992.2023.10394506","Natural Sciences and Engineering Research Council (NSERC) of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10394506","Domain adaptation;Emotion recognition;fusion approaches;Cross-domain adaptation;EEG","Training;Emotion recognition;Correlation;Electroencephalography;Noise measurement;Task analysis;Kernel","","","","26","IEEE","29 Jan 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition from EEG: Self-Attention and Differentiable Pooling Improve SOGNN Performance","S. Katyal; R. A. Ganesan","Department of Electrical Engineering, Indian Institute of Science, Bengaluru; Department of Electrical Engineering & Centre for Neuroscience, Indian Institute of Science, Bengaluru",2023 IEEE 20th India Council International Conference (INDICON),"27 Feb 2024","2023","","","281","286","This work utilizes differential entropy (DE) feature, functional connectivity, and the relationship between different time samples to recognize emotions from the EEG data accurately. This study employs the SEED IV and SEED V datasets, containing EEG corresponding to 4 and 5 emotions, from 15 and 16 subjects, respectively. The work modified the self-organized graph neural network (SOGNN) by (i) employing self-attention modules at different levels, and (ii) using differentiable pooling with some custom modifications. The achieved cross-subject average accuracy on SEED IV data is 77.9±4.1%, which is 2.6% higher than the results obtained by the SOGNN. Further, we ran the SOGNN code on SEED V data, which gave 74.5±7.9% accuracy, whereas our subject-independent result on SEED V data is 79.4±5.2%, which is 4.9% higher.","2325-9418","979-8-3503-0559-3","10.1109/INDICON59947.2023.10440863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440863","SOGNN;EEG;emotion recognition;graph neural network;self-attention;differentiable pooling;SEED V","Electrodes;Emotion recognition;Time-frequency analysis;Feature extraction;Brain modeling;Electroencephalography;Graph neural networks","","","","13","IEEE","27 Feb 2024","","","IEEE","IEEE Conferences"
"TESANet: Self-attention network for olfactory EEG classification","C. Tong; Y. Ding; K. L. Jun Liang; Z. Zhang; H. Zhang; C. Guan","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Wilmar International, Singapore; Agency for Science, Technology and Research, Institute for Infocomm Research, Singapore; Agency for Science, Technology and Research, Institute for Infocomm Research, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",2022 International Joint Conference on Neural Networks (IJCNN),"30 Sep 2022","2022","","","1","7","The olfactory system is known to be associated with emotion during odor stimulation. A well-designed computational model that can correctly recognize preference induced by odor stimulation can be vital in the food and perfume industries. Electroencephalogram (EEG) can be used to study the brain's response to odor stimulation due to its good temporal resolution and low acquisition cost. In this study, we proposed a novel self-attention deep learning framework: Temporal Segment Attention Network (TESANet) to classify the brain state of subjects when they are exposed to pleasant and unpleasant odors. Odor stimulation is a continuous process, the temporal dynamics of the EEG signal should reflect the continuous changes of the brain responses to the given odor, thus we design the model to capture the intercorrelation between time segments of the EEG by utilizing the self-attention mechanism. TESANet consists of a filter-bank layer to extract spectral features, a spatial convolution layer to extract spatial features, a temporal segmentation layer to split the data into overlapping time windows, a Long Short-Term Memory (LSTM) layer to encode the temporal segments, a self-attention layer to decode the temporal dynamics by learning the intercorrelation between time segments, and finally a fully connected layer for classification. Experiments on an olfactory EEG dataset demonstrated that the proposed method outperforms other competing deep learning methods for odor pleasantness classification.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892920","Brain Computer Interface;EEG;Olfactory;Deep Learning;Attention","Deep learning;Industries;Costs;Convolution;Olfactory;Feature extraction;Brain modeling","","3","","42","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Gender Difference in EEG Emotion Recognition with Overlapping Shifting Window","E. S. Pane; D. Risqiwati; A. D. Wibawa; M. H. Purnomo","Ministry of Industry, Industrial Training and Education of Surabaya, Surabaya, Indonesia; Department of Electrical Engineering, Faculty of Intelligent Electrical and Informatics Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Dept. of Computer Engineering, Faculty of Intelligent Electrical and Informatics Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Dept. of Computer Engineering, Faculty of Intelligent Electrical and Informatics Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",2022 Fifth International Conference on Vocational Education and Electrical Engineering (ICVEE),"9 Nov 2022","2022","","","54","59","Gender in HCI has become crucial part due to the rising acknowledgment that computers must understand and adapt to the user's gender and emotional states. Hence, this work analyses the gender difference in emotion recognition based on the EEG signals. This paper used the overlapping shifting window mechanism to improve the emotion classification accuracy. Considering the frequency band in brain signals, we also investigate the critical frequency bands in alpha. Following that, we use PCA to reduce the dataset's dimensionality and utilize SVM to make a binary classification of valence and arousal emotions. We use a public dataset of EEG-based emotions comprising 13 female and 15 male subjects. According to the experiment results, the low $\alpha$ frequency (8-10 Hz) is more reliable for recognizing emotion. As for the epochs of shifting, the shorter the epochs window, the better the emotion classification accuracy. The average results of emotion classification accuracies in females reach 79.4% for valence and 78.05% for arousal, while the males obtain 81.7% for valence and 81.4% for arousal. Females are more affected by the valence emotion than by the arousal mood. In males, however, there is little difference between arousal and valence emotion perception. Furthermore, females have more complex aspects of valence and arousal emotion recognition than males.","","978-1-6654-7581-5","10.1109/ICVEE57061.2022.9930381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930381","gender emotion;overlapping window;valence arousal;alpha band","Support vector machines;Human computer interaction;Electrical engineering;Computers;Emotion recognition;Mood;Education","","1","","24","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Statistical Measures and Auto-Regressive Modeling","A. E. Vijayan; D. Sen; A. P. Sudheer","Robotics Interest Group Mechatronics/Robotics Laboratory, National Institute of Technology Calicut, Kerala, India; Robotics Interest Group Mechatronics/Robotics Laboratory, National Institute of Technology Calicut, Kerala, India; Robotics Interest Group Mechatronics/Robotics Laboratory, National Institute of Technology Calicut, Kerala, India",2015 IEEE International Conference on Computational Intelligence & Communication Technology,"2 Apr 2015","2015","","","587","591","In this paper, a novel approach towards classification of various human emotions based on statistically weighed autoregressive modeling of Electroencephalogram (EEG) signals is discussed. The proposed algorithm was proven to be superior to many related works, in distinguishing different emotions such as happiness, fear, sadness etc. The findings discussed are based on the results obtained using benchmark emotion based EEG database called DEAP. In this work, epochs were extracted from data using statistical measures such as Shannon Entropy and higher order auto-regressive model was fit to the extracted features. The model was used for classifying human emotions by feeding it into a multi-class Support Vector Machine (MCSVM). The proposed algorithm is proven to be more efficient than existing algorithms as a classification accuracy of 94.097% was obtained.","","978-1-4799-6023-1","10.1109/CICT.2015.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7078771","EEG;Shannon Entropy;Auto-regression;Multi Class SVM","Electroencephalography;Emotion recognition;Entropy;Brain modeling;Accuracy;Feature extraction;Signal processing algorithms","","50","","39","IEEE","2 Apr 2015","","","IEEE","IEEE Conferences"
"EVNCERS: An Integrated Eigenvector Centrality-Variational Nonlinear Chirp Mode Decomposition-Based EEG Rhythm Separation for Automatic Emotion Recognition","K. S. Kamble; J. Sengupta","Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India",IEEE Sensors Journal,"14 Sep 2023","2023","23","18","21661","21669","Affective computing, which focuses on identifying emotions from physiological data, namely electroencephalography (EEG) is becoming increasingly significant. However, direct analysis of EEG is highly challenging due to its nonlinear and nonstationary character. Various EEG rhythms provide a reliable method for the automatic recognition of emotions. Therefore, an integrated eigenvector centrality-variational nonlinear chirp mode decomposition-based EEG rhythm separation (EVNCERS) is developed. For selecting the dominant channels, the eigenvector centrality method (EVCM) is used followed by variational nonlinear chirp mode decomposition (VNCMD) to retrieve the instantaneous frequency (IF) and instantaneous amplitude (IA) from EEG signals. The equivalent IA and IF are used to create the delta, theta, alpha, beta, and gamma rhythms. The rhythms are analyzed over several entropy-based features, chosen using statistical analysis [mean and standard deviation (STD)], then categorized using various machine-learning methods. The proposed EVNCERS has achieved the highest performance of accuracy and F1-score for arousal: (96.86%, 97.98%), valence: (96.87%, 97.82%), and dominance: (96.81%, 97.71%) using random rotation forest classifier. The performance revealed that the delta rhythm offered more insight into automatic emotion recognition. The DREAMER dataset results demonstrate that our model has the highest predictive ability, with area-under-the-curve (AUC) values of 0.98 for arousal and dominance and 0.99 for the valence category. The SEED dataset also shows a similar trend, with the delta rhythm achieving the highest accuracy of 87.25% and F1-score of 74.54%. The proposed EVNCERS model can help in real-time situations to automatically recognize affective emotions, which would give us a greater range of emotional states.","1558-1748","","10.1109/JSEN.2023.3304891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223717","Affective emotion recognition;channel selection;eigenvector centrality method (EVCM);electroencephalography (EEG);machine learning (ML) models;rhythms separation;variational nonlinear chirp mode decomposition","Electroencephalography;Rhythm;Feature extraction;Emotion recognition;Chirp;Brain modeling;Transforms","","9","","60","IEEE","17 Aug 2023","","","IEEE","IEEE Journals"
"EEG–Based Emotion Classification Using Convolutional Neural Networks","W. Kristianto; H. Candra","Faculty of Industrial Technology, Universitas Trisakti, Jakarta, Indonesia; Faculty of Industrial Technology, Universitas Trisakti, Jakarta, Indonesia",2019 2nd International Conference on Applied Engineering (ICAE),"13 Oct 2020","2019","","","1","4","Research in recognizing emotion has been done with many methods, one of them by using brain wave or electroencephalography (EEG). The main benefit of using EEG is because of its ability to track events within brain in millisecond accuracy. Psychologist proposed a model to help classify human emotions, which can be divided into four quadrants using two dimensional emotions of arousal and valence. In this paper, Convolutional Neural Network was proposed as a method for recognizing emotion in the EEG data due to its advantages, such as: local connections, shared weights, pooling and using series of layer to handle high dimensional data of EEG. The overfitting problem in EEG dataset arising from insufficient sample data has been successfully addressed using data augmentation process with effective window size of 4 seconds. The best model is achieved with the accuracy of 72% for arousal and 71% for valence.","","978-1-7281-2807-8","10.1109/ICAE47758.2019.9221673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9221673","EEG;Convolutional Neural Network","","","","","11","IEEE","13 Oct 2020","","","IEEE","IEEE Conferences"
"Stable Feature Selection for EEG-based Emotion Recognition","Z. Lan; O. Sourina; L. Wang; Y. Liu; R. Scherer; G. R. Müller-Putz","Fraunhofer Singapore, Singapore; Fraunhofer Singapore, Singapore; School of EEE Nanyang Technological University Singapore; Fraunhofer Singapore, Singapore; Institute of Neural Engineering Graz University of Technology Graz, Austria; Technische Universitat Graz, Graz, Steiermark, AT",2018 International Conference on Cyberworlds (CW),"27 Dec 2018","2018","","","176","183","Affective brain-computer interface (aBCI) introduces personal affective factors into human-computer interactions, which could potentially enrich the user's experience during the interaction with a computer. However, affective neural patterns are volatile even within the same subject. To maintain satisfactory emotion recognition accuracy, the state-of-the-art aBCIs mainly tailor the classifier to the subject-of-interest and require frequent re-calibrations for the classifier. In this paper, we demonstrate that the recognition accuracy of aBCIs deteriorates when re-calibration is ruled out during the long-term usage for the same subject. Then, we propose a stable feature selection method to choose the most stable affective features, for mitigating the accuracy deterioration to a lesser extent and maximizing the aBCI performance in the long run. We validate our method on a dataset comprising six subjects' EEG data collected during two sessions per day for each subject for eight consecutive days.","","978-1-5386-7315-7","10.1109/CW.2018.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8590037","Electroencephalography (EEG), stable feature, feature selection, emotion recognition, intra correlation coefficient (ICC)","Feature extraction;Electroencephalography;Emotion recognition;Correlation coefficient;Fractals;Data collection","","4","","38","IEEE","27 Dec 2018","","","IEEE","IEEE Conferences"
"Facial Expressions and EEG Signals for Emotion Recognition Using Deep2 Face Algorithm","S. Swetha; N. V. Babu","Division of Computer Science and Engineering, Karunya Institute of Technology and Science, Coimbatore, Tamil Nadu, India; Division of Data Science and Cyber Security, Karunya Institute of Technology and Science, Coimbatore, Tamil Nadu, India",2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS),"23 Oct 2024","2024","1","","2245","2250","Emotions, the essence of human connection and understanding, take center stage in this pioneering research on emotion recognition. We look at identifying common emotions including joy, sorrow, neutral, anger, fear, and surprise. Introducing an innovative approach, we seamlessly blend advanced computer vision algorithms and intricate signal processing techniques to concurrently analyze facial expressions and EEG signals. The heart of our methodology lies in the utilization of the state-of- the-art Deep Face Analyzer, guided by the deep2face algorithm for meticulous interpretation of facial features, coupled with a spectrographic analyzer translating EEG signals into spectrograms. The overarching goal is to enhance the accuracy and resilience of emotion recognition, with the added dimension of a real-time Waveform Extractor for dynamic evaluation of audio signals. Within the landscape of diverse algorithms and modalities, the Hybrid Model (CNN + LSTM) emerges as a standout performer, achieving an impressive 88.2% accuracy by synergizing facial expressions and EEG signals. The strategic integration of long short-term memory (LSTM) networks with convolutional neural networks (CNNs) underscores the model's exceptional capacity to capture the nuanced intricacies of human emotions. Beyond mere technological advancement, this study underscores the importance of embracing multiple modalities for a holistic understanding of emotions, positioning the Hybrid Model as a powerful and nuanced tool for precise and multiclass emotion identification across diverse real-world scenarios. In essence, our research charts a path where the realm of emotions converges with technological innovation, fostering a deeper understanding of the human experience","2575-7288","979-8-3503-8436-9","10.1109/ICACCS60874.2024.10717306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10717306","Emotion Recognition;Facial Expressions;EEG signals;Feature Extraction;Feature Fusion;Deep Learning Detection;Machine Learning;Hybrid model","Emotion recognition;Accuracy;Face recognition;Signal processing algorithms;Brain modeling;Electroencephalography;Real-time systems;Convolutional neural networks;Long short term memory;Resilience","","","","18","IEEE","23 Oct 2024","","","IEEE","IEEE Conferences"
"Advancing Emotional Health Assessments: A Hybrid Deep Learning Approach Using Physiological Signals for Robust Emotion Recognition","A. Waheed Awan; I. Taj; S. Khalid; S. Muhammad Usman; A. S. Imran; M. Usman Akram","Department of Computer Engineering, Bahria University, Islamabad, Pakistan; College of Interdisciplinary Studies, Zayed University, Abu Dhabi, United Arab Emirates; Department of Computer Engineering, Bahria University, Islamabad, Pakistan; Department of Computer Science, Bahria School of Engineering and Applied Sciences, Bahria University, Islamabad, Pakistan; Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, Norway; Department of Computer and Software Engineering, College of Electrical and Mechanical Engineering, National University of Sciences and Technology (NUST), Islamabad, Pakistan",IEEE Access,"4 Oct 2024","2024","12","","141890","141904","Emotional health significantly impacts physical and psychological well-being, with emotional imbalances and cognitive disorders leading to various health issues. Timely diagnosis of mental illnesses is crucial for preventing severe disorders and enhancing medical care quality. Physiological signals, such as Electrocardiograms (ECG) and Electroencephalograms (EEG), which reflect cardiac and neuronal activities, are reliable for emotion recognition as they are less susceptible to manipulation than physical signals. Galvanic Skin Response (GSR) is also closely linked to emotional states. Researchers have developed various methods for classifying signals to detect emotions. However, these signals are susceptible to noise and are inherently non-stationary, meaning they change constantly over time. Consequently, emotions can vary rapidly. Traditional techniques for analyzing physiological signals may not be adequate to study the dynamic changes in emotional states. This research introduces a deep learning approach using a combination of advanced signal processing and machine learning to analyze physiological signals for emotion recognition. We propose a CNN-Vision Transformer (CVT) based method with ensemble classification. The process involves decomposing signals into segments, removing noise, and extracting features using 1D CNN and Vision Transformers. These features are integrated into a single vector for classification by an ensemble of LSTM, ELM, and SVM classifiers. The outputs are then synthesized using Model Agnostic Meta Learning (MAML) to improve prediction accuracy. Validated on AMIGOS and DEAP datasets with 10-fold cross-validation, our method achieved accuracies up to 98.2%, sensitivity of 99.15%, and specificity of 99.53%, outperforming existing emotion charting techniques. This novel method provides significant improvements 3 to 4% in the accuracy, sensitivity, and specificity of emotion detection, leveraging physiological signals for comprehensive emotional assessments.","2169-3536","","10.1109/ACCESS.2024.3463746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684185","Emotion charting;emotion recognition;1D CNN;vision transformer (ViT);model agnostic meta learning (MAML);long-short time memory (LSTM);extreme machine learning (ELM)","Physiology;Feature extraction;Emotion recognition;Electroencephalography;Electrocardiography;Brain modeling;Noise measurement;Convolutional neural networks;Metalearning;Long short term memory;Machine learning","","2","","54","CCBYNCND","19 Sep 2024","","","IEEE","IEEE Journals"
"Emotion Recognition Method Based on EEG in Few Channels","X. Deng; X. Lv; P. Yang; K. Liu; K. Sun","College of Computer Science and Technology, Chongqing University of Posts and Telecommunication, Chongqing, China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunication, Chongqing, China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunication, Chongqing, China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunication, Chongqing, China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunication, Chongqing, China",2022 IEEE 11th Data Driven Control and Learning Systems Conference (DDCLS),"26 Aug 2022","2022","","","1291","1296","EEG acquired by wearable devices can effectively monitor human emotion. Some traditional methods based on some artificial designed features and deep learning technology have achieved good results in emotion recognition based on the EEG signals. However, in the actual experimental process, there are still some problems, such as the high number of channels, the feature redundancy and so on. These challenges hinder the application of the emotion recognition in portable wearable devices. This paper proposes a 4-channel method to achieve the classification accuracy in more channels (62 channels in SEED dataset and 14 channels in our dataset). In this paper, a parallel signal processing method, which is based on the intrinsic time scale decomposition (ITD), discrete wavelet transform (DWT), variational mode decomposition (VMD) and phase space reconstruction(PSR), is proposed to obtain more modes of signals. The differential entropy for each mode is extracted as the feature, and the linear dynamic system is used to smooth the feature. Based on the artificial features, this paper designs a simple CNN model with fewer parameters to complete the classification task. It also applies this method to SEED dataset and our own dataset, and carries out three types of experiments. Experimental results show that the average accuracy of the method in 4 channels can reach the performance of more channels. Due to the versatility of this method, it is expected to be more widely used in wearable devices with weak computing power under 4 channels.","2767-9861","978-1-6654-9675-9","10.1109/DDCLS55054.2022.9858390","Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-msxmX0284); Chongqing Municipal Education Commission(grant numbers:KJQN202000625); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858390","4-Channel EEG;emotion recognition;wearable EEG devices;parallel signal processing;simple CNN","Training;Emotion recognition;Wearable computers;Transforms;Feature extraction;Brain modeling;Electroencephalography","","1","","30","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Emotion recognition and brain mapping for sentiment analysis: A review","K. Hulliyah; N. S. Awang Abu Bakar; A. R. Ismail","Syarif Hidayatullah State Islamic University (UIN), Indonesia; International Islamic University Malaysia, Kuala Lumpur, Wilayah Persekutuan, MY; International Islamic University Malaysia (IIUM), Malaysia",2017 Second International Conference on Informatics and Computing (ICIC),"5 Feb 2018","2017","","","1","5","The rapid growth of the Internet has caused the increase in the amount of textual information available, such as in blogs, discussion forums and review sites on the web, where the texts surely have the emotion content. Emotion is one appearence of people behaviour and it is an important performance in human computer interaction (HCI). Human express the emotion in the form of facial expression, speech and writing text. Recently, researchers in computational linguistic (CL) areas are interested in the attention of emotion for Sentiment Analysis (SA). SA naturally observes the emotion conveyed by a text, and at the same time, distinguishing positive and negative valence. The wide areas of CL research, actually considerable for investigating the emotion dimension detection and searching the approaches and techniques in the term of emotion recognition (ER). There are two significant trends of research in the area, the emotion recognition based on state affective computing and the real time using brain signal machines. The two areas have the same aim for getting the improvement result in sentiment analysis with the mapping of emotion recognition provided. The exclusive work on emotion detection is comparatively rare and lacks empirical evaluation research. This paper provides the overview of past and recent research on emotion detection as well as some approaches and techniques used and shows the linked between both SA and ER.","","978-1-5386-2985-7","10.1109/IAC.2017.8280568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280568","Emotion recognition;brain mapping;sentiment analysis;affective computing","Emotion recognition;Sentiment analysis;Electroencephalography;Affective computing;Brain mapping;Hidden Markov models","","28","","32","IEEE","5 Feb 2018","","","IEEE","IEEE Conferences"
"Investigation of EEG Signals for Emotion Recognition of People for Brain-Computer Interface","R. H. Jibon; M. Maniruzzaman","Electronics and Communication Engineering Discipline, Khulna University, Khulna, Bangladesh; Electronics and Communication Engineering Discipline, Khulna University, Khulna, Bangladesh",2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),"3 Nov 2021","2021","","","1","6","EEG-based emotion recognition has recently been taken a large space for several applications that are related to brain-computer interface. Machine learning algorithms make this process easier. This paper investigates how the EEG signals are changed with eight different classes of emotions, namely joy, relax, sad, fear, sleepy, calm, happy, and excited. The EEG dataset provided by ‘DEAP’ has been used for this purpose. The raw data is preprocessed, then some features are extracted using PCA technique, and finally, these features are used for classification purposes using CNN, SVM, KNN, AdaBoost, NB, and RF classifiers for different emotional states. An average accuracy of 99.84%, 98.06%, 98.98% for valence, and 99.53%, 97.89%, 98.51% for arousal states are respectively obtained for CNN, SVM, and RF classifiers. Besides the accuracies of 96.86%, 98.13%, 98.20% for dominance, and 98.61%, 98.28%, 98.20% for liking states are respectively obtained for CNN, SVM, and RF classifiers. Among all the mentioned classifiers CNN provides a better classification accuracy on an average of 99.68% comparing with SVM and RF in valence and arousal states.","","978-1-7281-8595-8","10.1109/ICCCNT51525.2021.9579764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9579764","BCI;CNN;EEG;emotion recognition;RF;SVM","Support vector machines;Radio frequency;Emotion recognition;Machine learning algorithms;Feature extraction;Brain modeling;Electroencephalography","","","","30","IEEE","3 Nov 2021","","","IEEE","IEEE Conferences"
"Domain-Adaptive Emotion Recognition Based on Horizontal Vertical Flow Representation of EEG Signals","Z. Bai; Z. Li; Z. Li; Y. Song; Q. Gao; Z. Mao","Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, TUT Maritime College, Tianjin University of Technology, Tianjin, China; Technical College for the Deaf, Tianjin University of Technology, Tianjin, China",IEEE Access,"8 Jun 2023","2023","11","","55023","55034","With the development of cognitive science and brain science, brain-computer interface technology can use Electroencephalogram (EEG) signals to better represent the inner changes of emotions. In this paper, A video-induced emotional stimulation experimental paradigm was designed, and the EEG signals of 15 hearing-impaired subjects under three emotions (positive, neutral, and negative) were collected. Considering the flow diffusion properties of EEG signals, we used the diffusion effect based on horizontal representation and vertical representation forms to obtain the spatial domain features. After EEG preprocessing, the differential entropy feature (DE) in the frequency domain is extracted. The frequency domain features of 62 channels are delivered to two Bi-directional Long Short-Term Memory (BiLSTM) to obtain spatial domain features of horizontal and vertical representations respectively, and then two kinds of domain features are fused by the residual network. The attention mechanism is applied to effectively extract emotional representational information from the fused features. To solve the cross-subject problem of emotion recognition, the domain adaptation method is utilized, and a center alignment loss function is applied to increase the distance of inter-class and reduce the distance of intra-class. According to the experimental results, the average accuracies of 75.89% (subject- dependent) and 69.59% (cross-subject) are obtained. Moreover, the validation was also performed on the public dataset SEED, achieving average accuracies of 93.99% (subject-dependent) and 84.22% (cross-subject), respectively.","2169-3536","","10.1109/ACCESS.2023.3270977","National Natural Science Foundation of China(grant numbers:62103299); 2022 Tianjin Postgraduate Research and Innovation Project(grant numbers:2022SKYZ249); Tianjin University of Technology Graduate Program(grant numbers:YJ2226); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10109720","EEG signals;emotion recognition;domain adaptation;deep learning","Electroencephalography;Emotion recognition;Feature extraction;Frequency-domain analysis;Motion pictures;Brain modeling;Physiology","","3","","37","CCBYNCND","27 Apr 2023","","","IEEE","IEEE Journals"
"Single-trial event-related potential emotional classification based on compressed sensing","X. Zhang; F. Li; J. Chang; L. Huang; Y. Sun; S. Duan","College of Information Engineering, Taiyuan University of Technology Taiyuan, China; College of Information Engineering, Taiyuan University of Technology Taiyuan, China; College of Information Engineering, Taiyuan University of Technology Taiyuan, China; College of Information Engineering, Taiyuan University of Technology Taiyuan, China; College of Information Engineering, Taiyuan University of Technology Taiyuan, China; College of Information Engineering, Taiyuan University of Technology Taiyuan, China",2017 International Conference on Orange Technologies (ICOT),"12 Apr 2018","2017","","","172","175","In this study, a robust classification method for emotional speech single-trial event-related potential (ERP) signal was developed. The classification method based on compression sensing (CS) theory. First, we use CS theory to reduce the dimensionality of the ERP signal. Second, the ERP signal was reconstructed by using K-SVD method to construct the over-complete redundant dictionary. Finally, the ERP signal was classified by calculating the residuals between the reconstructed samples and the test samples. The experimental results show that the proposed algorithm can effectively classify the noisy ERP signal and avoid the feature extraction process in the signal recognition.","","978-1-5386-3276-5","10.1109/ICOT.2017.8336115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8336115","Compression Sensing;EEG;Event-related Potential;Signal Classification","Dictionaries;Electroencephalography;Classification algorithms;Matching pursuit algorithms;Pattern classification;Training;Electronic mail","","1","","8","IEEE","12 Apr 2018","","","IEEE","IEEE Conferences"
"AsEmo: Automatic Approach for EEG-Based Multiple Emotional State Identification","S. -H. Kim; H. -J. Yang; N. A. T. Nguyen; S. -W. Lee","Department of Brain and Cognitive Engineering, Korea University, Seoul, Korea; Department of Computer Science, Chonnam National University, Gwangju, South Korea; Faculty of Information Technology, The University of DaNang – University of Science and Education, DaNang, Viet Nam; Department of Artificial Intelligence, Korea University, Seoul, Korea",IEEE Journal of Biomedical and Health Informatics,"11 May 2021","2021","25","5","1508","1518","An electroencephalogram (EEG) is the most extensively used physiological signal in emotion recognition using biometric data. However, these EEG data are difficult to analyze, because of their anomalous characteristic where statistical elements vary according to time as well as spatial-temporal correlations. Therefore, new methods that can clearly distinguish emotional states in EEG data are required. In this paper, we propose a new emotion recognition method, named AsEmo. The proposed method extracts effective features boosting classification performance on various emotional states from multi-class EEG data. AsEmo Automatically determines the number of spatial filters needed to extract significant features using the explained variance ratio (EVR) and employs a Subject-independent method for real-time processing of Emotion EEG data. The advantages of this method are as follows: (a) it automatically determines the spatial filter coefficients distinguishing emotional states and extracts the best features; (b) it is very robust for real-time analysis of new data using a subject-independent technique that considers subject sets, and not a specific subject; (c) it can be easily applied to both binary-class and multi-class data. Experimental results on real-world EEG emotion recognition tasks demonstrate that AsEmo outperforms other state-of-the-art methods with a 2-8% improvement in terms of classification accuracy.","2168-2208","","10.1109/JBHI.2020.3032678","National Research Foundation of Korea(grant numbers:NRF-2018R1A2B6006046); Korea University; National Research Foundation of Korea(grant numbers:NRF2020R1A4A1019191); Department of Artificial Intelligence, Korea University(grant numbers:2019-0-00079); Institute of Information and Communications Technology Planning and Evaluation(grant numbers:2015-0-00185); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234607","Electroencephalogram;emotion recognition;explained variance ratio;feature extraction;multi-class common spatial pattern;subject-independent","Feature extraction;Electroencephalography;Emotion recognition;Covariance matrices;Real-time systems;Data mining;Matrix decomposition","Biometry;Electroencephalography;Emotions;Humans;Research Design","13","","51","IEEE","21 Oct 2020","","","IEEE","IEEE Journals"
"A Computation Resource Friendly Convolutional Neural Network Engine For EEG-based Emotion Recognition","Y. Zhan; M. I. Vai; S. Barma; S. H. Pun; J. W. Li; P. U. Mak","State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau, China; State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau, China; Department of Electronics and Communication Engineering, Indian Institute of Information Technology Guwahati (IIITG), Guwahati, India; State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau, China; State Key Laboratory of Analog and Mixed-Signal VLSI, University of Macau, Macau, China; Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau",2019 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA),"20 Apr 2020","2019","","","1","6","EEG-based Emotion recognition is a crucial link in Human-Computer Interaction (HCI) application. Nowadays, Convolutional Neural Network (CNN) and its related CNN-hybrid approaches have achieved the state-of-art accuracy in this field. However, most of these existing techniques employ large-scale neural networks which cause performance bottleneck in portable systems. Moreover, traditional convolution kernel confuses EEG multiple frequency bands information, which is critical for investigating emotion status. To improve these issues, firstly, we extract power spectral features from four frequency bands (θ,α,β,γ) and transform obtained features into cortex-like frames while preserving spatial information of electrodes position, so that the multi-channel, multi-frequency bands and time series EEG signals can be efficiently represented. Then, we design a shallow depthwise parallel CNN inspired by Mobilenet technique to learn spatial representation from labeled frames. Segment-level emotion recognition experiments are implemented to verify the proposed architecture with DEAP database. Our approach achieves the competitive accuracy of 84.07% and 82.95% on arousal and valence respectively. Besides, the experimental results prove the computation-effectiveness of the proposed method. Compared with the state-of-art approach, our approach saves 69.23% GPU memory and reduces 30% GPU peak utilization with only 6.5% accuracy drop. Therefore, our method shows extensive application prospects for EEG-based emotion recognition on resource-limited devices.","2377-9322","978-1-5386-8344-6","10.1109/CIVEMSA45640.2019.9071594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9071594","EEG;emotion recognition;deep learning;CNN;depth-wise convolution;parallel convolution neural network","Convolution;Electroencephalography;Feature extraction;Brain modeling;Emotion recognition;Electrodes;Computational modeling","","8","","22","IEEE","20 Apr 2020","","","IEEE","IEEE Conferences"
"Classification of emotions based on ERP feature extraction","M. Goyal; M. Singh; M. Singh","EIED, Thapar University, Patiala, India; EIED, Thapar University, Patiala, India; EIED, Thapar University, Patiala, India",2015 1st International Conference on Next Generation Computing Technologies (NGCT),"11 Jan 2016","2015","","","660","662","Emotions are the feelings that represent the personality of any individual. Thus predicting emotions become necessary to understand the behavior of humans. Emotions can be predicted from gestures, sound processing but emotion recognition using EEG signals is very powerful method to know the internal state of mind accurately. This paper describes the acquisition of EEG signals on frontal electrodes such as F3, F4 and FZ from five subjects for classification of emotions into two classes. The emotions were induced by showing images from International Affective Picture System (IAPS) dataset to the subjects. The event related potential (ERP) features were determined from the processed EEG signals for every class of emotions. The classification was performed using LIBSVM classifier with 3 fold cross validation and RBF kernel to classify emotions into two classes along the arousal axis. It was found that accuracy remained consistently high on F4 electrode. An accuracy of 79.16% was obtained on F4 electrode, 76.19% on F3 electrode and 73.07% on FZ electrode when classifying emotions subject wise.","","978-1-4673-6809-4","10.1109/NGCT.2015.7375203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375203","EEG;Emotions;Event realted potential(ERP) Library for Support vector machines(LIBSVM)","Electroencephalography;Electrodes;Emotion recognition;Biomedical imaging;MATLAB;Libraries","","8","","15","IEEE","11 Jan 2016","","","IEEE","IEEE Conferences"
"A channel selection method for EEG classification in emotion assessment based on synchronization likelihood","K. Ansari-Asl; G. Chanel; T. Pun","Computer Science Department, University of Geneva, Carouge-Geneva, Switzerland; Computer Science Department, University of Geneva, Carouge-Geneva, Switzerland; Computer Science Department, University of Geneva, Carouge-Geneva, Switzerland",2007 15th European Signal Processing Conference,"4 May 2015","2007","","","1241","1245","When assessing human emotion using EEG classification, one of the critical problems is to deal with the very large number of features to be classified. In this paper, we address this problem using synchronization likelihood as a new channel selection method. Applying this method, we could significantly reduce the number of EEG channels to be used in emotion assessment, with only slight (if any) loss of classification performance depending on the used feature. We report and compare the results obtained by employing a linear classifier on different features extracted either from all channels or from the selected subset of channels. These features include synchronization likelihood, Hjorth parameters, and fractal dimension.","","978-839-2134-04-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7099003","","Electroencephalography;Synchronization;Fractals;Scalp;Accuracy;Complexity theory;Physiology","","7","","21","","4 May 2015","","","IEEE","IEEE Conferences"
"Development and Validation of an EEG-Based Real-Time Emotion Recognition System Using Edge AI Computing Platform With Convolutional Neural Network System-on-Chip Design","W. -C. Fang; K. -Y. Wang; N. Fahier; Y. -L. Ho; Y. -D. Huang","Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electronics Engineering, National Chiao Tung University, Hsinchu, Taiwan",IEEE Journal on Emerging and Selected Topics in Circuits and Systems,"11 Dec 2019","2019","9","4","645","657","This study proposed an electroencephalogram (EEG)-based real-time emotion recognition hardware system architecture based on multiphase convolutional neural network (CNN) algorithm implemented on a 28-nm technology chip and on field programmable gate array (FPGA) for binary and quaternary classification. Sample entropy, differential asymmetry, short-time Fourier transform, and a channel reconstruction method were used for emotion feature extraction. In this work, six EEG channels were selected (FP1, FP2, F3, F4, F7, and F8), and EEG images were generated from spectrogram fusions. The complete CNN architecture included training and acceleration for efficient artificial intelligence (AI) edge application, and we proposed a multiphase CNN execution method to accommodate hardware resource constraints. Datasets of 32 subjects from the DEAP database were used to validate the proposed design, exhibiting mean accuracies for valance binary classification and valance-arousal quaternary classification of 83.36% and 76.67%, respectively. The core area and total power consumption of the CNN chip were 1.83 x 1.83 mm2, respectively, and 76.61 mW. The chip operation was validated using ADVANTEST V93000 PS1600, and the training process and real-time classification processing time took 0.12495 ms and 0.02634 ms for each EEG image, respectively. The proposed EEG-based realtime emotion recognition system included a dry electrode EEG headset, feature extraction processor, CNN chip platform, and graphical user interface, and the execution time costed 450 ms for each emotional state recognition.","2156-3365","","10.1109/JETCAS.2019.2951232","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8890664","Emotion recognition;convolutional neural network (CNN);system-on-chip;electroencephalography;affective computing","Electroencephalography;Emotion recognition;Real-time systems;Artificial intelligence;Feature extraction;Convolutional neural networks;System-on-chip","","57","","32","IEEE","4 Nov 2019","","","IEEE","IEEE Journals"
"LSTM vs Plot-based CNN for EEG Emotion Detection Tasks","J. Kelnhofer; M. Blaisdell; M. Ghandi","Wearable Computing REU Program, Washington State University, Pullman, WA, USA; Wearable Computing REU Program, Washington State University, Pullman, WA, USA; Wearable Computing REU Program, Washington State University, Pullman, WA, USA","2021 IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)","4 Feb 2022","2021","","","121","123","Emotion detection using machine learning and data gathered from an electroencephalogram (EEG) holds the potential for architecture and creating smart adaptive spaces which can respond to the user's current emotional state detected from the Neurophysiological data in real-time. This technology can help people with mental and physical disabilities to have a greater role in shaping their environment and live more independent lives. In this paper, two different machine learning approaches, the Long Short Term memory network, (LSTM) and Convolutional Neural Network (CNN) are compared in order to assess their potential to satisfy this goal of emotion detection. The LSTM network was trained on eight-channel time-series data which had undergone a Fast Fourier Transform, and the CNN was trained on the un-transformed data in the form of a unique plot-image based approach.","","978-1-6654-3965-7","10.1109/CHASE52844.2021.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9697929","component;CNN;LSTM;EEG;Fast-Fourier Transform;Short-time Fourier Transform","Emotion recognition;Electric potential;Fast Fourier transforms;Machine learning;Electroencephalography;Real-time systems;Convolutional neural networks","","2","","4","IEEE","4 Feb 2022","","","IEEE","IEEE Conferences"
"Developing a Physiological Signal-Based, Mean Threshold and Decision-Level Fusion Algorithm (PMD) for Emotion Recognition","Q. Zhang; H. Zhang; K. Zhou; L. Zhang","College of Computer Science, Sichuan University, Chengdu, China; August International Ltd., Hoddesdon, UK; August International Ltd., Hoddesdon, UK; College of Computer Science, Sichuan University, Chengdu, China",Tsinghua Science and Technology,"6 Jan 2023","2023","28","4","673","685","With the development of computers, artificial intelligence, and cognitive science, engagement in deep communication between humans and computers has become increasingly important. Therefore, affective computing is a current hot research topic. Thus, this study constructs a Physiological signal-based, Mean-threshold, and Decision-level fusion algorithm (PMD) to identify human emotional states. First, we select key features from electroencephalogram and peripheral physiological signals, and use the mean-value method to obtain the classification threshold of each participant and distinguish individual differences. Then, we employ Gaussian Naive Bayes (GNB), Linear Regression (LR), Support Vector Machine (SVM), and other classification methods to perform emotion recognition. Finally, we improve the classification accuracy by developing an ensemble model. The experimental results reveal that physiological signals are more suitable for emotion recognition than classical facial and speech signals. Our proposed mean-threshold method can solve the problem of individual differences to a certain extent, and the ensemble learning model we developed significantly outperforms other classification models, such as GNB and LR.","1007-0214","","10.26599/TST.2022.9010038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011177","electroencephalogram (EEG);peripheral physiological signals;machine learning;emotion recognition;multimodal fusion","Support vector machines;Computers;Emotion recognition;Linear regression;Gaussian processes;Speech recognition;Brain modeling","","22","","52","","6 Jan 2023","","","TUP","TUP Journals"
"EEG Emotion Recognition based on Hierarchy Graph Convolution Network","F. Zheng; B. Hu; S. Zhang; Y. Li; X. Zheng","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Key Laboratory of TCM Data Cloud Service in Universities of Shandong Shandong Management University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China",2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"14 Jan 2022","2021","","","1628","1632","Emotion recognition has become a research focus in the field of human-computer interaction (HCI). As an excellent physiological signal, electroencephalographic (EEG) is considered to be a favorable tool for emotion recognition. Most traditional methods focus on extracting features in time domain and frequency domain but the adjacent information and asymmetric information from adjacent and asymmetric channels are often ignored. Although several graph neural network (GNN) models are utilized to learn EEG features, most of the emotion recognition studies of GNN ignore the information existing between adjacent electrodes. In this paper, we propose an EEG emotion recognition method based on hierarchy graph convolution network (HGCN) named ERHGCN. Firstly, six different features including power spectral density (PSD), differential entropy (DE), differential asymmetry (DASM), rational asymmetry (RASM), asymmetry (ASM) and differential caudality (DCAU) from five frequency bands are extracted. Secondly, to improve graph convolution network (GCN) shortcoming of only extracting time and frequency features, HGCN is applied to extract deeper spatial feature by treating the longitudinal and transverse adjacent electrode pairs in different ways. Finally, six extracted features are fed into the HGCN model, then all features are integrated by two full connection layers. We conducted extensive experiments on DEAP dataset and experimental results show that the proposed method can obtain 90.56% and 88.79% recognition accuracies for valence and arousal classification tasks.","","978-1-6654-0126-5","10.1109/BIBM52615.2021.9669465","Natural Science Foundation of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669465","EEG;Emotion recognition;HGCN;Adjacent feature;Asymmetric feature","Electrodes;Human computer interaction;Emotion recognition;Time-frequency analysis;Convolution;Feature extraction;Brain modeling","","11","","13","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"Research on EEG signal recognition based on channel selection","Q. Meng; J. Yan; H. Xu","College of Electronic Information and Control Enginering, Beijing University of Technology Beijing, China; College of Electronic Information and Control Enginering, Beijing University of Technology Beijing, China; College of Electronic Information and Control Enginering, Beijing University of Technology Beijing, China",2017 Chinese Automation Congress (CAC),"1 Jan 2018","2017","","","6413","6417","Emotional recognition as the key technology in the field of emotion computing has received more and more attentions in applications such as human-computer interaction, medical-assisted diagnosis and multimedia intelligence recommendation, and it has important research and application value. EEG recognition based on EEG is a commonly used and effective method of emotion recognition. More and more scholars are concerned with the emotional recognition of specific brain regions and specific frequency bands in the study of emotion recognition based on EEG. They will be based on the distance between the brain electrode and the principle of symmetry divided into different regions to extract EEG characteristics, follow-up identification study. But this way ignores the emotional correlation and difference between the channels, which in turn affects the EEG feature representation and recognition effect. In this paper, we propose a channel selection method based on the maximum correlation minimum redundancy idea, and then it obtains 10 ideal channels for the futher EEG emotional analysis.","","978-1-5386-3524-7","10.1109/CAC.2017.8243933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8243933","maximum correlation minimum redundancy;channel selection","Electroencephalography;Correlation;Electrodes;Emotion recognition;Redundancy;Feature extraction;Physiology","","3","","6","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"Reducing the Calibration Effort of EEG Emotion Recognition using Domain Adaptation with Soft Labels","Z. Li; H. Chen; M. Jin; J. Li","HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, China; HwaMei Hospital, University of Chinese Academy of Sciences, Ningbo, China",2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"9 Dec 2021","2021","","","5962","5965","Electroencephalogram (EEG)-based emotion recognition has made great progress in recent years. The current pipelines collect EEG training data in a long-time calibration session for each new subject, which is time consuming and user unfriendly. To reduce the time required for the calibration session, there have been many studies using domain adaptation (DA) approaches to transfer knowledge from existing subjects (source domain) to the new subject (target domain) for reducing the dependence on the calibration session. Existing DA methods usually require substantial unlabeled EEG data of the new subject. However, the real scenario is that there are a small number of labeled samples in the calibration session of the target. Motivated by this, we introduce a novel domain adaptation architecture based on adversarial training to learn domain-invariant feature representations across subjects. To improve the performance when there are few labeled EEG data in the calibration session, we add a soft label loss to the architecture, which can ensure that the inter-class relationships learned from the source domain are transferred to target domain. We evaluate the method on the SEED dataset, and the experimental results show that our method uses only 15 examples per trial in the calibration session to achieve an average accuracy of 87.28%, indicating the effectiveness of our framework.","2694-0604","978-1-7281-1179-7","10.1109/EMBC46164.2021.9629649","University of Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629649","","Training;Emotion recognition;Correlation;Pipelines;Training data;Feature extraction;Electroencephalography","Adaptation, Physiological;Algorithms;Calibration;Electroencephalography;Emotions;Humans","1","","14","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition for Multi Channel Fast Empirical Mode Decomposition using VGG-16","M. A. Asghar; Fawad; M. J. Khan; Y. Amin; A. Akram","Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Telecommunication Engineering, University of Engineering and Technology, Taxila, Pakistan",2020 International Conference on Engineering and Emerging Technologies (ICEET),"30 Mar 2020","2020","","","1","7","Much attention has been paid to the recognition of human emotions with the help of EEG signals based on machine learning methods. Human Emotion recognition is yet a difficult task to perform due to the non-linear property of the EEG signals. This paper presents an advanced signal processing method using the deep neural function to extract features using VGG-16 from all channels related to emotion. To reduce the computational costs of emotion recognition and achieve better results, this article presents a Fast Empirical mode decomposition (FEMD) model which significantly reduce the feature size for fast processing. In the proposed method, we convert the signal into a two-dimensional wavelet spectrogram and calculate the characteristics of each subject. An EEG-based emotional classification model using a Deep Neural Network (DNN) model is proposed on the SJTU SEED and DEAP datasets. Random Forest, SVM and k-NN are used to classify data into positive / negative / neutral dimensions for SEED data sets and Arousal/Valence dimensions for DEAP dataset. The proposed model achieves better accuracy on the SEED and DEAP datasets, as compared to other advanced methods of human emotion recognition.","2409-2983","978-1-7281-4581-5","10.1109/ICEET48479.2020.9048217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9048217","Emotion recognition;Brain Computer Interface;Machine Learning;DNN;Fast Empirical Mode Decomposition","Electroencephalography;Emotion recognition;Empirical mode decomposition;Continuous wavelet transforms;Brain modeling;Electrodes;Discrete wavelet transforms","","10","","40","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"Fixed low-rank EEG spatial filter estimation for emotion recognition induced by movies","K. Yano; T. Suyama","Department of Dynamic Brain Imaging Cognitive Mechanisms Laboratories, Advanced Telecommunications Research Institute International, Seika-cho, Soraku-gun, Kyoto, JAPAN; Department of Dynamic Brain Imaging Cognitive Mechanisms Laboratories, Advanced Telecommunications Research Institute International, Seika-cho, Soraku-gun, Kyoto, JAPAN",2016 International Workshop on Pattern Recognition in Neuroimaging (PRNI),"1 Sep 2016","2016","","","1","4","In this paper, we propose a fixed low-rank spatial filter estimation for brain computer interface (BCI) systems with an application that recognizes emotions elicited by movies. The proposed approach unifies such tasks as feature extraction, feature selection, and classification, which are often independently tackled in a ""bottom-up"" manner, under a regularized loss minimization problem. We explicitly derive the loss function from the conventional BCI approach and solve its minimization by optimization with a non-convex fixed low-rank constraint. For evaluation, we conducted an experiment to induce emotions by movies for dozens of young adult subjects and estimated the emotional states using the proposed method. Our results show competitive performance against conventional methods using CSP. The advantage of the proposed method is the holistic approach, which combines feature extraction, feature selection and classification. The obtained results are also plausible from the standpoint of neurophysiological interpretation.","","978-1-4673-6530-7","10.1109/PRNI.2016.7552327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552327","","Electroencephalography;Feature extraction;Error analysis;Motion pictures;Emotion recognition;Covariance matrices;Optimization","","3","","9","IEEE","1 Sep 2016","","","IEEE","IEEE Conferences"
"Time-Varying Graph Signal Processing Based Cross-Subject Emotion Classification from Multi-Electrode EEG Signals","S. Anand; V. Kumar Chakka; P. Mathur","Dept. of Electrical Engineering, Shiv Nadar University, Greater Noida, India; Dept. of Electrical Engineering, Shiv Nadar University, Greater Noida, India; Dept. of Electrical Engineering, Shiv Nadar University, Greater Noida, India",2022 IEEE 19th India Council International Conference (INDICON),"16 Feb 2023","2022","","","1","6","Emotion classification plays an important role in the domain of human-computer interaction (HCI). In this paper, a novel framework for learning emotion-specific brain functional connectivity from EEG signals, with blockwise time-varying graph signal processing (GSP), is proposed. Graph corresponding to the last temporal block, which captures the spatio-temporal smoothness from all of the previous blocks is considered for extracting the Laplacian-based graph spectral features. The deviation range of the eigenvalues and their ratio corresponding to low-frequency and high-frequency components are proposed in the form of a convex sum feature. This feature is further utilized to classify cross-subject based positive and negative emotions using the KNN classifier. Simulation results on the benchmark DREAMER database validate the performance of the proposed method with metrics of accuracy, sensitivity, and specificity and are comparable with the existing state-of-the-art techniques.","2325-9418","978-1-6654-7350-7","10.1109/INDICON56171.2022.10040075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040075","Electroencephalogram(EEG);Human-Computer Interaction (HCI);Graph Signal Processing(GSP);Time-varying graph;Spatio-temporal smoothness;KNN classifier","Human computer interaction;Measurement;Simulation;Signal processing algorithms;Signal processing;Sensitivity and specificity;Feature extraction","","","","20","IEEE","16 Feb 2023","","","IEEE","IEEE Conferences"
"Emotion Feature Analysis and Recognition Based on Reconstructed EEG Sources","G. Chen; X. Zhang; Y. Sun; J. Zhang","College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan, China",IEEE Access,"22 Jan 2020","2020","8","","11907","11916","Emotion plays a significant role in perceiving external events or situations in daily life. Due to ease of use and relative accuracy, Electroencephalography (EEG)-based emotion recognition has become a hot topic in the affective computing field. However, scalp EEG is a mixed-signal and cannot directly indicate the exact information about active cortex sources of different emotions. In this paper, we analyze the significant differences of active source regions and frequency bands for pairs of emotions-based reconstructed EEG sources using sLORETA, and 26 Brodmann areas are selected as the regions of interest (ROI). And then, six kinds of time- and frequency-domain features from significant active regions and frequency bands are extracted to classify different emotions using support vector machines. Furthermore, we compare the classification performances of emotion features extracted from active source regions and EEG sensors. We have demonstrated that the features from selected source regions can improve the classification accuracy by extensive experiments on the DEAP and TYUT 2.0 EEG-based datasets.","2169-3536","","10.1109/ACCESS.2020.2966144","National Natural Science Foundation of China(grant numbers:61371193); Natural Science Foundation for Young Scientists of Shanxi Province, China(grant numbers:201701D221117); Taiyuan University of Technology(grant numbers:2016QN24); Key Research and Development Project of Shanxi Province, China(grant numbers:201803D31045); Scientific and Technological Innovation Project in Higher Education Institutions of Shanxi Province, China(grant numbers:2019L0189); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957051","Emotion recognition;EEG source reconstruction;inverse solution;difference analysis of active source;time- and frequency-domain features","Electroencephalography;Feature extraction;Emotion recognition;Brain modeling;Scalp;Sensors;Computational modeling","","44","","40","CCBY","13 Jan 2020","","","IEEE","IEEE Journals"
"Comparison of Baseline Reduction Methods for Emotion Recognition Based On Electroencephalogram Signals","I. M. Agus Wirawan; R. Wardoyo; D. Lelono; S. Kusrohmaniah; S. Asrori","Education of Informatics Engineering Department, Universitas Pendidikan Ganesha, Bali, Indonesia; Department of Computer Science and Electronics, Universitas Gadjah Mada, Yogyakarta, Indonesia; Department of Computer Science and Electronics, Universitas Gadjah Mada, Yogyakarta, Indonesia; Department of Psychology, Universitas Gadjah Mada, Yogyakarta, Indonesia; Faculty of Social and Political Sciences, UIN Syarif Hidayatullah, Jakarta, Indonesia",2021 Sixth International Conference on Informatics and Computing (ICIC),"9 Dec 2021","2021","","","1","7","Emotions play an essential role in human social interactions. Its importance has sparked research on emotion recognition mainly based on electroencephalogram signals. However, differences in individual characteristics significantly affect the electroencephalogram signal pattern and impact the emotion recognition process. Several studies have used the baseline reduction approach with the Difference method to represent the differences in individual characteristics on electroencephalogram signals. On the other hand, the baseline reduction process on signal data, in general, can also use the Relative Difference and Fractional Difference methods. Therefore, the contribution of this research is to compare the performance of the three baseline reduction methods on emotion recognition based on electroencephalogram signals. In this study, feature extraction and representation were also carried out using Differential Entropy and 3D Cube. Furthermore, Convolutional Neural Network and Decision Tree methods are used to classify emotions. The experimental results using the DEAP dataset show that the Relative Difference and Fractional Difference methods are superior in reducing the baseline electroencephalogram signal compared to the Difference method. In addition, the Relative Difference and Fractional Difference methods produce a smoother electroencephalogram signal pattern in the baseline reduction process.","","978-1-6654-2155-3","10.1109/ICIC54025.2021.9632948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9632948","emotion recognition;baseline reduction methods;difference method;relative difference;fractional difference;electroencephalogram","Emotion recognition;Three-dimensional displays;Feature extraction;Electroencephalography;Entropy;Decision trees;Convolutional neural networks","","1","","29","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"An Automated Framework for Human Emotion Detection From Multichannel EEG Signals","A. Nalwaya; K. Das; R. B. Pachori","Department of Electrical Engineering, Indian Institute of Technology at Indore, Indore, Madhya Pradesh, India; Department of Electrical Engineering, Indian Institute of Technology at Indore, Indore, Madhya Pradesh, India; Department of Electrical Engineering, Indian Institute of Technology at Indore, Indore, Madhya Pradesh, India",IEEE Sensors Journal,"28 Jun 2024","2024","24","13","20920","20927","This article presents an electroencephalogram (EEG) rhythm-based novel approach for emotion recognition. Recognizing multiple classes of emotion has been a challenging task, and several attempts have been made earlier to recognize emotion. The proposed work presents a simplistic and efficient framework for emotion recognition. Instead of using different methods for signal quality enhancement and signal component extraction, the current study focuses on a single advanced signal processing method which addresses the above mentioned issue. A joint time–frequency (JTF) domain-based feature is proposed. The proposed joint features help in estimating the effect of emotion elicitation over the time–frequency distribution of each rhythm calculated across all the channels. Additionally, channel-wise separated EEG rhythm features are extracted, and these features are used to determine the emotional state using a machine learning (ML) model. In EEG, several oscillatory rhythms exist which reflect the brain’s neural activity. The current study assesses changes in EEG rhythms due to audiovisual elicitation. Four classes of emotion, namely happy, sad, fear, and neutral, are studied in this article. The subject-wise mean accuracy obtained is 95.91%. The proposed framework uses a multivariate variational mode decomposition (MVMD) method to separate the raw signal into various EEG rhythms. Also, it has been found that higher frequency rhythms have more information related to emotion than the lower frequency rhythms. A simplistic approach with good accuracy makes the proposed methodology significant.","1558-1748","","10.1109/JSEN.2024.3398050","Council of Scientific and Industrial Research funded Research Project, Government of India(grant numbers:22/20/EMR-II); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534195","Electroencephalogram (EEG);emotion recognition;joint time–frequency (JTF) analysis;multivariate variational mode decomposition (MVMD);rhythms","Electroencephalography;Emotion recognition;Feature extraction;Rhythm;Databases;Recording;Electrodes","","1","","38","IEEE","17 May 2024","","","IEEE","IEEE Journals"
"Automatic Accuracy versus Complexity Characterization for Embedded Emotion-Sensing Platforms in Healthcare Applications","G. Mezzina; D. De Venuto","Dept. of Electrical and Information Engineering, Politecnico di Bari, Bari, Italy; Dept. of Electrical and Information Engineering, Politecnico di Bari, Bari, Italy","2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)","10 Aug 2022","2022","","","676","683","In this paper, we present a novel framework to realize an automatic “accuracy versus complexity” characterization for embeddable emotion recognition systems in healthcare applications. This framework is based on a series of grid searches able to identify highly descriptive features from the input signals, while taking into account implementation ease, memory usage, and classification performance. This paper is articulated on a proof of concept, in which the proposed framework is used to extract an emotion recognition architecture able to discriminate up to 8 emotions, by employing the users' cortical activity and a 3D model for the emotion characterization. The implemented series of grid searches lead to a final low-memory, low-resources and low-complexity emotions recognition processing chain suitable for embedded platforms. The processing chain extracted by the proposed framework has been implemented as a real-time task on a personal care robot processing core. The extracted emotion recognition system tested on cortical signals from a publicly available dataset, showed an accuracy of ~75 % (average) in discriminating 8 emotions and a memory reduction of 70% from a canonical implementation of the same feature extraction step.","0730-3157","978-1-6654-8810-5","10.1109/COMPSAC54236.2022.00116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9842581","Emotions recognition;EEG;Machine Learning;Feature Extraction;Feature Selection","Emotion recognition;Solid modeling;Medical services;Feature extraction;Brain modeling;Electroencephalography;Complexity theory","","","","40","IEEE","10 Aug 2022","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Classification with Per Class Feature Importance Using Multi-Domain Features","K. Devi; B. Deka","Dept. of ECE, Tezpur University, Tezpur, India; Dept. of ECE, Tezpur University, Tezpur, India","2023 International Conference on Computational Intelligence, Networks and Security (ICCINS)","7 Mar 2024","2023","","","1","6","Emotion dysfunction, characterized by an elevated level of negative emotions compared to positive emotions, can often be indicative of neurological disorders such as depression. This paper introduces an efficient approach for emotion recognition using the electroencephalogram (EEG) signals. Experiments are conducted using the DEAP [1] dataset. This study basically focused on finding the most significant features from multiple domains and then select necessary emotion classes based on the per-class feature importance function (PCFIF) to reduce computational complexity and enhance recognition accuracy. The Support Vector Machine (SVM) classifier with leave-one-out cross-validation (LOOCV) and 10-fold cross-validation methods are applied to achieve an average accuracy of 72.72 %.","","979-8-3503-1379-6","10.1109/ICCINS58907.2023.10450043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450043","Affective computing;electroencephalogram;emotion classification","Support vector machines;Neurological diseases;Emotion recognition;Feature extraction;Electroencephalography;Security;Principal component analysis","","","","13","IEEE","7 Mar 2024","","","IEEE","IEEE Conferences"
"A Bayesian Graph Neural Network for EEG Classification — A Win-Win on Performance and Interpretability","J. Wang; X. Ning; W. Shi; Y. Lin","School of Computer and Information Technology, Beijing Jiaotong University, China; School of Computer and Information Technology, Beijing Jiaotong University, China; School of Computer and Information Technology, Beijing Jiaotong University, China; School of Computer and Information Technology, Beijing Jiaotong University, China",2023 IEEE 39th International Conference on Data Engineering (ICDE),"26 Jul 2023","2023","","","2126","2139","With the deepening of neuroscience research, data mining of brain signals is becoming an emerging topic. Among various brain signals, electroencephalography (EEG) has attracted more and more attention due to its advantages of non-invasiveness, portability, and low cost. EEG modeling and analysis play a vital role in human healthcare. Although many machine learning algorithms have been successfully applied to data mining of EEG signals, few of them achieve a win-win in classification performance and interpretability. In this paper, we propose a Bayesian graph neural network named BayesEEGNet. Considering an electrical impulse between two nodes in the brain as a Poisson process, the countless electrical impulses generated by the brain in a period are represented as an infinite number of connection probability graphs. After coupling and transforming these probability graphs, we interpret the brain’s electrical activity state as the brain’s perceptual state. Benefiting from the joint optimization of Bayesian modules and deep neural networks, our model shows superior classification performance in sleep stage classification and emotion recognition tasks. Meanwhile, our model is able to learn interpretable functional connectivity relationships between EEG channels without any prior knowledge.","2375-026X","979-8-3503-2227-9","10.1109/ICDE55515.2023.00165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10184782","Physiological time series mining;bayesian deep learning;EEG classification;sleep stage classification;emotion recognition","Analytical models;Neuroscience;Time series analysis;Brain modeling;Electroencephalography;Physiology;Graph neural networks","","4","","68","IEEE","26 Jul 2023","","","IEEE","IEEE Conferences"
"Performance Comparison of EEG Channels in Emotion Recognition","S. Kul; P. O. Durdu; O. Akbulut","Kocaeli Universitesi, Kocaeli, TR; Kocaeli Universitesi, Kocaeli, TR; Kocaeli Universitesi, Kocaeli, TR",2019 27th Signal Processing and Communications Applications Conference (SIU),"22 Aug 2019","2019","","","1","4","Electroencephalogram (EEG) is one of the low-cost Brain Computer Interface (BBA) methods that understands the emotional and cognitive activity of the users and maps them to a meaningful signal. In the EEG method, which is commonly used for emotion recognition, the number of channels may affect the emotion recognition performance. In this work, the performance of different number of EEG channels on emotion recognition has been explored, in detail. Experimental findings show that a better emotion classification performance can be achieved with fewer channels.","2165-0608","978-1-7281-1904-5","10.1109/SIU.2019.8806538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806538","emotion recognition;EEG channel selection;classification","Electroencephalography;Emotion recognition;Brain modeling;Signal processing;Brain-computer interfaces;Feature extraction;Computer science","","3","","","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Virtual Image from EEG to Recognize Appropriate Emotion using Convolutional Neural Network","M. R. Islam; M. Ahmad","Electrical and Electronic Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Electrical and Electronic Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh","2019 1st International Conference on Advances in Science, Engineering and Robotics Technology (ICASERT)","19 Dec 2019","2019","","","1","4","Electroencephalogram (EEG) that measures the electrical activity of the brain has been used extensively to recognize emotion. Normally feature based emotion recognition requires a strong effort to design the perfect feature or feature set related to the classification of emotion. To curtail the manual human effort we designed a model by using a virtual image from EEG with Convolutional Neural Network (CNN). Initially, we calculated Pearson's correlation coefficients form different sub-bands of EEG to formulate a virtual image. Later, this virtual image was fed into a CNN architecture to classify emotion. We made two distinct protocols; between these, protocol-1 was to classify positive and negative emotion and protocol-2 was to classify four distinct emotions. An overall maximum accuracy of 81.51% on valence and 79.42% on arousal was obtained by using internationally authorized DEAP dataset. Our proposed method is helpful in recognizing emotions efficiently.","","978-1-7281-3445-1","10.1109/ICASERT.2019.8934760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8934760","EEG;Convolutional Neural Network;emotion;correlation;virtual image","Electroencephalography;Emotion recognition;Convolution;Convolutional neural networks;Correlation;Brain modeling;Correlation coefficient","","8","","11","IEEE","19 Dec 2019","","","IEEE","IEEE Conferences"
"EEG Channel Analysis for Energy Data Visualizations Using GAF Encoding and Deep Learning","O. F. Kucukler; A. Amira; H. Malekmohamadi","Institute of Artificial Intelligence, De Montfort University, Leicester, UK; Department of Computer Science, University of Sharjah, UAE; Institute of Artificial Intelligence, De Montfort University, Leicester, UK","2023 International Conference on Electrical, Communication and Computer Engineering (ICECCE)","27 Feb 2024","2023","","","1","6","Electroencephalography (EEG) is a technique used to assess brain activity during cognitive tasks. EEG signals are generated by a variety of sensors, and due to their inherent complexity, analyzing EEGs can be difficult to retrieve task-related information. To improve the performance of EEG applications, selecting useful EEG channels and minimizing irrelevant channel information can improve the quality of analysis and results. This study focuses on the classification performance evaluation of EEG channels. A unique EEG dataset containing EEG signals is used in response to energy data visualization stimuli to capture users' emotions. To analyze valence emotion in data visualizations, EEG signals are transformed into Gramian Angular Fields (GAF) images, and Deep Learning (DL) features retrieved from GAFs using the Convolutional Neural Network (CNN) -Long-Short Term Memory (LSTM) model. To evaluate valence emotion, a boosting classifier is utilized using a DL feature set reduced by a Principal Component Analysis (PCA) approach. The results demonstrate the success of the technique employed to identify the best channels. The F3 channel has the highest accuracy of 99.76%, while the Fp2 channel has a comparable accuracy of 99.52%. Overall, the suggested technique shows promising outcomes in identifying valence emotion for energy data representations.","","979-8-3503-6969-4","10.1109/ICECCE61019.2023.10442121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10442121","Brain-computer interface;Data visualization;Energy;Electroencephalography;Gramian Angular Fields;Deep Learning","Deep learning;Data visualization;Electroencephalography;Sensors;Convolutional neural networks;Task analysis;Principal component analysis","","","","28","IEEE","27 Feb 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition from Brain Signals Using Hybrid Adaptive Filtering and Higher Order Crossings Analysis","P. C. Petrantonakis; L. J. Hadjileontiadis","Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece",IEEE Transactions on Affective Computing,"6 Jan 2011","2010","1","2","81","97","This paper aims at providing a new feature extraction method for a user-independent emotion recognition system, namely, HAF-HOC, from electroencephalograms (EEGs). A novel filtering procedure, namely, Hybrid Adaptive Filtering (HAF), for an efficient extraction of the emotion-related EEG-characteristics was developed by applying Genetic Algorithms to the Empirical Mode Decomposition-based representation of EEG signals. In addition, Higher Order Crossings (HOCs) analysis was employed for feature extraction realization from the HAF-filtered signals. The introduced HAF-HOC scheme incorporated four different classification methods to accomplish a robust emotion recognition performance. Through a series of facial-expression image projection, as a Mirror Neuron System-based emotion elicitation process, EEG data related to six basic emotions (happiness, surprise, anger, fear, disgust, and sadness) have been acquired from 16 healthy subjects using three EEG channels. Experimental results from the application of the HAF-HOC to the collected EEG data and comparison with previous approaches have shown that the HAF-HOC scheme clearly surpasses the latter in the field of emotion recognition from brain signals for the discrimination of up to six distinct emotions, providing higher classification rates up to 85.17 percent. The promising performance of the HAF-HOC surfaces the value of EEG signals within the endeavor of realizing more pragmatic, affective human-machine interfaces.","1949-3045","","10.1109/T-AFFC.2010.7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557842","EEG;emotion recognition;EMD;genetic algorithms;higher order crossings analysis;hybrid adaptive filtering;mirror neuron system.","Electroencephalography;Emotion recognition;Feature extraction;Brain;Neurons","","206","1","69","IEEE","26 Aug 2010","","","IEEE","IEEE Journals"
"Effect-size-based electrode and feature selection for emotion recognition from EEG","R. Jenke; A. Peer; M. Buss","Institute of Automatic Control Engineering, Technische Universität München, Germany; Institute of Automatic Control Engineering, Technische Universität München, Germany; Institute of Automatic Control Engineering, Technische Universität München, Germany","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","1217","1221","Emotion recognition from EEG signals allows the direct assessment of the “inner” state of the user which is considered an important factor in Human-Machine-Interaction. Given the vast amount of possible features from scalp recordings and the high variance between subjects, a major challenge is to select electrodes and features that separate classes well. In most cases, this decision is made based on neuro-scientific knowledge. We propose a statistically-motivated electrode/feature selection procedure, based on Cohen's effect size f2. We compare inter- and intra-individual selection on a self-recorded database. Classification is evaluated using quadratic discriminant analysis (QDA). We found both feature selection versions based on f2 yield comparable results. While highest accuracies up to 57,5% (5 classes) are reached by applying intra-individual selection, inter-individual analysis successfully finds features that perform with lower variance in recognition rates across subjects than combinations of electrodes/features suggested in literature.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6637844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637844","Emotion Recognition;EEG;Feature Selection;Machine Learning","Electrodes;Electroencephalography;Emotion recognition;Feature extraction;Accuracy;Reactive power;Complexity theory","","11","","19","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"Effects of Data Augmentation Method Borderline-SMOTE on Emotion Recognition of EEG Signals Based on Convolutional Neural Network","Y. Chen; R. Chang; J. Guo","College of Information and Computer Engineering, Northeast Forestry University, Harbin, China; College of Information and Computer Engineering, Northeast Forestry University, Harbin, China; College of Information and Computer Engineering, Northeast Forestry University, Harbin, China",IEEE Access,"30 Mar 2021","2021","9","","47491","47502","In recent years, with the continuous development of artificial intelligence and brain-computer interface technology, emotion recognition based on physiological signals, especially electroencephalogram signals, has become a popular research topic and attracted wide attention. However, the imbalance of the data sets themselves, affective features’ extraction from electroencephalogram signals, and the design of classifiers with excellent performance, pose a great challenge to the subject. Motivated by the outstanding performance of deep learning approaches in pattern recognition tasks, we propose a method based on convolutional neural network with data augmentation method Borderline-synthetic minority oversampling technique. First, we obtain 32-channel electroencephalogram signals from DEAP data set, which is the standard data set of emotion recognition. Then, after data pre-processing, we extract features in frequency domain and data augmentation based on the data augmentation algorithm above for getting more balanced data. Finally, we train a one dimensional convolutional neural network for three classification on two emotional dimensions valence and arousal. Meanwhile, the proposed method is compared with some traditional machine learning methods and some existing methods by other researchers, which is proved to be effective in emotion recognition, and the average accuracy rate of 32 subjects on valence and arousal are 97.47% and 97.76% respectively. Compared with other existing methods, the performance of the proposed method with data augmentation algorithm Borderline-SMOTE shows its advantage in affective emotional recognition than that without Borderline-SMOTE.","2169-3536","","10.1109/ACCESS.2021.3068316","National Natural Science Foundation of China(grant numbers:61300098); Natural Science Foundation of Heilongjiang Province(grant numbers:F201347); Fundamental Research Funds for the Central Universities(grant numbers:2572015DY07); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385376","Electroencephalogram;emotion recognition;Borderline-Synthetic minority oversampling technique;convolutional neural network","Feature extraction;Emotion recognition;Electroencephalography;Physiology;Brain modeling;Classification algorithms","","36","","43","CCBYNCND","24 Mar 2021","","","IEEE","IEEE Journals"
"PGCN: Pyramidal Graph Convolutional Network for EEG Emotion Recognition","M. Jin; C. Du; H. He; T. Cai; J. Li","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Research Center for Brain-Inspired Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; No. 2 Hospital, Ningbo, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China",IEEE Transactions on Multimedia,"21 Aug 2024","2024","26","","9070","9082","Emotion recognition is essential in the diagnosis and rehabilitation of various mental diseases. In the last decade, electroencephalogram (EEG)-based emotion recognition has been intensively investigated due to its prominative accuracy and reliability, and graph convolutional network (GCN) has become a mainstream model to decode emotions from EEG signals. However, the electrode relationship, especially long-range electrode dependencies across the scalp, may be underutilized by GCNs, although such relationships have been proven to be important in emotion recognition. The small receptive field makes shallow GCNs only aggregate local nodes. On the other hand, stacking too many layers leads to over-smoothing. To solve these problems, we propose the pyramidal graph convolutional network (PGCN), which aggregates features at three levels: local, mesoscopic, and global. First, we construct a vanilla GCN based on the 3D topological relationships of electrodes, which is used to integrate two-order local features; Second, we construct several mesoscopic brain regions based on priori knowledge and employ mesoscopic attention to sequentially calculate the virtual mesoscopic centers to focus on the functional connections of mesoscopic brain regions; Finally, we fuse the node features and their 3D positions to construct a numerical relationship adjacency matrix to integrate structural and functional connections from the global perspective. Experimental results on four public datasets indicate that PGCN enhances the relationship modelling across the scalp and achieves state-of-the-art performance in both subject-dependent and subject-independent scenarios. Meanwhile, PGCN makes an effective trade-off between enhancing network depth and receptive fields while suppressing the ensuing over-smoothing.","1941-0077","","10.1109/TMM.2024.3385676","Guangdong Provincial Key Laboratory of Human Digital Twin(grant numbers:2022B1212010004); National Natural Science Foundation of China(grant numbers:62106248); Ningbo Clinical Research Center for Medical Imaging(grant numbers:2021L003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496191","Electroencephalogram;emotion recognition;graph convolutional network;knowledge-based modelling","Electroencephalography;Emotion recognition;Feature extraction;Electrodes;Convolutional neural networks;Scalp;Knowledge engineering","","13","","57","IEEE","10 Apr 2024","","","IEEE","IEEE Journals"
"Twin Neural Networks for Efficient EEG Signal Classification","H. Pant; S. Soman; Jayadeva; M. Sharma","Department of Electrical Engineering, Indian Institute of Technology, Delhi, India; Department of Electrical Engineering, Indian Institute of Technology, Delhi, India; Department of Electrical Engineering, Indian Institute of Technology, Delhi, India; Department of Electrical Engineering, Indian Institute of Technology, Delhi, India",2018 International Joint Conference on Neural Networks (IJCNN),"14 Oct 2018","2018","","","1","7","Classification of ElectroEncephaloGram (EEG) signals has found several applications in developing Brain Computer Interfaces (BCIs), as well as other clinical and nonclinical applications based on EEG signals. Processing of EEG signals in this context is challenged by its non-stationarity, high dimensionality and the problem of class imbalance for training classifiers, particularly in case of multi-class classification. Our recent work demonstrated the utility of Twin Support Vector Machine (TWSVM) classifiers for robust classification of imbalanced datasets, specifically EEG signal classification. However, the architecture of the TWSVM is not scalable for large datasets as it involves computing the kernel and matrix inversion operations. In this paper, we present an application of the recently proposed neural network architecture for the Twin SVM, the Twin Neural Network (Twin NN), for robust classification of EEG signals. Results on datasets from BCI competitions illustrate the improved generalization and scalability of the Twin NN for the binary and multi-class classification tasks.","2161-4407","978-1-5090-6014-6","10.1109/IJCNN.2018.8489352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489352","","Electroencephalography;Support vector machines;Biological neural networks;Artificial neural networks;Training;Computer architecture","","9","","29","IEEE","14 Oct 2018","","","IEEE","IEEE Conferences"
"Emotion Classification Using EEG Brain Signals and the Broad Learning System","S. Issa; Q. Peng; X. You","School of Electronic Information and Communication, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communication, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communication, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","18 Nov 2021","2021","51","12","7382","7391","This article presents a new user-independent emotion classification method that classifies four distinct emotions using electroencephalograph (EEG) signals and the broad learning system (BLS). The public DEAP and MAHNOB-HCI databases are used. Just one EEG electrode channel is selected for the feature extraction process. Continuous wavelet transform (CWT) is then utilized to extract the proposed gray-scale image (GSI) feature which describes the EEG brain activation in both time and frequency domains. Finally, the new BLS is constructed for the emotion classification process, which successfully upgrades the efficiency of emotion classification based on EEG brain signals. The experiment results show that the proposed work produces a robust system with high accuracy of approximately 93.1% and training process time of approximately 0.7 s for the DEAP database, as well as, the high average accuracy of approximately 94.4% and training process time of approximately 0.6 s for MAHNOB-HCI database.","2168-2232","","10.1109/TSMC.2020.2969686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8998137","Broad learning system (BLS);continuous wavelet transform (CWT);electroencephalograph (EEG);emotion classification;gray-scale image (GSI)","Electroencephalography;Feature extraction;Electrodes;Databases;Support vector machines;Continuous wavelet transforms;Training data","","53","","49","IEEE","13 Feb 2020","","","IEEE","IEEE Journals"
"Transfer components between subjects for EEG-based emotion recognition","W. -L. Zheng; Y. -Q. Zhang; J. -Y. Zhu; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2015 International Conference on Affective Computing and Intelligent Interaction (ACII),"7 Dec 2015","2015","","","917","922","Addressing the structural and functional variability between subjects for robust affective brain-computer interface (aBCI) is challenging but of great importance, since the calibration phase for aBCI is time-consuming. In this paper, we propose a subject transfer framework for electroencephalogram (EEG)-based emotion recognition via component analysis. We compare two state-of-the-art subspace projecting approaches called transfer component analysis (TCA) and kernel principle component analysis (KPCA) for subject transfer. The main idea is to learn a set of transfer components underlying source domain (source subjects) and target domain (target subject). When projected to this subspace, the difference of feature distributions of both domains can be reduced. From the experiments, we show that the two proposed approaches, TCA and KPCA, can achieve an improvement on performance with the best mean accuracies of 71.80% and 79.83%, respectively, in comparison of the baseline of 58.95%. The significant improvement shows the feasibility and efficiency of our approaches for subject transfer emotion recognition from EEG signals.","2156-8111","978-1-4799-9953-8","10.1109/ACII.2015.7344684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344684","emotion recognition;transfer learning;EEG","Electroencephalography;Kernel;Emotion recognition;Feature extraction;Matrices;Calibration;Training","","44","","26","IEEE","7 Dec 2015","","","IEEE","IEEE Conferences"
"A Grouped Dynamic EEG Channel Selection Method for Emotion Recognition","L. Yang; S. Chao; Q. Zhang; P. Ni; D. Liu","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China",2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"14 Jan 2022","2021","","","3689","3696","EEG signals directly reflect the active state of the brain, so they are widely used for emotion recognition. At present, many researchers have achieved noteworthy results by using multi-channel EEG signals. However, too many EEG channels will cause slow transmission, high experimental costs, and low efficiency. This paper proposed a grouped dynamic EEG channel selection method based on ReliefF and random forest (RF), termed GDCSBR. We divided all channels into four groups, which provided more choices and flexibility to subsequent dynamic channel selection. GDCSBR selected channels iteratively. With the iteration increased, the number of alternative channels decreased. At each iteration, we adopted the strategy with a minor loss of recognition accuracy. Finally, the results on subject-independent data were taken as the final choice, since there were relatively large differences between subjects. Experiments were carried out on the DEAP dataset. The recognition accuracy for valence reaches 81.27% while 10 channels are selected. As for arousal scale, 11 channels can obtain 82.36% of classification accuracy. In addition, we found that high-frequency bands play a crucial part in emotion recognition, and the selected channels were mostly located in the frontal and parietal lobes. These findings are coincident with previous work. Experimental results demonstrate the effectiveness of the proposed method.","","978-1-6654-0126-5","10.1109/BIBM52615.2021.9669889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669889","Emotion recognition;EEG signal;Channel selection;ReliefF;Random forest","Radio frequency;Emotion recognition;Costs;Conferences;Diversity reception;Forestry;Feature extraction","","6","","29","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"A Comparative Analysis of the Brain Lobes and Feature Techniques in the Detection of Emotion","S. Bansal; R. Mohammed; U. Kumar; T. Taori","B.Tech Biomedical Engineering, Ajeenkya DY Patil University, Pune, India; B.Tech Biomedical Engineering, Ajeenkya DY Patil University, Pune, India; School of Engineering, Ajeenkya DY Patil University, Pune, India; School of Engineering, Ajeenkya DY Patil University, Pune, India",2023 3rd International Conference on Intelligent Technologies (CONIT),"7 Aug 2023","2023","","","1","6","The role of emotions in our daily lives is of great importance as they have a significant impact on our behaviour and interactions with others. Numerous attempts and research have been undertaken to classify EEG signals for emotion detection. This study presents a multi-class classification of four emotions, namely happy, sad, neutral, and fear, using various classifiers, along with 5- and 10-folds cross-validation methods. To evaluate which lobe shows the highest accuracy, four lobes of the brain, namely frontal, temporal, parietal, and occipital, were compared by selecting electrodes based on their positions in the brain. The results demonstrate that the parietal lobe resulted in the highest accuracy of 96.9% with 10-fold cross-validation using Fine KNN classifier, while the temporal lobe showed the lowest accuracy of 89.8% with 5-folds cross-validation using Fine KNN classifier. Four commonly used performance metrics (accuracy, recall, precision and F1-score) have been used to evaluate the accuracy and effectiveness of the model. Conducted a comparison between the feature extraction methods using Differential Entropy (DE) and Power Spectral Density (PSD) techniques. One study stated that the highest level of accuracy, at 98.4%, was achieved using data that had been processed using DE features, compared to data processed using PSD features, or a combination of both DE and PSD features. The best-performing algorithms were Fine KNN, Cubic SVM, Fine Gaussian SVM, Weighted KNN, and Ensembled Bagged Trees model. This study highlights the potential of using EEG signals and machine learning algorithms to accurately recognize emotions, which can have applications in various fields such as psychology and neuroscience.","","979-8-3503-3860-7","10.1109/CONIT59222.2023.10205783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205783","Multi-class classification;KNN;Power Spectral Density (PSD);Differential Entropy (DE);SVM;Ensemble","Support vector machines;Emotion recognition;Temporal lobe;Machine learning algorithms;Psychology;Feature extraction;Brain modeling","","","","12","IEEE","7 Aug 2023","","","IEEE","IEEE Conferences"
"EEG-based emotion recognition using nonlinear feature","J. Tong; S. Liu; Y. Ke; B. Gu; F. He; B. Wan; D. Ming","Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China; Department of Biomedical Engineering, Tianjin University, Tianjin, China",2017 IEEE 8th International Conference on Awareness Science and Technology (iCAST),"15 Jan 2018","2017","","","55","59","Emotions are ubiquitous components of everyday life, as they influence behavior to a large extent. And Emotion recognition is one of the most important and necessary parts in the field of emotion research. Its accuracy relies heavily on the ability to generate representative features. However, this is a very challenging problem. In this study, EEG nonlinear features, power spectrum entropy and correlation dimension, were extracted to differentiate emotions. International Affective Picture System (IAPS) pictures with different valence but similar arousal level were used to induce the emotions with 8 valence levels. The results showed that the valence levels were positively correlated with these two features, especially in the frontal lobe. Based on the two features, SVM gave an average accuracy of 82.22%. Analyzing the nonlinear features of EEGs is an efficient way to classify emotions.","2325-5994","978-1-5386-2965-9","10.1109/ICAwST.2017.8256518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8256518","EEG;emotion recognition;power spectrum entropy;correlation dimension","Electroencephalography;Entropy;Correlation;Feature extraction;Emotion recognition;Frequency-domain analysis;Support vector machines","","16","","23","IEEE","15 Jan 2018","","","IEEE","IEEE Conferences"
"BiCCT: A Compact Convolutional Transformer for EEG Emotion Recognition","G. Cao; L. Yang; C. Tang; Q. Zhang; H. He","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China",2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Jan 2024","2023","","","4792","4799","Emotion is a manifestation of human’s internal psychological and physiological reactions. Understanding and recognizing emotions is one of the important ways to understand human behavior and human-computer interaction. However, with the widespread application of deep learning in the field of EEG emotion recognition, the number of parameters and model size have increased accordingly. In this paper, we combined the Bi-hemisphere asymmetry theory and Compact Convolutional Transformer to propose a model named BiCCT to recognize emotions, which has fewer training parameters and can achieve higher recognition performance. We first constructed three different matrices of the recorded EEG information according to the international 10-20 system to preserve the temporal information and spatial information of the EEG signals. Next, we applied an improved Transformer architecture, which achieves fewer model parameters and a lightweight structure through the token pooling module and the Convolutional Tokenizer module. We conducted a set of subject-dependent and a set of subject-dependent shuffle experiments on the DEAP dataset. The first set of experiments used a subject-known movie to predict a completely unknown movie. In the second set of experiments, all the data of the subject were randomly divided into training set and test set. We obtained 67.42% for valence and 67.81% for arousal in the first set of experiments. We achieved 94.41% accuracy in the valence dimension and 95.15% accuracy in the arousal dimension in the second set of experiments. At the same time, our model parameters are only 0.17M, which is far lower than other models. It means that our model is lighter and faster in training speed, and has the ability to be deployed in some scenarios with limited computing resources potential.","2156-1133","979-8-3503-3748-8","10.1109/BIBM58861.2023.10385574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385574","Bi-hemispheric Asymmetry;Affective Computing;EEG;Emotion Classification","Training;Emotion recognition;Convolution;Computational modeling;Biological system modeling;Brain modeling;Transformers","","1","","41","IEEE","18 Jan 2024","","","IEEE","IEEE Conferences"
"Decoding Brain Age: A Self-Supervised Graph Neural Network Framework for EEG Analysis","Z. Cook; C. Zhao; L. Murray; J. Kesan; N. Belacel; S. M. Doesburg; G. Medvedev; V. A. Vakorin; P. Xi","National Research Council Canada, University of Waterloo, Waterloo, Canada; University of Waterloo, National Research Council Canada, Waterloo, Canada; National Research Council Canada, University of Waterloo, Waterloo, Canada; National Research Council Canada, University of Waterloo, Waterloo, Canada; National Research Council Canada, Digital Technologies, Ottawa, Canada; Biomedical Physiology and Kinesiology, Simon Fraser University, Burnaby, Canada; Division of Neurology, Fraser Health Authority, Vancouver, Canada; Biomedical Physiology and Kinesiology, Simon Fraser University, Burnaby, Canada; National Research Council Canada, Digital Technologies, Ottawa, Canada",2024 IEEE SENSORS,"17 Dec 2024","2024","","","1","4","Electroencephalogram (EEG) recordings are valuable for capturing neuro-physiological states, with brain age prediction providing key insights into brain health. To scale this diagnostic technique, we propose a computer-aided system using self-supervised learning (SSL) and Graph Neural Networks (GNNs) for EEG analysis. SSL reduces the need for fully labeled data by pre-training models on large unlabeled EEG datasets. We tackle temporal-spectral feature learning challenges with GNNs, employing graph-based representations of EEG data to depict the brain's interconnectedness and extract meaningful features. Furthermore, we enhance the explainability of brain age predictions by visualizing channel-wise maps, highlighting critical EEG channels Influencing the model's decisions.","2168-9229","979-8-3503-6351-7","10.1109/SENSORS60989.2024.10784646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10784646","EEG Analysis;Brain Age Prediction;Self-Supervised Learning;Graph Neural Networks","Representation learning;Accuracy;Self-supervised learning;Predictive models;Brain modeling;Feature extraction;Electroencephalography;Graph neural networks;Data models;Sensors","","","","29","Crown","17 Dec 2024","","","IEEE","IEEE Conferences"
"Bimodal Physiological Emotion Recognition Using Information Contrastive Learning","B. Xing; Y. Xue; H. Chang","School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China",2023 International Conference on Engineering Education and Information Technology (EEIT),"2 Apr 2024","2023","","","36","40","Recently, several methods have been developed to enhance the performance of bimodal physiological emotion recognition. However, these methods have overlooked the need to identify common features related to emotions and to address the modality discrepancy. To address these issues, we propose a novel method called information contrastive learning (InfoCon), which extracts common features across different modalities and ensures that the extracted common features are highly related to emotions, thereby reducing the modality gap. We have compared the performance of InfoCon with state-of-the-art approaches on two public datasets, SEED-IV (EEG, eye movement) and MPED (EEG, GSR, RSP, ECG). Our results have shown that InfoCon achieves the best recognition accuracy of 92.86% on the SEED-IV dataset and promising results on the MPED dataset for all three protocols. Furthermore, we have visualized the features before and after the InfoCon transformation on the SEED-IV dataset and observed that the transformed features are more discriminative between different emotions. Overall, our proposed method provides a new approach for bimodal physiological emotion recognition by identifying common features and reducing the modality gap.","","979-8-3503-2684-0","10.1109/EEIT58928.2023.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483392","Bimodal physiological Emotion Recognition;Contrastive learning;Common features","Emotion recognition;Visualization;Protocols;Self-supervised learning;Feature extraction;Physiology;Electroencephalography","","","","22","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"A Pursuit of Affective Computing System of EEG Multi-leads Time-Frequency Analysis and Multi-scale Entropy Fusion","X. Wu; W. Shi; C. -H. Yeh","School of Information and Electronics, Beijing Institution of Technology, Beijing, China; School of Information and Electronics, Beijing Institution of Technology, Beijing, China; School of Information and Electronics, Beijing Institution of Technology, Beijing, China","2024 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","12 Feb 2025","2024","","","1","6","Precise electrophysiological feature fusion plays an important role in Brain-Computer Interaction technology. Geared to the critical demands of affective computing, this paper discusses the synergic effect among multiple encephalic regions and diverse emotions based on datasets of the human electroencephalogram (EEG). Based on the database of 14-leads EEG signals of 23 healthy subjects whose emotions are evoked by watching a video clip, we reconcile time-frequency analysis, power spectrum analysis, and multi-scale entropy analysis methods to collect multi-dimensional feature indexes from 5 EEG frequency bands for fusion comparison under 3 different emotion categories and 9 subcategories. Then we summarize the classification basis and carry out verification with statistical significance analysis. It is found that the sampling entropy of the stimuli state is gradually higher than that of the baseline state with the increase of frequency, and the indexes of the 3 kinds of emotion categories or 9 kinds of emotion subcategories are generally significantly different, which could provide an important basis for emotion recognition, and to untangling the time-frequency dynamics of EEG energy and entropy under varying emotions.","","979-8-3315-1566-9","10.1109/ICSIDP62679.2024.10868874","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10868874","affective computing;time-frequency analysis;EEG;multi-scale entropy;ANOVA","Time-frequency analysis;Emotion recognition;Affective computing;Databases;Feature extraction;Entropy;Electroencephalography;Regulation;Indexes;Spectral analysis","","","","16","IEEE","12 Feb 2025","","","IEEE","IEEE Conferences"
"Adaptive Emotional Information Retrieval From EEG Signals in the Time-Frequency Domain","P. C. Petrantonakis; L. J. Hadjileontiadis","Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Greece; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Greece",IEEE Transactions on Signal Processing,"12 Apr 2012","2012","60","5","2604","2616","This paper aims at developing adaptive methods for electroencephalogram (EEG) signal segmentation in the time-frequency domain, in order to effectively retrieve the emotion-related information within the EEG recordings. Using the multidimensional directed information analysis supported by the frontal brain asymmetry in the case of emotional reaction, a criterion, namely asymmetry index , is used to realize the proposed segmentation processes that take into account both the time and frequency (in the empirical mode decomposition domain) emotionally related EEG components. The efficiency of the -based “emotional” filters was justified through an extensive classification process, using higher-order crossings and cross-correlation as feature-vector extraction techniques and a support vector machine classifier for six different classification scenarios in the valence/arousal space. This resulted in mean classification rates from 64.17% up to 82.91% in a user-independent base, revealing the potential of establishing such a filtering for reliable EEG-based emotion recognition systems.","1941-0476","","10.1109/TSP.2012.2187647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6151844","Electroencephalogram (EEG);emotion recognition (ER);empirical mode decomposition;frontal brain asymmetry;multidimensional directed information","Electroencephalography;Feature extraction;Time frequency analysis;Time series analysis;Indexes;Correlation","","73","","34","IEEE","13 Feb 2012","","","IEEE","IEEE Journals"
"The Effects of Word Priming on Emotion Classification from Neurological Signals","C. Schmitz; T. Sweet; D. E. Thompson","Electrical and Computer Engineering Department, Kansas State University, KS, USA; Electrical and Computer Engineering Department, Kansas State University, KS, USA; Electrical and Computer Engineering Department, Kansas State University, KS, USA",2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"8 Sep 2022","2022","","","410","413","Affective states play an important role in human behavior and decision-making. In recent years, several affective brain-computer interface (aBCI) studies have focused on developing an emotion classifier based on elicited emotions within the user. However, it is difficult to achieve consistency in elicited emotions across populations, which can lead to dataset imbalances. The experimental design presented in this paper seeks to avoid consistency issues by asking the participant to classify the emotion portrayed in images of facial expressions, rather than their own emotions. Priming is also a common technique used in psychology studies that is known to influence emotional perception. To improve participant accuracy, we investigated matching and mis-matched word priming for the facial expression images. Electro-encephalogram (EEG) data were used to generate images fed into a classifier based on the Big Transfer model, BiT-M R101x1. The primed images resulted in higher classification accuracy overall. Further, by building different classifier models for both mis-matched primed images and matching primed images, we were able to achieve classification accuracies above 90%. We also provided the classifier with the true labels of the photographs instead of the labels generated by the participants and achieved similar results. The experimental paradigm of measuring brain activity during the emotional classification of another individual provides consistently high, balanced classification accuracies.","2694-0604","978-1-7281-2782-8","10.1109/EMBC48229.2022.9871624","National Science Foundation(grant numbers:1910526); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9871624","","Emotion recognition;Sociology;Decision making;Psychology;Image representation;Brain modeling;Particle measurements","Brain-Computer Interfaces;Emotions;Humans;Motor Activity;Research Design","","","12","IEEE","8 Sep 2022","","","IEEE","IEEE Conferences"
"Emotion Recognition using Electroencephalography in Response to High Dynamic Range Videos","M. Riaz; M. Majid; J. Mir","Department of Computer Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Computer Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Electrical Engineering, Univerity of Engineering and Technology, Taxila, Pakistan",2021 International Conference on Information Technology (ICIT),"26 Jul 2021","2021","","","565","570","Emotions are the human's internal feelings that result in physiological and physical changes influencing behaviour and thought. Emotion recognition is critical in affective computing for user modelling and human-computer interaction. Previously, user's emotional states have been recognized by measuring physiological activities in response to various audiovisual standard dynamic range (SDR) emotional stimuli. High dynamic range (HDR) content provides a better visual and immersive experience than SDR stimuli due to more brightness, enhanced contrast, and saturated colors. However, the impact of HDR multimedia on emotion elicitation and recognition has not been studied yet. In this paper, four audio-visual HDR stimuli were selected that evoke discrete emotions in each quadrant of the arousal-valence plane. Electroencephalography (EEG) signals of 27 subjects were recorded while watching HDR stimuli using a commercially available Emotiv Insight headset. Time-domain features from 5 channels of EEG signals are extracted and selected to classify emotions in two arousal and valence states using a support vector machine (SVM) classifier. Classification accuracies of 80.55% and 70.37% are achieved for arousal and valence, respectively. Our findings show that HDR videos can act as powerful stimuli for emotion recognition.","","978-1-6654-2870-5","10.1109/ICIT52682.2021.9491662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491662","Emotion recognition;high dynamic range;multimedia;emotion classification;EEG","Support vector machines;Emotion recognition;Visualization;Dynamic range;Feature extraction;Electroencephalography;Physiology","","3","","27","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
"An Efficient LSTM Network for Emotion Recognition From Multichannel EEG Signals","X. Du; C. Ma; G. Zhang; J. Li; Y. -K. Lai; G. Zhao; X. Deng; Y. -J. Liu; H. Wang","Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Informatics, Cardiff University, Cardiff, Wales, U.K.; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, China; Beijing Key Laboratory of Human Computer Interactions, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Affective Computing,"2 Sep 2022","2022","13","3","1528","1540","Most previous EEG-based emotion recognition methods studied hand-crafted EEG features extracted from different electrodes. In this article, we study the relation among different EEG electrodes and propose a deep learning method to automatically extract the spatial features that characterize the functional relation between EEG signals at different electrodes. Our proposed deep model is called ATtention-based LSTM with Domain Discriminator (ATDD-LSTM), a model based on Long Short-Term Memory (LSTM) for emotion recognition that can characterize nonlinear relations among EEG signals of different electrodes. To achieve state-of-the-art emotion recognition performance, the architecture of ATDD-LSTM has two distinguishing characteristics: (1) By applying the attention mechanism to the feature vectors produced by LSTM, ATDD-LSTM automatically selects suitable EEG channels for emotion recognition, which makes the learned model concentrate on the emotion related channels in response to a given emotion; (2) To minimize the significant feature distribution shift between different sessions and/or subjects, ATDD-LSTM uses a domain discriminator to modify the data representation space and generate domain-invariant features. We evaluate the proposed ATDD-LSTM model on three public EEG emotional databases (DEAP, SEED and CMEED) for emotion recognition. The experimental results demonstrate that our ATDD-LSTM model achieves superior performance on subject-dependent (for the same subject), subject-independent (for different subjects) and cross-session (for the same subject) evaluation.","1949-3045","","10.1109/TAFFC.2020.3013711","National Key Research and Development Program of China(grant numbers:2016YFB1001200); National Natural Science Foundation of China(grant numbers:U1736220,61725204,61872346); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9154557","Emotion recognition;multichannel EEG;LSTM;attention mechanism;domain adaptation","Electroencephalography;Feature extraction;Brain modeling;Emotion recognition;Electrodes;Data models;Frequency-domain analysis","","167","","60","IEEE","3 Aug 2020","","","IEEE","IEEE Journals"
"Emotion Dictionary Learning With Modality Attentions for Mixed Emotion Exploration","F. Liu; P. Yang; Y. Shu; F. Yan; G. Zhang; Y. -J. Liu","Department of Computer Science and Technology, and MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Department of Computer Science and Technology, and MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; Department of Computer Science and Technology, and MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China; School of Computer Science and Technology, Changchun University of Science and Technology, Changchun, China; Institute for Visualisation and Interactive Systems, University of Stuttgart, Stuttgart, Germany; Department of Computer Science and Technology, and MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, China",IEEE Transactions on Affective Computing,"5 Sep 2024","2024","15","3","1289","1302","Most existing multi-modal emotion recognition studies are targeted at a classification task that aims to assign a specific emotion category to a combination of several heterogeneous input data, including multimedia signals and physiological signals. A growing number of recent psychological evidence suggests that different discrete emotions may co-exist at the same time, which promotes the development of mixed-emotion recognition to identify a mixture of basic emotions. In this work, we focus on a challenging situation where both positive and negative emotions are presented simultaneously, and propose a multi-modal mixed emotion recognition framework, namely EmotionDict. The key characteristics of our EmotionDict include the following. (1) Inspired by the psychological evidence that such a mixed state can be represented by combinations of basic emotions, we address mixed emotion recognition as a label distribution learning task. An emotion dictionary has been designed to disentangle the mixed emotion representations into a weighted sum of a set of basic emotion elements in a shared latent space and their corresponding weights. (2) We incorporate physiological and overt behavioral multi-modal signals, including electroencephalogram (EEG), peripheral physiological signals, and facial videos, which directly display the subjective emotions. These modalities have diverse characteristics given that they are related to the central or peripheral nervous system, and the motor cortex. (3) We further design auxiliary tasks to learn modality attentions for modality integration. Experiments on two datasets show that our method outperforms existing state-of-the-art approaches on mixed-emotion recognition.","1949-3045","","10.1109/TAFFC.2023.3334520","National Key R&D Program of China(grant numbers:2022ZD0117900); National Natural Science Foundation of China(grant numbers:62332019,U2336214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323171","Emotion distribution learning;mixed emotion;modality attention;multi-modal","Emotion recognition;Physiology;Task analysis;Brain modeling;Electroencephalography;Dictionaries;Visualization","","2","","67","IEEE","20 Nov 2023","","","IEEE","IEEE Journals"
"Causal Graph Convolutional Neural Network for Emotion Recognition","W. Kong; M. Qiu; M. Li; X. Jin; L. Zhu","College of Computer Science, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou, China; College of Computer Science, Hangzhou Dianzi University, Hangzhou, China",IEEE Transactions on Cognitive and Developmental Systems,"14 Dec 2023","2023","15","4","1686","1693","Graph convolutional neural network (GCNN)-based methods have been widely used in electroencephalogram (EEG)-related works due to their advantages of considering the symmetrical connections of brain regions. However, the current GCNN-based methods do not fully explore other correlations between EEG channels. Many studies have proved that definite causal connections exist between brain regions. Therefore, this article proposes a causal GCNN (CGCNN) using the Granger causality (GC) test to calculate interchannel interactions. First, we consider causal relations between EEG channels and construct an asymmetric causal graph with direction. Then, we adopt depthwise separable convolution to extract emotional features from multichannel EEG signals. Experiments carried out on SEED and SEED-IV show that CGCNN has the ability to represent the causal information flow in different emotional states, and improve the classification accuracy to 93.36% on SEED and 75.48% on SEED-IV, respectively. The results outperform other existing methods, indicating that GC is more effective in revealing the correlations between EEG channels in emotion recognition.","2379-8939","","10.1109/TCDS.2022.3175538","National Key Research and Development Program of China(grant numbers:2017YFE0116800); National Natural Science Foundation of China(grant numbers:U20B2074,U1909202); Science and Technology Program of Zhejiang Province(grant numbers:2018C04012); Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province(grant numbers:2020E10010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9776518","Electroencephalogram (EEG) signal;emotion recognition;Granger causality (GC);graph convolutional neural network (GCNN)","Electroencephalography;Brain modeling;Feature extraction;Emotion recognition;Convolutional neural networks;Predictive models;Graph neural networks","","21","","36","IEEE","17 May 2022","","","IEEE","IEEE Journals"
"Multimodal Emotion Classification Method and Analysis of Brain Functional Connectivity Networks","X. Sun; X. Zheng; T. Li; Y. Li; L. Cui","School of Information Science and Engineering, Shandong Normal University, Shandong, China; School of Information Science and Engineering, Shandong Normal University, Shandong, China; Faculty of Education, Shandong Normal University, Shandong, China; Key Laboratory of TCM Data Cloud Service in Universities of Shandong, Shandong Management University, Shandong, China; Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University, Shandong, China",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"26 Jul 2022","2022","30","","2022","2031","Since multimodal emotion classification in different human states has rarely been studied, this paper explores the emotional mechanisms of the brain functional connectivity networks after emotional stimulation. We devise a multimodal emotion classification method fusing a brain functional connectivity network based on electroencephalography (EEG) and eye gaze (ECFCEG) to study emotional mechanisms. First, the nonlinear phase lag index (PLI) and phase-locked value (PLV) are calculated to construct the multiband brain functional connectivity networks, which are then converted into binary brain networks, and the seven features of the binary brain networks are extracted. At the same time, the features of the eye gaze signals are extracted. Then, a fusion algorithm called kernel canonical correlation analysis, based on feature level and randomization (FRKCCA), is executed for feature-level fusion (FLF) of brain functional connectivity networks and eye gaze. Finally, support vector machines (SVMs) are utilized to classify positive and negative emotions in multiple frequency bands with single modal features and multimodal features. The experimental results demonstrate that multimodal complementary representation properties can effectively improve the accuracy of emotion classification, achieving a classification accuracy of 91.32±1.81%. The classification accuracy of pupil diameter in the valence dimension is higher than that of additional features. In addition, the average emotion classification effect of the valence dimension is preferable to that of arousal. Our findings demonstrate that the brain functional connectivity networks of the right brain exhibit a deficiency. In particular, the information processing ability of the right temporal (RT) and right posterior (RP) regions is weak in the low frequency after emotional stimulation; Conversely, phase synchronization of the brain functional connectivity networks based on PLI is stronger than that of PLV.","1558-0210","","10.1109/TNSRE.2022.3192533","Natural Science Foundation of Shandong Province(grant numbers:ZR2020LZH008,ZR2021MF118,ZR2019MF071); Shandong Provincial Key Research and Development Program (Major Scientific and Technological Innovation Project)(grant numbers:2021CXGC010506,2021SFGC0104); National Nature Science Foundation of China(grant numbers:91846205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833538","Eye gaze;brain functional connectivity network;multimodal feature fusion;emotion classification","Feature extraction;Electroencephalography;Physiology;Time-frequency analysis;Frequency conversion;Wavelet coefficients;Pupils","Arousal;Brain;Electroencephalography;Emotions;Humans;Support Vector Machine","7","","51","CCBY","20 Jul 2022","","","IEEE","IEEE Journals"
"A Hierarchical Cross-Modal Spatial Fusion Network for Multimodal Emotion Recognition","M. Xu; T. Shi; H. Zhang; Z. Liu; X. He","Department of Automation, Tsinghua University, Beijing, P. R. China; Shenzhen ZNV Technology Company Ltd, Shenzhen, P. R. China; Shenzhen ZNV Technology Company Ltd, Shenzhen, P. R. China; Department of Automation, Tsinghua University, Beijing, P. R. China; Department of Automation, Tsinghua University, Beijing, P. R. China",IEEE Transactions on Artificial Intelligence,"30 Apr 2025","2025","6","5","1429","1438","Recent advancements in emotion recognition research based on physiological data have been notable. However, existing multimodal methods often overlook the interrelations between various modalities, such as video and electroencephalography (EEG) data, in emotion recognition. In this article, a feature fusion-based hierarchical cross-modal spatial fusion network (HCSFNet) is proposed that effectively integrates EEG and video features. By designing an EEG feature extraction network based on 1-D convolution and a video feature extraction network based on 3-D convolution, corresponding modality features are thoroughly extracted. To promote sufficient interaction between the two modalities, a hierarchical cross-modal coordinated attention module is proposed in this article. Additionally, to enhance the network's perceptual ability for emotion-related features, a multiscale spatial pyramid pooling module is also designed. Meanwhile, a self-distillation method is introduced, which enhances the performance while reducing the number of parameters in the network. The HCSFNet achieved an accuracy of 97.78% on the valence–arousal dimension of the Database for Emotion Analysis using Physiological Signals (DEAP) dataset, and it also obtained an accuracy of 60.59% on the MAHNOB-human-computer interaction (HCI) dataset, reaching the state-of-the-art level.","2691-4581","","10.1109/TAI.2024.3523250","Beijing Natural Science Foundation(grant numbers:L241016); National Natural Science Foundation of China(grant numbers:62473223,62163012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10820048","Attention;cross modal;Database for Emotion Analysis using Physiological Signals (DEAP);emotion recognition;multimodal;self-distillation","Electroencephalography;Feature extraction;Emotion recognition;Convolution;Accuracy;Brain modeling;Three-dimensional displays;Artificial intelligence;Physiology;Transformers","","","","61","IEEE","1 Jan 2025","","","IEEE","IEEE Journals"
"EEG-Based Cross-Subject Emotion Recognition Using Sparse Bayesian Learning with Enhanced Covariance Alignment","W. Wang; F. Qi; W. Huang; Y. Li; Z. Yu; W. Wu","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Internet Finance and Information Engineering, Guangdong University of Finance, Guangzhou, Guangdong, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Songjiang Hospital and Songjiang Research Institute, Shanghai Key Laboratory of Emotions and Affective Disorders, Shanghai Jiao Tong University School of Medicine, Shanghai, China",IEEE Transactions on Affective Computing,"","2024","PP","99","1","16","EEG (Electroencephalography)-based emotion recognition has emerged as a crucial area of research due to its potential applications in mental health, brain-computer interfaces (BCIs), and affective computing. However, the inherent variability in EEG signals across individuals, coupled with limited dataset sizes, significantly hinders the development of robust and generalizable emotion recognition models. To overcome these challenges, we propose the Sparse Bayesian Learning with Enhanced Covariance Alignment (SBLECA) algorithm. SBLECA formulates cross-subject emotion recognition as an end-to-end decoding problem, integrating spatiotemporal filtering and classification within a sparse Bayesian learning (SBL) framework. Crucially, SBLECA incorporates a novel covariance alignment technique to mitigate inter-subject variability in EEG patterns. Rigorous evaluations on two publicly available emotion datasets demonstrate that SBLECA consistently outperforms state-of-the-art methods. Furthermore, SBLECA offers valuable insights into the neural correlates of emotion through interpretable visualizations of learned spatial and temporal filters. SBLECA holds promise as a valuable EEG decoding tool to advance the development and translation of neurotechnologies and biomarkers for brain disorders. Code is available at https://github.com/EEGdecoding/Code-SBLECA.","1949-3045","","10.1109/TAFFC.2024.3497897","National Natural Science Foundation of China(grant numbers:62276102,62376098); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10753003","emotion recognition;electroencephalography (EEG);sparse Bayesian learning;covariance alignment;transfer learning","Electroencephalography;Emotion recognition;Feature extraction;Decoding;Brain modeling;Spatiotemporal phenomena;Filters;Classification algorithms;Bayes methods;Affective computing","","","","","IEEE","14 Nov 2024","","","IEEE","IEEE Early Access Articles"
"Multimodal Emotion Recognition Based on EEG and EOG Signals Evoked by the Video-Odor Stimuli","M. Wu; W. Teng; C. Fan; S. Pei; P. Li; G. Pei; T. Li; W. Liang; Z. Lv","Anhui Province Key Laboratory of Multimodal Cognitive Computation and the School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation and the School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation and the School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation and the School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation and the School of Computer Science and Technology, Anhui University, Hefei, China; Zhejiang Laboratory, Institute of Artificial Intelligence, Hangzhou, China; Zhejiang Laboratory, Institute of Artificial Intelligence, Hangzhou, China; Google Inc., Mountain View, CA, USA; Zhejiang Laboratory, Institute of Artificial Intelligence, Hangzhou, China",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"20 Sep 2024","2024","32","","3496","3505","Affective data is the basis of emotion recognition, which is mainly acquired through extrinsic elicitation. To investigate the enhancing effects of multi-sensory stimuli on emotion elicitation and emotion recognition, we designed an experimental paradigm involving visual, auditory, and olfactory senses. A multimodal emotional dataset (OVPD-II) that employed the video-only or video-odor patterns as the stimuli materials, and recorded the electroencephalogram (EEG) and electrooculogram (EOG) signals, was created. The feedback results reported by subjects after each trial demonstrated that the video-odor pattern outperformed the video-only pattern in evoking individuals’ emotions. To further validate the efficiency of the video-odor pattern, the transformer was employed to perform the emotion recognition task, where the highest accuracy reached 86.65% (66.12%) for EEG (EOG) modality with the video-odor pattern, which improved by 1.42% (3.43%) compared with the video-only pattern. What’s more, the hybrid fusion (HF) method combined with the transformer and joint training was developed to improve the performance of the emotion recognition task, which achieved classify accuracies of 89.50% and 88.47% for the video-odor and video-only patterns, respectively.","1558-0210","","10.1109/TNSRE.2024.3457580","National Natural Science Foundation of China(grant numbers:62476004); Excellent Youth Foundation of Anhui Scientific Committee(grant numbers:2208085J05); National Key Research and Development Program of China(grant numbers:2021ZD0201502); Special Fund for Key Program of Science and Technology of Anhui Province(grant numbers:202203a07020008); Natural Science Foundation of Anhui Province(grant numbers:2108085MF207); Open Research Projects of Zhejiang Laboratory(grant numbers:2021KH0AB06); Open Projects Program of National Laboratory of Pattern Recognition(grant numbers:202200014); Open Fund Project of Key Laboratory of Civil Aviation Flight Technology and Flight Safety(grant numbers:FZ2022KF15); Cloud Ginger XR-1(grant numbers:FZ2022KF15); Cloud Ginger(grant numbers:XR-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10672559","Electroencephalogram (EEG);electrooculogram (EOG);emotion recognition;video-odor stimuli;multi-modal fusion","Electroencephalography;Videos;Emotion recognition;Electrooculography;Physiology;Feature extraction;Electrodes","Humans;Electroencephalography;Emotions;Male;Female;Young Adult;Electrooculography;Adult;Odorants;Algorithms;Video Recording;Photic Stimulation;Reproducibility of Results;Healthy Volunteers","2","","45","CCBYNCND","10 Sep 2024","","","IEEE","IEEE Journals"
"Multi-Output Regression for Integrated Prediction of Valence and Arousal in EEG-Based Emotion Recognition","H. Choi; C. Woo; J. Kong; B. H. Kim","Dept. Electrical and Computer Engineering, Inha University, Incheon, Republic of Korea; Dept. Electrical and Computer Engineering, Inha University, Incheon, Republic of Korea; Dept. Computer Engineering, Inha University, Incheon, Republic of Korea; Dept. Artificial Intelligence, Inha University, Incheon, Republic of Korea",2024 12th International Winter Conference on Brain-Computer Interface (BCI),"2 Apr 2024","2024","","","1","6","Previous research in electroencephalogram (EEG)– based emotion recognition has assumed valence and arousal as independent dimensions and derived results using separate classification models for each. Our study aims to introduce multi- output regression to continuously predict the correlated valence and arousal. We compared the prediction performance of a total of three models: Single-output Regression, Multi-output Regression, and Multi-output Regression with a Chain structure. The analysis yielded the following results: 1) The performance of multi-output and single-output was similar, but the multi-output method is more useful as it can produce multiple outputs at once. 2) Predictions and actuals for valence and arousal were similar. Furthermore, considering that human emotions are not constant during certain intervals, it can provide insights close to the ground truth for understanding changes in emotional states.","2572-7672","979-8-3503-0943-0","10.1109/BCI60775.2024.10480527","Inha University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10480527","Multi-output Regression;Emotion Recognition;EEG;Valence;Arousal","Emotion recognition;Predictive models;Brain modeling;Market research;Electroencephalography;Brain-computer interfaces;Mirrors","","1","","22","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Feature Extraction of EEG Signals and Classification Using FCM","S. A. M. Aris; M. N. Taib; S. Lias; N. Sulaiman","Razak School of Engineering, Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Selangor Darul Ehsan, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Selangor Darul Ehsan, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Selangor Darul Ehsan, Malaysia",2011 UkSim 13th International Conference on Computer Modelling and Simulation,"21 Apr 2011","2011","","","54","58","EEG data were collected between two conditions, relax wakefulness (close-eyes) and non-relax (IQ test). Data segmentation and linear regression model is used to extract the EEG features and to obtain the slope and the mean relative power from 43 participants. All of the data were then normalized and classified using Fuzzy C-Means (FCM) clustering. Results shown that there are different of activities exist in the EEG which proved that the feature extraction using linear regression model manage to discern between two different brain behaviors.","","978-1-61284-705-4","10.1109/UKSIM.2011.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5754207","EEG;Linear Regression Technique;FCM","Electroencephalography;Feature extraction;Linear regression;Brain modeling;Humans;Emotion recognition","","13","","24","IEEE","21 Apr 2011","","","IEEE","IEEE Conferences"
"Generative Listener EEG for Speech Emotion Recognition Using Generative Adversarial Networks With Compressed Sensing","J. Chang; Z. Zhang; Z. Wang; J. Li; L. Meng; P. Lin","Institute of Big Data Science and Industry, the School of Computer and Information Technology, Shanxi University, Taiyuan, China; Institute of Big Data Science and Industry, the School of Computer and Information Technology, Shanxi University, Taiyuan, China; Institute of Big Data Science and Industry, the School of Computer and Information Technology, Shanxi University, Taiyuan, China; Institute of Big Data Science and Industry, the School of Computer and Information Technology, Shanxi University, Taiyuan, China; School of Physical Education, Shanxi University, Taiyuan, China; Center for Mind&Brain Sciences and Institute of Interdisciplinary Studies, Hunan Normal University, Changsha, China",IEEE Journal of Biomedical and Health Informatics,"4 Apr 2024","2024","28","4","2025","2036","Currently, emotional features in speech emotion recognition are typically extracted from the speeches, However, recognition accuracy can be influenced by factors such as semantics, language, and cross-speech datasets. Achieving consistent emotional judgment with human listeners is a key challenge for AI to address. Electroencephalography (EEG) signals prove to be an effective means of capturing authentic and meaningful emotional information in humans. This positions EEG as a promising tool for detecting emotional cues conveyed in speech. In this study, we proposed a novel approach named CS-GAN that generates listener EEGs in response to a speaker's speech, specifically aimed at enhancing cross-subject emotion recognition. We utilized generative adversarial networks (GANs) to establish a mapping relationship between speech and EEGs to generate stimulus-induced EEGs. Furthermore, we integrated compressive sensing theory (CS) into the GAN-based EEG generation method, thereby enhancing the fidelity and diversity of the generated EEGs. The generated EEGs were then processed using a CNN-LSTM model to identify the emotional categories conveyed in the speech. By averaging these EEGs, we obtained the event-related potentials (ERPs) to improve the cross-subject capability of the method. The experimental results demonstrate that the generated EEGs by this method outperform real listener EEGs by 9.31% in cross-subject emotion recognition tasks. Furthermore, the ERPs show an improvement of 43.59%, providing evidence for the effectiveness of this method in cross-subject emotion recognition.","2168-2208","","10.1109/JBHI.2024.3360151","Shanxi Province Foundation for Youths(grant numbers:20210302124556); National Natural Science Foundation of China(grant numbers:62071177); Fundamental Research Program of Shanxi Province(grant numbers:202303021211023,202303021221075); Central guidance for Local scientific and technological development funds(grant numbers:YDZJSX20231B001); special fund for Science and Technology lnnovation Teams of Shanxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416653","Compressed sensing;cross-subject;EEG emotion recognition;EEG generation;generative adversarial networks","Electroencephalography;Emotion recognition;Brain modeling;Speech recognition;Generative adversarial networks;Feature extraction;Convolutional neural networks","","5","","29","IEEE","30 Jan 2024","","","IEEE","IEEE Journals"
"EEG Signal Analysis to Verify the Impact of Music on Human Emotions","D. Das; K. Shankar; J. Hazarika","Electronics and Instrumentation Engg., NIT Silchar, Silchar, India; Electronics and Instrumentation Engg., NIT Silchar, Silchar, India; Electronics and Instrumentation Engg., NIT Silchar, Silchar, India",2024 International Conference on Brain Computer Interface & Healthcare Technologies (iCon-BCIHT),"19 Feb 2025","2024","","","156","161","Emotions are a vital aspect of humanity and under-standing them is one of the most challenging tasks. In our study, EEG data was collected from 10 healthy subjects, comprising 5 males and 5 females by using a 3-electrode system. The experiment involved assessing EEG signals across four stages with various stimuli to evoke emotions, particularly focusing on changes while subjects listened to music. We used both statistical analysis and machine learning algorithm to classify emotions based on time-domain features extracted from EEG signal data. Statistical analysis revealed emotional variations during music listening, indicated by changes in the Root Mean Square (RMS) value. By using machine learning, we classified six discrete emotions (Happy, Enjoy, Calm, Sad, Excited, Bored) and arousal and valence states using five classifiers: Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Multi-Layer Perceptron (MLP), Random Forest, and Decision Tree. This approach effectively distinguished each emotional states and provided insights into arousal and valence states.","","979-8-3315-4006-7","10.1109/iCon-BCIHT63907.2024.10882297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10882297","Human emotion recognition;Electroencephalogram (EEG);Machine Learning","Support vector machines;Machine learning algorithms;Statistical analysis;Music;Nearest neighbor methods;Electroencephalography;Multiple signal classification;Decision trees;Root mean square;Random forests","","","","31","IEEE","19 Feb 2025","","","IEEE","IEEE Conferences"
"EEG based emotion recognition using SVM and PSO","R. Nivedha; M. Brinda; D. Vasanth; M. Anvitha; K. V. Suma","Dept. of Electronics & Communication, Ramaiah Institute of Technology, Bangalore, India; Dept. of Electronics & Communication, Ramaiah Institute of Technology, Bangalore, India; Dept. of Electronics & Communication, Ramaiah Institute of Technology, Bangalore, India; Dept. of Electronics & Communication, Ramaiah Institute of Technology, Bangalore, India; Dept. of Electronics & Communication, Ramaiah Institute of Technology, Bangalore, India","2017 International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)","23 Apr 2018","2017","","","1597","1600","Machine learning has fueled real breakthroughs in affective computing in making the machines more emphatic to the user. This emotion recognition capability of machines enables them to act according to the observed mental state. Human feelings and emotions are triggered by stimuli which are external or internal and manifest themselves in the form of pulse rate, tone, facial expressions and many more. In this paper we classify human emotions using EEG signals into four discrete states, namely happy, sad, angry and relaxed. The preprocessed signals from the DEAP database is used and spectral and statistical features are extracted by discrete wavelet transform. These features are classified using a SVM classifier and the performance of the classifier is optimized using the PSO algorithm. An overall emotional accuracy of 80.625% was obtained for a combination of 32 electrodes with a valence and arousal accuracy of 86.25% and 88.125%.","","978-1-5090-6106-8","10.1109/ICICICT1.2017.8342809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342809","EEG;Wavelet transform;Emotion recognition;SVM;PSO","Feature extraction;Electroencephalography;Support vector machines;Emotion recognition;Discrete wavelet transforms;Electrodes;Brain modeling","","19","","13","IEEE","23 Apr 2018","","","IEEE","IEEE Conferences"
"Correlation between stimulated emotion extracted from EEG and its manifestation on facial expression","A. Chakraborty; P. Bhowmik; S. Das; A. Halder; A. Konar; A. K. Nagar","Faculty of ETCE Department, Jadavpur University, India; Department of Electronics and Tele-Communication Engineering, Jadavpur University, Calcutta, India; Department of Electronics and Tele-Communication Engineering, Jadavpur University, Calcutta, India; Department of Electronics and Tele-Communication Engineering, Jadavpur University, Calcutta, India; Department of Electronics and Tele-Communication Engineering, Jadavpur University, Calcutta, India; Department of Computer Science, Liverpool Hope University, Liverpool, UK","2009 IEEE International Conference on Systems, Man and Cybernetics","4 Dec 2009","2009","","","3132","3137","Determining correlation between aroused emotion and its manifestation on facial expression, voice, gesture and posture have interesting applications in psychotherapy. A set of audiovisual stimulus, selected by a group of experts, is used to excite emotion of the subjects. EEG and facial expression of the subjects excited by the selected audio-visual stimulus are collected, and the nonlinear-correlation from EEG to facial expression, and vice-versa is obtained by employing feed-forward neural network trained with back-propagation algorithm. Experiments undertaken reveals that the trained network can reproduce the correlated EEG-facial expression trained instances with 100 % accuracy, and is also able to predict facial expression (EEG) from unknown EEG (facial expression) of the same subject with an accuracy of around 95.2%.","1062-922X","978-1-4244-2793-2","10.1109/ICSMC.2009.5346157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346157","Facial expression;EEG;Nonlinear Correlation;Feedforward neural learning","Electroencephalography;Psychology;Feedforward systems;Biological neural networks;Neural networks;Feedforward neural networks;Feature extraction;Computer science;Cybernetics;USA Councils","","5","","12","IEEE","4 Dec 2009","","","IEEE","IEEE Conferences"
"Functional Connectivity Patterns Learning for EEG-based Emotion Recognition","C. Shi; C. L. P. Chen; S. Li; T. Zhang","Guangdong Provincial Key Laboratory of Computational Intelligence and Cyberspace Information, School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Computational Intelligence and Cyberspace Information, School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Computational Intelligence and Cyberspace Information, School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Computational Intelligence and Cyberspace Information, School of Computer Science and Engineering, South China University of Technology, Guangzhou, China",IEEE Transactions on Cognitive and Developmental Systems,"","2024","PP","99","1","15","Neuroscience research reveals that different emotions are associated with different functional connectivity structures of brain regions. However, many existing EEG-based emotion recognition methods use these connectivity patterns broadly without distinguishing between specific emotions. Additionally, the non-stationarity of EEG signals often results in high variations across different periods, leading models to extract time-specific features instead of emotional features. This paper proposes a Functional Connectivity Patterns Learning network (FCPL) for EEG-based emotion recognition to address these challenges. FCPL includes a coefficient branch, a graph construction module, and a period domain adversarial module. These components capture individual characteristics and specific emotional connectivity patterns and reduce period-related variations, respectively. FCPL achieves state-of-the-art results: 42.04%/28.81% for seven-class subject-dependent/independent experiments on the MPED dataset, 97.45%/89.88% for subject-dependent/ independent experiments on the SEED dataset, and 95.98%/96.19% for valence/arousal subject-dependent experiments and 67.90%/65.60% for valence/arousal subject-independent experiments on the DREAMER dataset. This work advances the exploration of functional connectivity structures in EEG signals from coarse-grained emotion-related patterns to fine-grained emotional distinctions, promoting neuroscience and EEG-based emotion recognition technologies.","2379-8939","","10.1109/TCDS.2024.3470248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10700842","EEG emotion recognition;functional connectivity patterns;non-stationarity;graph neural network","Electroencephalography;Feature extraction;Emotion recognition;Brain modeling;Graph neural networks;Data mining;Vectors;Training;Electrodes;Accuracy","","1","","","IEEE","30 Sep 2024","","","IEEE","IEEE Early Access Articles"
"PR-PL: A Novel Prototypical Representation Based Pairwise Learning Framework for Emotion Recognition Using EEG Signals","R. Zhou; Z. Zhang; H. Fu; L. Zhang; L. Li; G. Huang; F. Li; X. Yang; Y. Dong; Y. -T. Zhang; Z. Liang","School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China; Department of Mathematics and Information Technology, The Education University of Hong Kong, Ting Kok, Hong Kong; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; The Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, School of Life Science and Technology, Center for Information in Medicine, University of Electronic Science and Technology of China, Chengdu, China; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China; School of Data Science, Hong Kong Center for Cerebro-Cardiovascular Health Engineering (COCHE), Hong Kong Science and Technology Park, City University of Hong Kong, Kowloon Tong, Hong Kong; Department of Biomedical Engineering, Hong Kong Center for Cerebro-Cardiovascular Health Engineering (COCHE), Hong Kong Science and Technology Park, City University of Hong Kong, Kowloon Tong, Hong Kong; School of Biomedical Engineering, Medical School, Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen University, Shenzhen, China",IEEE Transactions on Affective Computing,"30 May 2024","2024","15","2","657","670","Affective brain-computer interface based on electroencephalography (EEG) is an important branch in the field of affective computing. However, the individual differences in EEG emotional data and the noisy labeling problem in the subjective feedback seriously limit the effectiveness and generalizability of existing models. To tackle these two critical issues, we propose a novel transfer learning framework with Prototypical Representation based Pairwise Learning (PR-PL). The discriminative and generalized EEG features are learned for emotion revealing across individuals and the emotion recognition task is formulated as pairwise learning for improving the model tolerance to the noisy labels. More specifically, a prototypical learning is developed to encode the inherent emotion-related semantic structure of EEG data and align the individuals’ EEG features to a shared common feature space under consideration of the feature separability of both source and target domains. Based on the aligned feature representations, pairwise learning with an adaptive pseudo labeling method is introduced to encode the proximity relationships among samples and alleviate the label noises effect on modeling. Extensive results on two benchmark databases (SEED and SEED-IV) under four different cross-validation evaluation protocols validate the model reliability and stability across subjects and sessions. Compared to the literature, the average enhancement of emotion recognition across four different evaluation protocols is 2.04% (SEED) and 2.58% (SEED-IV).","1949-3045","","10.1109/TAFFC.2023.3288118","National Natural Science Foundation of China(grant numbers:62276169,62071310,61906122); Shenzhen-Hong Kong Institute of Brain Science-Shenzhen Fundamental Research Institutions(grant numbers:2021SHIBS0003); Shenzhen Science and Technology Research and Development Fund for Sustainable Development(grant numbers:KCXFZ20201221173613036); Tencent “Rhinoceros Birds”-Scientific Research Foundation for Young Teachers of Shenzhen University; High Level University Construction(grant numbers:000002110133); Hong Kong Center for Cerebro-cardiovascular Health Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160130","EEG;emotion recognition;affective brain-computer interface;transfer learning;affective computing","Brain modeling;Electroencephalography;Emotion recognition;Transfer learning;Adaptation models;Labeling;Computational modeling","","22","","64","IEEE","23 Jun 2023","","","IEEE","IEEE Journals"
"SSTD: A Novel Spatio-Temporal Demographic Network for EEG-Based Emotion Recognition","R. Li; C. Ren; C. Li; N. Zhao; D. Lu; X. Zhang","Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Institute of Engineering Medicine, Beijing Institute of Technology, Beijing, China; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, China",IEEE Transactions on Computational Social Systems,"31 Jan 2023","2023","10","1","376","387","Emotion recognition is the key to making machines more intelligent. This study proposes a novel sing-link end-to-end spatio-temporal demographic network (SSTD) that fuses spatial, temporal, and demographic information for electroencephalography (EEG)-based emotion recognition. In the SSTD model, an adaptive time window using single-link hierarchical clustering based on Riemannian metrics was realized for data preprocessing to solve the problem of individual differences. Then, the preprocessed EEG data acted as a gate recurrent unit (GRU) network input to calculate high-level time-domain features. At the same time, the EEG covariance matrices were fed into the symmetric positive definite matrix network (SPDNet) to calculate high-level spatial features. Given the correlation between EEG signals and individual demographic information, gender and age factors were integrated into the spatio-temporal model, resulting in more effective high-level features for EEG-based emotion recognition. Finally, extensive comparative experiments were conducted on two public datasets: DEAP and DREAMER. The average accuracy of valence and arousal on the DEAP dataset are 68.28% and 71.48%, respectively. The average accuracy of valence and arousal on the DREAMER dataset are 76.81% and 81.64%, respectively. Experimental results show that the SSTD model has an excellent recognition performance.","2329-924X","","10.1109/TCSS.2022.3188891","National Key Research and Development Program of China(grant numbers:2019YFA0706200); National Natural Science Foundation of China(grant numbers:62072219); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829548","Demographic;electroencephalography (EEG);emotion recognition;gate recurrent unit (GRU);Riemannian manifold;spatio-temporal;symmetric positive definite matrix network (SPDNet)","Covariance matrices;Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Data preprocessing;Measurement","","8","","49","IEEE","14 Jul 2022","","","IEEE","IEEE Journals"
"Colour Prediction using Vision Transformer and Continous Wavelet Transform on EEG signals","P. Mishra; M. Antonakakis; K. K. Singh; M. Zervakis","National Institute of Technology Jamshedpur (NIT JSR), Jamshedpur, Jharkhand, India; School of Electrical and Computer Engineering, Technical University of Crete (TUC), Chania, Crete, Greece; National Institute of Technology Jamshedpur (NIT JSR), Jamshedpur, Jharkhand, India; School of Electrical and Computer Engineering, Technical University of Crete (TUC), Chania, Crete, Greece",2023 IEEE 23rd International Conference on Bioinformatics and Bioengineering (BIBE),"19 Feb 2024","2023","","","181","186","Electroencephalography (EEG)-based classification of brain disease such as epilepsy or schizophrenia, decoding brain activity during movement and vision have been shown promising results in the last years. Here, we introduce a novel pipeline for the presence of speech information carried on EEG signals. The proposed work includes a new conducted EEG dataset of 15 subjects and a deep learning model to predict the colour information. With a unique experimental set up, the data successfully captures the information about the mental enunciation of the set of used colors. The primary goal is to perform multiclass classification using our custom EEG data which records the brain activity of individuals during mental enunciation and thought about a class of objects, in our case, colours. Continuous Wavelet Transform (CWT) is applied on each of the EEG channels of each participant to obtain time-frequency (TF) based characteristics. A Vision Transformer (ViT) based model is then developed and used to capture information from these TF. The method deals with a 6-class classification problem, for which, the 6 different colors are used as target classes for our model. The proposed model achieves 91.36% cross validation accuracy, 5.48x the random guess accuracy. These results clearly demonstrate the existence of speech information in EEG signals and lay the foundational stone for future research in speech assistive technologies.","2471-7819","979-8-3503-9311-8","10.1109/BIBE60311.2023.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431804","EEG;BCI;Colour Prediction;Deep Learning;Vision Transformer","Continuous wavelet transforms;Image color analysis;Biological system modeling;Predictive models;Brain modeling;Transformers;Electroencephalography","","","","37","IEEE","19 Feb 2024","","","IEEE","IEEE Conferences"
"Cross-Subject Emotion Recognition Using Flexible Analytic Wavelet Transform From EEG Signals","V. Gupta; M. D. Chopda; R. B. Pachori","Discipline of Electrical Engineering, Indian Institute of Technology Indore, Indore, India; Discipline of Electrical Engineering, Indian Institute of Technology Indore, Indore, India; Discipline of Electrical Engineering, Indian Institute of Technology Indore, Indore, India",IEEE Sensors Journal,"15 Feb 2019","2019","19","6","2266","2274","Human emotion is a physical or psychological process which is triggered either consciously or unconsciously due to perception of any object or situation. The electroencephalogram (EEG) signals can be used to record ongoing neuronal activities in the brain to get the information about the human emotional state. These complicated neuronal activities in the brain cause non-stationary behavior of the EEG signals. Thus, emotion recognition using EEG signals is a challenging study and it requires advanced signal processing techniques to extract the hidden information of emotions from EEG signals. Due to poor generalizability of features from EEG signals across subjects, recognizing cross-subject emotion has been difficult. Thus, our aim is to comprehensively investigate the channel specific nature of EEG signals and to provide an effective method based on flexible analytic wavelet transform (FAWT) for recognition of emotion. FAWT decomposes the EEG signal into different sub-band signals. Furthermore, we applied information potential to extract the features from the decomposed sub-band signals of EEG signal. The extracted feature values were smoothed and fed to the random forest and support vector machine classifiers that classified the emotions. The proposed method is applied to two different publicly available databases which are SJTU emotion EEG dataset and database for emotion analysis using physiological signal. The proposed method has shown better performance for human emotion classification as compared to the existing method. Moreover, it yields channel specific subject classification of emotion EEG signals when exposed to the same stimuli.","1558-1748","","10.1109/JSEN.2018.2883497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546787","Human emotions;EEG;FAWT;random forest;SVM","Electroencephalography;Feature extraction;Emotion recognition;Sensors;Databases;Transforms;Support vector machines","","227","","48","IEEE","27 Nov 2018","","","IEEE","IEEE Journals"
"CSP- LSTM Based Emotion Recognition from EEG Signals","J. T. Panachakel; R. H; S. P. K; S. Sidharth; A. A. Samuel","Department of Electronics and Communication Engineering, College of Engineering Trivandrum, India; Department of Electronics and Communication Engineering, College of Engineering Trivandrum, India; Department of Electronics and Communication Engineering, College of Engineering Trivandrum, India; Department of Electronics and Communication Engineering, College of Engineering Trivandrum, India; Department of Electronics and Communication Engineering, College of Engineering Trivandrum, India","2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)","1 Feb 2024","2023","","","289","294","Emotion recognition from the electroencephalogram (EEG) signals is a growing area of research in the field of neuroscience and medical technology. The ability to accurately identify and classify emotions can have significant implications for various applications, including the diagnosis and treatment of emotional disorders, human-computer interaction, etc. A deep learning-based emotion identification method is proposed here. The EEG data is divided into four frequency bands (alpha, beta, lower gamma and upper gamma) using a temporal domain filter, and then the common spatial pattern (CSP) feature extraction is applied to obtain a set of discriminative features that are used to train the long short-term memory (LSTM) for emotion classification. The LSTM was trained for each band and the predictions of each of these models (same architecture with different weights) were combined using a majority voting classifier (MVC) method. All subjects achieved accuracy values exceeding 90%, resulting in a mean accuracy of 97.34% when considering the average across all subjects. The utilisation of this method for bio-signal modeling and interpretation has the capability to enhance the identification and management of emotional disorders. etc.","","979-8-3503-0080-2","10.1109/MetroXRAINE58569.2023.10405666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10405666","EEG;emotions;CSP;LSTM;emotion classification;identification;deep learning;neuroscience;philosophy","Emotion recognition;Neuroscience;Neural engineering;Predictive models;Feature extraction;Brain modeling;Electroencephalography","","","","22","IEEE","1 Feb 2024","","","IEEE","IEEE Conferences"
"Application of EEMD-HHT Method on EEG Analysis for Speech Evoked Emotion Recognition","J. Chen; H. Li; L. Ma; H. Bo; X. Gao","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Shenzhen Academy of Aerospace Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China",2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),"25 Aug 2020","2020","","","376","381","Electroencephalograph (EEG) is widely used to study human brain activities. However, the interpretation of EEG signals is still a challenging computational task. Emerging evidence has shown that the non-stationary traits of EEG signals hinder the way of informative interpretation. Compared to the classical Welch frequency analysis method (short-time Fourier transform), Hilbert Huang Transform(HHT) is more suitable for non-linear and non-stationary signals. This paper proposes a band energy extraction method based on EEMD-HHT for time-frequency analysis of EEG signals. We evaluate the method on an EEG database obtained through the emotional cognitive experiment. The auditory stimulus in this paper are selected from CHEAVD2 which is a speech emotion database of the Chinese Academy of Sciences. The correlation coefficients between the predict and target values reach 0.51 and 0.43 for arousal and valence dimension, respectively. This method shows great potentials in applications of computational neuroscience and cognition of art creation.","","978-1-7281-4272-2","10.1109/MIPR49039.2020.00082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9175519","Hilbert-Huang Transform (HHT), Ensemble Empirical Mode Decomposition(EEMD), Electro encephalograph (EEG), Time frequency analysis, Computational neuroscience","Electroencephalography;Feature extraction;Transforms;Emotion recognition;Time-frequency analysis;Frequency conversion;Databases","","8","","15","IEEE","25 Aug 2020","","","IEEE","IEEE Conferences"
"Fractal Spiking Neural Network Scheme for EEG-Based Emotion Recognition","W. Li; C. Fang; Z. Zhu; C. Chen; A. Song","School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China",IEEE Journal of Translational Engineering in Health and Medicine,"4 Dec 2023","2024","12","","106","118","Electroencephalogram (EEG)-based emotion recognition is of great significance for aiding in clinical diagnosis, treatment, nursing and rehabilitation. Current research on this issue mainly focuses on utilizing various network architectures with different types of neurons to exploit the temporal, spectral, or spatial information from EEG for classification. However, most studies fail to take full advantage of the useful Temporal-Spectral-Spatial (TSS) information of EEG signals. In this paper, we propose a novel and effective Fractal Spike Neural Network (Fractal-SNN) scheme, which can exploit the multi-scale TSS information from EEG, for emotion recognition. Our designed Fractal-SNN block in the proposed scheme approximately simulates the biological neural connection structures based on spiking neurons and a new fractal rule, allowing for the extraction of discriminative multi-scale TSS features from the signals. Our designed training technique, inverted drop-path, can enhance the generalization ability of the Fractal-SNN scheme. Sufficient experiments on four public benchmark databases, DREAMER, DEAP, SEED-IV and MPED, under the subject-dependent protocols demonstrate the superiority of the proposed scheme over the related advanced methods. In summary, the proposed scheme provides a promising solution for EEG-based emotion recognition.","2168-2372","","10.1109/JTEHM.2023.3320132","Basic Research Project of Leading Technology of Jiangsu Province(grant numbers:BK20192004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266337","Electroencephalogram;fractal spiking neural network;inverted drop-path;emotion recognition","Electroencephalography;Feature extraction;Emotion recognition;Fractals;Biological neural networks;Neurons;Three-dimensional displays","Fractals;Emotions;Recognition, Psychology;Electroencephalography;Neural Networks, Computer","10","","37","CCBY","28 Sep 2023","","","IEEE","IEEE Journals"
"Advancing EEG-Based Emotion Recognition: Unleashing the Power of Graph Neural Networks for Dynamic and Topology-Aware Models","L. Galluccio; L. D'Errico; M. Giordano; M. Staffa","University of Naples ""Parthenope""; University of Naples ""Federico II""; National Research Council (ICAR-CNR); University of Naples ""Parthenope""",2024 International Joint Conference on Neural Networks (IJCNN),"9 Sep 2024","2024","","","1","8","In recent years, numerous studies on emotion recognition have employed non-invasive electroencephalography (EEG) to measure neuronal activity. Various Machine Learning and Deep Learning techniques have been applied to classify emotions based on EEG signals. However, Graph Neural Networks (GNNs) remain relatively unexplored in this domain. Given that GNNs can accommodate dynamic and variablesized data as input, we hypothesize their potential for superior performance compared to traditional models like Support Vector Machines (SVMs) or Convolutional Neural Networks (CNNs), which rigidly fix the distance among electrodes in a pixel-like matrix. Furthermore, GNNs are designed to leverage the biological topology between different brain regions, capturing both local and global relationships among EEG channels. In this study, two GNN models are experimented with: a Dynamical Graph Convolutional Neural Network (DGCNN) and a Regularized Graph Neural Network (RGNN). Results indicate that the DGCNN model proposed in this work outperforms state-of-the-art models and the RGNN, achieving an average accuracy of 95%, compared to 53% for the latter. Additionally, the DGCNN exhibits significantly faster training times, opening up new avenues for research in this field.","2161-4407","979-8-3503-5931-2","10.1109/IJCNN60899.2024.10650427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10650427","Emotion recognition;EEG signal;Graph Neural Network;RGNN;DCGNN","Training;Emotion recognition;Convolution;Biological system modeling;Neurons;Brain modeling;Graph neural networks","","","","31","IEEE","9 Sep 2024","","","IEEE","IEEE Conferences"
"Emotion Recognition System Based on Two-Level Ensemble of Deep-Convolutional Neural Network Models","M. Hussain; E. -U. -H. Qazi; H. A. AboAlSamh; I. Ullah","Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Insigth SFI Research Center for Data Analytics, School of Computer Science, University of Galway, Galway, Ireland",IEEE Access,"24 Feb 2023","2023","11","","16875","16895","Emotions play a crucial role in human interaction and healthcare. This study introduces an automatic emotion recognition system based on deep learning using electroencephalogram signals. A lightweight pyramidal one-dimensional convolutional neural network model is proposed that involves a small number of learnable parameters. Using the model, a two-level ensemble classifier is designed. Each channel is scanned incrementally in the first level to generate predictions, which are fused using the majority vote. The second level fuses the predictions of all the channels of a signal using a majority vote to predict the emotional state. The method was validated using the public domain challenging benchmark DEAP dataset. The electroencephalogram signals over five brain regions were analyzed. The results indicate that the frontal brain region plays a dominant role, achieving accuracies of 98.43% and 97.65% for two emotion recognition problems (distinguishing high valence vs. low valence and high arousal vs. low arousal states).","2169-3536","","10.1109/ACCESS.2023.3245830","Deputyship for Research and Innovation, Ministry of Education in Saudi Arabia(grant numbers:IFKSURG-2-109); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045346","Convolutional neural networks;deep learning;electroencephalography;emotion recognition;expert systems;feature extraction;machine learning;pattern classification;psychology;signal processing","Feature extraction;Electroencephalography;Brain modeling;Emotion recognition;Support vector machines;Convolutional neural networks;Deep learning","","9","","66","CCBYNCND","15 Feb 2023","","","IEEE","IEEE Journals"
"EEG Based Human Emotion Analysis","A. Garg; K. Mankar; S. S. Manhas; I. Naik; H. Jain; H. Patil; P. Pimpale; D. Kalathil; S. Rahise; S. Poddar; L. Deshmukh; A. Hake; K. Talele","Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Electronics Engineering, Sardar Patel Institute of Technology, Mumbai, India",2022 IEEE Region 10 Symposium (TENSYMP),"29 Aug 2022","2022","","","1","6","Emotions accompany everyone in their daily lives, play an important role in nonverbal communication, and are critical to understanding human behaviours. Text, speech, facial expressions, and gestures can all be used to recognise emotions. In this paper we propose a model using electroencephalograph signal that are decomposed into different frequency bands and spectral features are extracted from them. In the proposed solution the dataset has been trained and tested using different machine learning algorithm to get all the possibilities and greater accuracy. The dataset testing mainly involved three emotions and has been tested using random forest algorithm. Using the dataset samples are collected and for each sample a decision tree is built to produce the result. In this paper various frequency bands are used with different naming with a primary objective to verify whether the naming makes a specific difference.","2642-6102","978-1-6654-6658-5","10.1109/TENSYMP54529.2022.9864507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864507","EEG;electroencephalogram;DEAP dataset;emotion recognition","Emotion recognition;Machine learning algorithms;Text recognition;Speech recognition;Feature extraction;Wavelet analysis;Electroencephalography","","","","17","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"Stress analysis of EEG signal using K-means clustering and hybrid classifier","P. Pallav; K. Srivastava","Department of Electronics and Communication Engg, Ajay Kumar Garg Engineering College, Ghaziabad, India; Department of Electronics and Communication Engg, Ajay Kumar Garg Engineering College, Ghaziabad, India",2024 2nd International Conference on Advancements and Key Challenges in Green Energy and Computing (AKGEC),"11 Feb 2025","2024","","","1","5","A significant factor that affects both mental and physical health is stress. Measuring accurately the stress level is very important. In this research, we approach measuring the stress using an EEG signal. There are many techniques to measure this stress; the most common ones are PET, ECG, EMG, and MRI, which help in measuring the stress levels of people. Another technique that is very useful and common, i.e., EEG, is mostly used to do such data analysis. EEG plays a very important role in detecting and correcting brain illness. EEG measures the electrical activity of the brain using small metal electrodes attached to the scalp. This activity results in wave lines of EEG signals. Here, we used a combination of the K-means algorithm and SVM-supervised machine learning algorithm to analyze the EEG data. K-means algorithm is used to divide and attempt to cluster the data while the SVM algorithm is used to classify the stress level based on these clusters. The main objective of this model is to improve the accuracy of stress detection by using the strength of both supervised and unsupervised machine learning algorithms. Using this hybrid approach, we attempt to provide a more precise and reliable method for detecting stress in real-time EEG data, with wide application in biomedical diagnosis, mental health monitoring, and many more.","","979-8-3503-9017-9","10.1109/AKGEC62572.2024.10869062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10869062","Classifier;EEG;SVM;K-means;Hybrid algorithm;cross-validation;accuracy;real-time stress analysis","Support vector machines;Machine learning algorithms;Accuracy;Clustering algorithms;Human factors;Electroencephalography;Real-time systems;Classification algorithms;Reliability;Stress measurement","","","","17","IEEE","11 Feb 2025","","","IEEE","IEEE Conferences"
"Emotion Recognition by Learning the Manifold of Fused Multiscale Information of EEG Signals","C. Li; S. Zhang; Y. Mu; L. Yang; Y. Peng; F. Li; Y. Zhang; Z. Liang; Z. Cao; F. Wan; D. Yao; P. Li; P. Xu","Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Technology, Southwest University of Science and Technology, Mianyang, China; School of Biomedical Engineering, Medical School, Shenzhen University, Shenzhen, China; STEM, Mawson Lakes Campus, University of South Australia, Adelaide, SA, Australia; Department of Electrical and Computer Engineering, Faculty of Science and Technology, and the Centre for Cognitive and Brain Sciences, Institute of Collaborative Innovation, University of Macau, Macau, China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Bioinfomatics, Chongqing University of Posts and Telecommunications, Chongqing, China; Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation and School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, China",IEEE Transactions on Affective Computing,"","2025","PP","99","1","17","Recent research has consistently indicated that the fusion of electroencephalography (EEG) features from multiple modalities can integrate cognitive state expressions across diverse dimensions, resulting in a substantial increase in emotion recognition accuracy. However, redundant information within the fused multimodal features could lead to the curse of dimensionality and overfitting of the learning model. In this work, we propose a multiscale EEG feature fusion and representation strategy for EEG emotion recognition named manifold of multiscale information fusion (MMIF), in which the optimal manifold of the multiscale fusion of local and global brain activation patterns can be automatically learned to realize an efficient representation of emotional EEG signals. To evaluate the performance, in this work, both off- and online EEG emotion recognition experiments were conducted, and the experimental results consistently verified the effectiveness and feasibility of the MMIF applied in real-time emotion decoding systems. Furthermore, the analytical experiments confirmed the discriminative capabilities and cognitive interpretability of the MMIF. In summary, the proposed MMIF model may provide an efficient avenue for exploring representations and enhancing the discrimination of multimodal fusion features, which may also provide a promising solution for designing online affective braincomputer interaction systems.","1949-3045","","10.1109/TAFFC.2025.3555226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10943131","Affective braincomputer interaction systems;electroencephalography (EEG);emotion recognition;manifold learning;multiscale information fusion","Electroencephalography;Emotion recognition;Feature extraction;Brain modeling;Accuracy;Manifolds;Cognition;Artificial intelligence;Affective computing;Decoding","","","","","IEEE","28 Mar 2025","","","IEEE","IEEE Early Access Articles"
"Graph-Embedded Convolutional Neural Network for Image-Based EEG Emotion Recognition","T. Song; W. Zheng; S. Liu; Y. Zong; Z. Cui; Y. Li","Key Laboratory of Child Development and Learning Science, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science, Ministry of Education, Southeast University, Nanjing, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Key Laboratory of Child Development and Learning Science, Ministry of Education, Southeast University, Nanjing, China",IEEE Transactions on Emerging Topics in Computing,"2 Sep 2022","2022","10","3","1399","1413","Emotion recognition from electroencephalograph (EEG) signals has long been essential for affective computing. In this article, we evaluate EEG emotion recognition by converting EEG signals from multiple channels into images such that richer spatial information can be considered and the question of EEG-based emotion recognition can be converted into image recognition. To this end, we propose a novel method to generate continuous images from discrete EEG signals by introducing offset variables following a Gaussian distribution for each EEG channel to alleviate the biased electrode coordinates during image generation. In addition, a novel graph-embedded convolutional neural network (GECNN) method is proposed to combine the local convolutional neural network (CNN) features with global functional features to provide complementary emotion information. In GECNN, the attention mechanism is applied to extract more discriminative local features. Simultaneously, dynamical graph filtering explores the intrinsic relationships between different EEG regions. The local and global functional features are finally fused for emotion recognition. Extensive experiments in subject-dependent and subject-independent protocols are conducted to evaluate the performance of the proposed GECNN model on four datasets, i.e., SEED, SDEA, DREAMER, and MPED.","2168-6750","","10.1109/TETC.2021.3087174","National Natural Science Foundation of China(grant numbers:U2003207,61921004,62076064,61902064); Jiangsu Frontier Technology Basic Research Project(grant numbers:BK20192004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448460","EEG emotion recognition;graph convolutional neural network;EEG image generation;graph embedded convolutional neural network","Electroencephalography;Feature extraction;Electrodes;Emotion recognition;Brain modeling;Image recognition;Data mining","","68","","45","IEEE","8 Jun 2021","","","IEEE","IEEE Journals"
"Improving Cross-Subject Emotion Recognition Performance with an Encoder-Decoder Structure","H. Cui; H. Shi; B. -L. Lu; W. -L. Zheng","School of Life Sciences and Biotechnology, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","Emotion recognition based on EEG is an important task in the field of affective computing. Due to individual differences, emotion recognition performance of cross-subject models is significantly lower than that of subject-dependent models. To minimize the degradation of emotion recognition performance due to the differences in the EEG distributions of different subjects, domain adaptation algorithms have been used to transfer the knowledge from source to target domains, and have achieved good performance in the task of cross-subject emotion recognition. However, most domain adaptation methods do not take into account the possible correspondence between the samples in the source and target domains. Therefore, we adopt an encoder-decoder architecture, the EEG converter, which utilizes the time alignment condition between the source and target domains during training. In the EEG converter structure, the encoder consists of a series of convolutional layers and max pooling layers, and the decoder consists of a series of upsampling layers and convolutional layers. We use the EEG converter to transfer the differential entropy features of EEG signals, from one subject to another, on datasets SEED, SEED-IV, and SEEDV. The results show that the transfer effect of the EEG converter significantly improves the performance of emotion recognition and outperforms existing domain adaptation algorithms.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782033","National Natural Science Foundation of China; Shanghai Jiao Tong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782033","","Training;Degradation;Emotion recognition;Adaptation models;Convolution;Computational modeling;Brain modeling;Electroencephalography;Entropy;Engineering in medicine and biology","Humans;Electroencephalography;Emotions;Algorithms;Signal Processing, Computer-Assisted","1","","14","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Electroencephalogram Emotion Recognition Using Combined Features in Variational Mode Decomposition Domain","Z. -T. Liu; S. -J. Hu; J. She; Z. Yang; X. Xu","School of Automation and Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, China University of Geosciences, Wuhan, China; School of Automation and Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, China University of Geosciences, Wuhan, China; School of Engineering, Tokyo University of Technology, Tokyo, Hachioji, Japan; Department of Rehabilitation, Union Hospital Tongji Medical College, Wuhan, China; School of Automation and Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, China University of Geosciences, Wuhan, China",IEEE Transactions on Cognitive and Developmental Systems,"7 Sep 2023","2023","15","3","1595","1604","Using electroencephalogram (EEG) to recognize human emotion has attracted increasing attention. However, feature extraction from EEG is a challenging work because it is a nonstationary continuous sequential signal. To obtain more pattern information, a combined feature extraction method in the variational mode decomposition (VMD) domain is proposed, which can extract local features of EEG signals to overcome the effects caused by nonstationarity. This method first decomposes EEG into several components using VMD and then combined features of differential entropy (DE) and short-time energy (STE) are extracted from each component. To optimize combined features, the important features are selected by tree modes, and the feature set is dimensionally reduced by further using linear discriminant analysis (LDA). Moreover, an XGBoost classifier with Bayesian optimization is presented to classify different emotional states. Binary-class and multiclass EEG emotion recognition are conducted on the DEAP data set, from which the experimental results show that accuracy of binary-class classification is 81.77% for high/low valence and 80.47% for high/low arousal, and accuracy of 91.41%, 94.27%, 94.27%, and 93.49% are obtained for HVHA, LVHA, LVLA, and HVLA, respectively, which demonstrate its effectiveness.","2379-8939","","10.1109/TCDS.2022.3233858","National Natural Science Foundation of China(grant numbers:61976197,61403422,61273102); Hubei Provincial Natural Science Foundation of China(grant numbers:2018CFB447,2015CFA010); Wuhan Applied Foundational Frontier Project(grant numbers:2020010601012175); Wuhan Applied Basic Research Programs Project(grant numbers:2017010201010133); Higher Education Discipline Innovation Project(grant numbers:B17040); Fundamental Research Funds for National University, China University of Geosciences (Wuhan)(grant numbers:1910491T01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005145","Bayesian optimization;combined features;electroencephalogram (EEG);emotion recognition;variational mode decomposition (VMD)","Feature extraction;Electroencephalography;Emotion recognition;Bayes methods;Optimization;Bandwidth;Frontal lobe","","9","","58","IEEE","3 Jan 2023","","","IEEE","IEEE Journals"
"EEG-Enabled Affective Applications","O. Sourina; Y. Liu","Fraunhofer IDM@NTU, Nanyang Technological University, Singapore; Fraunhofer IDM@NTU, Nanyang Technological University, Singapore",2013 Humaine Association Conference on Affective Computing and Intelligent Interaction,"12 Dec 2013","2013","","","707","708","Using Electroencephalogram (EEG) signals for affective interaction can make interfaces more intuitive. This project includes development of different affective applications based on an EEG-based real-time emotion recognition algorithm. The algorithm is subject-dependent one and consists from two parts: feature extraction and classification. The algorithm can recognize up to eight emotions with good accuracy. The demo includes affective games and emotional avatar applications. After a short session to train the classifier, an application is able to monitor the user's emotions, and the recognized emotions are used as the input to the applications.","2156-8111","978-0-7695-5048-0","10.1109/ACII.2013.125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681516","EEG;emotion classification;affective computing;affective applications","Games;Emotion recognition;Real-time systems;Electroencephalography;Avatars;Color;Classification algorithms","","8","","6","IEEE","12 Dec 2013","","","IEEE","IEEE Conferences"
"Increasing the Stability of EEG-based Emotion Recognition with a Variant of Neural Processes","Y. -K. Liu; W. -B. Jiang; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2022 International Joint Conference on Neural Networks (IJCNN),"30 Sep 2022","2022","","","1","6","Electroencephalography (EEG) signals provide an incredible promising access to decode human emotions in affective computing. Many approaches have been applied to building EEG-based affective models and much endeavor is made to improve the performance of affective models, where test data has similar quality as training data. However, due to the strong sensitivity of EEG to external factors such as body movement and electromagnetic interference, EEG signals usually have a lot of noise and subjects have to remain as motionless as possible in a quiet environment, which is difficult to be satisfied in real applications and severely influences the user experience. To deal with this problem, a common way is to drop those channels with intensive noise. However, this results in the loss of critical information and neither existing machine learning (ML) nor deep learning (DL) approaches can handle this situation well, especially when too many channels are missed. In this paper, we propose a robust variant of the neural processes model and evaluate the stability of our model under various circumstances to simulate random data corruption in real applications. We conduct two categories of experiments by controlling the number and the places of missing channels separately and compare with classical ML and DL models. The results demonstrate that the performance of our model significantly outperforms the existing models, even using only five channels. We also explore the critical brain regions in the current EEG electrode distribution. Final performance manifests our model has extreme stability in dealing with intractable situations and sheds light on the widespread usage of portable EEG-based affective computing.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892825","National Natural Science Foundation of China(grant numbers:61976135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892825","EEG;Stability;Emotion Recognition;Affective Computing;Neural Processes;Critical Brain Regions","Emotion recognition;Affective computing;Sensitivity;Computational modeling;Training data;Brain modeling;Electroencephalography","","2","","18","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Sparse Bayesian Learning for End-to-End EEG Decoding","W. Wang; F. Qi; D. P. Wipf; C. Cai; T. Yu; Y. Li; Y. Zhang; Z. Yu; W. Wu","School of Automation Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China; School of Internet Finance and Information Engineering, Guangdong University of Finance, Guangzhou, Guangdong, China; Amazon Shanghai AI Lab, Shanghai, China; National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, Hubei, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China; Department of Bioengineering, Lehigh University, Bethlehem, PA, USA; School of Automation Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China; Alto Neuroscience Inc., Los Altos, CA, USA",IEEE Transactions on Pattern Analysis and Machine Intelligence,"3 Nov 2023","2023","45","12","15632","15649","Decoding brain activity from non-invasive electroencephalography (EEG) is crucial for brain-computer interfaces (BCIs) and the study of brain disorders. Notably, end-to-end EEG decoding has gained widespread popularity in recent years owing to the remarkable advances in deep learning research. However, many EEG studies suffer from limited sample sizes, making it difficult for existing deep learning models to effectively generalize to highly noisy EEG data. To address this fundamental limitation, this paper proposes a novel end-to-end EEG decoding algorithm that utilizes a low-rank weight matrix to encode both spatio-temporal filters and the classifier, all optimized under a principled sparse Bayesian learning (SBL) framework. Importantly, this SBL framework also enables us to learn hyperparameters that optimally penalize the model in a Bayesian fashion. The proposed decoding algorithm is systematically benchmarked on five motor imagery BCI EEG datasets ($N=192$N=192) and an emotion recognition EEG dataset ($N=45$N=45), in comparison with several contemporary algorithms, including end-to-end deep-learning-based EEG decoding algorithms. The classification results demonstrate that our algorithm significantly outperforms the competing algorithms while yielding neurophysiologically meaningful spatio-temporal patterns. Our algorithm therefore advances the state-of-the-art by providing a novel EEG-tailored machine learning tool for decoding brain activity.","1939-3539","","10.1109/TPAMI.2023.3299568","National Natural Science Foundation of China(grant numbers:61836003,61876063,61906048,62007013); Technology Innovation 2030(grant numbers:2022ZD0211700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197212","Electroencephalography (EEG);brain-computer interface (BCI);emotion recognition;decoding;spatio-temporal filtering;sparse Bayesian learning","Electroencephalography;Decoding;Classification algorithms;Finite impulse response filters;Filtering algorithms;Brain modeling;Feature extraction","Algorithms;Brain;Bayes Theorem;Machine Learning;Electroencephalography;Brain-Computer Interfaces;Imagination","36","","54","IEEE","28 Jul 2023","","","IEEE","IEEE Journals"
"EEG-Based Human Emotion Prediction Using an LSTM Model","S. Mohsen; A. G. Alharbi","Electronics and Communications Engineering Department, Al-Madina Higher Institute for Engineering and Technology, Giza, Egypt; Department of Electrical Engineering, Faculty of Engineering, Jouf University, Saudi Arabia",2021 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS),"13 Sep 2021","2021","","","458","461","It is deemed essential to identify and classify human emotions via deep learning with computers. Therefore, electroencephalogram (EEG) is extensively used as a physiological source of emotions. In this paper, a long short-term memory (LSTM) model is proposed for classification of positive, neutral, and negative emotions. This model is applied to a dataset that includes three classes of emotions with a total of 2,100 EEG samples from two subjects. The proposed model is trained using TensorFlow library with a tuning method to achieve maximum accuracy for emotion prediction. To appraise the model performance, receiver operating characteristic (ROC) curve is utilized. Experimental results demonstrate that the proposed model attains a high performance in the classification of human emotions. Furthermore, the proposed LSTM model has a testing accuracy of 98.13%, a macro average precision of 98.14%, and the area under the ROC curve for this model is 100%.","1558-3899","978-1-6654-2461-5","10.1109/MWSCAS47672.2021.9531707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9531707","Deep Learning;Electroencephalogram;Long Short-Term Memory;Emotions","Computational modeling;Receivers;Predictive models;Brain modeling;Electroencephalography;Physiology;Libraries","","9","","19","IEEE","13 Sep 2021","","","IEEE","IEEE Conferences"
"Interpretability for Multimodal Emotion Recognition using Concept Activation Vectors","A. R. Asokan; N. Kumar; A. V. Ragam; S. S. Shylaja","Dept. of Computer Science, PES University, Bangalore, India; Dept. of Computer Science, PES University, Bangalore, India; Dept. of Computer Science, PES University, Bangalore, India; Dept. of Computer Science, PES University, Bangalore, India",2022 International Joint Conference on Neural Networks (IJCNN),"30 Sep 2022","2022","","","01","08","Multimodal Emotion Recognition refers to the classification of input video sequences into emotion labels based on multiple input modalities (usually video, audio and text). In recent years, Deep Neural networks have shown remarkable performance in recognizing human emotions, and are on par with human-level performance on this task. Despite of recent advancements in this field, emotion recognition systems are yet to be accepted for real world setups due to the obscure nature of their reasoning and decision-making process. Most of the research in this field deals with novel architectures to improve the performance for this task, with a few attempts at providing explanations for these models' decisions. In this paper, we address the issue of interpretability for neural networks in the context of emotion recognition using Concept Activation Vectors (CAVs). To analyse the model's latent space, we define human-understandable concepts specific to Emotion AI and map them to the widely-used IEMOCAP multimodal database. We then evaluate the influence of our proposed concepts at multiple layers of the Bi-directional Contextual LSTM (BC-LSTM) network to show that the reasoning process of neural networks for emotion recognition can be represented using human-understandable concepts. Finally, we perform hypothesis testing on our proposed concepts to show that they are significant for interpretability of this task.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892315","Multimodal Emotion Recognition;Interpretability;Concept Activation Vectors;Emotion AI","Deep learning;Emotion recognition;Databases;Neural networks;Video sequences;Decision making;Cognition","","7","","31","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Spatio-temporal EEG signal descriptors for recognition of negative emotional states","F. N. Feradov; T. D. Ganchev","Faculty of Automation and Computing, Technical University of Varna, Varna, Bulgaria; Faculty of Automation and Computing, Technical University of Varna, Varna, Bulgaria",2016 XXV International Scientific Conference Electronics (ET),"24 Nov 2016","2016","","","1","4","In the following paper we present a study on novel signal descriptors for the purposes of automated recognition of negative emotional states from EEG signals - namely, the decorrelated values of the energy of the spatio-temporal distribution of EEG activity. Using the extracted features person-specific SVM models are created. The experimental setups are based on data taken from the DEAP database. The classification accuracy of the proposed features is evaluated using two experimental setups: valence detection and like/dislike detection. Recognition accuracy of 77.5% and 78.0%, respectfully, was achieved.","","978-1-5090-2883-2","10.1109/ET.2016.7753477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7753477","EEG;Negative emotion detection;spatial features","Electroencephalography;Feature extraction;Filter banks;Emotion recognition;Databases;Decorrelation;Physiology","","1","","9","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"AI-Assisted Analysis of EEG for Diagnosis of Autism Spectrum Disorder: Pilot Study","Y. Karpliuk; A. Popov; V. Kharytonov; K. Ivanko; H. Porieva","Department of Electronic Engineering, Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine; Department of Electronic Engineering, Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine; Child Psychiatry Department, Clinical Hospital “Psychiatry”, Kyiv, Ukraine; Department of Electronic Engineering, Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine; Department of Electronic Engineering, Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine",2024 IEEE 42nd International Conference on Electronics and Nanotechnology (ELNANO),"28 Nov 2024","2024","","","412","416","This paper addresses the intricate challenge of classifying electroencephalogram (EEG) signals to distinguish between individuals with Autism Spectrum Disorder (ASD) and Normal controls. Autism Spectrum Disorder is a complex neurodevelopmental condition exhibiting a diverse array of symptoms across social communication, behavior, and cognition. The EEGNetv4 deep learning model is employed, trained on data from a limited cohort of 9 subjects. The study utilizes a Leave-One-Subject-Out cross-validation methodology to assess the model's generalization across a small and unbalanced dataset. Following the cross-validation process, the model exhibits varying accuracies across folds, emphasizing the heterogeneous nature of ASD. Notably, the highest cross-validation accuracy is achieved in fold #3, reaching 83.53%, showcasing the model's proficiency in certain subject-specific contexts. Subsequently, the model trained on fold #3 is selected for further testing, demonstrating a test set accuracy of 74.49% on EEG data from two additional subjects, one representing a Normal control and the other diagnosed with ASD. This research underscores the inherent challenges posed by the small and unbalanced dataset, highlighting the necessity for cautious interpretation of results. Moreover, the absence of EEG manual preprocessing with labeling by experts regarding EEG background activity and the presence of significant artifacts that were not eliminated by prior events rejection underscores the potential impact of signal quality on classification outcomes. Despite these challenges, the study provides insights into the feasibility of utilizing EEGNetv4 for ASD classification and calls for future research to address dataset limitations and enhance model robustness.","2693-3535","979-8-3503-6817-8","10.1109/ELNANO63394.2024.10756937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756937","autism;autistic spectrum disorder;electroencephalogram;deep learning;machine learning;early diagnosis","Autism;Accuracy;Manuals;Brain modeling;Electroencephalography;Data models;Robustness;Labeling;Nanotechnology;Testing","","","","13","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"BCI-based Emotion recognition by Combining Kernel Classifiers and EEG Feature Selection","R. D. Varshney; R. K. Gadhia; Ritvik; A. Bhat","Department of Computer Science and Engineering, Delhi Technological University, Delhi, India; Department of Computer Science and Engineering, Delhi Technological University, Delhi, India; Department of Computer Science and Engineering, Delhi Technological University, Delhi, India; Department of Computer Science and Engineering, Delhi Technological University, Delhi, India",2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS),"8 Jun 2023","2023","","","1809","1815","This study focuses on creating an online emotion detection brain-computer interface (BCI) system that identifies emotional states from EEG data using advanced signal processing techniques. To achieve better results, the study proposes a feature selection method that measures the spectral power of theta, alpha, beta, and gamma frequencies for each electrode extracted from the EEG data using Welch’s approach. EEG channels are categorized into six groups based on their location in the brain, and a FIR filter in the provided model eliminates power line interference while keeping the spectral content within the intended frequency range. Cross-validation techniques are applied, and the best classifier is selected from support vector machine, KNN, and MLP for various EEG areas to improve emotion categorization. The obtained classification rates of 75% and 72% in arousal and valence, respectively, are encouraging when compared to recent comparable work.","2768-5330","979-8-3503-9725-3","10.1109/ICICCS56967.2023.10142682","Delhi Technological University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10142682","Electroencephalography;K-Nearest Neighbors;Multi-Layer Perceptron;Welch Algorithm;Multi-Class Classification Algorithm;DEAP;Brain-Computer Interface","Support vector machines;Emotion recognition;Computational modeling;Feature extraction;Brain modeling;Electroencephalography;Brain-computer interfaces","","1","","15","IEEE","8 Jun 2023","","","IEEE","IEEE Conferences"
"Improving ADHD Diagnosis with Real-Time EEG Artifact Correction and Emotional Insights","S. M; S. Parameswaran; S. S; I. K; S. M. S; S. T","Department of Biomedical Engineering, KSR Institute for Engineering and Technology, Tiruchengode, Tamilnadu, India; Department of Biomedical Engineering, KSR Institute for Engineering and Technology, Tiruchengode, Tamilnadu, India; Department of Biomedical Engineering, KSR Institute for Engineering and Technology, Tiruchengode, Tamilnadu, India; Department of Biomedical Engineering, KSR Institute for Engineering and Technology, Tiruchengode, Tamilnadu, India; Department of Biomedical Engineering, KSR Institute for Engineering and Technology, Tiruchengode, Tamilnadu, India; Department of Biomedical Engineering, KSR Institute for Engineering and Technology, Tiruchengode, Tamilnadu, India",2024 4th International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS),"13 Feb 2025","2024","","","1963","1971","‐ Attention Deficit Hyperactivity Disorder (ADHD) diagnosis is often subjective, posing challenges to accuracy. This study enhances diagnosis by integrating real-time EEG artifact correction and emotional insights. EEG, a non-invasive neuroimaging tool, detects brain activity patterns, such as abnormal theta-to-beta ratios, commonly linked to ADHD. However, EEG signals are prone to artifacts from muscle movements and external noise. To address this, Independent Component Analysis (ICA) and machine learning algorithms are employed for real-time artifact correction. In addition, emotion recognition through EEG provides insights into emotional dysregulation, often seen in ADHD. Results show significant improvements in EEG signal quality, emotional state classification, and diagnostic accuracy, achieving 90% accuracy. These findings highlight the potential of combining neural and emotional data to enhance ADHD diagnosis and foster personalized treatment strategies targeting both cognitive and emotional components of the disorder.","","979-8-3315-2963-5","10.1109/ICUIS64676.2024.10866175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10866175","ADHD;EEG;Artifact Correction;Emotional Insights;Machine Learning;Diagnostic Accuracy;Theta-Beta Ratio","Support vector machines;Technological innovation;Accuracy;Noise;Independent component analysis;Ubiquitous computing;Electroencephalography;Real-time systems;Regulation;Physiology","","","","23","IEEE","13 Feb 2025","","","IEEE","IEEE Conferences"
"A Multitask Framework for Emotion Recognition Using EEG and Eye Movement Signals with Adversarial Training and Attention Mechanism","W. Liu; Y. Luo; Y. Lu; Y. Lu","Clinical Neuroscience Center Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China; Clinical Neuroscience Center Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China; International Department, Shanghai Luwan Senior High School, Shanghai, China; Clinical Neuroscience Center Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China",2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"18 Jan 2024","2023","","","2551","2558","Affective brain-computer interface is one of the research frontiers which could promote the development of artificial intelligence and which might help the diagnosis and treatment of mental health diseases. In the field of emotion recognition with EEG and eye movement signals, however, it is challenging to build a model that can extract emotion-related information, fuse multiple modalities, and preserve the complementary characteristics at the same time. In this paper, we proposed a multi-task framework with adversarial training and attention mechanism (ATAM) for emotion recognition using EEG and eye movement signals. An adversarial training scheme is designed by maximizing mutual information loss within the same modalities and minimizing the cosine similarity loss between different modalities. With this design, the proposed ATAM not only preserved emotional information in EEG and eye movement signals, but also kept the complementary property between these two modalities. Attention mechanism was used to fuse multimodal features adaptively. Our evaluation of the model using the SEED and SEED-IV datasets revealed that the proposed method surpassed existing methodologies in recognition accuracy. Further analysis of the loss curves and attentional weight distributions indicated the effectiveness of ATAM in transforming and incorporating multi-modal properties. The adversarial training set-up, epitomized in our ablation study, was necessary for good performance.","2156-1133","979-8-3503-3748-8","10.1109/BIBM58861.2023.10385505","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385505","EEG;eye movement;emotion;adversarial training;attention mechanism","Training;Emotion recognition;Fuses;Transforms;Mental health;Brain modeling;Multitasking","","","","32","IEEE","18 Jan 2024","","","IEEE","IEEE Conferences"
"Unsupervised Domain Adaptation With Pseudo-Label Propagation for Cross-Domain EEG Emotion Recognition","X. -C. Zhong; Q. Wang; R. Li; Y. Liu; S. Duan; R. Yang; D. Liu; J. Sun","School of Instrumentation Science and Engineering, Harbin Institute of Technology, Harbin, China; School of Instrumentation Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Anesthesiology, Heilongjiang Provincial Hospital, Harbin, China; School of Instrumentation Science and Engineering, Harbin Institute of Technology, Harbin, China; School of Instrumentation Science and Engineering, Harbin Institute of Technology, Harbin, China; School of Instrumentation Science and Engineering, Harbin Institute of Technology, Harbin, China; School of Instrumentation Science and Engineering, Harbin Institute of Technology, Harbin, China; School of Instrumentation Science and Engineering, Harbin Institute of Technology, Harbin, China",IEEE Transactions on Instrumentation and Measurement,"8 Apr 2025","2025","74","","1","11","Emotion recognition from electroencephalography (EEG) signals is increasingly emerging as a critical research focus in brain-computer interfaces (BCIs). However, challenges such as the scarcity of emotion labels and distribution discrepancies in EEG signals significantly hinder the practical application of EEG-based emotion recognition. To overcome these challenges, this article fully exploits the continuity of emotion-related EEG data and proposes an unsupervised domain adaptation (DA) with pseudo-label propagation (PLP), termed DA method combined with PLP (DAPLP), for cross-domain EEG emotion recognition. Specifically, we first perform global distribution alignment (GDA) between the source and target domains and utilize the source classifier to generate pseudo-labels for the target domain. From these predictions, reliable pseudo-labels are then selected to guide label propagation, and the propagation process is further optimized with correct and smooth techniques. Systematic experiments conducted on the SEED, SEED-IV, and SEED-V datasets reveal that the proposed DAPLP accomplishes competitive performance compared to advanced existing methods, reaching average accuracies of 89.44%/74.57%/69.15% in cross-subject evaluation and 96.41%/82.20%/84.70% in cross-session evaluation, respectively. Moreover, our proposed DAPLP exhibits strong practical potential and robust performance in unsupervised cross-domain emotion recognition.","1557-9662","","10.1109/TIM.2025.3553234","Fundamental Research Funds for Central Universities(grant numbers:IR2021222); Future Science and Technology Innovation Team Project of Harbin Institute of Technology (HIT)(grant numbers:216506); Scientific Research Project of the Health Commission of Heilongjiang Province(grant numbers:20232020010430); Key Project of Higher Education Teaching Reform of Heilongjiang Province(grant numbers:SJGZY2024003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10944516","Label propagation;brain-computer interfaces (BCIs);domain adaptation (DA);electroencephalography (EEG);emotion recognition","Electroencephalography;Emotion recognition;Reliability;Feature extraction;Brain modeling;Accuracy;Adaptation models;Training;Statistical distributions;Fast Fourier transforms","","","","49","IEEE","28 Mar 2025","","","IEEE","IEEE Journals"
"A Weighted Co-Training Framework for Emotion Recognition Based on EEG Data Generation Using Frequency-Spatial Diffusion Transformer","Y. Yi; Y. Xu; B. Yang; Y. Tian","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Emergency Technology and Management, North China Institute of Science and Technology, Hebei, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China",IEEE Transactions on Affective Computing,"26 Nov 2024","2024","15","4","2055","2069","Emotion recognition based on EEG signals has been a challenging task. The acquisition of EEG signals is complex, time-consuming, and has a high overhead. Artificial Intelligence Generated Content technology has been developing rapidly in image and sound generation, but effective generative models for EEG signal generation have rarely appeared. To address the problem, we propose a weighted co-training framework for emotion recognition using a frequency-spatial diffusion transformer. We propose the EEG signal generation model by utilizing frequency-spatial correlation. In the first step, we use forward diffusion to add noise to the real samples and then use the proposed model to denoise and restore the real EEG signals. It ensures that the trained generative model can generate real EEG signals. Then, we use the denoising process of the proposed model to generate a large amount of data and then use the pseudo-data weighting module to evaluate the generated samples further. Finally, the actual samples and weighted pseudo-data are used to train the classifier for better generalization jointly. We conducted comprehensive experiments on three EEG emotion recognition benchmark datasets. The experimental results show that our method improves by 0.86%-1.91% compared to state-of-the-art methods. In addition, quantitative and qualitative analysis proves the effectiveness of our proposed method.","1949-3045","","10.1109/TAFFC.2024.3395359","National Key R&D Program of China(grant numbers:2020YFC0833102); Central Government Guides Local Funds for Science and Technology Development(grant numbers:236Z0307G); Fundamental Research Funds for the Central Universities(grant numbers:3142021002,3142023021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10510555","Weighted co-training framework;frequency-spatial diffusion transformer;emotion recognition;EEG data generation","Electroencephalography;Brain modeling;Emotion recognition;Generative adversarial networks;Feature extraction;Correlation;Transformers","","2","","80","IEEE","30 Apr 2024","","","IEEE","IEEE Journals"
"Brain Computer Interface based EEG for Emotion Recognition System: A Systematic Review","P. R. Bhise; S. B. Kulkarni; T. A. Aldhaheri","Department of Computer Science and Information Technology, Dr.Babasaheb Ambedkar Marathwada University, Aurangabad, Maharashtra, India; Department of Computer Science and Information Technology, Dr.Babasaheb Ambedkar Marathwada University, Aurangabad, Maharashtra, India; Department of Computer Science and Information Technology, Dr.Babasaheb Ambedkar Marathwada University, Aurangabad, Maharashtra, India",2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),"23 Apr 2020","2020","","","327","334","Currently the Brain-Compute Interface (BCI) is an outstanding research area where the goal is to create an interaction channel between system and person brain. It provides direct way to transform brainwaves into physical effects without using muscles. It presents an auspicious technology which granting individuals to deal with extraneous instruments through managing their brain signals. In this technology the noninvasive BCI technique that is electroencephalography plays a vital role for acquisition of brain signals and developing Emotion Recognition System. The Emotions are very important in our life for interaction, decision handling and cognitive process. This paper contains types of BCI system, it also explored to neuro imaging techniques for acquisition of brain signals, basic functioning of brain and comprehensive survey on EEG-Based BCI system for human emotion recognition. Various classification techniques are available for emotion classifications but among these support vector machine classification techniques is most preferred by the various researchers for analyzing the emotions.","","978-1-7281-4167-1","10.1109/ICIMIA48430.2020.9074921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9074921","Brain computer interface;EEG (Electro Encephalogram);emotion;ERP;ERD","Electroencephalography;Feature extraction;Support vector machines;Imaging;Brain modeling;Brain-computer interfaces","","19","","46","IEEE","23 Apr 2020","","","IEEE","IEEE Conferences"
"Human Emotion Classification using KNN Classifier and Recurrent Neural Networks with Seed Dataset","K. N. V. Satyanarayana; V. Tejasri; Y. S. N. Srujitha; K. N. S. Mounisha; S. T. Yerramsetti; G. D. Darapu","Department of ECE, SRKR Engineering College; Department of IT, SRKR Engineering College; Department of ECE, SRKR Engineering College; Department of ECE, SRKR Engineering College; Department of ECE, SRKR Engineering College; Department of ECE, SRKR Engineering College",2022 6th International Conference on Computing Methodologies and Communication (ICCMC),"13 Apr 2022","2022","","","1717","1722","Emotions which can be commonly called to be as human feelings, are variable and numerous. They vary according to the situation or according to perception. Analyzing and classifying those emotions are very crucial in current situations. For example, for knowing the review of the product, the developer can use this emotion detection to see whether the client is satisfied with the product and can understand the likeliness of the product. Accordingly, he can vary it, and in health care for finding the depression in a person. So, this makes the classification of human feelings more vulnerable. Here initially, the data is being collected from the brain via EEG Signals, and fed into a mock dataset, and then these EEG Signal features can be extracted by using KNN Classifier to classify the data but To improve several parameters like time of execution and accuracy this seed data can be classified using the RNN(recurrent neural networks). For a small dataset, K nearest neighbour may work efficiently, but for large datasets and more classifications, a Recurrent neural network is more efficient. Here when a small seed dataset is being considered, It produces good accuracy and classification of the data. Computing using this process produces the best accuracy of 96.22% by the KNN classifier and Test accuracy of 85.71% by Recurrent Neural Networks.","","978-1-6654-1028-1","10.1109/ICCMC53470.2022.9754091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754091","EEG Signal;Emotion Classification;Seed dataset;KNN Classifier;RNN","Emotion recognition;Recurrent neural networks;Medical services;Feature extraction;Depression;Electroencephalography;Data mining","","3","","48","IEEE","13 Apr 2022","","","IEEE","IEEE Conferences"
"Temporal-Spatial Prediction: Pre-Training on Diverse Datasets for EEG Classification","Z. Li; L. -M. Zhao; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Emotion Helper, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","1806","1810","Electroencephalogram (EEG) classification tasks have received increasing attention because its high application value. Meanwhile, the great success of general pre-training models in language processing areas inspires us to excavate the potential of an EEG pre-trained model. This model is expected to adapt to diverse downstream tasks. However, current studies either ignore the temporal or spatial domain in EEG signals, or only use single datasets in pre-training. The proposed Temporal-Spatial Prediction (TSP) model effectively solve these issues. Specifically, the output of the TSP encoder serves as the input of two tasks: spatial prediction, i.e., masked autoencoder, and temporal prediction, i.e, contrastive predictive coding. In addition, in order to provide more diverse information and thus benefit the downstream fine-tuning, we pre-train TSP on six large EEG datasets with four different numbers of channels. Results on three public downstream datasets SEED, SEED-IV, TUEV demonstrate that TSP achieves the state-of-the-art performance on different EEG classification tasks. In addition, according to the ablation experiments, TSP performs better than the single-domain method, i.e. Temporal Prediction (TP) model and Spatial Prediction (SP) model.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10447845","National Natural Science Foundation of China; Shanghai Jiao Tong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10447845","EEG;Self-supervised Learning;Emotion Recognition;Transformer;CNN","Adaptation models;Speech recognition;Predictive models;Predictive coding;Signal processing;Brain modeling;Excavation","","3","","21","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"4D Recurrent Neural Network Based on Time-Space-Frequency Domain Fusion for EEG Emotion Recognition","H. Yang; M. Wang","College of Arts and Sciences, Northeast Agricultural University, Harbin, China; College of Arts and Sciences, Northeast Agricultural University, Harbin, China",2022 IEEE 4th International Conference on Civil Aviation Safety and Information Technology (ICCASIT),"27 Dec 2022","2022","","","230","236","In recent years, EEG emotion recognition has had broad prospects in various research fields. Most of the existing EEG identification studies use a single feature and do not simultaneously consider the effects of EEG information on time, space and frequency. However, neuroscience shows that the response degree of emotion depends on the EEG channels of different frequencies. In this paper, A recursive neural network approach is proposed to analyze EEG signals at different times and regions by integrating the time domain, frequency domain, and space domain. Firstly, the 2D channel map is extracted to obtain frequency domain features, and then the 2D channel map with different frequency domain features is fused to obtain 3D spatial frequency domain data. Finally, the 3D data in different time periods are connected to obtain the 4D feature structure used for the training depth model. 4D data feature makes up for the defect that a single feature cannot perceive channel information of different frequencies and produce a more significant effect than single feature emotion recognition. Experiments on the SEED data set show that the multi-feature fusion model performs better than other models in EEG emotion recognition.","","978-1-6654-6766-7","10.1109/ICCASIT55263.2022.9986771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9986771","emotion recognition;feature fusion;RNN neural network","Training;Emotion recognition;Time-frequency analysis;Recurrent neural networks;Three-dimensional displays;Brain modeling;Feature extraction","","","","10","IEEE","27 Dec 2022","","","IEEE","IEEE Conferences"
"An EEG-Based Multi-Modal Emotion Database with Both Posed and Authentic Facial Actions for Emotion Analysis","X. Li; X. Zhang; H. Yang; W. Duan; W. Dai; L. Yin","Department of Computer Science, State University of New York at Binghamton, New York, USA; Department of Computer Science, State University of New York at Binghamton, New York, USA; Department of Computer Science, State University of New York at Binghamton, New York, USA; Department of Computer Science, State University of New York at Binghamton, New York, USA; Department of Computer Science, State University of New York at Binghamton, New York, USA; Department of Computer Science, State University of New York at Binghamton, New York, USA",2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020),"18 Jan 2021","2020","","","336","343","Emotion is an experience associated with a particular pattern of physiological activity along with different physiological, behavioral and cognitive changes. One behavioral change is facial expression, which has been studied extensively over the past few decades. Facial behavior varies with a person's emotion according to differences in terms of culture, personality, age, context, and environment. In recent years, physiological activities have been used to study emotional responses. A typical signal is the electroencephalogram (EEG), which measures brain activity. Most of existing EEG-based emotion analysis has overlooked the role of facial expression changes. There exits little research on the relationship between facial behavior and brain signals due to the lack of dataset measuring both EEG and facial action signals simultaneously. To address this problem, we propose to develop a new database by collecting facial expressions, action units, and EEGs simultaneously. We recorded the EEGs and face videos of both posed facial actions and spontaneous expressions from 29 participants with different ages, genders, ethnic backgrounds. Differing from existing approaches, we designed a protocol to capture the EEG signals by evoking participants' individual action units explicitly. We also investigated the relation between the EEG signals and facial action units. As a baseline, the database has been evaluated through the experiments on both posed and spontaneous emotion recognition with images alone, EEG alone, and EEG fused with images, respectively. The database will be released to the research community to advance the state of the art for automatic emotion recognition.","","978-1-7281-3079-8","10.1109/FG47880.2020.00050","Binghamton University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320173","EEG;Database;Multimodal;Emotion;Affection;Facial action;Action unit;Pain;Fusion feature;Emotion recognition;Facial expression recognition;Action unit recognition","Electroencephalography;Videos;Physiology;Databases;Pain;Feature extraction;Two dimensional displays","","17","","36","IEEE","18 Jan 2021","","","IEEE","IEEE Conferences"
"EEG-based Emotion Recognition Using Sub-Band Time-Delay Correlations *","F. A. Alskafi; A. H. Khandoker; F. Marzbanrad; H. F. Jelinek","Department of Biomedical Engineering, Khalifa University, Abu Dhabi, UAE; Department of Biomedical Engineering and the HEIC, Khalifa University, Abu Dhabi, UAE; Department of Electrical and Computer Systems Engineering, Monash University, Clayton, Australia; Department of Biomedical Engineering, the HEIC, the Biotechnology Center, Khalifa University, Abu Dhabi, UAE",2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"11 Dec 2023","2023","","","1","4","Emotion recognition is a challenging task with many potential applications in psychology, psychiatry, and human-computer interaction (HCI). The use of time-delay information in the controlled time-delay stability (cTDS) algorithm can help to capture the temporal dynamics of EEG signals, including sub-band information and bi-directional coupling that can aid in emotion recognition and identification of specific connectivity patterns between brain rhythms. Incorporating EEG frequency bands can be used to design better emotion recognition systems. This paper evaluates the cTDS algorithm for binary classification tasks of arousal and valence using EEG sub-band signals. This method achieved a high accuracy of 91.1% for arousal and 91.7% for valence based on one electrode recording site at Fp1. The cTDS algorithm is a promising approach to analyzing brain network interactions. It can be particularly applicable to arousal and valence classification tasks, especially within a complex, multimodal feature space associated with understanding psychiatric disorders and HCI applications.","2694-0604","979-8-3503-2447-1","10.1109/EMBC40787.2023.10340014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340014","","Human computer interaction;Electrodes;Couplings;Emotion recognition;Time-frequency analysis;Heuristic algorithms;Electroencephalography","Humans;Electroencephalography;Emotions;Brain;Algorithms;Software","","","25","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Emotion recognition algorithm based on convolution neural network","C. Cheng; X. Wei; Z. Jian","College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China",2017 12th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),"15 Jan 2018","2017","","","1","5","Emotional recognition is a very challenging nature of the topic in the field of Brain - Computer Interface (BCI). This technology has been applied in many fields, such as The electroencephalogram (EEG) signals. The EEG signals can intuitively express the human emotional state and has attracted attentions of many researchers. Besides, it has a strong correlation during a period. To preserve the correlation, this paper presents an emotion recognition algorithm based on convolution neural network (ERACNN). In this paper, the EEG signals are pretreated, and then the parameters of CNN are selected. Finally, the classification model of emotion recognition is trained. Experimental results show that the results based on ERACNN is more robust than these based on Support Vector Machine (SVM). Besides, ERACNN can improve the classification accuracy of emotion recognition compared with the similar CNN algorithm.","","978-1-5386-1829-5","10.1109/ISKE.2017.8258786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258786","convolution neural network;emotion recognition;EEG signals;data preprocessing;parameter selection","Emotion recognition;Electroencephalography;Convolution;Feature extraction;Videos;Cost function;Training","","13","","12","IEEE","15 Jan 2018","","","IEEE","IEEE Conferences"
"A Survey on Affective Computing for Psychological Emotion Recognition","V. S. Bakkialakshmi; T. Sudalaimuthu","Department of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India; Department of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India","2021 5th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques (ICEECCOT)","16 Feb 2022","2021","","","480","486","Human emotions are precious and it directly exposes the mental wellbeing of human beings. Understanding the real emotion of people creates clues for many decisions making in critical situations. With recent enhancements in the field of artificial intelligence and machine learning, affective computing becomes an interesting area of researches that adopts the emotional behavior of humans and improves the learning impacts that are closely related to behavioral phycology. Evaluation of machine learning algorithms improves the prediction quality, but the problem arises at a decision making that is closely related to each other. The problem of single and multiple modality issues arises during the final emotion identification that makes the decision resilient. The proposed study focused on a broad survey about different techniques used to detect human behavior and real emotion by considering the data analysis tools and platforms. The proposed study takes affective computing as an initiate, considers the physiological data and their relation in the detection of emotions. The study also considers the recent techniques in comparison like deep learning algorithms, machine learning, and toolsets related to that. To find the compact algorithm that works well in decision making, also helpful in creating a novel idea for emotion recognition even better the proposed study is formulated.","","978-1-6654-3272-6","10.1109/ICEECCOT52851.2021.9707947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9707947","Affective computing;Emotion recognition;Machine learning;Psychology analysis;Deep learning","Affective computing;Emotion recognition;Machine learning algorithms;Decision making;Psychology;Computer architecture;Predictive models","","3","","34","IEEE","16 Feb 2022","","","IEEE","IEEE Conferences"
"Research on Multimodal Emotion Recognition Based on Fusion of Electroencephalogram and Electrooculography","J. Yin; M. Wu; Y. Yang; P. Li; F. Li; W. Liang; Z. Lv","AHU-IAI AI Joint Laboratory, Anhui University, Hefei, China; Laboratory of Intelligent Information and Human-Computer Interaction, Anhui University, Hefei, China; State Key Laboratory of Brain and Cognitive Sciences, Institute of Biophysics (IBP), Chinese Academy of Sciences, Beijing, China; Laboratory of Intelligent Information and Human-Computer Interaction, Anhui University, Hefei, China; Key Laboratory of Flight Techniques and Flight Safety, Civil Aviation Flight University of China, Guanghan, China; Google Research, Mountain View, CA, USA; Key Laboratory of Flight Techniques and Flight Safety, Civil Aviation Flight University of China, Guanghan, China",IEEE Transactions on Instrumentation and Measurement,"6 Mar 2024","2024","73","","1","12","Emotion recognition plays a vital role in building a harmonious society and emotional interaction. Recent research has demonstrated that multimodal interchannel correlations and insufficient emotion elicitation plague deep learning-based emotion identification techniques. To cope with these problems, we propose a multimodal and channel attention fusion transformer (MCAF-Transformer). First, we employ an olfactory video approach to evoke emotional expression more fully and acquire electroencephalogram (EEG) and electrooculography (EOG) signal data. Second, the model makes full use of multimodal channel information, time-domain and spatial-domain information of EEG and EOG signals, captures the correlation of different channels using channel attention, and improves the accuracy of emotion recognition by focusing on the global dependence on the temporal order using the transformer. We conducted extensive experiments on the olfactory video sentiment dataset, and the experimental results were correct at 94.63%. The results show that olfactory videos evoke emotion more adequately than pure videos and that the MCAF-Transformer model significantly outperforms other emotion recognition methods.","1557-9662","","10.1109/TIM.2024.3370813","National Natural Science Foundation of China (NSFC)(grant numbers:61972437); Distinguish Youth Foundation of Anhui Scientific Committee(grant numbers:2208085J05); Open Fund of Key Laboratory of Flight Techniques and Flight Safety, Civil Aviation Administration of China (CAAC)(grant numbers:FZ2022KF15); Special Fund for Key Program of Science and Technology of Anhui Province(grant numbers:202203a07020008); Cloud Ginger XR-1 Platform; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10445754","Attention mechanisms;electroencephalogram (EEG) and electrooculography (EOG);multimodal;olfactory video emotion evocation;transformer","Electroencephalography;Feature extraction;Emotion recognition;Brain modeling;Transformers;Electrooculography;Time-domain analysis","","","","45","IEEE","27 Feb 2024","","","IEEE","IEEE Journals"
"Confusion State Induction and EEG-based Detection in Learning","Y. Zhou; T. Xu; S. Li; S. Li","Shaanxi Normal University, Xi'an, Shaanxi, CN; Northwestern Polytechnical University, Xi'an, Shaanxi, CN; Shaanxi Normal University, Xi'an, Shaanxi, CN; Northwestern Polytechnical University, Xi'an, Shaanxi, CN",2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"28 Oct 2018","2018","","","3290","3293","Confusion, as an affective state, has been proved beneficial for learning, although this emotion is always mentioned as negative affect. Confusion causes the learner to solve the problem and overcome difficulties in order to restore the cognitive equilibrium. Once the confusion is successfully resolved, a deeper learning is generated. Therefore, quantifying and visualizing the confusion that occurs in learning as well as intervening has gained great interest by researchers. Among these researches, triggering confusion precisely and detecting it is the critical step and underlies other studies. In this paper, we explored the induction of confusion states and the feasibility of detecting confusion using EEG as a first step towards an EEG-based Brain Computer Interface for monitoring the confusion and intervening in the learning. 16 participants EEG data were recorded and used. Our experiment design to induce confusion was based on tests of Raven’s Standard Progressive Matrices. Each confusing and not-confusing test item was presented during 15 seconds and the raw EEG data was collected via Emotiv headset. To detect the confusion emotion in learning, we propose an end-to-end EEG analysis method. End-to-end classification of Deep Learning in Machine Learning has revolutionized computer vision, which has gained interest to adopt this method to EEG analysis. The result of this preliminary study was promising, which showed a 71.36% accuracy in classifying users’ confused and unconfused states when they are inferring the rules in the tests.","1558-4615","978-1-5386-3646-6","10.1109/EMBC.2018.8512943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8512943","","Electroencephalography;Feature extraction;Training;Task analysis;Cognition;Standards;Headphones","Brain-Computer Interfaces;Electroencephalography;Emotions;Machine Learning","12","","15","IEEE","28 Oct 2018","","","IEEE","IEEE Conferences"
"Explainable Data Poison Attacks on Human Emotion Evaluation Systems Based on EEG Signals","Z. Zhang; S. Umar; A. Y. A. Hammadi; S. Yoon; E. Damiani; C. A. Ardagna; N. Bena; C. Y. Yeun","Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science, Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates; Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates; Dipartimento di Informatica, Università degli Studi di Milano, Milan, Italy; Dipartimento di Informatica, Università degli Studi di Milano, Milan, Italy; Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, United Arab Emirates",IEEE Access,"24 Feb 2023","2023","11","","18134","18147","The major aim of this paper is to explain the data poisoning attacks using label-flipping during the training stage of the electroencephalogram (EEG) signal-based human emotion evaluation systems deploying Machine Learning models from the attackers’ perspective. Human emotion evaluation using EEG signals has consistently attracted a lot of research attention. The identification of human emotional states based on EEG signals is effective to detect potential internal threats caused by insider individuals. Nevertheless, EEG signal-based human emotion evaluation systems have shown several vulnerabilities to data poison attacks. Besides, due to the instability and complexity of the EEG signals, it is challenging to explain and analyze how data poison attacks influence the decision process of EEG signal-based human emotion evaluation systems. In this paper, from the attackers’ side, data poison attacks occurring in the training phases of six different Machine Learning models including Random Forest, Adaptive Boosting (AdaBoost), Extra Trees, XGBoost, Multilayer Perceptron (MLP), and K-Nearest Neighbors (KNN) intrude on the EEG signal-based human emotion evaluation systems using these Machine Learning models. This seeks to reduce the performance of the aforementioned Machine Learning models with regard to the classification task of 4 different human emotions using EEG signals. The findings of the experiments demonstrate that the suggested data poison assaults are model-independently successful, although various models exhibit varying levels of resilience to the attacks. In addition, the data poison attacks on the EEG signal-based human emotion evaluation systems are explained with several Explainable Artificial Intelligence (XAI) methods including Shapley Additive Explanation (SHAP) values, Local Interpretable Model-agnostic Explanations (LIME), and Generated Decision Trees. And the codes of this paper are publicly available on GitHub.","2169-3536","","10.1109/ACCESS.2023.3245813","Technology Innovation Institute (TII)(grant numbers:8434000394); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045653","Cyber resilience;cyber security;data poisoning;EEG signals;explainable artificial intelligence;human emotion evaluation;label-flipping;machine learning","Computer security;Machine learning;Resilience;Artificial intelligence;Human factors;Emotion recognition;Data quality;Labeling;Electroencephalography","","6","","29","CCBYNCND","15 Feb 2023","","","IEEE","IEEE Journals"
"EEG Emotion Recognition using Parallel Hybrid Convolutional-Recurrent Neural Networks","N. A. Putri; E. Contessa Djamal; F. Nugraha; F. Kasyidi","Department of Informatics, Universitas Jenderal Achmad Yani, Cimahi, Indonesia; Department of Informatics, Universitas Jenderal Achmad Yani, Cimahi, Indonesia; Department of Informatics, Universitas Jenderal Achmad Yani, Cimahi, Indonesia; Department of Informatics, Universitas Jenderal Achmad Yani, Cimahi, Indonesia",2022 International Conference on Data Science and Its Applications (ICoDSA),"25 Aug 2022","2022","","","24","29","Electroencephalogram (EEG) signals of certain emotions contain waves with specific frequency bands. So, emotion recognition uses the network containing each wave to become relevant. EEG signals record electrical activity in the brain from several channels. Therefore, EEG signal processing needs consideration to spatial and temporal. Spatial is a signal between channels, while temporal is a sequence. Several methods were used, Convolutional Neural Networks (CNN) with various dimensions, Recurrent Neural Networks (RNN), and hybrid CNN-RNN. This paper proposed a hybrid 2D CNN-RNN method for identifying emotions from a parallel network of each wave. Two-dimensional CNN is used in channel extraction in a short time of the signal. Using short-time signals is intended to minimize the non-stationary characteristic of EEG signals. Meanwhile, the identification of emotions is carried out with RNN using the output of 2D CNN extraction. The modeling and testing used a dataset from SEED, with three emotion classes: positive, neutral, and negative. The experimental results show that using a split network of each wave increased accuracy from 80.92% to 84.71% and a decreased Loss value. While the use of 2D CNN only increased a less significant accuracy than 1D CNN. Evaluation of the waves shows that Beta and Gamma waves provided the best precision, 87-91%, and Theta waves gave 79-85% precision. Alpha wave degrades overall performance, which only has 56-61% precision, considering it is a mid-wave between Theta and Beta. It is necessary to choose the proper weight updating technique. Adaptive Moment (Adam) increased accuracy than AdaDelta, AdaGrad, and RMSprop.","","978-1-6654-8665-1","10.1109/ICoDSA55874.2022.9862853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9862853","emotion;EEG signal;wavelet;2D-CNN;GRU","Emotion recognition;Recurrent neural networks;Convolution;Data science;Brain modeling;Electroencephalography;Convolutional neural networks","","3","","26","IEEE","25 Aug 2022","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition with Deep Convolution Neural Network","H. -M. Shao; J. -G. Wang; Y. Wang; Y. Yao; J. Liu","School of Mechatronical Engineering and Automation, Shanghai University, Shanghai Key Lab of Power Station Automation Technology, Shanghai, China; School of Mechatronical Engineering and Automation, Shanghai University, Shanghai Key Lab of Power Station Automation Technology, Shanghai, China; School of Mechatronical Engineering and Automation, Shanghai University, Shanghai Key Lab of Power Station Automation Technology, Shanghai, China; Department of Chemical Engineering, National Tsing-Hua University, Hsin-Chu, Taiwan; Tube, Pipe and Bar Business Unit, Baoshan Iron and Steel Co Ltd, Shanghai, China",2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS),"25 Nov 2019","2019","","","1225","1229","Emotions are closely related to people's work and life. Emotional analysis and recognition is not only an urgent need to solve certain mental illnesses, but also has broad application prospects in the fields of human-computer interaction, entertainment and medical care. Therefore, it is of great value to classify emotional EEG signals. This paper introduces CNN(Convolutional Neural Networks)into the process of emotional EEG recognition. The innovation of this method is to adjustthe convolution kernel of the CNN to adapt to the input of EEG signals. The classification accuracy of 0.8579 is achieved in the three-classification emotional EEG signal.","","978-1-7281-1454-5","10.1109/DDCLS.2019.8908880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908880","CNN(Convolutional Neural Networks);EEG(Electroencephalogram);Emotional Recognition","","","9","","10","IEEE","25 Nov 2019","","","IEEE","IEEE Conferences"
"DC-ASTGCN: EEG Emotion Recognition Based on Fusion Deep Convolutional and Adaptive Spatio-Temporal Graph Convolutional Networks","X. Yang; Z. Zhu; G. Jiang; D. Wu; A. He; J. Wang","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; Smart Health Big Data Analysis and Location Services Engineering Research Center of Jiangsu Province, School of Geographic and Biologic Information, Nanjing University of Posts and Telecommunications, Nanjing, China",IEEE Journal of Biomedical and Health Informatics,"4 Apr 2025","2025","29","4","2471","2483","Thanks to advancements in artificial intelligence and brain-computer interface (BCI) research, there has been increasing attention towards emotion recognition techniques based on electroencephalogram (EEG) recently. The complexity of EEG data poses a challenge when it comes to accurately classifying emotions by integrating time, frequency, and spatial domain features. To address this challenge, this paper proposes a fusion model called DC-ASTGCN, which combines the strengths of deep convolutional neural network (DCNN) and adaptive spatio-temporal graphic convolutional neural network (ASTGCN) to comprehensively analyze and understand EEG signals. The DCNN focuses on extracting frequency-domain and local spatial features from EEG signals to identify brain region activity patterns, while the ASTGCN, with its spatio-temporal attention mechanism and adaptive brain topology layer, reveals the functional connectivity features between brain regions in different emotional states. This integration significantly enhances the model's ability to understand and recognize emotional states. Extensive experiments conducted on the DEAP and SEED datasets demonstrate that the DC-ASTGCN model outperforms existing state-of-the-art methods in terms of emotion recognition accuracy.","2168-2208","","10.1109/JBHI.2024.3449083","Xuzhou Key Research and Development Program(grant numbers:KC21304); National Natural Science Foundation of China(grant numbers:62176258); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10666790","Adaptive spatio-temporal graphic convolutional neural network (ASTGCN);deep convolutional neural network (DCNN);EEG;emotion recognition","Electroencephalography;Brain modeling;Emotion recognition;Feature extraction;Long short term memory;Convolutional neural networks;Convolution","Humans;Electroencephalography;Emotions;Neural Networks, Computer;Signal Processing, Computer-Assisted;Brain;Brain-Computer Interfaces;Deep Learning;Adult","","","58","IEEE","5 Sep 2024","","","IEEE","IEEE Journals"
"Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)","L. Santamaria-Granados; M. Munoz-Organero; G. Ramirez-González; E. Abdulhay; N. Arunkumar","Faculty of Systems Engineering, Universidad Santo Tomás, Tunja, Colombia; Telematics Engineering Department, UC3M-BS Institute of Financial Big Data, Universidad Carlos III de Madrid, Leganes, Spain; Telematics Department, University of Cauca, Popayán, Colombia; Department of Biomedical Engineering, Jordan University of Science and Technology, Irbid, Jordan; Department of Electronics and Instrumentation, SASTRA University, Thanjavur, India",IEEE Access,"3 Jan 2019","2019","7","","57","67","Recommender systems have been based on context and content, and now the technological challenge of making personalized recommendations based on the user emotional state arises through physiological signals that are obtained from devices or sensors. This paper applies the deep learning approach using a deep convolutional neural network on a dataset of physiological signals (electrocardiogram and galvanic skin response), in this case, the AMIGOS dataset. The detection of emotions is done by correlating these physiological signals with the data of arousal and valence of this dataset, to classify the affective state of a person. In addition, an application for emotion recognition based on classic machine learning algorithms is proposed to extract the features of physiological signals in the domain of time, frequency, and non-linear. This application uses a convolutional neural network for the automatic feature extraction of the physiological signals, and through fully connected network layers, the emotion prediction is made. The experimental results on the AMIGOS dataset show that the method proposed in this paper achieves a better precision of the classification of the emotional states, in comparison with the originally obtained by the authors of this dataset.","2169-3536","","10.1109/ACCESS.2018.2883213","Government of Colombia; Colciencias; Governorate of Boyacá; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543567","Emotion recognition;deep convolutional neural network;physiological signals;machine learning;AMIGOS dataset","Physiology;Emotion recognition;Feature extraction;Electrocardiography;Videos;Electroencephalography;Brain modeling","","167","","52","OAPA","23 Nov 2018","","","IEEE","IEEE Journals"
"Consumer Grade Brain Sensing for Emotion Recognition","P. Lakhan; N. Banluesombatkul; V. Changniam; R. Dhithijaiyratn; P. Leelaarporn; E. Boonchieng; S. Hompoonsup; T. Wilaiprasitporn","Bio-Inspired Robotics and Neural Engineering Lab, School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand; Bio-Inspired Robotics and Neural Engineering Lab, School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand; Department of Tool and Materials Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand; Department of Electrical Engineering, Chulalongkorn University, Bangkok, Thailand; Bio-Inspired Robotics and Neural Engineering Lab, School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand; Center of Excellence in Community Health Informatics, Chiang Mai University, Chiang Mai, Thailand; Learning Institute, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand; Bio-Inspired Robotics and Neural Engineering Lab, School of Information Science and Technology, Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand",IEEE Sensors Journal,"4 Oct 2019","2019","19","21","9896","9907","For several decades, electroencephalography (EEG) has featured as one of the most commonly used tools in emotional state recognition via monitoring of distinctive brain activities. An array of datasets has been generated with the use of diverse emotion-eliciting stimuli and the resulting brainwave responses conventionally captured with high-end EEG devices. However, the applicability of these devices is to some extent limited by practical constraints and may prove difficult to be deployed in highly mobile context omnipresent in everyday happenings. In this study, we evaluate the potential of OpenBCI to bridge this gap by first comparing its performance to research grade EEG system, employing the same algorithms that were applied on benchmark datasets. Moreover, for the purpose of emotion classification, we propose a novel method to facilitate the selection of audio-visual stimuli of high/low valence and arousal. Our setup entailed recruiting 200 healthy volunteers of varying years of age to identify the top 60 affective video clips from a total of 120 candidates through standardized self assessment, genre tags, and unsupervised machine learning. In addition, 43 participants were enrolled to watch the pre-selected clips during which emotional EEG brainwaves and peripheral physiological signals were collected. These recordings were analyzed and extracted features fed into a classification model to predict whether the elicited signals were associated with a high or low level of valence and arousal. As it turned out, our prediction accuracies were decidedly comparable to those of previous studies that utilized more costly EEG amplifiers for data acquisition.","1558-1748","","10.1109/JSEN.2019.2928781","Robotics AI and Intelligent Solution Project; Thailand Research Fund; Thailand Research Fund(grant numbers:MRG6180028); National Science and Technology Development Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762012","Consumer grade EEG;low-cost EEG;OpenBCI;emotion recognition;affective computing","Electroencephalography;Emotion recognition;Biomedical monitoring;Feature extraction;Sensors;Brain;Prediction algorithms","","65","","74","IEEE","15 Jul 2019","","","IEEE","IEEE Journals"
"Classification of Five Emotions from EEG and Eye Movement Signals: Discrimination Ability and Stability over Time","T. -H. Li; W. Liu; W. -L. Zheng; B. -L. Lu","Center for Brain-Like Computing and Machine Intelligence, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, Shanghai Jiao Tong University, Shanghai, China; Center for Brain-Like Computing and Machine Intelligence, Shanghai Jiao Tong University, Shanghai, China",2019 9th International IEEE/EMBS Conference on Neural Engineering (NER),"20 May 2019","2019","","","607","610","This paper explores the discrimination ability and stability of electroencephalogram (EEG) and eye movement signals over time for classifying five emotions: happy, sad, fear, disgust and neutral. We develop a multimodal emotion dataset called SEED-V with 16 subjects. Two classifiers are trained based on the EEG and eye movement signals. Topographic maps are used to depict the neural patterns of EEG signal. The classification result based on EEG, eye movement, and feature level fusion (FLF) reaches the average accuracies of 70.8%, 59.87% and 75.13%, respectively. The experiment result indicates that: a) the EEG and eye movement signals have good discrimination ability for five emotion classification problem; b) the beta and gamma bands of EEG signal have better discrimination ability than the delta, theta and alpha bands; c) the stable neural patterns of different emotions do exist and are common across sessions; and d) the neural pattern of disgust emotion has high gamma response in the frontal area, while fear emotion has low activation at the top of brain in the gamma band.","1948-3554","978-1-5386-7921-0","10.1109/NER.2019.8716943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716943","","Electroencephalography;Brain modeling;Streaming media;Support vector machines;Feature extraction;Stability criteria","","44","","14","IEEE","20 May 2019","","","IEEE","IEEE Conferences"
"Robust EEG-based Emotion Recognition using Multi-feature Joint Sparse Representation","D. Wu; X. Han; H. Wang; R. Wang","School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, China","2020 International Conference on Computing, Networking and Communications (ICNC)","30 Mar 2020","2020","","","802","807","The key problem of emotion recognition lie in effective electroencephalography (EEG) signal processing and feature extraction. Traditional methods mostly extract single feature or simply combine different features together, but emotion recognition based merely on a single feature may be unreliable and simple feature combination will be affected by complex feature interdependencies. To improve recognition accuracy, we propose a multi-feature emotion recognition method based on Joint Sparse Representation (JSR) to transform the simple feature fusion into an optimization problem. Specifically, sparse matrices for each individual feature are combined to obtain the JSR of these features, and three different EEG features including wavelet energy, Hurst index, and fractal dimension are employed to produce multi-feature fusion results. Due to high computational complexity and restrictions on kernel function selection of Support Vector Machine (SVM), we adopt Relevance Vector Machine (RVM) to classify emotions. Simulation results show our proposed multi-feature fusion algorithm has an average recognition accuracy of over 85%, which is 8% higher than the traditional method.","2325-2626","978-1-7281-4905-9","10.1109/ICNC47757.2020.9049689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9049689","Emotion recognition;electroencephalography;feature fusion;joint sparse representation;relevance vector machine","Feature extraction;Electroencephalography;Emotion recognition;Sparse matrices;Brain modeling;Support vector machines;Microsoft Windows","","3","","24","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"Investigation of window size in classification of EEG-emotion signal with wavelet entropy and support vector machine","H. Candra; M. Yuwono; R. Chai; A. Handojoseno; I. Elamvazuthi; H. T. Nguyen; S. Su","Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Faculty of Engineering and Information Technolnov., Universirv of Technologies, Sydney, New South Wales, Australia; Dept. of Electrical and Electronic Engineering Universiti Teknologi PETRONAS, Tronoh, Malaysia",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"5 Nov 2015","2015","","","7250","7253","When dealing with patients with psychological or emotional symptoms, medical practitioners are often faced with the problem of objectively recognizing their patients' emotional state. In this paper, we approach this problem using a computer program that automatically extracts emotions from EEG signals. We extend the finding of Koelstra et. al [IEEE trans. affective comput., vol. 3, no. 1, pp. 18-31, 2012] using the same dataset (i.e. the DEAP: dataset for emotion analysis using electroencephalogram, physiological and video signals), where we observed that the accuracy can be further improved using wavelet features extracted from shorter time segments. More precisely, we achieved accuracy of 65% for both valence and arousal using the wavelet entropy of 3 to 12 seconds signal segments. This improvement in accuracy entails an important discovery that information on emotions contained in the EEG signal may be better described in term of wavelets and in shorter time segments.","1558-4615","978-1-4244-9271-8","10.1109/EMBC.2015.7320065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320065","","Accuracy;Electroencephalography;Discrete wavelet transforms;Entropy;Feature extraction;Support vector machines;Emotion recognition","Arousal;Electroencephalography;Emotions;Entropy;Humans;Support Vector Machine;Wavelet Analysis","106","","11","IEEE","5 Nov 2015","","","IEEE","IEEE Conferences"
"Exploiting the Intrinsic Neighborhood Semantic Structure for Domain Adaptation in EEG-based Emotion Recognition","Y. Yang; Z. Wang; Y. Song; Z. Jia; B. Wang; T. -P. Jung; F. Wan","Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau; Macao Centre for Mathematical Sciences, and the Respiratory Disease AI Laboratory on Epidemic Intelligence and Medical Big Data Instrument Applications, Faculty of Innovation Engineering, Macau University of Science and Technology, Macau, China; School of Electrical Engineering and Automation, with Tianjin Key Laboratory of New Energy Power Conversion, Transmission and Intelligent Control, Tianjin University of Technology, Tianjin, China; Beijing Key Laboratory of Brainnetome and Brain-Computer Interface, and Brainnetome Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, and the Brain Mind Institute, Western University, London, ON, Canada; Swartz Center for Computational Neuroscience and Institute for Neural Computation, University of California at San Diego, La Jolla, CA, USA; Department of Electrical and Computer Engineering, Faculty of Science and Technology, University of Macau, Macau",IEEE Transactions on Affective Computing,"","2025","PP","99","1","13","Due to the inherent non-stationarity and individual differences present in electroencephalogram (EEG) signals, developing a generalizable model that performs well on new subjects is challenging in EEG-based emotion recognition. Most existing domain adaptation (DA) methods typically mitigate these discrepancies by aligning the marginal distributions of domain feature representations. However, when there is a significant difference in the class-conditional distribution between domain features and labels, the domain-invariant features learned by aligning marginal distributions may have limited discriminative ability for unlabeled target instances or even prove counterproductive. To address this issue, we propose a Neighborhood Semantic Aware Learning-based Dynamic Graph Attention Convolution (NSAL-DGAT) approach that learns target semantic information by considering the inter-domain semantic topological structure, thereby improving classifier adaptation for target instances. Specifically, the proposed NSAL framework is designed to capitalize on the insight that after domain feature alignment, some target samples and their neighboring source samples exhibit similar semantics. By leveraging the neighborhood topological structure, we extract and incorporate semantic target features to train a more transferable classifier. Besides, we implement an entropy weighting mechanism to emphasize representative target semantic information, encouraging target instances to prioritize high-confidence individuals within the source neighborhood. We have conducted extensive experiments on the public SEED dataset and our collected the Hearing-Impaired EEG Dataset (HIED). The experimental results underscore the efficacy of our proposed NSAL-DGAT approach, showcasing state-of-the-art accuracy in subject-dependent as well as subject-independent scenarios. The source code is available at https://github.com/YYingDL/NSAL-DGAT.","1949-3045","","10.1109/TAFFC.2025.3564272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10976537","Domain adaptation;emotion recognition;brain-computer interface;EEG","Semantics;Electroencephalography;Emotion recognition;Feature extraction;Brain modeling;Adaptation models;Graph convolutional networks;Training;Psychology;Entropy","","1","","","IEEE","24 Apr 2025","","","IEEE","IEEE Early Access Articles"
"Merged LSTM Model for emotion classification using EEG signals","A. Garg; A. Kapoor; A. K. Bedi; R. K. Sunkaria","Department of Electronics and Communication Engineering, Dr. B.R. Ambedkar National Institute of Technology, Jalandhar, India; Department of Electronics and Communication Engineering, Dr. B.R. Ambedkar National Institute of Technology, Jalandhar, India; Department of Electronics and Communication Engineering, Dr. B.R. Ambedkar National Institute of Technology, Jalandhar, India; Department of Electronics and Communication Engineering, Dr. B.R. Ambedkar National Institute of Technology, Jalandhar, India",2019 International Conference on Data Science and Engineering (ICDSE),"30 Jan 2020","2019","","","139","143","The applicability of contemporary deep learning techniques have seen considerable improvements in the field of biomedical signal analysis. Emotion analysis using EEG signals is one such problem that has been studied and worked upon extensively in recent times. In this paper we have proposed a novel methodology to classify emotions using signal processing techniques such as wavelet transform and statistical measures for feature extraction and dimensionality reduction followed by developing state of the art neural architecture for the classification task. A merged LSTM model has been proposed for binary classification of emotions. The model's applicability and accuracy has been validated using DEAP dataset which is the benchmark dataset for emotion recognition.","","978-1-7281-2087-4","10.1109/ICDSE47409.2019.8971484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971484","deep recurrent neural networks (RNN);long short term memory (LSTM);discrete wavelet transform;statistics;dimensionality reduction;feature extraction;merged LSTM;DEAP dataset","","","27","","26","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"Deep Learning Techniques for Emotion Recognition From EEG Signals: Improving Accuracy and Efficiency","P. R; P. C","ECE department, Amrita school of engineering, Amrita viswavidyapeetam, Bangalore, India; ECE department, Amrita school of engineering, Amrita viswavidyapeetam, Bangalore, India","2023 International Conference on Computational Intelligence, Networks and Security (ICCINS)","7 Mar 2024","2023","","","1","6","Acknowledgment of emotions is One of the major problems with the subject of emotional technology is its significant scientific value and numerous real-world applications. Due to their intrinsic limitations, the single iris ID in the current circumstances has a problem with inadequate accuracy in feeling categorization. This message's content is intended to supply a need for abundant expression-EEG highly changeable fusion emotion identification devices in order to overcome this issue. In order to acquire high-quality EEG signal traits, this strategy leverages the improved emotional data network structure to swiftly extract facial expression features. Artefacts are eliminated from the electroencephalogram (EEG) signal employing a wavelet soft filter approach. The system is then constructed and tested using the electrical feature data gathered via the expression-EEG bimodality method to finish the finished bi-modal melting mood grouping as a detection study. The results of this study depend on both extended and short-memory neural networks and the option-fused technique. The proposed approach is then confirmed with the feelings collection. The recommended model can effectively detect things with a degree of test reliability of 99.87%, according to testing results. Comparing the suggested methodology to the traditional way, the total time for each detection technique will be decreased to around 20 seconds.","","979-8-3503-1379-6","10.1109/ICCINS58907.2023.10450142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450142","Emotion Recognition;Deep Learning;EEG Signals;Fusion;Neural Networks","Deep learning;Emotion recognition;Time-frequency analysis;Brain modeling;Feature extraction;Electroencephalography;Security","","","","15","IEEE","7 Mar 2024","","","IEEE","IEEE Conferences"
"Hybrid CNN-SVM Classifier for Emotion Recognition Using Brain Waves","F. A. Yassine; G. Abdelkader","Department of Mathematics and Computer Sciences, Tahri Mohammed University, Bechar, Algeria; Department of Mathematics and Computer Sciences, Tahri Mohammed University, Bechar, Algeria",2024 4th International Conference on Embedded & Distributed Systems (EDiS),"13 Dec 2024","2024","","","49","54","The field of information fusion and deep learning (DL) has advanced significantly in recent years, enabling computers and other devices to perceive, recognize, and evaluate emotions. In addition to distinct physiological rhythms and alterations in behavior, mental health, and physical attributes, emotions constitute a mental state. The first three techniques might not be effective because people occasionally accidentally or purposely hide their true feelings. We call this social masking phenomena. By using physiological clues, emotion detection could become more objective and reliable. Important features of emotional states can be inferred from electroencephalography (EEG) signals since they react to changes in emotional states faster and more sensitively than peripheral neurophysiological signals. Automatic electroencephalography (EEG) emotion identification is one challenging component of human-computer interaction (HCI). Inspired by the powerful feature learning capabilities of recently emerging deep learning algorithms, several advanced deep learning models have been used more often to obtain high-level feature representations for EEG emotion recognition. In order to classify EEG emotions more quickly and accurately, this research introduces a hybrid model based on convolutional neural networks (CNN) and support vector machines (SVM). Experimental results show that the suggested strategy outperforms previous approaches in modeling training with a smaller loss value and more generalization. The categorization accuracy as a whole was 98.361%. This percentage is higher when compared to other learning strategies. Furthermore, compared to other models, the proposed model takes less time and much less complex.","","979-8-3315-1987-2","10.1109/EDiS63605.2024.10783403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10783403","brain computer interface;convolutional neural networks;electroencephalography;emotion recognition;support vector machines","Deep learning;Support vector machines;Emotion recognition;Accuracy;Computational modeling;Brain modeling;Electroencephalography;Physiology;Convolutional neural networks;Reliability","","","","61","IEEE","13 Dec 2024","","","IEEE","IEEE Conferences"
"Recognizing Emotions Evoked by Music Using CNN-LSTM Networks on EEG Signals","S. Sheykhivand; Z. Mousavi; T. Y. Rezaii; A. Farzamnia","Biomedical Engineering Department, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Department of Mechanical Engineering, Faculty of Mechanical Engineering, University of Tabriz, Tabriz, Iran; Biomedical Engineering Department, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Faculty of Engineering, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia",IEEE Access,"5 Aug 2020","2020","8","","139332","139345","Emotion is considered to be critical for the actual interpretation of actions and relationships. Recognizing emotions from EEG signals is also becoming an important computer-aided method for diagnosing emotional disorders in neurology and psychiatry. Another advantage of this approach is recognizing emotions without clinical and medical examination, which plays a major role in completing the Brain-Computer Interface (BCI) structure. Emotions recognition ability, without traditional utilization strategies such as self-assessment tests, is of paramount importance. EEG signals are considered the most reliable technique for emotions recognition because of the non-invasive nature. Manual analysis of EEG signals is impossible for emotions recognition, so an automatic method of EEG signals should be provided for emotions recognition. One problem with automatic emotions recognition is the extraction and selection of discriminative features that generally lead to high computational complexity. This paper was design to prepare a new approach to automatic two-stage classification (negative and positive) and three-stage classification (negative, positive, and neutral) of emotions from EEG signals. In the proposed method, directly apply the raw EEG signal to the convolutional neural network and long short-term memory network (CNN-LSTM), without involving feature extraction/selection. In prior literature, this is a challenging method. The suggested deep neural network architecture includes 10-convolutional layers with 3-LSTM layers followed by 2-fully connected layers. The LSTM network in a fusion of the CNN network has been used to increase stability and reduce oscillation. In the present research, we also recorded the EEG signals of 14 subjects with music stimulation for the process. The simulation results of the proposed algorithm for two-stage classification (negative and positive) and three-stage classification (negative, neutral and positive) of emotion for 12 active channels showed 97.42% and 96.78% accuracy and Kappa coefficient of 0.94 and 0.93 respectively. We also compared our proposed LSTM-CNN network (end-to-end) with other hand-crafted methods based on MLP and DBM classifiers and achieved promising results in comparison with similar approaches. According to the high accuracy of the proposed method, it can be used to develop the human-computer interface system.","2169-3536","","10.1109/ACCESS.2020.3011882","Research and Innovation Management Center (PPPI) and the Faculty of Engineering, Universiti Malaysia Sabah (UMS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148598","Emotions Recognition;CNN;LSTM;EEG","Electroencephalography;Feature extraction;Emotion recognition;Brain modeling;Music;Physiology;Multiple signal classification","","94","","48","CCBY","27 Jul 2020","","","IEEE","IEEE Journals"
"Individual Similarity Guided Transfer Modeling for EEG-based Emotion Recognition","X. Zhang; W. Liang; T. Ding; J. Pan; J. Shen; X. Huang; J. Gao","Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University",2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"6 Feb 2020","2019","","","1156","1161","Intelligent recognition of electroencephalogram (EEG) signals has been an important means to recognize emotions. Traditional user-independent method, which treats each individual's EEG data as independent and identically distributed (i.i.d.) samples and ignores destruction on i.i.d. condition caused by individual differences, usually has lower generalization performance. Although user-dependent method could alleviate abovementioned problem, it faces difficulty in collection of sufficient training EEG data for each individual. In order to construct user-dependent model merely based on a small amount of training EEG data, we incorporate transfer learning framework and propose a individual similarity guided transfer modeling method for EEG-based emotion recognition. We first measure the similarities between individuals using maximum mean discrepancy (MMD), then utilize pre-existing EEG data of similar individuals to assist construction of user-dependent model for the target individual using an instance-based transfer learning algorithm named TrAdaBoost. We compared this method with traditional user-independent and user-dependent methods on DEAP dataset. Experimental results showed that our method could transfer useful knowledge from other individuals for user-dependent emotion recognition, which achieved classification accuracies of 66.1% and 66.7% on arousal and valence dimentions, respectively.","","978-1-7281-1867-3","10.1109/BIBM47256.2019.8982972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982972","electroencephalography;emotion recognition;individual differences;transfer learning","","","20","","29","IEEE","6 Feb 2020","","","IEEE","IEEE Conferences"
"A Convolutional Neural Network Feature Fusion Framework with Ensemble Learning for EEG-based Emotion Classification","K. Guo; H. Mei; X. Xie; X. Xu","South China University of Technology, Guangzhou, China; South China University of Technology, Guangzhou, China; South China University of Technology, Guangzhou, China; South China University of Technology, Guangzhou, China",2019 IEEE MTT-S International Microwave Biomedical Conference (IMBioC),"29 Jul 2019","2019","1","","1","4","In recent years, mental health has received more and more attention. With the development of artificial intelligence, machine learning has also been widely used in the field of mental health, e.g., emotion analysis. We propose a feature fusion framework based on convolutional neural network with correlation coefficient matrix and synchronization likelihood matrix of EEG signals for emotion analysis. To further improve the performance, we take the proposed fusion framework as a feature extractor, i.e., taking the output of the layer before softmax as feature, and use stacking strategy for ensemble learning. Experiments on the DEAP database show the effectiveness of the proposed method.","","978-1-5386-7395-9","10.1109/IMBIOC.2019.8777738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8777738","","Feature extraction;Correlation;Electroencephalography;Stacking;Synchronization;Videos;Task analysis","","7","","15","IEEE","29 Jul 2019","","","IEEE","IEEE Conferences"
"User-Independent Classification of Emotions in a Mixed Arousal-Valence Model","M. Nascimben; T. Zoëga Ramsøy; L. E. Bruni","Neurons Inc, Taastrup, Denmark; Neurons Inc., Taastrup, Denmark; Augmented Cognition Lab, Aalborg University Copenhagen, Copenhagen, Denmark",2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE),"27 Dec 2019","2019","","","445","449","In this work we classified EEG features connected with emotions elicited by musical videos. To detect emotions, we used a user-independent approach with data coming from multiple participants in order to test the ""peak-end rule"". Participant's video ratings were processed to create a mixed valence-arousal labelling. Input features were refined using a combination of feature ranking and data reduction based on intrinsic dimensionality search. Compared to previous literature, our results show that the proposed mixed arousal-valence classification is compatible with previous works applying a distinct arousal or valence classification.","2471-7819","978-1-7281-4617-1","10.1109/BIBE.2019.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941735","EEG, emotion recognition, human computer-interaction","Electroencephalography;Feature extraction;Videos;Music;Frequency-domain analysis;Brain modeling;Complexity theory","","1","","20","IEEE","27 Dec 2019","","","IEEE","IEEE Conferences"
"IMGWOFS: A Feature Selector with Trade-off between Conflict Objectives for EEG-based Emotion Recognition","G. Luo; S. Sun; C. Yan; S. Qu; D. Wang; N. Chu; X. Liu; F. Tian; K. Qian; X. Li; B. Hu","Key Laboratory of Brain Health Intelligent Evaluation and Intervention, Ministry of Education (Beijing Institute of Technology), Beijing, China; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Key Laboratory of Brain Health Intelligent Evaluation and Intervention, Ministry of Education (Beijing Institute of Technology), Beijing, China; Key Laboratory of Brain Health Intelligent Evaluation and Intervention, Ministry of Education (Beijing Institute of Technology), Beijing, China; Key Laboratory of Brain Health Intelligent Evaluation and Intervention, Ministry of Education (Beijing Institute of Technology), Beijing, China; Key Laboratory of Brain Health Intelligent Evaluation and Intervention, Ministry of Education (Beijing Institute of Technology), Beijing, China; Key Laboratory of Brain Health Intelligent Evaluation and Intervention, Ministry of Education (Beijing Institute of Technology), Beijing, China; Key Laboratory of Brain Health Intelligent Evaluation and Intervention, Ministry of Education (Beijing Institute of Technology), Beijing, China; Key Laboratory of Brain Health Intelligent Evaluation and Intervention, Ministry of Education (Beijing Institute of Technology), Beijing, China; Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Key Laboratory of Brain Health Intelligent Evaluation and Intervention, Ministry of Education (Beijing Institute of Technology), Beijing, China",IEEE Transactions on Affective Computing,"","2024","PP","99","1","13","Feature selection is a crucial step in EEG emotion recognition. However, it was often used as a single objective problem to either reduce the number of features or maximize classification accuracy, while neglecting their balance. To address the issue, we proposed Improved Multi-objective Grey Wolf Optimization Feature Selection (IMGWOFS). Firstly, we designed a population initialization operator via discriminability and independence of features to accelerate search speed. Secondly, we employed a two-stage update strategy to improve the global search capabilities of the EEG feature subsets. Finally, we incorporated an adaptive mutation operator to escape the local optima. We conducted experiments on SEED and DEAP datasets, and the accuracy were 86.87$\pm$1.62 % and 60.65$\pm$1.51 % in the beta band using a smaller number of EEG features. In addition, the frontal lobe was related to emotion processing. In conclusion, IMGWOFS is an effective and feasible feature selection method for EEG-based emotion recognition.","1949-3045","","10.1109/TAFFC.2024.3450573","Ministry of Science and Technology of the People's Republic of China(grant numbers:2022ZD0208500,2021ZD0202000,2021ZD0200601,2021ZD0201900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10652254","EEG;Emotion Recognition;Feature Selection;Multi-objective","Emotion recognition;Electroencephalography;Feature extraction;Optimization;Accuracy;Minimization;Mathematical models","","","","","IEEE","27 Aug 2024","","","IEEE","IEEE Early Access Articles"
"EEG-Music Emotion Recognition: Challenge Overview","S. Calcagno; S. Carnemolla; I. Kavasidis; S. Palazzo; D. Giordano; C. Spampinato","University of Catania, Catania, Italy; University of Catania, Catania, Italy; University of Catania, Catania, Italy; University of Catania, Catania, Italy; University of Catania, Catania, Italy; University of Catania, Catania, Italy","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","3","As our understanding of emotions continues to evolve, the ability of machines to accurately interpret and respond to emotional cues is more important than ever. Traditional methods of emotion recognition often fall short, particularly when it comes to the subtle and complex responses elicited by music. The EEG-Music Emotion Recognition Challenge aims to leverage electroencephalography (EEG) to decode emotional states from brain signals while subjects listen to music. This initiative seeks to uncover the intricate relationship between neural activity and emotional responses, offering insights for advancing adaptive user interfaces. We propose two tracks: (1) Person Identification aims to identify the subject from whom the EEG was recorded, while (2) Emotion Recognition targets the decoding of emotional state of the subject while listening to a musical stimulus.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888506","Emotion Recognition;EEG analysis;Data Challenge","Emotion recognition;Technological innovation;Target tracking;Neural activity;Speech recognition;Electroencephalography;Emotional responses;Recording;Multiple signal classification;Speech processing","","","","12","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"fMRINet: Repurposing the EEGNet model to identify emotional arousal states in fMRI data","D. Agostinho; M. Castelo-Branco; M. Simões","CISUC, Department of Informatics Engineering, University of Coimbra, Coimbra, Portugal; CIBIT, Institute for Nuclear Sciences Applied to Health, University of Coimbra, Coimbra, Portugal; CISUC, Department of Informatics Engineering, University of Coimbra, Coimbra, Portugal",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","In recent years, functional magnetic resonance imaging (fMRI) transformed our understanding of the intricate relationship between emotions and the brain. The precise classification of emotional states from fMRI data poses challenges for traditional machine learning methods dealing with high-dimensional data. The limitations of these conventional approaches have spurred a growing interest in exploring the potential of deep learning (DL) models. This study introduces a novel approach to classifying emotional arousal levels using fMRI data, specifically tailored for projects with limited data. The approach involves the adaptation of the EEGNet architecture, originally designed for the classification of electroencephalography (EEG) signals, to fMRI data. By mapping the fMRI signal into brain regions using a brain atlas, fMRINet is applied to the two-dimensional fMRI time series, achieving a promising performance in identifying emotional states in both typical and clinical participants (balanced accuracy between 70% and 72%). Our findings highlight the successful integration of the EEGNet architecture into fMRI data and contribute to the broader field of brain state classification.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782984","Deep Learning;fMRI;Brain Decoding;Emotion Classification","Adaptation models;Electric potential;Time series analysis;Functional magnetic resonance imaging;Brain modeling;Feature extraction;Data models;Electroencephalography;Decoding;Engineering in medicine and biology","Magnetic Resonance Imaging;Humans;Emotions;Arousal;Brain;Electroencephalography;Image Processing, Computer-Assisted;Signal Processing, Computer-Assisted;Deep Learning;Male;Adult;Female;Brain Mapping","","","16","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Emotion Classification Using Single-Channel EEG","B. R; H. S. Dayal; K. Sankpal","SCSE, VIT Chennai Campus, Chennai, India; SCSE, VIT Chennai Campus, Chennai, India; SCSE, VIT Chennai Campus, Chennai, India","2019 International Conference on Computing, Power and Communication Technologies (GUCON)","27 Dec 2019","2019","","","360","366","The discovery of EEG signal in 1929 by the German psychiatrist Hans Berger changed the way we understand the structure and functioning of brain.The interaction between a machine and computer is increasing day by day and the need of the hour is to develop a Brain Computer Interface(BCI) which can help the humanity.EEG Emotion recognition system can be used to predict the emotions felt by disabled people.Studies have found that corticolimbic Theta electroencephalographic (EEG) oscillation is responsible for the emotions that one feels. Alpha, Theta, Beta and Delta sub-bands of EEG play a major role in brains emotion processing. The goal of this study is to identify emotions from an EEG data collected from a Single Channel EEG headset.13 subjects of varying age participated in the EEG experiment which were shown videos that helped in evoking three emotional states: neutral, calm and fear. After each video the participants were asked to rate the video on the basis of SAM Model and Valence-Arousal scale.The method used is based on Digital Signal Processing Techniques in order to remove arte-facts,clubbed with machine learning in order to design a system for predicting emotions using EEG signal.Stationary Wavelet Transform (SWT) with haar wavelet at level 6 decomposition with Garrote Thresholding is used to clean the signal and remove the noise. Higuchi Fractal Dimension is also calculated and added as one of the features and is found to have increase the classification accuracy due to its ability to identify the patterns from the data.Experimental results show that EEG based emotion classification can predict emotions with an average of 76% in case of pure EEG signal and 85% in case of EEG signals with Valence-Arousal scale using Recurrent Neural Networks.","","978-93-5351-098-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8940473","Statinary Wavelet Transform;Savitzky Golay Filter;Short time Fourier Transform;Higuchi Fractal Dimension;Deep Neural Network;Support Vector Machine;Recurrent Neural Network","Electroencephalography;Videos;Brain modeling;Wavelet transforms;Fractals;Prediction algorithms","","","","13","","27 Dec 2019","","","IEEE","IEEE Conferences"
"Emotion Classification Using Fast Fourier Transform and Recurrent Neural Networks","H. G. Prawira; S. Sundari; E. C. Djamal; A. Wulandari","Department of Informatics, Univ. Jenderal Achmad Yani, Cimahi, Indonesia; Department of Informatics, Univ. Jenderal Achmad Yani, Cimahi, Indonesia; Department of Informatics, Univ. Jenderal Achmad Yani, Cimahi, Indonesia; Department of Informatics, Univ. Jenderal Achmad Yani, Cimahi, Indonesia","2021 International Conference on Instrumentation, Control, and Automation (ICA)","1 Dec 2021","2021","","","94","99","Electroencephalogram (EEG) capture electric activity in the brain. The choice of feature extraction method depends on the characteristics of the variables being reviewed. Often frequency extraction is advantageous. Besides removing the noise recorded in the signal, the emotion variable has characteristics in the frequency band. Fast Fourier Transform (FFT) is one of the methods used as a frequency filter. However, for EEG signals, it is necessary the short time to be approached as a stationary signal, and FFT gives fast computing. This research proposed FFT as a method filter to classify emotion before using Recurrent Neural Networks (RNN). RNN is a method that can capture the connection between signals so that it is suitable for EEG signals. FFT filtered one-second signal in frequency 4-45 Hz to get Alpha, Beta, Theta, and Gamma waves. Three classes of emotions, Positive, Negative, and Neutral. The experiment shows that the FFT filter increased the accuracy from 68.16% without FFT to 80.60%. FFT reduced the classification time from 0.88 seconds up to 0.05 seconds. There are three weight correction techniques used Adaptive Moment Estimation with Maximum (AdaMax), Stochastic Gradient Descent (SGD), and Root Mean Square Propagation (RMSprop), which gave near to the same accuracy. At the same time, a small learning rate (0.0001) made much better accuracy. We examined one-second segment was better compared to a five-second at processing emotional EEG signals. These results indicate that the FFT has a limited processing duration to close to a stationary signal.","2639-5045","978-1-6654-3295-5","10.1109/ICA52848.2021.9624479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9624479","Emotion classification;EEG signal;FFT;RNN","Emotion recognition;Recurrent neural networks;Fast Fourier transforms;Instruments;Stochastic processes;Estimation;Feature extraction","","3","","31","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"Compact unsupervised EEG response representation for emotion recognition","X. Zhuang; V. Rozgić; M. Crystal","Speech, Language and Multimedia Business Unit, Raytheon BBN Technologies, Cambridge, MA, USA; Speech, Language and Multimedia Business Unit, Raytheon BBN Technologies, Cambridge, MA, USA; Speech, Language and Multimedia Business Unit, Raytheon BBN Technologies, Cambridge, MA, USA",IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI),"26 Jul 2014","2014","","","736","739","In this work, we propose a compact and un-supervised EEG response representation. Instead of directly extracting features from the whole response, as is commonly done for EEG signal processing, the proposed representation employs segment-level feature extraction and leverages a robust two-part unsupervised generative model to transform the segment-level features to a low-dimensional vector. The proposed method leads to rich and compact representation capability, and robust unsupervised estimation. While some previous work [1] based on segment-level features needs labeled training responses to transforms segment-level features to a response representation, the proposed method produces an EEG response representation in an unsupervised fashion, which can be directly used in various EEG response classification problems. We perform binary classification and regression of emotion dimensions on the DEAP dataset (Dataset for Emotion Analysis using electroencephalogram, Physiological and Video Signals) and demonstrate competitive performances.","2168-2208","978-1-4799-2131-7","10.1109/BHI.2014.6864469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6864469","","Electroencephalography;Brain modeling;Feature extraction;Robustness;Training;Vectors;Support vector machines","","28","","17","IEEE","26 Jul 2014","","","IEEE","IEEE Conferences"
"DW-FBCSP: EEG emotion recognition algorithm based on scale distance weighted optimization","H. Peng; W. Lin; G. Cai; S. Huang; Y. Pei; T. Ma","Department of Electronic & Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Electronic & Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Electronic & Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Electronic & Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Electronic & Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Electronic & Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China",2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"9 Dec 2021","2021","","","430","433","Emotion calibration is measured by the valence and arousal scales and the ideal center is used to directly divide valence arousal into high scores and low scores. This division method has a big classification and labeling defect, and the influence of emotion stimulation material on the subjects cannot be accurately measured. To address this problem, this paper proposes an EEG emotion recognition algorithm (DW-FBCSP: Distance Weighted Filter Bank Common Spatial Pattern) based on scale distance weighted optimization to optimize the classification according to the distance of the scores from ideal center. This method is a natural extension of CSP that optimize the user’s EEG signal projection matrix. Then, the LDA classifier is used to recognize emotions using the features set which fused the selected features and the features extracted by the projection matrix. The results show that the mean correct rate of the valence and arousal achieves 81.14% and 84.45% using the DEAP dataset. The results demonstrate that our proposed method outperforms better than some other results published in recent years.","2694-0604","978-1-7281-1179-7","10.1109/EMBC46164.2021.9629850","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629850","","Emotion recognition;Filter banks;Feature extraction;Electroencephalography;Biology;Classification algorithms;Calibration","Algorithms;Arousal;Electroencephalography;Emotions;Humans","2","","12","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"SemConv: Deep Learning for Emotion Decoding in Autism Individuals using CNN","A. Chaukade; M. K; K. Chakrapani","Dept. of Networking and Communications, SRM Institute of Science & Technology KTR, Chennai, India; Dept. of Networking and Communications, SRM Institute of Science & Technology KTR, Chennai, India; Dept. of Information Technology, Sastra Deemed University, Thanjavur, Tamil Nadu, India",2024 IEEE 3rd World Conference on Applied Intelligence and Computing (AIC),"29 Oct 2024","2024","","","461","466","This research addresses advancements in medical decision support systems aimed at overcoming challenges in neuroscience, specifically for individuals with Autism Spectrum Disorder. This technology utilizes the convolution neural network for efficiently analyzing the neural activities of individuals suffering from ASD (Autism Spectrum Disorder), which is a functional disorder occurring due to neural oscillatory malfunctioning. The proposed technology will work to detect the EEG signals in ASD individuals. Applying learned filters can be able to analyze and inspect the part of the EEG spectrum that is being utilized for reading and decoding the emotions [1]. The range of band suppression present in ASD individuals is in 2 categories – α (high) and β (high). The highα (9-13 Hz) and highβ (13-30 Hz). The deep learning model is trained using the dataset training with validation and tracking accuracy. The model consists of multiple dense layers – ReLU and dropout layer [2] which complies with the dataset using the optimizer function and sparse categorical cross-entropy loss. The evaluation will select the random EEG data sample, predict its emotion label and plot EEG data from the selected sample for visualizations. The results align with existing neuroscientific findings on emotion recognition in individuals with behavioral deficits, demonstrating that the system maintains high performance and accuracy while enhancing interpretability.","","979-8-3503-8459-8","10.1109/AIC61668.2024.10730830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10730830","ASD;EEG;ReLU;neural oscillatory;crossentropy","Deep learning;Training;Autism;Accuracy;Neuroscience;Runtime;Predictive models;Brain modeling;Electroencephalography;Decoding","","","","28","IEEE","29 Oct 2024","","","IEEE","IEEE Conferences"
"Electroencephalogram Emotion Recognition Based on Dispersion Entropy Feature Extraction Using Random Oversampling Imbalanced Data Processing","X. -W. Ding; Z. -T. Liu; D. -Y. Li; Y. He; M. Wu","School of Automation, Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, and Engineering Research Center of Intelligent Technology for Geo-Exploration, Ministry of Education, China University of Geosciences, Wuhan, China; School of Automation, Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, and Engineering Research Center of Intelligent Technology for Geo-Exploration, Ministry of Education, China University of Geosciences, Wuhan, China; School of Automation, Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, and Engineering Research Center of Intelligent Technology for Geo-Exploration, Ministry of Education, China University of Geosciences, Wuhan, China; School of Automation, Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, and Engineering Research Center of Intelligent Technology for Geo-Exploration, Ministry of Education, China University of Geosciences, Wuhan, China; School of Automation, Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, and Engineering Research Center of Intelligent Technology for Geo-Exploration, Ministry of Education, China University of Geosciences, Wuhan, China",IEEE Transactions on Cognitive and Developmental Systems,"8 Sep 2022","2022","14","3","882","891","Electroencephalogram (EEG) is the brain’s electrical activity measure, which can reflect people’s inner emotional states objectively. In this article, a dispersion entropy (DispEn) feature extraction-based EEG emotion recognition method is proposed. In feature extraction, the DispEn is computed for the four typical frequency bands, i.e.,  $\theta $ ,  $\alpha $ ,  $\beta $ , and  $\gamma $  of EEG signals which are filtered from 32 channels. Furthermore, a random oversampling algorithm is employed to balance the data for the emotional labels to avoid majority biases. The proposed method not only has a better ability to characterize EEG signals but also has a faster recognition speed. In the experiments, the DEAP dataset is used to validate the effectiveness of the proposal, in which the DispEn is extracted from the undecomposed signal and four typical EEG rhythms are compared for emotion recognition by using a support vector machine (SVM). Besides, comparison experiments using DispEn, sample entropy (SampEn), permutation entropy (PerEn), and three other commonly used statistical features are performed. The experimental results show that the proposed method achieves recognition accuracy in high valence (HV)/low valence (LV) and high arousal (HA)/low arousal (LA) is 72.95% and 76.67%, respectively. The computation cost of DispEn feature extraction is  ${O}(N)$  that better than some state-of-the-art methods.","2379-8939","","10.1109/TCDS.2021.3074811","National Natural Science Foundation of China(grant numbers:61976197,61403422,61703375,61273102); Hubei Provincial Natural Science Foundation of China(grant numbers:2018CFB447,2015CFA010); Wuhan Science and Technology Project(grant numbers:2020010601012175,2017010201010133); 111 Project(grant numbers:B17040); Fundamental Research Funds for National University, China University of Geosciences (Wuhan)(grant numbers:1910491T01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9409950","Dispersion entropy (DispEn);electroencephalogram (EEG) signals;emotion recognition;random oversampling;support vector machine (SVM)","Electroencephalography;Entropy;Emotion recognition;Feature extraction;Time series analysis;Dispersion;Support vector machines","","21","","72","IEEE","21 Apr 2021","","","IEEE","IEEE Journals"
"Spatiotemporal Gated Graph Transformer for EEG-Based Emotion Recognition","Y. Chang; X. Zheng; Y. Chen; X. Li; Q. Miao","Key Lab of Digital Signal and Image Processing of Guangdong Province, Department of Electronic Engineering, Shantou University, Shantou, China; School of Mathematics and Big Data, Foshan University, Foshan, China; Key Lab of Digital Signal and Image Processing of Guangdong Province, Department of Electronic Engineering, Shantou University, Shantou, China; Key Lab of Digital Signal and Image Processing of Guangdong Province, Department of Electronic Engineering, Shantou University, Shantou, China; School of Mathematics and Big Data, Foshan University, Foshan, China",IEEE Signal Processing Letters,"20 Jun 2024","2024","31","","1630","1634","The availability of accurate and reliable electroencephalography (EEG) signal data makes emotion recognition feasible. In recent years, an increasing number of deep learning methods have been applied to emotion recognition tasks based on EEG signals, but they all have their own drawbacks, including the inability to simultaneously capture spatial and temporal information, the loss of spatial information, and even the improper selection of data scales. To address the above problems, this paper proposes a novel method for EEG-based emotion recognition named Spatiotemporal Gated Graph Transformer (SGGT). The method takes advantage of graph structure to enrich spatial information and uses a two-tower transformer to encode spatial information and temporal information, respectively. For the spatial feature extraction, we adopt four encoding methods to compensate for the shortcomings of traditional transformers in graph learning tasks. Different from the previous processing methods, our method uses graph pooling to compress the graph, forcing the model to learn the relationship between different time stamps. In addition, a gating mechanism is used to merge the two-tower transformer to realize the fusion of temporal and spatial features. Our method is validated on the SEED, SEED-IV and SEED-V datasets, and it outperforms the baseline methods.","1558-2361","","10.1109/LSP.2024.3410044","National Natural Science Foundation of China(grant numbers:61901116,61471229,62001115); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515140111,2019A1515010789,2021A1515012289,2019A1515110136); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10549782","EEG;emotion recognition;graph transformer;graph pooling;gating mechanism","Transformers;Electroencephalography;Brain modeling;Encoding;Feature extraction;Emotion recognition;Data models","","1","","27","IEEE","5 Jun 2024","","","IEEE","IEEE Journals"
"Identification of Affective Mental Activity Based on Multichannel EEG Signals","F. Feradov; T. Ganchev","Faculty of Computer Sciences and Automation, Technical University of Varna, Varna, Bulgaria; Faculty of Computer Sciences and Automation, Technical University of Varna, Varna, Bulgaria",2020 International Conference on Biomedical Innovations and Applications (BIA),"4 Nov 2020","2020","","","101","104","Efficient and effective human-robot collaboration depends on the capacity of both sides to ensure robust communication and mutual awareness of actions, intentions, context, timing, and numerous extra factors that precondition specific behaviors. On the machine side, this would require an intelligent Human-Machine Interface (iHMI), which among other functionalities is also sensitive to the human cognitive and affective states. In this regard, we propose a method for the automated identification of affective mental activity based on EEG signals. The proposed method is computationally lightweight as it operates directly on the samples of the time-domain signal, without the need for complex preprocessing, frequency-domain transformation, or another feature extraction steps. The method is validated in an experimental setup based on the DEAP dataset and was shown to significantly outperform two other setups. The experimental results support the usefulness of the proposed method and we deem it will facilitate the development of iHMI, which are sensitive to the four major categories of affective states as defined in the arousal-valence space.","","978-1-7281-7073-2","10.1109/BIA50171.2020.9244279","Bulgarian National Science Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9244279","human-robot collaboration;human-machine interfaces (HMI);affective computing;emotion recognition;electroencephalography (EEG)","Technological innovation;Frequency-domain analysis;Collaboration;Feature extraction;Electroencephalography;Computational efficiency;Timing","","1","","17","IEEE","4 Nov 2020","","","IEEE","IEEE Conferences"
"Identification of Human Stress Based on EEG Signals Using Machine Learning","N. H. Saputra; N. Nafi’Iyah","Electrical Engineering Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Engineering Universitas Islam Lamongan, Lamongan, Indonesia",2022 1st International Conference on Information System & Information Technology (ICISIT),"7 Sep 2022","2022","","","176","180","Mental health greatly affects human physical health. Mental health can be a source of thinking as well as the response center of all activities. The pressures faced, the burden of thoughts, and food patterns can be a source of human psychological conditions. If the human psychological condition is under stress, it can cause disease. The development of intelligent system technology can take advantage of electroencephalogram (EEG) signals to recognize human mental conditions (stressed and normal). The purpose of this research was to determine the most appropriate method in identifying human psychology (stress and normal) from EEG. Based on the EEG signal taken through the recording of the response signal of the human brain, feature extraction is performed. The features taken are the mean, standard deviation, and MAV (Mean Absolute Value) of each subband, and channel. The total data of respondents studied were 20 people, with 10 normal criteria, and 10 stress. Each of the mean, standard deviation, and MAV features was modeled using the Naive Bayes, SVM, KNN, Backpropagation, Regression Logistics, Deep Learning, ID3 methods. The best method for detecting stress and normal is KNN with 97% accuracy.","","978-1-6654-0200-2","10.1109/ICISIT54091.2022.9872815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9872815","stress identification and normal;EEG;feature mean;standard deviation;MAV","Support vector machines;Deep learning;Backpropagation;Mental health;Electroencephalography;Recording;Intelligent systems","","2","","12","IEEE","7 Sep 2022","","","IEEE","IEEE Conferences"
"Emotion Recognition in Response to Haptic Enhanced Multimedia Using EEG","Shams-Ul-Haq; A. Raheel; M. Majid","Department of Computer Engineering, University of Engineering and Technology, Taxila, Taxila, Pakistan; Department of Computer Engineering, University of Engineering and Technology, Taxila, Taxila, Pakistan; Department of Computer Engineering, University of Engineering and Technology, Taxila, Taxila, Pakistan",2021 International Conference on Information Technology (ICIT),"26 Jul 2021","2021","","","910","913","The purpose of this study is to enrich the human emotional experience by engaging vision, auditory, and haptic senses using haptic enhanced multimedia (HEM). Four different multimedia clips are selected from different movies and then converted into HEM clips by synchronizing them with a haptic jacket to add haptic effect. The EEG data from 30 participants is recorded in which 15 of them participated in the traditional multimedia and 15 for HEM. After each clip, self assessment manikin scale is used to record user’s valence and arousal scores to recognize the stimulated emotions. 17 different time domain features are extracted and three classifiers namely support vector machine (SVM), the Naive bayes (NB), and random forest (RF) are applied to classify two states of valence and arousal. An accuracy of 60% against traditional multimedia and 66.67% against HEM clips is achieved, which shows that by engaging haptic sense, the performance of emotion recognition enhances.","","978-1-6654-2870-5","10.1109/ICIT52682.2021.9491786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491786","Emotion recognition;Electroencephalogram;haptic;multimedia;classification","Support vector machines;Radio frequency;Emotion recognition;Feature extraction;Motion pictures;Electroencephalography;Haptic interfaces","","","","24","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
"Time Domain Analysis for Emotional EEG Signals of Stroke Patient and Normal Subject","E. Vincen; W. Khairunizam; C. W. Yean; W. Azani Mustafa","Faculty of Electrical Engineering & Technology, Universiti Malaysia Perlis (UniMAP), Arau, Malaysia; Faculty of Electrical Engineering & Technology, Universiti Malaysia Perlis (UniMAP), Arau, Malaysia; Faculty of Electrical Engineering & Technology, Universiti Malaysia Perlis (UniMAP), Arau, Malaysia; Faculty of Electrical Engineering & Technology, Universiti Malaysia Perlis (UniMAP), Arau, Malaysia",2023 15th International Conference on Computer and Automation Engineering (ICCAE),"3 May 2023","2023","","","372","375","This paper aims to analyze the emotional Electroencephalogram (EEG) signals of different time windows. The time window of the signals is one of the variables that affect the efficiency of the EEG signal analysis. In this research, a total of 30 subjects are analyzed from three different groups namely 10 left brain damage (LBD), 10 right brain damage (RBD), and 10 normal control (NC) for six different emotional states. The 14-Channel Wireless Emotiv EPOC device with a sampling frequency of 128 Hz is used to extract EEG signal from the subjects. The 6th Order Butterworth Bandpass filter is used to extract the EEG signals with the frequency band of 8-49 Hz, which are alpha to gamma waves. The EEG signals are segmented in 2s, 4s, 6s, and 8s time windows for all frequency bands. In addition, the K-Nearest Neighbor (KNN) and Probabilistic Neural Network (PNN) classifiers are used to classify the six emotions in LBD, RBD and NC. The beta and gamma bands are the best performing EEG frequency band for emotion classification. In the investigation, 6s time windows have the highest classification accuracy for KNN with 81.90% and 8s time window for PNN classifier with 82.15%.","2154-4360","979-8-3503-9622-5","10.1109/ICCAE56788.2023.10111252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10111252","electroencephalogram (EEG);time window;emotions;stroke patients","Band-pass filters;Wireless communication;Deep learning;Time-frequency analysis;Stroke (medical condition);Feature extraction;Probabilistic logic","","","","16","IEEE","3 May 2023","","","IEEE","IEEE Conferences"
"Multi-Modal Emotion Recognition Based On deep Learning Of EEG And Audio Signals","Z. Li; G. Zhang; J. Dang; L. Wang; J. Wei","College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China",2021 International Joint Conference on Neural Networks (IJCNN),"20 Sep 2021","2021","","","1","6","Automatic recognition of human emotional states has attracted many researchers' attention in Human-Computer Interactions and emotional brain-computer interface recently. However, the accuracy of emotion recognition is not satisfying. Considering the advantage of information supplement based on deep learning of multi-modal signals related to emotion, this study proposed a novel emotion recognition architecture to fuse emotional features from brain electroencephalography (EEG) signal and the corresponding audio signal in emotion recognition on DEAP dataset. We used convolutional neural network (CNN) to extract EEG features and bidirectional long short term memory (BiLSTM) neural networks to extract audio features. After that, we combine the multi-modal features into a deep learning architecture to recognize arousal and valence levels. Results showed an improved accuracy compared with previous studies that merely used the EEG signals in both arousal level and valence level, which suggests the effectiveness of our proposed multi-modal fused emotion recognition model. In future work, multi-modal data from nature interaction scenes will be collected and inputted into this architecture to further validate the effectiveness of the method.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533663","National Natural Science Foundation of China(grant numbers:61876126); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533663","affective computing;brain-machine interfaces;electroencephalography (EEG);deep learning;multi-modal fusion","Deep learning;Emotion recognition;Speech recognition;Computer architecture;Feature extraction;Brain modeling;Electroencephalography","","9","","19","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Deep Learning Based Emotion Detection Using Discrete Wavelet Transform","D. Nguyen; M. T. Nguyen; K. Yamada","Graduate School of Science and Technology Gunma University, Gunma, Japan; Faculty of Telecommunication Engineering, Posts and Telecommunications Institute of Technology, Hanoi, Vietnam; Graduate School of Science and Technology Gunma University, Gunma, Japan",2024 International Conference on Advanced Technologies for Communications (ATC),"7 Mar 2025","2024","","","785","790","Emotion detection plays essential role in brain-computer interface, which applies in various fields such as healthcare, security, education. In this paper, a novel algorithm is proposed for detection of emotion using electroencephalogram signal. The method selects convolutional neural networks as the proposed classifier among different intelligent algorithms. Discrete wavelet transform is employed for signal decomposition into 4 bands namely theta, alpha, beta, and gamma, which are then used for feature extraction. A total of 800 features are chosen as the most relevant subset among 1920 features by recursive feature elimination algorithm in combination with 5-fold cross validation procedure and K-nearest neighbors model. The proposed algorithm is then validated with the selected feature subset on the validation set using 5-fold cross validation based statistic method. The performance results are accuracy of 65.5 % and 65.51 % for valence and arousal detection. Moreover, relative high F1_score values of 78.87% and 79.11 % for both valence and arousal classification, respectively, show potentiality of the proposed algorithm for recognition of emotion in practical environments.","2162-1039","979-8-3503-5398-3","10.1109/ATC63255.2024.10908189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10908189","Emotion detection;Discrete wavelet transform;Deep learning;Machine learning;Recursive feature elimination","Emotion recognition;Accuracy;Nearest neighbor methods;Feature extraction;Brain modeling;Classification algorithms;Discrete wavelet transforms;Convolutional neural networks;Security;Signal resolution","","","","24","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Multi-task Feature Learning for EEG-based Emotion Recognition Using Group Nonnegative Matrix Factorization","A. Hajlaoui; M. Chetouani; S. Essid","LTCI, Université Paris-Saclay; Institut des Systèmes Intelligents et de Robotique, Université Pierre et Marie Curie; LTCI, Université Paris-Saclay",2018 26th European Signal Processing Conference (EUSIPCO),"2 Dec 2018","2018","","","91","95","Electroencephalographic sensors have proven to be promising for emotion recognition. Our study focuses on the recognition of valence and arousal levels using such sensors. Usually, ad hoc features are extracted for such recognition tasks. In this paper, we rely on automatic feature learning techniques instead. Our main contribution is the use of Group Nonnegative Matrix Factorization in a multi-task fashion, where we exploit both valence and arousal labels to control valence-related and arousal-related feature learning. Applying this method on HCI MAHNOB and EMOEEG, two databases where emotions are elicited by means of audiovisual stimuli and performing binary inter-session classification of valence labels, we obtain significant improvement of valence classification Fl scores in comparison to baseline frequency-band power features computed on predefined frequency bands. The valence classification F1 score is improved from 0.56 to 0.69 in the case of HCI MAHNOB, and from 0.56 to 0.59 in the case of EMOEEG.","2076-1465","978-9-0827-9701-5","10.23919/EUSIPCO.2018.8553390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8553390","Electroencephalography;Valence;Arousal;Nonnegative Matrix Factorization;Group NMF;Common Spectral Patterns","Human computer interaction;Feature extraction;Task analysis;Electroencephalography;Brain modeling;Electrodes;Dictionaries","","","","26","","2 Dec 2018","","","IEEE","IEEE Conferences"
"Frame Work For EEG Based Emotion Recognition Based On Hybrid Neural Network","J. M. Jose; A. J","Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India","2021 Seventh International conference on Bio Signals, Images, and Instrumentation (ICBSII)","4 Jun 2021","2021","","","1","7","In recent years, there were many attempts to classify human emotions based on corporeal signals including ECG, EEG, EMG. EEG based emotion classification is more accurate because it cannot be tainted by subjects' will. The recent development in CNNs has made it easier to systematically extract features from EEG easily. But again, the traditional CNNs fail to comprehend the multi-channel aspect of EEG. In this work, a simple and efficient pre-processing method by considering baseline signals is proposed to enhance the accuracy of recognition and we proposed a hybrid neural network which combines Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) to identify human emotions by extracting spatial and temporal features from raw EEG stream effectively. In CNN, the 1D EEG sequence is then efficiently converted into a 2D frame structure. In order to extract the inter-channel connection between physically adjacent EEG signals, the CNN module is used, and to extract the contextual information, the LSTM module is used. Using this logic, we were able to create a deep learning model which predicts arousal and valence emotions with 86.98% and 85.82% accuracy respectively.","","978-1-6654-4126-1","10.1109/ICBSII51839.2021.9445130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9445130","EEG;CNN;RNN;LSTM;Spatial-temporal;SFV;TFV","Deep learning;Emotion recognition;Recurrent neural networks;Biological system modeling;Predictive models;Feature extraction;Brain modeling","","8","","29","IEEE","4 Jun 2021","","","IEEE","IEEE Conferences"
"FEELing (key)Pressed: Implicit Touch Pressure Bests Brain Activity for Modeling Emotion Dynamics in the Space Between Stressed & Relaxed","X. L. Cang; R. R. Guerra; B. Guta; P. Bucci; L. Rodgers; H. Mah; Q. Feng; A. Agrawal; K. E. MacLean","University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada",IEEE Transactions on Haptics,"19 Sep 2024","2024","17","3","310","318","In-body lived emotional experiences can be complex, with time-varying and dissonant emotions evolving simultaneously; devices responding in real-time to estimate personal human emotion should evolve accordingly. Models assuming generalized emotions exist as discrete states fail to operationalize valuable information inherent in the dynamic and individualistic nature of human emotions. Our multi-resolution emotion self-reporting procedure allows the construction of emotion labels along the Stressed-Relaxed scale, differentiating not only what the emotions are, but how they are transitioning – e.g., “hopeful but getting stressed” vs. “hopeful and starting to relax”. We trained participant-dependent hierarchical models of contextualized individual experience to compare emotion classification by modality (brain activity and keypress force from a physical keyboard), then benchmarked classification performance at F1-scores = [0.44, 0.82] (chance $F1=0.22$, $\sigma =0.01$) and examined high-performing features. Notably, when classifying emotion evolution in the context of an experience that realistically varies in stress, pressure-based features from keypress force proved to be the more informative modality, and more convenient when considering intrusiveness and ease of collection and processing. Finally, we present our FEEL (Force, EEG and Emotion-Labelled) dataset, a collection of brain activity and keypress force data, labelled with self-reported emotion collected during tense videogame play (N = 16) and open-sourced for community exploration.","2329-4051","","10.1109/TOH.2023.3308059","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10239097","Affective touch;dynamic emotion classification;emotion labelling methods;keypress force;brain activity","Brain modeling;Electroencephalography;Biological system modeling;Video games;Protocols;Emotion recognition;Labeling","Humans;Emotions;Male;Brain;Female;Adult;Electroencephalography;Young Adult;Touch Perception;Pressure;Stress, Psychological;Touch","1","","58","IEEE","4 Sep 2023","","","IEEE","IEEE Journals"
"An Area-Energy Efficient EEG Detection System Based On PWL and Nonlinear SVM","G. Cao; W. Shi; J. Wu; Z. Mo; Y. Gao","College of Electronic and Information Engineering, Shenzhen University; State Key Laboratory of Radio Frequency Heterogeneous Integration, Shenzhen University; College of Electronic and Information Engineering, Shenzhen University; College of Electronic and Information Engineering, Shenzhen University; College of Electronic and Information Engineering, Shenzhen University",2024 IEEE Biomedical Circuits and Systems Conference (BioCAS),"23 Dec 2024","2024","","","1","5","This paper presents a low-power and area-efficient detection chip design for Support Vector Machine (SVM) classification of EEG signals. The design incorporates a 256-point FFT module for feature extraction of EEG signals. To further reduce power consumption and circuit complexity, an approximate computing approach is introduced for operations such as energy calculation, exponential functions and squaring. Additionally, a custom compression-based Lagrange multiplier encoding technique is employed. The proposed detection system has been implemented on FPGAs, achieving a $\mathbf{1 0 \% - 3 0 \%}$ reduction in resource utilization compared to recent related works, with power consumption as low as 0.201 W, significantly lower than existing solutions. In chip design, the energy efficiency reaches $0.31 \mu \mathrm{~J} /$ Class, which is an improvement of over 25%, and the area is reduced by more than 40%. Overall, the proposed detection system effectively reduces resource and power consumption through approximate computing circuits and encoding methods.","2766-4465","979-8-3503-5495-9","10.1109/BioCAS61083.2024.10798185","Natural Science Foundation of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10798185","Electroencephalogram (EEG);SVM;piecewise line fitting(PWL);multi-classification","Support vector machines;Power demand;Approximate computing;Feature extraction;Electroencephalography;Energy efficiency;Encoding;Resource management;Chip scale packaging;System analysis and design","","","","24","IEEE","23 Dec 2024","","","IEEE","IEEE Conferences"
"Enhancing EEG-Based Emotion Recognition Using Asymmetric Windowing Recurrence Plots","D. Wahyu Prabowo; N. Akhmad Setiawan; J. Debayle; H. A. Nugroho","Department of Electrical and Information Engineering, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, Indonesia; Department of Electrical and Information Engineering, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, Indonesia; Department of Electrical and Information Engineering, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, Indonesia; Department of Electrical and Information Engineering, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, Indonesia",IEEE Access,"25 Jun 2024","2024","12","","85969","85982","Time-series classification (TSC) has been widely utilized across various domains, including brain-computer interfaces (BCI) for emotion recognition through electroencephalogram (EEG) signals. However, traditional methods often struggle to capture the complex emotional patterns present in EEG data. Recent advancements in encoding techniques have provided promising avenues for improving emotion recognition. This study introduces asymmetric windowing recurrence plots (AWRP) as a novel encoding technique to efficiently encapsulate the dynamic characteristics of EEG signals into texture-rich image representations. This study systematically compares the impact of conventional thresholded and unthresholded recurrence plots (RP) versus the proposed AWRP in emotion recognition tasks. Empirical validations conducted across benchmark datasets, such as DEAP and SEED, demonstrate that the AWRP method achieves classification accuracies of 99.84% and 99.69%, respectively, outperforming existing state-of-the-art methodologies. This study emphasizes the significance of input formulation, highlighting that richer input textures, as provided by AWRP, significantly enhance emotion recognition performance while ensuring computational memory usage efficiency. These findings have significant implications in the domain of EEG-based emotion recognition and offer a novel perspective that can guide future research.","2169-3536","","10.1109/ACCESS.2024.3409384","High-Impact Publication Assistance Program at Universitas Gadjah Mada and the Article Processing Charges (APC) IEEE-UGM Token; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10547242","Time-series classification (TSC);electroencephalogram (EEG);asymmetric windowing recurrence plots (AWRP);emotion recognition;brain-computer interfaces (BCI)","Electroencephalography;Emotion recognition;Encoding;Nonlinear dynamical systems;Windows;Brain modeling;Trajectory;Time series analysis;Brain-computer interfaces","","1","","44","CCBYNCND","4 Jun 2024","","","IEEE","IEEE Journals"
"Human emotion recognition based on multi-channel EEG signals","Z. Du","CS, Sun Yat-sen University, Guangzhou, China",2023 17th International Conference on Complex Medical Engineering (CME),"25 Jun 2024","2023","","","24","31","EEG is a kind of sequential data. In order to learn the relationship of EEG in time series, this paper proposes to use long-term memory network to combine convolutional neural network and self-attention mechanism to extract features from EEG. In order to verify the superiority of this method, I design a complete experiment based on SEED data set, and the experimental strategy is an independent leave one method cross-validation experiment for subjects. The experimental results show that the proposed method can effectively learn the EEG temporal relationship, and can improve the convergence speed and have a high accuracy.","","979-8-3503-1611-7","10.1109/CME60059.2023.10565389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10565389","Emotion classification;EEG;physiological signals;signal processing;pattern classification;affective computing","Training;Emotion recognition;Accuracy;Design methodology;Time series analysis;Feature extraction;Brain modeling","","","","23","IEEE","25 Jun 2024","","","IEEE","IEEE Conferences"
"EEG Emotion Recognition via Graph-based Spatio-Temporal Attention Neural Networks","S. Sartipi; M. Torkamani-Azar; M. Cetin","Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA; School of Computing, University of Eastern Finland, Joensuu, Finland; Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA",2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"9 Dec 2021","2021","","","571","574","Emotion recognition based on electroencephalography (EEG) signals has been receiving significant attention in the domains of affective computing and brain-computer interfaces (BCI). Although several deep learning methods have been proposed dealing with the emotion recognition task, developing methods that effectively extract and use discriminative features is still a challenge. In this work, we propose the novel spatio-temporal attention neural network (STANN) to extract discriminative spatial and temporal features of EEG signals by a parallel structure of multi-column convolutional neural network and attention-based bidirectional long-short term memory. Moreover, we explore the inter-channel relationships of EEG signals via graph signal processing (GSP) tools. Our experimental analysis demonstrates that the proposed network improves the state-of-the-art results in subject-wise, binary classification of valence and arousal levels as well as four-class classification in the valence-arousal emotion space when raw EEG signals or their graph representations, in an architecture coined as GFT-STANN, are used as model inputs.","2694-0604","978-1-7281-1179-7","10.1109/EMBC46164.2021.9629628","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629628","","Deep learning;Emotion recognition;Computer architecture;Tools;Signal processing;Feature extraction;Electroencephalography","Arousal;Brain-Computer Interfaces;Electroencephalography;Emotions;Neural Networks, Computer","14","","24","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"EEG Data Analysis for Stress Detection using K-Nearest Neighbor","R. Shashidhar; P. Kadakol; D. Sreeniketh; P. Patil; K. H. Krishnappa; R. Madhura","Department of Electronics and Communication Engineering, JSS Science and Technology University, Mysuru, India; Department of Electronics and Communication Engineering, JSS Science and Technology University, Mysuru, India; Department of Electronics and Communication Engineering, JSS Science and Technology University, Mysuru, India; Department of Electronics and Communication Engineering, JSS Science and Technology University, Mysuru, India; Dept. of Computer Science, Southern University and A&M College, Baton Rouge, Louisina; Department of Electronics and Communication Engineering, Dayananda Sagar College of Engineering, Bengaluru, India",2023 International Conference on Integrated Intelligence and Communication Systems (ICIICS),"8 Feb 2024","2023","","","1","7","Stress is a impactful condition affecting individuals’ well-being, making its objective assessment and management vital. In recent years, electroencephalography (EEG) has emerged as a promising tool for real-time stress measurement. This paper presents a comprehensive review of EEG-based stress detection methods and their applications. Notably, the proposed model showcased remarkable performance, achieving 100% accuracy in classifying stress and relaxation levels using EEG signals. The review underscores the identification of distinct EEG patterns associated with stress, such as elevated beta and theta power alongside decreased alpha and beta coherence. Various feature extraction techniques have been leveraged to extract pertinent information from EEG signals. Machine learning algorithms like K-Nearest Neighbor (KNN) have been instrumental in accurately classifying stress levels. This model when compared to the CIS-KNORAU DES model described in the research article with reference number of 17 as in the references.","","979-8-3503-1545-5","10.1109/ICIICS59993.2023.10420898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10420898","Electroencephalography (EEG);Stress detection;Machine Learning;Feature Extraction;Beta and theta power","Brain modeling;Feature extraction;Electroencephalography;Real-time systems;Data mining;Stress;Stress measurement","","","","17","IEEE","8 Feb 2024","","","IEEE","IEEE Conferences"
"A Spectro-Statistical Approach for Emotion Identification from EEG Signals","L. R. Sookha; G. Sharma; M. A. Ganaie; A. Dhall","Indian Institute of Technology Ropar, India; Indian Institute of Technology Ropar, India; Indian Institute of Technology Ropar, India; Flinders University, Australia",2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG),"11 Jul 2024","2024","","","1","5","Automatic identification of emotions is important in human-centered computing. It allows machines to better understand user emotions. Identifying emotions via neural sensing techniques such as electroencephalogram (EEG) is a promising approach. In this paper, we aim to identify the emotions class from EEG signals. We frame emotion identification as a classification task and apply spectral and statistical encoders to extract the relevant features. We validate our approach on EmoNeuroDB dataset. Our method outperforms the EmoNeuroDB baseline, achieving a 42.10% increase in class prediction accuracy.","2770-8330","979-8-3503-9494-8","10.1109/FG59268.2024.10582002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10582002","","Human computer interaction;Face recognition;Gesture recognition;Probability;Feature extraction;Prediction algorithms;Electroencephalography","","","","26","IEEE","11 Jul 2024","","","IEEE","IEEE Conferences"
"Toward EEG-Based Objective Assessment of Emotion Intensity","P. -H. Ho; Y. -S. Chen; C. -S. Wei","Department of Computer Science, NYCU, Hsinchu, Taiwan; Department of Computer Science, NYCU, Hsinchu, Taiwan; Department of Computer Science, NYCU, Hsinchu, Taiwan",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","Understanding the temporal dynamics of emotion poses a significant challenge due to the lack of methods to measure them objectively. In this study, we propose a novel approach to tracking intensity (EI) based on electroencephalogram (EEG) during continuous exposure to affective stimulation. We design selective sampling strategies to validate the association between the prediction outcome of an EEG-based emotion recognition model and the prominence of emotion-related EEG patterns, evidenced by the improvement in the classification task of discriminating arousal and valence by 2.01% and 1.71%, respectively. This study constitutes a breakthrough in the objective evaluation of the temporal dynamics of emotions, proposing a promising avenue to refine EEG-based emotion recognition models through intensity-selective sampling. Furthermore, our findings can contribute to future affective studies by providing a reliable and objective measurement method to profile emotion dynamics.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10781662","National Science and Technology Council; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10781662","","Emotion recognition;Biological system modeling;Predictive models;Brain modeling;Electroencephalography;Emotional responses;Reliability;Engineering in medicine and biology","Electroencephalography;Humans;Emotions;Male;Female;Adult;Algorithms;Signal Processing, Computer-Assisted","","","31","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Advancing Semi-Supervised EEG Emotion Recognition through Feature Extraction with Mixup and Large Language Models","S. Yao; L. Liu; J. Lu; D. Wu; Y. Li","Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"10 Jan 2025","2024","","","2772","2779","The scarcity of labeled EEG data presents a significant challenge in emotion recognition. To address this issue, we propose PAWS, a semi-supervised learning framework specifically designed to enhance EEG-based emotion recognition, particularly in scenarios where labeled data is extremely limited. PAWS leverages the power of Intermediate Mixup, which improves domain adaptation by generating more robust features through strategic augmentation. Additionally, PAWS integrates large language models (LLMs) for enhanced feature extraction, enabling the framework to capture complex patterns in EEG signals even with minimal labeled data. In experiments with 1%, 5%, 10%, 15%, and 20% labeled data, PAWS consistently outperformed existing methods, with the most significant performance gains observed in scenarios with very few labeled samples. Notably, with 20% labeled data, PAWS achieved an accuracy of 95.81% ± 0.52% on the SEED dataset and 83.41% ± 0.31% on the SEED-IV dataset. These results demonstrate PAWS’s effectiveness in leveraging limited labeled data to achieve superior emotion recognition performance. This framework not only advances the state-of-the-art in EEG-based emotion recognition but also provides a robust foundation for future research in semi-supervised learning. Code is available at https://github.com/dragonlfy/PAWS.","2156-1133","979-8-3503-8622-6","10.1109/BIBM62325.2024.10822473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10822473","EEG-based Emotion Recognition;Semi-Supervised Learning;Large Language Models;Data Augmentation","Emotion recognition;Codes;Large language models;Semisupervised learning;Performance gain;Feature extraction;Brain modeling;Electroencephalography;Robustness;Optimization","","","","29","IEEE","10 Jan 2025","","","IEEE","IEEE Conferences"
"From Regional to Global Brain: A Novel Hierarchical Spatial-Temporal Neural Network Model for EEG Emotion Recognition","Y. Li; W. Zheng; L. Wang; Y. Zong; Z. Cui","Key Laboratory of Child Development and Learning Science (Ministry of Education), School of Information Science and Engineering, Southeast University, Nanjing, Jiangsu, China; Key Laboratory of Child Development and Learning Science (Ministry of Education), School of Biological Sciences and Medical Engineering, Southeast University, Nanjing, Jiangsu, China; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; Key Laboratory of Child Development and Learning Science (Ministry of Education), School of Biological Sciences and Medical Engineering, Southeast University, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China",IEEE Transactions on Affective Computing,"1 Jun 2022","2022","13","2","568","578","In this paper, we propose a novel Electroencephalograph (EEG) emotion recognition method inspired by neuroscience with respect to the brain response to different emotions. The proposed method, denoted by R2G-STNN, consists of spatial and temporal neural network models with regional to global hierarchical feature learning process to learn discriminative spatial-temporal EEG features. To learn the spatial features, a bidirectional long short term memory (BiLSTM) network is adopted to capture the intrinsic spatial relationships of EEG electrodes within brain region and between brain regions, respectively. Considering that different brain regions play different roles in the EEG emotion recognition, a region-attention layer into the R2G-STNN model is also introduced to learn a set of weights to strengthen or weaken the contributions of brain regions. Based on the spatial feature sequences, BiLSTM is adopted to learn both regional and global spatial-temporal features and the features are fitted into a classifier layer for learning emotion-discriminative features, in which a domain discriminator working corporately with the classifier is used to decrease the domain shift between training and testing data. Finally, to evaluate the proposed method, we conduct both subject-dependent and subject-independent EEG emotion recognition experiments on SEED database, and the experimental results show that the proposed method achieves state-of-the-art performance.","1949-3045","","10.1109/TAFFC.2019.2922912","National Key Research and Development Program of China(grant numbers:2015CB351704); National Key R&D Program of China(grant numbers:2018YFB1305200); National Natural Science Foundation of China(grant numbers:61572009,61772276); Jiangsu Provincial Key Research and Development Program(grant numbers:BE2016616); China Scholarship Council(grant numbers:201706090224); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736804","EEG emotion recognition;regional to global;spatial-temporal network","Electroencephalography;Feature extraction;Emotion recognition;Brain modeling;Electrodes;Biological neural networks;Computational modeling","","163","","47","IEEE","14 Jun 2019","","","IEEE","IEEE Journals"
"EEG based automatic emotion recognition using EMD and Random forest classifier","G. K. P. Veeramallu; Y. Anupalli; S. k. Jilumudi; A. Bhattacharyya","Department of Electronics and Communication Engineering, National Institute of Technology, Andhra Pradesh, Tadepalligudem, INDIA; Department of Electronics and Communication Engineering, National Institute of Technology, Andhra Pradesh, Tadepalligudem, INDIA; Department of Electronics and Communication Engineering, National Institute of Technology, Andhra Pradesh, Tadepalligudem, INDIA; Department of Electronics and Communication Engineering, National Institute of Technology, Andhra Pradesh, Tadepalligudem, INDIA","2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","30 Dec 2019","2019","","","1","6","Electroencephalogram (EEG) signal based automatic emotion recognition is an inter-disciplinary field which has been studied for a long time. In this study, empirical mode decomposition (EMD) is used for EEG-based automatic emotion classification. The EMD is used for decomposition of non-stationary and nonlinear EEG signals into different modes called intrinsic mode functions (IMFs). From the resulting IMFs different nonlinear features namely Hjorth parameters, Shannon entropy, collision entropy, differential entropy, and Higuchis fractal dimension are extracted in order to determine the characteristics associated with emotional states. The obtained features are smoothed using moving average filter with a window size of 5 samples for removing unwanted rapid fluctuations in the extracted features. Finally, the processed features are fed to random forest (RF) classifier and the accuracies are noted for different profiles of 4, 6 and 12 channels. The proposed method is applied to SJTU emotion EEG dataset (SEED). We have obtained highest classification accuracies of 89.59%, 91.45%, 93.87% using activity feature for the above 3 channel profiles in classifying positive, neutral and negative emotions respectively. Finally a comparative analysis is performed in terms of computed classification accuracies with other above mentioned features. As compared to existing methods, the proposed method has shown better performance and can be considered for recognition of human emotional states.","","978-1-5386-5906-9","10.1109/ICCCNT45670.2019.8944903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944903","Electroencephalogram (EEG);empirical mode decomposition (EMD);intrinsic mode function (IMF);moving average filter;random forest (RF)","Electroencephalography;Feature extraction;Emotion recognition;Entropy;Complexity theory;Fractals;Electrodes","","16","","25","IEEE","30 Dec 2019","","","IEEE","IEEE Conferences"
"Analysis of brain signal on visual noise affecting driver’s attention using deep learning","M. A. H. Sojun; M. N. Rahaman; A. K. Datta; S. H. Shehab; T. Hasan","Department of Computer Science and Engineering(CSE), American International University-Bangladesh, Dhaka, Bangladesh; Department of Computer Science and Engineering(CSE), American International University-Bangladesh, Dhaka, Bangladesh; Department of Computer Science and Engineering(CSE), American International University-Bangladesh, Dhaka, Bangladesh; Department of Computer Science and Engineering(CSE), American International University-Bangladesh, Dhaka, Bangladesh; Department of Computer Science and Engineering(CSE), American International University-Bangladesh, Dhaka, Bangladesh",2022 12th International Conference on Electrical and Computer Engineering (ICECE),"4 Apr 2023","2022","","","100","103","One of the main reasons for road accidents is driver’s brain activities during driving, some of which is heavily influenced by roadside visuals like posters, billboards and other objects around. Our objective is to prove that those posters, billboards, and roadside activities are some of the reasons for driver’s distraction that create high arousal on negative valence in certain part of driver’s brain and causes frequent road accidents. Our experiment shows that images of visually distasteful creates quite opposite reaction to that images of visually pleasing to human brain. Taking pictures from different locations of Dhaka city representing the actual roadside scenario that produces high stimulation in brain and can lead to human errors. In this research, brain data was collected using EEG device by displaying categorically positive, negative, and neutral pictures to the viewers. Collected brain data was analyzed using Deep Learning techniques. From the result, this research established strong correlations between different categories of images and the neural activities in the brain. The results are conclusive about perturbation on visually distracting elements along the roadside which may contribute to minor to major road accidents.","2771-7917","979-8-3503-9879-3","10.1109/ICECE57408.2022.10089091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089091","Visual Noise;EEG;Deep Learning;Emotion State;Brain Signal","Deep learning;Visualization;Road accidents;Correlation;Perturbation methods;Urban areas;Neural activity","","","","16","IEEE","4 Apr 2023","","","IEEE","IEEE Conferences"
"An EEG-Driven Framework for Emotion Recognition During Gameplay","K. Shahzad; Z. Ali; U. Rauf; A. U. Rehman; S. Khan; S. H. Noorani","Department of Elect & Comp Engineering, COMSAT University, Islamabad, Pakistan; Department of Computer Science, Bahria University, Islamabad, Pakistan; Department of Computer Engineering, University of Engineering & Technology, Taxila, Pakistan; Department of Computer Science, Bahria University, Islamabad, Pakistan; Department of Computer Engineering, University of Engineering & Technology, Taxila, Pakistan; Department of Computer Engineering, University of Engineering & Technology, Taxila, Pakistan",2024 5th International Conference on Advancements in Computational Sciences (ICACS),"21 Mar 2024","2024","","","1","6","Gaming transcends mere entertainment, serving as a unique realm to explore human emotions. This research utilizes Brain-Computer Interface (BCI) technology, specifically Electroencephalogram (EEG) signals, to unravel player emotions. Pioneering an innovative approach, our study decodes and interprets emotions exhibited during gaming experiences. Through meticulous analysis, we demonstrate the efficacy of EEG signals in revealing emotional states. Findings illuminate the dynamic interplay between the brain and gaming environments. Leveraging datasets like GAMEEMO, our study employs various data and feature extraction processes to detect emotions (boring, calm, horror, funny) with a accuracy of 98.21%, achieved by the Random Forest model.","","979-8-3503-9478-8","10.1109/ICACS60934.2024.10473280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473280","emotions;brain computer interface (BCI);human sentiments;electroencephalogram (EEG);player engagement;interface design;neuroscience","Emotion recognition;Psychology;Entertainment industry;Medical services;Feature extraction;Electroencephalography;Wearable devices","","","","27","IEEE","21 Mar 2024","","","IEEE","IEEE Conferences"
"Emotion recognition from multi-channel EEG data through Convolutional Recurrent Neural Network","X. Li; D. Song; P. Zhang; G. Yu; Y. Hou; B. Hu","School of Computing and Communications, The Open University, United Kindom; Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, China; Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, China; Tianjin University, Tianjin, Tianjin, CN; School of Information Science and Engineering, Lanzhou University, China; Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, China",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"19 Jan 2017","2016","","","352","359","Automatic emotion recognition based on multi-channel neurophysiological signals, as a challenging pattern recognition task, is becoming an important computer-aided method for emotional disorder diagnoses in neurology and psychiatry. Traditional approaches require designing and extracting a range of features from single or multiple channel signals based on extensive domain knowledge. This may be an obstacle for non-domain experts. Moreover, traditional feature fusion method can not fully utilize correlation information between different channels. In this paper, we propose a preprocessing method that encapsulates the multi-channel neurophysiological signals into grid-like frames through wavelet and scalogram transform. We further design a hybrid deep learning model that combines the `Convolutional Neural Network (CNN)' and `Recurrent Neural Network (RNN)', for extracting task-related features, mining inter-channel correlation and incorporating contextual information from those frames. Experiments are carried out, in a trial-level emotion recognition task, on the DEAP benchmarking dataset. Our results demonstrate the effectiveness of the proposed methods, with respect to the emotional dimensions of Valence and Arousal.","","978-1-5090-1611-2","10.1109/BIBM.2016.7822545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822545","CNN;EEG;emotion recognition;LSTM;physiological signal","Emotion recognition;Electroencephalography;Feature extraction;Continuous wavelet transforms;Correlation","","174","1","24","IEEE","19 Jan 2017","","","IEEE","IEEE Conferences"
"BrainNets: Human Emotion Recognition Using an Internet of Brian Things Platform","H. Lu; H. Kim; Y. Li; Y. Zhang","Dept. of Mech. And Control Engineering, Kyushu Institute of Technology, Kitakyushu, Japan; Dept. of Mech. And Control Engineering, Kyushu Institute of Technology, Kitakyushu, Japan; Faculty of Engineering, Fukuoka University, Fukuoka, Japan; School of Information and Safety Engineering, Zhongnan University of Economics and Law, Wuhan, China",2018 14th International Wireless Communications & Mobile Computing Conference (IWCMC),"30 Aug 2018","2018","","","1313","1316","Human wearable helmet is a useful tool for monitoring the status of miners in the mining industry. However, there is little research regarding human emotion recognition in an extreme environment. In this paper, an emotional state evoked paradigm is designed to identify the brain area where the emotion feature is most evident. Next, the correct electrode position is determined for the collection of the negative emotion by the electroencephalograph (EEG) based on the international 10-20 system of electrode placement. And then, a fusion algorithm of the anxiety level is proposed to evaluate the person's mental state using the θ, α, and β rhythms of an EEG. Experiments demonstrate that the position Fp2 is the best electrode position for obtaining the anxiety level parameter. The most visible EEG changes appear within the first two seconds following stimulation. The amplitudes of the θ rhythm increase most significantly in the negative emotional state.","2376-6506","978-1-5386-2070-0","10.1109/IWCMC.2018.8450382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8450382","Emotion recognition;EEG;Cloud computing;Internet of Things","Electroencephalography;Emotion recognition;Electrodes;Frequency-domain analysis;Feature extraction;Time-domain analysis;Fatigue","","3","","20","IEEE","30 Aug 2018","","","IEEE","IEEE Conferences"
"Emotion Classification with DEAP Dataset:Survey","H. P. Ünal; G. Gökmen; M. Yumurtacı","Mekatronik Mühendisliği, Marmara Üniversitesi, İstanbul, Türkiye; Mekatronik Mühendisliği, Marmara Üniversitesi, İstanbul, Türkiye; Elektrik-Elektronik Mühendisliği, Afyon Kocatepe Üniversitesi, Afyon, Türkiye",2020 Innovations in Intelligent Systems and Applications Conference (ASYU),"23 Nov 2020","2020","","","1","6","Emotions affect our decision making as well as human interactions in our daily lives. Therefore, contributing to commercial activities such as the classification of emotions, neuromarketing and game industry; to be able to communicate with people who cannot express their feelings due to their disability; It has been a popular research topic for many purposes such as being able to determine the accuracy of expressions during the query and creating emotions in humanoid robots. In the researches, it is aimed to increase the classification success by using various methods. Since it is difficult for people to interfere with EEG signals in emotion classification, more realistic determinations can be made. Therefore, the examination of EEG data for emotion classification is an area that needs to be updated and developed. The most popular open source dataset used in emotion classification studies with EEG is the DEAP dataset. In this article, it is aimed to shed light on future studies by examining the classification studies made with DEAP data set.","","978-1-7281-9136-2","10.1109/ASYU50717.2020.9259797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9259797","EEG;emotion classification;DEAP","Electroencephalography;Mathematical model;Brain modeling;Matlab;Electromyography;Internet;Industries","","4","","0","IEEE","23 Nov 2020","","","IEEE","IEEE Conferences"
"A Multimodal-Driven Fusion Data Augmentation Framework for Emotion Recognition","A. Li; M. Wu; R. Ouyang; Y. Wang; F. Li; Z. Lv","Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Key Laboratory of Flight Techniques and Flight Safety, CAAC; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China",IEEE Transactions on Artificial Intelligence,"","2025","PP","99","1","16","The pursuit of imbuing computers with emotional intelligence has driven extensive research into physiological signal analysis for emotion recognition. Deep learning techniques offer promising avenues for analyzing physiological signals in this domain. Despite numerous studies on emotion recognition using various physiological signals, challenges persist in classifying multimodal physiological signals due to data scarcity. Current research lacks focus on addressing data insufficiency for multimodal physiological signals. This paper proposes an innovative method to address this issue and improve the effect of emotion recognition using multimodal physiological signal data. Our model comprises a physiological signal encoder, a multimodal data generator, and a multimodal emotion recognizer. Specifically, we introduce a customized ConvNeXt-Attention fusion model (CNXAF) to fuse diverse physiological signals, generating fused multimodal data. The multimodal data generator employs a conditional Self-Attention Generative Adversarial Network (c-SAGAN) to synthesize additional data across different categories, augmenting original datasets. Finally, the multimodal emotion recognizer utilizes the ConvNeXt-t classifier for emotion recognition on the extended dataset. Through extensive experimentation, our model achieves accuracies of 96.06% on the DEAP dataset and 95.70% on the WESAD dataset, demonstrating the effectiveness of our approach in accurately recognizing emotions. Experimental results underscore the superior performance of our method compared to existing approaches in multimodal emotion recognition research. Our code is publicly available at https://github.com/suprola1017/Multimodal-Data-Enhance-Framework-for-Emotion-Recognation.","2691-4581","","10.1109/TAI.2025.3537965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10896942","Multimodal Emotion Recognition;Data Fusion;Data Augmentation;Physiological Signals;Deep Learning","Emotion recognition;Physiology;Brain modeling;Data models;Electroencephalography;Artificial intelligence;Generators;Data augmentation;Training;Accuracy","","","","","IEEE","20 Feb 2025","","","IEEE","IEEE Early Access Articles"
"EEG Emotion Recognition via a Lightweight 1DCNN-BiLSTM Model in Resource-Limited Environments","H. Liu; S. Zhang; J. Shi; H. Liu; Y. Zhang; W. Wu; B. Li","School of Microelectronics, Xidian University, Xi’an, China; Beijing Sunwise Space Technology Ltd., Beijing, China; School of Microelectronics, Xidian University, Xi’an, China; Beijing Sunwise Space Technology Ltd., Beijing, China; School of Microelectronics, Xidian University, Xi’an, China; School of Microelectronics, Xidian University, Xi’an, China; Beijing Sunwise Space Technology Ltd., Beijing, China",IEEE Sensors Journal,"3 Feb 2025","2025","25","3","5723","5730","In the application of wearable medical monitoring devices, electroencephalogram (EEG) emotion recognition tasks need to be implemented in resource-constrained environments. Therefore, the proposed lightweight 1DCNN-BiLSTM network aims to achieve comparable emotion recognition accuracy to existing models while significantly reducing computational costs and memory usage on resource-constrained devices. First, a low computational cost preprocessing method is used to eliminate the interference of baseline signals in the raw EEG signal. Second, a shallow hybrid network of 1DCNN-BiLSTM is proposed to extract spatial features between different channels and temporal forward–backward features in EEG signals. Finally, quantize the trained model to reduce memory consumption and replace floating-point operations with fixed-point operations. Experiments on DEAP and DREAMER datasets achieve more than 90% recognition accuracy. The memory usage of the quantized network is 17.4 KB, and the computation for a single classification is 1.7 MFLOPs. The model is ultimately deployed on an embedded processor, attaining an inference speed of 352.51 mss, thereby enabling emotion recognition in resource-constrained environments.","1558-1748","","10.1109/JSEN.2024.3514094","Special Fund for Research on National Major Research Instruments of the National Natural Science Foundation of China (NSFC)(grant numbers:82327810); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10812798","Bidirectional long short-term memory;convolutional neural network;electroencephalogram (EEG);emotion recognition;lightweight","Electroencephalography;Brain modeling;Emotion recognition;Feature extraction;Logic gates;Bidirectional long short term memory;Accuracy;Sensors;Computational modeling;Convolution","","1","","40","IEEE","23 Dec 2024","","","IEEE","IEEE Journals"
"Analysis of Dynamics of EEG Signals in Emotional Valence Using Super-Resolution Superlet Transform","H. Kumar; N. Ganapathy; R. Swaminathan","Department of Applied Mechanics, Biomedical Engineering, Indian Institute of Technology Madras, Chennai, India; Department of Biomedical Engineering, Indian Institute of Technology Hyderabad, Sangareddy, India; Department of Applied Mechanics, Biomedical Engineering, Indian Institute of Technology Madras, Chennai, India",IEEE Sensors Letters,"28 Jan 2025","2025","9","2","1","4","Electroencephalography (EEG)-based emotional state assessment is widely preferred due to its noninvasiveness and nonradiation approach. However, these signals are highly nonstationary and multicomponent, demonstrating large intrasubject variability. Extracting time and frequency information simultaneously from EEG addresses these challenges to effectively recognise the valence emotional states. Traditional time–frequency (TF) approaches optimise either temporal or frequency resolution, resulting in failure to identify fast transient oscillatory emotional events. In this letter, an attempt has been made to recognize emotional valence using super-resolution-based superlet transform (SLT). For this, the preprocessed EEG signals during emotion-evoking audio–visual stimuli from publicly available database is considered. The EEG signals are decomposed into theta, alpha, beta, and gamma frequency bands and are subjected to SLT. The TF skewness and kurtosis are extracted from the SLT. The statistical significance of features is evaluated, and the features are applied to three machine learning algorithms: random forest, Adaboost, and k-nearest neighbor. The results show that the SLT-based TF spectrum is able to provide variations of frequency components associated with emotional valence. Both the features exhibit statistically significant $(p < 0.05)$ difference in the high-frequency gamma bands to characterize emotional valence. Among the classifiers, AdaBoost stands out as the most robust performer (F1 = 70.16%). Feature importance analysis highlights that SLT features from the fronto-central and parieto-occipital brain regions play a crucial role in valence detection. It appears that this method could be useful in analyzing various mental well-being conditions in clinical settings.","2475-1472","","10.1109/LSENS.2025.3526907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829970","Sensor signal processing;classification;electroencephalogram;emotions;superlet transform (SLT);time–frequency representation (TFR)","Electroencephalography;Time-frequency analysis;Signal resolution;Feature extraction;Transforms;Electrodes;Wavelet transforms;Wavelet analysis;Spectrogram;Emotion recognition","","","","22","IEEE","7 Jan 2025","","","IEEE","IEEE Journals"
"Multimodal Emotion Recognition Based on Facial Expressions, Speech, and EEG","J. Pan; W. Fang; Z. Zhang; B. Chen; Z. Zhang; S. Wang","School of Software, South China Normal University, Guangzhou, China; School of Software, South China Normal University, Guangzhou, China; School of Software, South China Normal University, Guangzhou, China; School of Software, South China Normal University, Guangzhou, China; Shenzhen Medical Biometrics Perception and Analysis Engineering Laboratory, Harbin Institute of Technology, Shenzhen, China; School of Computing and Mathematical Sciences, University of Leicester, Leicester, U.K.",IEEE Open Journal of Engineering in Medicine and Biology,"14 Jun 2024","2024","5","","396","403","Goal: As an essential human-machine interactive task, emotion recognition has become an emerging area over the decades. Although previous attempts to classify emotions have achieved high performance, several challenges remain open: 1) How to effectively recognize emotions using different modalities remains challenging. 2) Due to the increasing amount of computing power required for deep learning, how to provide real-time detection and improve the robustness of deep neural networks is important. Method: In this paper, we propose a deep learning-based multimodal emotion recognition (MER) called Deep-Emotion, which can adaptively integrate the most discriminating features from facial expressions, speech, and electroencephalogram (EEG) to improve the performance of the MER. Specifically, the proposed Deep-Emotion framework consists of three branches, i.e., the facial branch, speech branch, and EEG branch. Correspondingly, the facial branch uses the improved GhostNet neural network proposed in this paper for feature extraction, which effectively alleviates the overfitting phenomenon in the training process and improves the classification accuracy compared with the original GhostNet network. For work on the speech branch, this paper proposes a lightweight fully convolutional neural network (LFCNN) for the efficient extraction of speech emotion features. Regarding the study of EEG branches, we proposed a tree-like LSTM (tLSTM) model capable of fusing multi-stage features for EEG emotion feature extraction. Finally, we adopted the strategy of decision-level fusion to integrate the recognition results of the above three modes, resulting in more comprehensive and accurate performance. Result and Conclusions: Extensive experiments on the CK+, EMO-DB, and MAHNOB-HCI datasets have demonstrated the advanced nature of the Deep-Emotion method proposed in this paper, as well as the feasibility and superiority of the MER approach.","2644-1276","","10.1109/OJEMB.2023.3240280","STI 2030–Major Projects(grant numbers:2022ZD0208900); National Natural Science Foundation of China(grant numbers:62076103,62271217); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026861","Multimodal emotion recognition;electroencephalogram;facial expressions;speech","Emotion recognition;Brain modeling;Feature extraction;Electroencephalography;Speech recognition;Convolution;Deep learning","","28","","35","CCBYNCND","27 Jan 2023","","","IEEE","IEEE Journals"
"Performance of k-NN classifier for emotion detection using EEG signals","V. L. Kaundanya; A. Patil; A. Panat","Cummins College of Engineering for women, Pune; Cummins College of Engineering for women, Pune; Cummins College of Engineering for women, Pune",2015 International Conference on Communications and Signal Processing (ICCSP),"12 Nov 2015","2015","","","1160","1164","This paper describes the performance of k-NN classifier to classify the different emotions. The human brain is a superimposition of the diverse processes. This complex structure of brain is recognized through EEG signals. EEG signals indicate the changes in the state of brain. Electroencephalograph (EEG) measurements are commonly used in different research areas under the field of medical. Data acquisition is done for different emotions with the help of ADInsruments' power lab instrument. The real life EEG signals are collected with the help of Ground Truth Method. In this paper, proposed method consists of four steps, viz., acquisition of data, Pre-processing, Feature extraction and Classification. Subjects are stimulated for Sad and Happy emotions. Statistical features are then given to a k-NN classifier. The k Nearest Neighbor classifier gives different accuracy of classification for different combinations of training and testing dataset. The system has been tested on number of subjects to observe the performance of k-NN classifier.","","978-1-4799-8081-9","10.1109/ICCSP.2015.7322687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322687","Brain Computer Interfacing;EEG signals;Electroencephalography;Emotions;k-NN classifier;Statistical Features","Electroencephalography;Instruments;Artificial neural networks;Feature extraction;Standards","","14","","10","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A Survey of EEG Analysis based on Graph Neural Network","Y. Li","College of Information Engineering, Capital Normal University, Beijing, China","2021 2nd International Conference on Electronics, Communications and Information Technology (CECIT)","1 Apr 2022","2021","","","151","155","EEG signals reflect the activity of the brain. Previous studies based on EEG signal recognition focused on extracting temporal features of EEG and have not yet considered the topology of EEG signal channels, missing important spatial information. Graph neural networks view EEG signals as vertices of a graph and capture the implicit topological relationships between signals. In this review, we review the basic models of graph networks and their variants, including graph convolutional neural networks, graph attention networks, etc., summarize the applications in the fields of emotion recognition, epilepsy diagnosis, and finally present an outlook based on current challenges.","","978-1-6654-3757-8","10.1109/CECIT53797.2021.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9742089","EEG;Topology;Graph convolutional neural network;Graph attention networks","Emotion recognition;Network topology;Epilepsy;Feature extraction;Electroencephalography;Graph neural networks;Topology","","3","","40","IEEE","1 Apr 2022","","","IEEE","IEEE Conferences"
"Multi-Target Positive Emotion Recognition From EEG Signals","G. Zhao; Y. Zhang; G. Zhang; D. Zhang; Y. -J. Liu","CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, P. R. China; CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing, P. R. China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, P. R. China; Department of Psychology, Tsinghua University, Beijing, P. R. China; Department of Computer Science and Technology, BNRist, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, Beijing, P. R. China",IEEE Transactions on Affective Computing,"28 Feb 2023","2023","14","1","370","381","Compared with the widely studied negative emotions in which different classes are easy to distinguish, nowadays less attention is paid to the recognition of positive emotions that are not fully independent. In this article, we propose to recognize multiple continuous positive emotions that exhibit statistical dependencies using multi-target regression — by analyzing brain activities when an individual watches emotional film clips — and explore the neural representation of different positive emotions. Thirty-seven participants volunteered to participate in our study, in which their brain activities were recorded when watching five selected film clips (corresponding to five positive emotions: amusement, happiness, romance, tenderness and warmth). First, 150 well-known power features extracted from Electroencephalography (EEG) signals and 105 multimedia content analysis features were collected as the pool of candidate features. Second, based on the collected features, we propose to use a linear model (linear regression) and a nonlinear model (long short-term memory network, LSTM) to predict the percentage of five positive emotions. Then, percentage values were converted to ranking numbers and Kendall rank correlation coefficients were calculated. Our results showed that (1) ensemble of regressor chains (ERC) using LSTM as unit regressor obtained both the best regression results (with lowest RMSE = 8.325 and highest $\text{R }^{2} = 0.346$R2=0.346) and the best Kendall rank correlation coefficient (0.165) on EEG features merely, and (2) selective features from alpha frequency bands of EEG signals could represent different positive emotions. These results demonstrate the effectiveness of selective EEG features on recognizing different positive emotions.","1949-3045","","10.1109/TAFFC.2020.3043135","National Natural Science Foundation of China(grant numbers:31771226,U1736220,61725204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286502","Emotion recognition;positive emotion;multi-target;regression model;EEG","Emotion recognition;Electroencephalography;Brain modeling;Feature extraction;Psychology;Task analysis;Predictive models","","19","","63","IEEE","8 Dec 2020","","","IEEE","IEEE Journals"
"EEG-Based Emotion Recognition via Efficient Convolutional Neural Network and Contrastive Learning","C. Li; X. Lin; Y. Liu; R. Song; J. Cheng; X. Chen","Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Neurosurgery, The First Affiliated Hospital of USTC, and the Department of Electronic Engineering and Information Science, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China",IEEE Sensors Journal,"17 Oct 2022","2022","22","20","19608","19619","Convolutional neural networks (CNNs) have achieved better performance than traditional algorithms in electroencephalogram (EEG)-based emotion recognition tasks in recent years. However, as the number of convolution layers increases, the number of network parameters increases sharply. Furthermore, emotional labels are not fully utilized by most supervised learning methods. To achieve a simple and effective model with supervised learning, we propose an efficient CNN and contrastive learning (ECNN-C) method for EEG-based emotion recognition. We utilize a novel convolutional block to replace the standard convolution to reduce the computational burden of the model. In addition, we adopt supervised contrastive learning to make full use of emotion labels, which allows us to pull EEG samples with the same emotional state together, push samples of the different emotional states apart from each other at the same time. Experimental results prove the effectiveness of the ECNN-C method for EEG-based emotion recognition.","1558-1748","","10.1109/JSEN.2022.3202209","National Key Research and Development Program of China(grant numbers:2021YFF1200600); National Natural Science Foundation of China(grant numbers:61922075,41901350,62176081,62171176,32150017); National Defense Basic Scientific Research Program of China(grant numbers:JCKY2019548B001); Anhui Key Project of Research and Development Plan(grant numbers:202104d07020005); Fundamental Research Funds for the Central Universities(grant numbers:JZ2021HGTB0078,PA2021KCPY0051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9875025","Contrastive learning;efficient convolutional neural network (ECNN);electroencephalogram (EEG);emotion recognition","Convolution;Feature extraction;Brain modeling;Emotion recognition;Electroencephalography;Task analysis;Computational modeling","","21","","47","IEEE","2 Sep 2022","","","IEEE","IEEE Journals"
"Correlated Attention Networks for Multimodal Emotion Recognition","J. -L. Qiu; X. -Y. Li; K. Hu","School of Electronic, Shanghai Jiao Tong University, Dongchuan Road 800, Shanghai, China; East China Normal University, Dongchuan Road 500, Shanghai, China; School of Electronic, Shanghai Jiao Tong University, Dongchuan Road 800, Shanghai, China",2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),"24 Jan 2019","2018","","","2656","2660","Emotion is a subjective, conscious experience when people face different kinds of stimuli. In this paper, we propose a new model, Correlated Attention Network (CAN), to make multimodal emotion recognition. Correlated Attention Network is an extension of attention based recurrent neural network with correlation calculated of different gated recurrent units to take correlation of EEG and eye movement extracted signals into attention mechanism and takes advantage of coordinated representation with complementary features. In experiments on 3 real world datasets, we find that our model can significantly contribute to higher emotion classification accuracy when higher correlation is acquired. Our experiment results indicate that the Correlated Attention Network model performs better than the state-of-the-art methods with a mean accuracy of of 94.03% on SEED dataset, 87.71% on SEED IV dataset, 88.51% and 85.62% for four classification and two dichotomies on DEAP dataset, respectively.","","978-1-5386-5488-0","10.1109/BIBM.2018.8621129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621129","Attention Mechanism;Recurrent Neural Network;EEG;Deep Canonical Correlation Analysis","Brain modeling;Electroencephalography;Correlation;Logic gates;Feature extraction;Emotion recognition;Recurrent neural networks","","12","","23","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Emotion Recognition and Treatment Based on Multi-Feature Signal Fusion and Deep Learning Model","J. Z. Yin Shen","Dulwich College Beijing, Beijing, China","2023 3rd International Signal Processing, Communications and Engineering Management Conference (ISPCEM)","18 Apr 2024","2023","","","26","30","Music is an expression of mood and emotion, and listening to music is an effective non-pharmacological tool with many benefits for quality of life and well-being. Good music can have a positive effect on mood, allowing human listeners to reduce anxiety, increase mental energy and motivation, and is believed to have analgesic and anxiolytic properties. The various physiological and psychological effects of music therapy on humans have been reported in the literature, and the effects of music on physiology and behavior have been studied in animals. Many studies have concluded that animals are also affected by music. Both the rhythmic and spectral distributions of music can convey emotional meaning, and these features have been shown in altering the behavior of animals. In this paper, we first constructed an emotion recognition model for cats based on multi-channel physiological signals, selecting EEG, ECG, and respiratory signals that are closely related to emotions, and utilizing the representational complementarity between multiple signals to construct a feature-level fusion emotion recognition model, which can effectively recognize a variety of emotions in cats. The experimental results show that the average accuracy of recognition has been effectively improved. Second, a music classification recognition network based on LSTM network was designed so as to obtain the most effective music styles and attributes that enhance the cat's emotions and help the cat alleviate negative emotions. The network shares the interpretation layer of classification on the basis of LSTM network, which can greatly reduce the learning of model parameters and improve the learning efficiency.","","979-8-3503-0702-3","10.1109/ISPCEM60569.2023.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499373","Medical Data Enhancement;Improved WGAN Network;Multi-Channel Fusion CNN;Gastric Cancer Detection","Emotion recognition;Animals;Mood;Music;Electrocardiography;Signal processing;Brain modeling","","1","","14","IEEE","18 Apr 2024","","","IEEE","IEEE Conferences"
"Light-Weight Gated Recurrent Unit for Electroencephalogram Emotion Classification","S. Sitjongsataporn; W. Du; P. Sakarin","Dept. of Electronic Engineering, Mahanakorn Institute of Innovation (MII) Faculty of Engineering and Technology, Mahanakorn University of Technology, Bangkok, Thailand; Dept. of Computer Engineering, Mahanakorn University of Technology International College (MUTIC) Faculty of Engineering and Technology Mahanakorn University of Technology; Dept. of Electronic Engineering, Mahanakorn Institute of Innovation (MII) Faculty of Engineering and Technology, Mahanakorn University of Technology, Bangkok, Thailand",2025 13th International Electrical Engineering Congress (iEECON),"9 May 2025","2025","","","1","4","This experiment investigates the impact of reducing GRU units and dense layer neurons in a lightweight GRU architecture (LW-GRU-RU) compared to a baseline GRU model for electroencephalogram (EEG) emotion classification. A baseline GRU model is used as a reference, with an optimized GRU variant utilizing feature selection through a Random Forest-based algorithm. Experiments are conducted on a labeled emotion dataset, comparing accuracy and training efficiency across five trials. Results highlight the trade-off between model complexity, accuracy, and computational efficiency, providing insights for practical applications. Both models are evaluated on an emotion dataset for accuracy and training efficiency. The lightweight model achieves a competitive accuracy of 97.486% while reducing the average training time to 0.19 seconds per epoch, showcasing its potential for efficient real-world applications.","","979-8-3315-4395-2","10.1109/iEECON64081.2025.10987663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10987663","Electroencephalogram (EEG) signal;Baseline GRU model;GRU with Feature selection","Training;Hands;Accuracy;Computational modeling;Neurons;Computer architecture;Brain modeling;Feature extraction;Electroencephalography;Computational efficiency","","","","11","IEEE","9 May 2025","","","IEEE","IEEE Conferences"
"Graph Convolutional Neural Network and Attention Mechanism based Emotion Classification","P. Gao; X. Zheng; G. Guo; Y. Zhang","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China","2024 International Conference on Identification, Information and Knowledge in the Internet of Things (IIKI)","17 Mar 2025","2024","","","30","36","Emotion is the subjective cognitive experiences and the corresponding behavioral responses, which plays an important role in interpersonal communication. Emotion classification is an important component of Human Computer Interaction(HCI) that automatically identifies an individual’s emotional state by acquiring physiological or non-physiological signals. Traditional emotion classification methods cannot comprehensively exploit the global and local features contained in EEG signals generated upon stimulation. In this paper, we proposed a graph convolutional neural network and attention mechanism based emotion classification (GCNAEC). First, cross-frequency coupling (CFC) network is leveraged to build adjacency matrices instead of simply combining connections by relative distances and spatial electrode positions of EEG channels. Second, a novel graph attention approach is developed to calculate the attention score of the current graph node and each neighboring node for investigating the differences between single EEG signal and functional connectivity patterns. Third, a novel graph processing method called GAT using PPC is introduced to reduce dimensionality and retain the global information of raw data in each layer. Finally, the experimental results show that the proposed method achieves better performance in emotion classification, in which the classification accuracies are 83.24% for valence dimension and 89.70% for arousal dimension, compared with state-of-the-art methods. In addition, our model outperforms the general GCN model both in terms of recognition accuracy and training time.","","979-8-3315-1063-3","10.1109/IIKI65561.2024.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917601","Emotion Classification;EEG;CFC Network;Graph Convolutional Neural Network;Graph Attention Mechanism","Training;Knowledge engineering;Attention mechanisms;Accuracy;Neural networks;Brain modeling;Electroencephalography;Physiology;Convolutional neural networks;Internet of Things","","","","15","IEEE","17 Mar 2025","","","IEEE","IEEE Conferences"
"STAR: A Spatial-Temporal Autoencoder for EEG Restoration in Emotion Recognition","H. -L. Yin; W. -L. Zheng; B. -L. Lu","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Research in emotion recognition using electroencephalography (EEG) has advanced rapidly, and affective EEG-based Brain-computer Interface (aBCI) technology is increasingly moving from lab research to real-world application. Nevertheless, EEG signals are inherently delicate and prone to noise and artifacts, especially in real-world environments where data quality often lags behind laboratory standards. This disparity poses substantial challenges for models trained on high-quality datasets. Conventional methods, such as data interpolation or exclusion, limit model efficacy. To overcome these challenges, we introduce the Spatial-Temporal Autoencoder for EEG Restoration (STAR). STAR leverages dynamic channel and temporal masking to mimic real-world signal degradation and incorporates a spatial-temporal alternating attention mechanism to encapsulate intricate spatiotemporal dynamics within EEG data. Our evaluations on three premium emotion recognition EEG datasets reveal that STAR effectively restores signals across varying corruption levels, significantly bolstering the performance of emotion recognition models in suboptimal conditions.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889545","National Natural Science Foundation of China; Shanghai Jiao Tong University; Shanghai Jiao Tong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889545","EEG restoration;EEG-based emotion recognition","Emotion recognition;Attention mechanisms;Autoencoders;Stars;Speech recognition;Brain modeling;Electroencephalography;Data models;Spatiotemporal phenomena;Standards","","","","25","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Multi-Scale Masked Autoencoders for Cross-Session Emotion Recognition","M. Pang; H. Wang; J. Huang; C. -M. Vong; Z. Zeng; C. Chen","School of Electronics and Information Engineering, Wuyi University, Jiangmen, China; School of Electronics and Information Engineering, Wuyi University, Jiangmen, China; School of Electronics and Information Engineering, Wuyi University, Jiangmen, China; Department of Computer and Information Science, University of Macau, Macau, China; School of Electronics and Information Engineering, Wuyi University, Jiangmen, China; School of Electronics and Information Engineering, Wuyi University, Jiangmen, China",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"22 Apr 2024","2024","32","","1637","1646","Affective brain-computer interfaces (aBCIs) have garnered widespread applications, with remarkable advancements in utilizing electroencephalogram (EEG) technology for emotion recognition. However, the time-consuming process of annotating EEG data, inherent individual differences, non-stationary characteristics of EEG data, and noise artifacts in EEG data collection pose formidable challenges in developing subject-specific cross-session emotion recognition models. To simultaneously address these challenges, we propose a unified pre-training framework based on multi-scale masked autoencoders (MSMAE), which utilizes large-scale unlabeled EEG signals from multiple subjects and sessions to extract noise-robust, subject-invariant, and temporal-invariant features. We subsequently fine-tune the obtained generalized features with only a small amount of labeled data from a specific subject for personalization and enable cross-session emotion recognition. Our framework emphasizes: 1) multi-scale representation to capture diverse aspects of EEG signals, obtaining comprehensive information; 2) an improved masking mechanism for robust channel-level representation learning, addressing missing channel issues while preserving inter-channel relationships; and 3) invariance learning for regional correlations in spatial-level representation, minimizing inter-subject and inter-session variances. Under these elaborate designs, the proposed MSMAE exhibits a remarkable ability to decode emotional states from a different session of EEG data during the testing phase. Extensive experiments conducted on the two publicly available datasets, i.e., SEED and SEED-IV, demonstrate that the proposed MSMAE consistently achieves stable results and outperforms competitive baseline methods in cross-session emotion recognition.","1558-0210","","10.1109/TNSRE.2024.3389037","National Natural Science Foundation of China(grant numbers:62201402); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2023A1515011978,2020A1515111154); Projects for International Scientific and Technological Cooperation of Guangdong Province(grant numbers:2023A0505050144); Department of Education of Guangdong Province(grant numbers:2021KTSCX136); Hong Kong and Macau Joint Research and Development Fund of Wuyi University(grant numbers:2021WGALH19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500357","EEG-based emotion recognition;self-supervised learning;cross-session;transformer","Electroencephalography;Emotion recognition;Feature extraction;Brain modeling;Task analysis;Data models;Data mining","Humans;Emotions;Electroencephalography;Brain-Computer Interfaces;Algorithms;Female;Male;Machine Learning;Artifacts;Adult;Neural Networks, Computer","4","","43","CCBY","15 Apr 2024","","","IEEE","IEEE Journals"
"Fuzzy logic based emotion classification","J. W. Matiko; S. P. Beeby; J. Tudor","Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Electronics and Computer Science, University of Southampton, Southampton, United Kingdom","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","14 Jul 2014","2014","","","4389","4393","Emotions affect many aspects of our daily lives including decision making, reasoning and physical wellbeing. Researchers have therefore addressed the detection of emotion from individuals' heart rate, skin conductance, pupil dilation, tone of voice, facial expression and electroencephalogram (EEG). This paper presents an algorithm for classifying positive and negative emotions from EEG. Unlike other algorithms that extract fuzzy rules from the data, the fuzzy rules used in this paper are obtained from emotion classification research reported in the literature and the classification output indicates both the type of emotion and its strength. The results show that the algorithm is more than 90 times faster than the widely used LIBSVM and the obtained average accuracy of 63.52 % is higher than previously reported using the same EEG dataset. This makes this algorithm attractive for real time emotion classification. In addition, the paper introduces a new oscillation feature computed from local minima and local maxima of the signal.","2379-190X","978-1-4799-2893-4","10.1109/ICASSP.2014.6854431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6854431","Emotions;Fuzzy Logic;Classification","Classification algorithms;Electroencephalography;Fuzzy logic;Support vector machines;Oscillators;Pragmatics;Accuracy","","24","","34","IEEE","14 Jul 2014","","","IEEE","IEEE Conferences"
"Research on Positive Emotion Recognition Based on EEG Signals","X. Chen; W. Liu","Faculty of Data Science, City University of Macau, Macau, China; Faculty of Data Science, City University of Macau, Macau, China","2023 5th International Conference on Communications, Information System and Computer Engineering (CISCE)","12 Jun 2023","2023","","","70","79","Human logical decision-making, perception, learning, and many functions, which influence people's decision-making on things and perception of external things, are all significantly influenced by emotions. An essential component of emotional computing is emotional cognition. By examining the psychological and psychological traits of the client, it evaluates the psychological condition of the service object. Currently, most of the research on emotions based on electroencephalogram (EEG) signals focuses on classifying positive, neutral, and negative emotions, or studying negative emotions, with less attention paid to specifically identifying positive emotions. This experiment proposed an experimental design that utilized virtual reality technology as an inducing method to stimulate positive emotions, and identified and evaluated the emotions of happiness, desire, and healing. This experiment collected datasets on three positive emotions of happiness, desire, and healing, and collected the Positive Affect and Negative Affect Scale (PANAS) and self-assessment (SAM) form of the participants. Divided the experiment into two groups, one was the experimental group wearing VR, and the other was the experimental group not wearing VR. The immersion feeling scale was used to study the effect of VR on stimulating emotions. Through the design of emotion induction experiments, the EEG signals of experiencing happiness, desire, and healing were collected. The EEG signals were input into a CNN network for feature extraction, both in the form of images and time series. The Resnet18 network was used for image-based emotion recognition with an accuracy of 93%. The time-series data was processed using an LSTM network for emotion recognition with an accuracy of 94.9%.","2833-2423","979-8-3503-2679-6","10.1109/CISCE58541.2023.10142342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10142342","EEG;CNN;emotion recognition;feature extraction;LSTM","Emotion recognition;Decision making;Time series analysis;Psychology;Virtual reality;Feature extraction;Electroencephalography","","1","","35","IEEE","12 Jun 2023","","","IEEE","IEEE Conferences"
"Emotional stress recognition system using EEG and psychophysiological signals","K. P. Vyshali Rao; H. K. Ashwini; S. Akshatha","Department of Information Science and Engineering, MVJ College of Engineering, Bangalore, India; UVCE, Bangalore, India; Department of Computer Science and Engineering, MVJ College of Engineering, Bangalore, India","2021 International Conference on Advancements in Electrical, Electronics, Communication, Computing and Automation (ICAECA)","18 Jan 2022","2021","","","1","6","There are situations where Stress can affect a person on a larger scale. Early detection and analysis of such factors that lead to stress can be very helpful in ensuring good health of a person. Hence, we propose to use EEG Signals such that given a set of brain signal Dataset (DS), each of which is associated with electrical activity. E.g., thinking., the Emotional Stress Recognition using EEG and psychophysiological signals finds the emotion that is running inside a person from the given set of brain signals (e.g., a happy or sad), and covers a user-specified set of categories (e.g., positive or negative). An approach for the general case is to obtain the best signal from the group of signals and classify them as the emotion. We also propose to implement a feature that helps you find out if the person wants to communicate but is not able to express it verbally. Motivated by this, we propose a system which can convert the signals into understandable form. This can further help in knowing what the person desires to express in form of thoughts. This can confirm that the proposed solution is efficient than the existing model.","","978-1-6654-2829-3","10.1109/ICAECA52838.2021.9675782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9675782","EEG Signals;psychophysiological signals;Emotional Stress;Genetic Algorithm;Support Vector Machine;wavelet features","Support vector machines;Emotion recognition;Automation;Biological system modeling;Machine learning;Brain modeling;Electroencephalography","","","","14","IEEE","18 Jan 2022","","","IEEE","IEEE Conferences"
"Extracting the features of emotion from EEG signals and classify using affective computing","R. Chinmayi; G. J. Nair; M. Soundarya; D. S. Poojitha; G. Venugopal; J. Vijayan","Department of Electronics and Communication, Engineering\, Amrita University, INDIA; Department of Electronics and Communication, Engineering\, Amrita University, INDIA; Department of Electronics and Communication, Engineering\, Amrita University, INDIA; Department of Electronics and Communication, Engineering\, Amrita University, INDIA; Department of Electronics and Communication, Engineering\, Amrita University, INDIA; Commercial Bank International, Dubai, UAE","2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)","22 Feb 2018","2017","","","2032","2036","The extraction of features, for the recognition of affective states through various means such as gestures of the body, facial images and electroencephalogram (EEG), is very important in affective computing. The brain-machine interface (BMI) using emotions, are used in medical robots, neuroergonomics, and auto-navigation and security systems. Emotions can be identified using analysis of scalp EEGs. The EEG data with audio-visual stimulus is collected and analyzed to extract the features of five emotions viz., happy, sad, fear, neutral and disgust. The raw EEG data is used to create the database, EEG_Amrita_emote. Features of EEG data are extracted using independent component analysis (ICA), and are classified using K Nearest Neighbor (KNN) algorithm. Cluster centroids are identified using k-Mean Clustering. The spectral energy of emotional activities in the brain is taken as one of the features. The EEG data is collected from male subjects of age group between 20 and 30. The locations of high intensity spectral energy is calculated for every emotion. The primary centroids of emotions are happy at (26.58, -99.97), neutral at (-69.18, 12.89), sad at (66.45, 29.52), fear at (74.22, -9.65) and disgust at (63.05, 38.68) respectively.","","978-1-5090-4442-9","10.1109/WiSPNET.2017.8300118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300118","EEG;affective computing;feature extraction;ICA;BMI;KNN;clustering;brain lobes;total energy;topographic plots;cortex;emotion","Electroencephalography;Feature extraction;Scalp;Limbic system;Conferences;Affective computing;Electrodes","","8","","7","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"A Multi-Label EEG Dataset for Mental Attention State Classification in Online Learning","H. Liu; Y. Zhang; G. Liu; X. Du; H. Wang; D. Zhang","School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Life Sciences and Technology, Xi’an Jiaotong University, Xi’an, China; Department of Computer Science, Aalborg University, Denmark","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Attention is a vital cognitive process in the learning and memory environment, particularly in the context of online learning. Traditional methods for classifying attention states of online learners based on behavioral signals are prone to distortion, leading to increased interest in using electroencephalography (EEG) signals for authentic and accurate assessment. However, the field of attention state classification based on EEG signals in online learning faces challenges, including the scarcity of publicly available datasets, the lack of standardized data collection paradigms, and the requirement to consider the interplay between attention and other psychological states. In light of this, we present the Multi-label EEG dataset for classifying Mental Attention states (MEMA) in online learning. We meticulously designed a reliable and standard experimental paradigm with three attention states: neutral, relaxing, and concentrating, considering human physiological and psychological characteristics. This paradigm collected EEG signals from 20 subjects, each participating in 12 trials, resulting in 1,060 minutes of data. Emotional state labels, basic personal information, and personality traits were also collected to investigate the relation-ship between attention and other psychological states. Extensive quantitative and qualitative analysis, including a multi-label correlation study, validated the quality of the EEG attention data. The MEMA dataset and analysis provide valuable insights for advancing research on attention in online learning. The dataset is publicly available at https://github.com/XJTU-EEG/MEMA.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889126","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889126","EEG dataset;Attention classification;Multi-label;Data validation","Statistical analysis;Psychology;Reliability engineering;Multitasking;Distortion;Electroencephalography;Physiology;Speech processing;Standards;Faces","","","","39","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"EEG-based emotion recognition approach for e-healthcare applications","M. Ali; A. H. Mosa; F. Al Machot; K. Kyamakya","Institute of Smart System Technologies, Alpen-Adria University, Klagenfurt, Austria; Institute of Smart System Technologies, Alpen-Adria University, Klagenfurt, Austria; Institute of Smart System Technologies, Alpen-Adria University, Klagenfurt, Austria; Institute of Smart System Technologies, Alpen-Adria University, Klagenfurt, Austria",2016 Eighth International Conference on Ubiquitous and Future Networks (ICUFN),"11 Aug 2016","2016","","","946","950","Emotions play an extremely important role in how we make a decision, planning, reasoning and other human mental states. The recognition of these emotions is becoming a vital task for e-healthcare systems. Using bio-sensors such as Electroencephalogram (EEG) to recognise the mental state of patients that could need a special care offers an important feedback for Ambient Assisted Living (AAL). This paper presents an EEG-based emotion recognition approach to detect the emotional state of patients. The proposed approach combines wavelet energy, modified energy, wavelet entropy and statistical features to classify four emotion states. Three different classifiers are used (quadratic discriminant analysis, k-nearest neighbor, and support vector machines) to recognise the emotion of patients robustly. The new approach is tested based on the EEG database “DEAP” using four electrodes. It shows high performance compared to existing algorithms. An overall classification accuracy of 83.87% is obtained.","2165-8536","978-1-4673-9991-3","10.1109/ICUFN.2016.7536936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7536936","","Electroencephalography;Discrete wavelet transforms;Feature extraction;Emotion recognition;Support vector machines;Electrodes;Entropy","","71","","37","IEEE","11 Aug 2016","","","IEEE","IEEE Conferences"
"Spatio-Temporal EEG Representation Learning on Riemannian Manifold and Euclidean Space","G. Zhang; A. Etemad","Department of Electrical and Computer Engineering, Ingenuity Labs Research Institute, Queen's University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, Ingenuity Labs Research Institute, Queen's University, Kingston, ON, Canada",IEEE Transactions on Emerging Topics in Computational Intelligence,"27 Mar 2024","2024","8","2","1469","1483","We present a novel deep neural architecture for learning electroencephalogram (EEG). To learn the spatial information, our model first obtains the Riemannian mean and distance from spatial covariance matrices (SCMs) on a Riemannian manifold. We then project the spatial information onto a Euclidean space via tangent space learning. Following, two fully connected layers are used to learn the spatial information embeddings. Moreover, our proposed method learns the temporal information via differential entropy and logarithm power spectrum density features extracted from EEG signals in a Euclidean space using a deep long short-term memory network with a soft attention mechanism. To combine the spatial and temporal information, we use an effective fusion strategy, which learns attention weights applied to embedding-specific features for decision making. We evaluate our proposed framework on four public datasets across three popular EEG-related tasks, notably emotion recognition, vigilance estimation, and motor imagery classification, containing various types of tasks such as binary classification, multi-class classification, and regression. Our proposed architecture outperforms other methods on SEED-VIG, and approaches the state-of-the-art on the other three datasets (SEED, BCI-IV 2 A, and BCI-IV 2B), showing the robustness of our framework in EEG representation learning.","2471-285X","","10.1109/TETCI.2023.3332549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10328680","EEG representations learning;emotion recognition;motor imagery classification;riemannian manifold","Electroencephalography;Feature extraction;Manifolds;Neural networks;Emotion recognition;Computer architecture;Image processing","","8","","78","IEEE","24 Nov 2023","","","IEEE","IEEE Journals"
"Bi-CapsNet: A Binary Capsule Network for EEG-Based Emotion Recognition","Y. Liu; Y. Wei; C. Li; J. Cheng; R. Song; X. Chen","Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China",IEEE Journal of Biomedical and Health Informatics,"7 Mar 2023","2023","27","3","1319","1330","In recent years, deep learning has gained widespread attention in electroencephalogram (EEG)-based emotion recognition. However, deep learning methods are usually time-consuming with a large amount of memory usage, which obstructs their practical usage on resource-constrained devices. In this paper, we propose a binary capsule network (Bi-CapsNet) for EEG emotion recognition with low computational cost and memory usage. The Bi-CapsNet binarizes 32-bit weights and activations to 1 b, and replaces floating-point operations with efficient bitwise operations. To address the issue of function discontinuity in backward propagation, we use a continuous function to approximate the binarization process. Two popular EEG emotion databases, namely, DEAP and DREAMER, are used for performance evaluation. In comparison to its full-precision counterpart, the Bi-CapsNet achieves a $>\!25\times$reduction on the computational cost and a $>\!5\times$ reduction on the memory usage, while with only a $< $1% drop on the recognition accuracy. Compared to some state-of-the-art EEG emotion recognition methods, the proposed method obtains more competitive performance. In addition, the Bi-CapsNet is implemented on a mobile phone via an open-source binary inference framework named Bolt, and it achieves an $\sim\! 5\times$ inference acceleration in comparison to its full-precision counterpart.","2168-2208","","10.1109/JBHI.2022.3232514","National Key Research and Development Program of China(grant numbers:2019YFA0706203); National Natural Science Foundation of China(grant numbers:62176081,41901350,32271431,61922075,62171176,62271186); National Defense Basic Scientific Research Program of China(grant numbers:JCKY2019548B001); Fundamental Research Funds for the Central Universities(grant numbers:JZ2020HGPA0111,JZ2021HGTB0078,JZ2021HGPA0061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999486","Electroencephalogram (EEG);emotion recognition;binary capsule network;inference acceleration;memory usage","Electroencephalography;Emotion recognition;Brain modeling;Feature extraction;Deep learning;Computational modeling;Computational efficiency","","5","","64","IEEE","27 Dec 2022","","","IEEE","IEEE Journals"
"A Machine Learning Model for Recognition and Classification of Emotion Using Facial Expressions","R. Kamalraj; S. K. Jain; T. Aggarwal; K. K. Brar; M. Al-Farouni; A. Gehlot","Department of CSE and IT, Jain Deemed to be University, Bangalore, India; Department of Electrical Engineering, Vivekananda Global University, Jaipur, India; MSET, Maharishi University of IT, Uttar Pradesh, India; Department of Electronics and Communication Engineering, Chandigarh University, Mohali, Punjab, India; The Islamic University, Najaf, Iraq; Department of ECE, Uttaranchal Institute of Technology, Dehradun, India",2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON),"11 Feb 2025","2024","","","1","5","Emotion recognition has become a major area of multidisciplinary research in recent years and has many potential applications in the development of artificial intelligence and human-computer interface related design of human health detection. Expansion experiments are carried out by researchers using various stimuli, attributes, and categorization methods. This paper makes use of hybrid feature optimization algorithms i. e, IWSS Embedded and PSO for recognizing emotions using facial expressions. A pretrained model ResNet50V2 is used as a feature extractor and an image filter called Discretization is applied in order to improve the accuracy of the proposed approach. The results revealed that the methodology attained an overall accuracy of 75%.","","979-8-3315-1859-2","10.1109/DELCON64804.2024.10866484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10866484","FER 2013;Emotions;Face emotion Recognition;Machine Learning and CNN","Emotion recognition;Accuracy;Face recognition;Filtering algorithms;Multilayer perceptrons;Feature extraction;Brain modeling;Electroencephalography;Optimization;Load modeling","","","","44","IEEE","11 Feb 2025","","","IEEE","IEEE Conferences"
"EEG Data Augmentation for Emotion Recognition Using Diffusion Model","Y. -D. Zhao; Y. -K. Liu; W. -L. Zheng; B. -L. Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","4","Electroencephalogram (EEG) signals are playing an increasingly important role in affective computing, especially in emotion recognition. However, the process of collecting EEG signals is very complex, requiring subjects to conduct long-term experiments under ideal conditions, which has high requirements for both the subjects and the experimental environment. Therefore, how to quickly obtain a large number of high-quality EEG signals has become an issue. In recent years, the success of diffusion models in the field of image generation has attracted a large number of researchers’ interest. Compared to traditional generative models, diffusion models have excellent properties such as better generation performance and a more stable training process. In this paper, we apply diffusion models to emotion recognition tasks for the first time. We optimize the sampling stage of the diffusion model to make it more suitable for generating high-quality EEG signals, and then enhance the original EEG signals and apply them to emotion recognition tasks. Additionally, we explore the impact of the quantity of generated data on task performance. We use three datasets in the experiment, SEED, SEED-IV, and DEAP datasets. The experimental results indicate that our data augmentation method can significantly improve emotion recognition.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782956","National Natural Science Foundation of China; Shanghai Jiao Tong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782956","EEG;emotion recognition;data augmentation;diffusion model","Training;Emotion recognition;Image synthesis;Biological system modeling;Diffusion models;Brain modeling;Data augmentation;Electroencephalography;Data models;Software development management","Electroencephalography;Humans;Emotions;Algorithms;Signal Processing, Computer-Assisted","","","13","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"KGViT: Knowledge Graph Integrated Vision Transformer for EEG-Based Emotion Recognition","Q. Chen; W. Weng; J. Shen; Y. Chen; Y. Gu","Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China",2024 IEEE Smart World Congress (SWC),"24 Mar 2025","2024","","","535","542","Emotion recognition using electroencephalography (EEG) has attracted considerable attention and achieved significant advancements in the brain-computer interface field. Although many deep learning methods have shown superior performance over traditional approaches in emotion recognition tasks, existing studies neglect auxiliary information, such as participants’ details and descriptions of stimuli, focusing solely on EEG data. This oversight may lead to poor generalization across different participants. To address this issue, we propose a knowledge graph integrated with a vision transformer (KGViT). The KGViT model utilizes the vision transformer to capture effective features from the 3D temporal-spectral-spatial representations of EEG data. Innovatively, we design a knowledge graph feature extraction module using a graph convolutional network. This module processes node and relation matrices constructed from the EEG dataset, enabling the model to leverage prior knowledge for improved performance. The hybrid feature fusion module is introduced to calculate dynamic weights for knowledge graph features based on EEG features and integrate them into a unified representation. Extensive experiments were conducted on the DEAP and DREAMER datasets to assess the effectiveness of KGViT, with outcomes surpassing the state-of-the-art baseline. Accuracy rates of $\mathbf{9 7. 7 8 \%}, \mathbf{9 8. 0 6 \%}, \mathbf{9 8. 8 9 \%}$, and $\mathbf{9 7. 4 7 \%}$ were achieved on DEAP-Valence, DEAP-Arousal, DREAMER-Valence, and DREAMER-Arousal, respectively.","2993-396X","979-8-3315-2086-1","10.1109/SWC62898.2024.00104","Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10924891","emotion recognition;vision transformer;knowledge graph;feature fusion","Emotion recognition;Computer vision;Accuracy;Computational modeling;Knowledge graphs;Brain modeling;Transformers;Feature extraction;Electroencephalography;Data models","","","","32","IEEE","24 Mar 2025","","","IEEE","IEEE Conferences"
"Introducing Attention Mechanism for EEG Signals: Emotion Recognition with Vision Transformers","A. Arjun; A. S. Rajpoot; M. Raveendranatha Panicker","Electrical Engineering, IIT Palakkad, India; Computer Science and Engineering, IIT Palakkad, India; Electrical Engineering, IIT Palakkad, India",2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"9 Dec 2021","2021","","","5723","5726","The accurate emotional assessment of humans can prove beneficial in health care, security investigations and human interaction. In contrast to emotion recognition from facial expressions which can prove to be inaccurate, analysis of electroencephalogram (EEG) activity is a more accurate representation of one’s state of mind. With advancements in deep learning, various methods are being employed for this task. In this research, importance of attention mechanism in EEG signals is introduced through two vision transformer based methods for the classification of EEG signals on the basis of emotions. The first method utilizes 2-D images generated through continuous wavelet transform (CWT) of the raw EEG signals and the second method directly operates on the raw signal. The publicly available and widely accepted DEAP dataset has been utilized in this research for validating the proposed approaches. The proposed approaches report very high accuracies of 97% and 95.75% using CWT and 99.4% and 99.1% using raw signal for valence and arousal classifications respectively, which clearly highlights the significance of attention mechanism for EEG signals. The proposed methodology also ensures faster training and testing time which suits the clinical purposes.Clinical Relevance— This work establishes a highly accurate algorithm for emotion recognition using EEG signals which has potential applications in music-based therapy.","2694-0604","978-1-7281-1179-7","10.1109/EMBC46164.2021.9629837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629837","","Training;Emotion recognition;Continuous wavelet transforms;Neural networks;Transformers;Electroencephalography;Real-time systems","Arousal;Electroencephalography;Emotions;Facial Expression;Humans;Wavelet Analysis","28","","18","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"MaDeNet: Disentangling Individuality of EEG Signals through Feature Space Mapping and Detachment","S. -E. Moon; J. -S. Lee","Department of Integrated Technology, Yonsei University, Seoul, Republic of Korea; Department of Integrated Technology, Yonsei University, Seoul, Republic of Korea",2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),"27 Aug 2020","2020","","","260","263","The cross-subject variability, or individuality, of electroencephalography (EEG) signals often has been an obstacle to extracting target-related information from EEG signals for classification of subjects' perceptual states. In this paper, we propose a deep learning-based EEG classification approach, which learns feature space mapping and performs individuality detachment to reduce subject-related information from EEG signals and maximize classification performance. Our experiment on EEG-based video classification shows that our method significantly improves the classification accuracy.","2694-0604","978-1-7281-1990-8","10.1109/EMBC44109.2020.9176301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9176301","","Electroencephalography;Feature extraction;Entropy;Training;Brain modeling;Electrodes;Data mining","Electroencephalography;Individuality","2","","16","IEEE","27 Aug 2020","","","IEEE","IEEE Conferences"
"JAN-Enhanced Capsule Networks with Pre-trained Features for Cross-Subject Emotion Recognition","L. Shi; X. Ma; S. Hu","School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China; School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China; School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China",2024 IEEE 17th International Conference on Signal Processing (ICSP),"23 Jan 2025","2024","","","116","119","In EEG-based emotion recognition, traditional shallow convolutional methods often struggle with effective feature capture, particularly in cross-subject scenarios where domain alignment is crucial. This study proposes a novel approach: Utilizing a pre-trained model for rapid feature extraction, followed by the integration of Joint Maximum Mean Discrepancy(JMMD) loss within the Joint Adaptation Network(JAN) framework and dynamic routing in Capsule Networks, forming the dynamic domain adaptation model DA-PCN. The pre-trained VGG16 enhances the capture of complex emotional patterns and improves generalization across subjects. These features are further refined through a dual-layer Capsule Network, which aligns them across different domains at each layer. Assessments across various subjects and multiple videos from the same subjects using the DEAP dataset support the effectiveness of this method.","2164-5221","979-8-3503-8738-4","10.1109/ICSP62129.2024.10846608","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10846608","EEG emotion recognition;pre-trained models;capsule network;dynamic domain adaption","Knowledge engineering;Adaptation models;Emotion recognition;Convolution;Network architecture;Brain modeling;Feature extraction;Routing;Electroencephalography;Videos","","","","23","IEEE","23 Jan 2025","","","IEEE","IEEE Conferences"
"Emotion Recognition Based on Fusion of Local Cortical Activations and Dynamic Functional Networks Connectivity: An EEG Study","F. Al-Shargie; U. Tariq; M. Alex; H. Mir; H. Al-Nashash","Department of Electrical Engineering, American University of Sharjah, Sharjah, UAE; Department of Electrical Engineering, American University of Sharjah, Sharjah, UAE; Department of Electrical Engineering, American University of Sharjah, Sharjah, UAE; Department of Electrical Engineering, American University of Sharjah, Sharjah, UAE; Department of Electrical Engineering, American University of Sharjah, Sharjah, UAE",IEEE Access,"14 Oct 2019","2019","7","","143550","143562","In this paper, we present a method to improve emotion recognition based on the fusion of local cortical activations and dynamic functional network patterns. We estimate the cortical activations using power spectral density (PSD) with the Burg autoregressive model. On the other hand, we estimate the functional connectivity networks by utilizing the phase locking value (PLV). The results of cortical activations and connectivity networks show different patterns across three emotions at all frequency bands. Similarly, the results of fusion significantly improve the classification rate in terms of accuracy, sensitivity, specificity and the area under the receiver operator characteristics curve (AROC), p <; 0.05. The average improvement with fusion in all evaluation metrics are 6.84% and 4.1% when compared to PSD and PLV alone, respectively. The results clearly demonstrate the advantage of fusion of cortical activations with dynamic functional networks for developing human-computer interaction system in real-world applications.","2169-3536","","10.1109/ACCESS.2019.2944008","American University of Sharjah(grant numbers:BBRI-FRG18,EFRG-EN0244); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8849985","Emotion;electroencephalogram (EEG);cortical activation;functional connectivity network patterns;fusion;classification","Electroencephalography;Brain modeling;Emotion recognition;Databases;Band-pass filters;Physiology;Signal resolution","","40","","85","CCBY","26 Sep 2019","","","IEEE","IEEE Journals"
"Permuted layer-based CNN for Emotion Detection with Multi-Modality Physiological Signals","A. Tripathi; T. Choudhury","School of Computer Science, University of Petroleum and Energy Studies, Dehradun, Uttarakhand, India; CSE Dept, Symbiosis Institute of technology, Symbiosis International University, Lavale Campus, Pune, Maharashtra, India",2023 IEEE International Conference on Contemporary Computing and Communications (InC4),"29 Sep 2023","2023","1","","1","5","Human emotion detection is a challenging task. Emotions are expressed via facial expressions, body/face temperature, EEG recordings, voice recordings. In this paper, a novel CNN based model is reported to detect human emotion using multimodal human emotion representation data. The model is based on the concepts of one-dimensional convolution on the permuted-ensembled feature space representing EEG signals. The model addresses the issues with localized one-dimensional convolution over high-dimensional feature spaces and attempts to improve the classification performance. Results indicate improved detection accuracy over different emotions as compared to other popular machine learning approaches.","","979-8-3503-3577-4","10.1109/InC457730.2023.10263176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10263176","Emotion detection;Ensemble learning;permuted CNNs","Emotion recognition;Convolution;Machine learning;Brain modeling;Feature extraction;Electroencephalography;Physiology","","4","","22","IEEE","29 Sep 2023","","","IEEE","IEEE Conferences"
"Multisource Transfer Learning for Cross-Subject EEG Emotion Recognition","J. Li; S. Qiu; Y. -Y. Shen; C. -L. Liu; H. He","Research Center for Brain-Inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-Inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Research Center for Brain-Inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Cybernetics,"17 Jun 2020","2020","50","7","3281","3293","Electroencephalogram (EEG) has been widely used in emotion recognition due to its high temporal resolution and reliability. Since the individual differences of EEG are large, the emotion recognition models could not be shared across persons, and we need to collect new labeled data to train personal models for new users. In some applications, we hope to acquire models for new persons as fast as possible, and reduce the demand for the labeled data amount. To achieve this goal, we propose a multisource transfer learning method, where existing persons are sources, and the new person is the target. The target data are divided into calibration sessions for training and subsequent sessions for test. The first stage of the method is source selection aimed at locating appropriate sources. The second is style transfer mapping, which reduces the EEG differences between the target and each source. We use few labeled data in the calibration sessions to conduct source selection and style transfer. Finally, we integrate the source models to recognize emotions in the subsequent sessions. The experimental results show that the three-category classification accuracy on benchmark SEED improves by 12.72% comparing with the nontransfer method. Our method facilitates the fast deployment of emotion recognition models by reducing the reliance on the labeled data amount, which has practical significance especially in fast-deployment scenarios.","2168-2275","","10.1109/TCYB.2019.2904052","National Natural Science Foundation of China(grant numbers:91520202,81701785); Chinese Academy of Sciences (CAS) Scientific Equipment Development Project(grant numbers:YJKYYQ20170050); Beijing Municipal Science and Technology Commission(grant numbers:Z181100008918010); Youth Innovation Promotion Association CAS; Strategic Priority Research Program of CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675478","Brain–computer interface;emotion recognition;transfer learning (TL)","Brain modeling;Electroencephalography;Emotion recognition;Data models;Training;Calibration;Training data","Adult;Brain;Brain-Computer Interfaces;Electroencephalography;Emotions;Humans;Machine Learning;Pattern Recognition, Automated;Young Adult","194","","48","IEEE","27 Mar 2019","","","IEEE","IEEE Journals"
"A Comparative Study on Prominent Connectivity Features for Emotion Recognition From EEG","M. A. Maria; M. A. H. Akhand; A. B. M. A. Hossain; M. A. S. Kamal; K. Yamada","Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Department of Electronics and Communication Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Department of Electronics and Communication Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Graduate School of Science and Technology, Gunma University, Kiryu, Japan; Graduate School of Science and Technology, Gunma University, Kiryu, Japan",IEEE Access,"21 Apr 2023","2023","11","","37809","37831","Classifying distinct human emotions, the fundamental purpose of brain-computer interface research, is essential for providing instant personalized services and assistance to individuals. With such emerging applications for individuals, several techniques have been proposed recently to explore interactions between brain regions, such as correlation, synchronization, and dependence. Notably, functional and effective connectivity methods are applied to assess the relationships between different brain areas. The primary objective of this study is to compare the frequently used functional and effective connectivity methods to recognize emotion using Electroencephalogram (EEG) signals. This paper uses a benchmark emotional EEG dataset consisting of 32 channels of EEG signals collected from 32 subjects while they were watching 40 emotional music videos. Specifically, correlation, phase synchronization, and mutual information are used to measure functional brain connectivity, and transfer entropy is used to acquire effective brain connectivity. After extracting the features, they are represented in a two-dimensional connectivity feature map (CFM). The CFMs are then used to classify emotions by a convolutional neural network model. The results of classified emotions are analyzed regarding compatible EEG bands, accuracy, and time. Notably, the Gamma band is found as the most compatible band. The comparative study has demonstrated that though the connectivity method named Pearson correlation coefficient requires less time, the normalized mutual information is the most accurate method with advantageous detecting capability of nonlinear dependencies.","2169-3536","","10.1109/ACCESS.2023.3264845","Japan Society for the Promotion of Science (JSPS) Grant-in-Aids for Scientific Research (C)(grant numbers:21K03930); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10092746","Connectivity feature;convolutional neural network;electroencephalography (EEG);emotion recognition;feature extraction","Feature extraction;Electroencephalography;Emotion recognition;Entropy;Time-frequency analysis;Brain modeling;Correlation","","10","","82","CCBYNCND","5 Apr 2023","","","IEEE","IEEE Journals"
"Multi-Domain Feature Fusion for Emotion Classification Using DEAP Dataset","M. Khateeb; S. M. Anwar; M. Alnowami","Department of Software Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Software Engineering, University of Engineering and Technology, Taxila, Pakistan; Department of Nuclear Engineering, King Abdulaziz University, Jeddah, Saudi Arabia",IEEE Access,"22 Jan 2021","2021","9","","12134","12142","Emotion recognition in real-time using electroencephalography (EEG) signals play a key role in human-computer interaction and affective computing. The existing emotion recognition models, that use stimuli such as music and pictures in controlled lab settings and limited number of emotion classes, have low ecological validity. Moreover, for effective emotion recognition identifying significant EEG features and electrodes is important. In our proposed model, we use the DEAP dataset consisting of physiological signals collected from 32 participants as they watched 40 movie (each of 60 seconds) clips. The main objective of this study is to explore multi-domain (time, wavelet, and frequency) features and hence, identify the set of stable features which contribute towards emotion classification catering to a larger number of emotion classes. Our proposed model is able to identify nine classes of emotions including happy, pleased, relaxed, excited, neutral, calm, distressed, miserable, and depressed with an average accuracy of 65.92%. Towards this end, we use support vector machine as a classifier along with 10-fold and leave-one-out cross-validation techniques. We achieve a significant emotion classification accuracy which could be vital towards developing solutions for affective computing and deal with a larger number of emotional states.","2169-3536","","10.1109/ACCESS.2021.3051281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321314","Affective computing;electroencephalography;emotions classifications;features extraction;machine learning","Electroencephalography;Brain modeling;Electrodes;Emotion recognition;Feature extraction;Hidden Markov models;Physiology","","88","","47","CCBY","13 Jan 2021","","","IEEE","IEEE Journals"
"Phase-Amplitude Coupling of EEG Applied in Music-Induced Emotional Recognition Tasks","M. Zhao; G. Fortunelli; Z. He; X. Gong; A. Cohn","Tongji University, Shanghai, China; Tongji University, Politecnico di Torino, Turin, Italy; Tongji University, Shanghai, China; Tongji University, Shanghai, China; University of Leeds, Leeds, United Kingdom","2024 20th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","8 Oct 2024","2024","","","1","6","Phase-Amplitude Coupling (PAC), as an important electroencephalogram (EEG) feature, which contains information about the brain's telematic mechanisms and cross-frequency relations, has gained more attention recently compared to conventional spectral features. Music has been recognized as a powerful way to induce emotions, and among the various emotional responses to music, musical chills are a unique and notable phenomenon, researching which can improve our knowledge of both musicology and cognitive neuroscience. Our experiment induced emotions, quantified into levels of arousal, valence, and preference, in 19 subjects using specific music extracts. During the experiment, the subjects experienced over 50 instances of musical chills with their EEG being recorded. This work extracts PAC, 15 conventional spectral statistical features, and 7 brain network features, and utilises a support vector machine (SVM) for emotion classification and recognition of musical chills. The results of emotion classification show that PAC performs the best in all emotional levels - arousal with an accuracy of 87%, valence 88% and preference 86%. In the musical chills recognition task, even though the result of PAC is lower than in the emotion classification task, it is the highest compared with the other two features. Furthermore, PAC with gamma band always shows the best results. These results suggest that PAC can be an essential factor for emotional recognition tasks, thus giving more insights into the field of affective computing.","","979-8-3503-5632-8","10.1109/ICNC-FSKD64080.2024.10702312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10702312","","Support vector machines;Couplings;Emotion recognition;Affective computing;Music;Feature extraction;Telematics;Knowledge discovery;Picture archiving and communication systems;Electroencephalography","","","","35","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"EEG-Based Emotion Recognition Using Regularized Graph Neural Networks","P. Zhong; D. Wang; C. Miao","Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Nanyang Technological University, Singapore; Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Nanyang Technological University, Singapore; Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Nanyang Technological University, Singapore",IEEE Transactions on Affective Computing,"2 Sep 2022","2022","13","3","1290","1301","Electroencephalography (EEG) measures the neuronal activities in different brain regions via electrodes. Many existing studies on EEG-based emotion recognition do not fully exploit the topology of EEG channels. In this article, we propose a regularized graph neural network (RGNN) for EEG-based emotion recognition. RGNN considers the biological topology among different brain regions to capture both local and global relations among different EEG channels. Specifically, we model the inter-channel relations in EEG signals via an adjacency matrix in a graph neural network where the connection and sparseness of the adjacency matrix are inspired by neuroscience theories of human brain organization. In addition, we propose two regularizers, namely node-wise domain adversarial training (NodeDAT) and emotion-aware distribution learning (EmotionDL), to better handle cross-subject EEG variations and noisy labels, respectively. Extensive experiments on two public datasets, SEED, and SEED-IV, demonstrate the superior performance of our model than state-of-the-art models in most experimental settings. Moreover, ablation studies show that the proposed adjacency matrix and two regularizers contribute consistent and significant gain to the performance of our RGNN model. Finally, investigations on the neuronal activities reveal important brain regions and inter-channel relations for EEG-based emotion recognition.","1949-3045","","10.1109/TAFFC.2020.2994159","Alibaba Innovative Research Program; Alibaba-NTU Singapore Joint Research Institute(grant numbers:Alibaba-NTU-AIR2019B1); National Innovation Challenge on Active and Confident Ageing(grant numbers:MOH/NIC/COG04/2017,MOH/NIC/HAIG03/2017); National Research Foundation Singapore; NRF Investigatorship Programme(grant numbers:NRF-NRFI05-2019-0002); AI Singapore Programme(grant numbers:AISG-GC-2019-003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091308","Affective computing;EEG;graph neural network;SEED","Electroencephalography;Brain modeling;Emotion recognition;Noise measurement;Convolution;Biological neural networks;Feature extraction","","408","","67","IEEE","11 May 2020","","","IEEE","IEEE Journals"
"A Novel Semi-Supervised Deep Learning Framework for Affective State Recognition on EEG Signals","X. Jia; K. Li; X. Li; A. Zhang","School of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, USA; School of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, USA; School of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, USA; School of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, USA",2014 IEEE International Conference on Bioinformatics and Bioengineering,"9 Feb 2015","2014","","","30","37","Nowadays the rapid development in the area of human-computer interaction has given birth to a growing interest on detecting different affective states through smart devices. By using the modern sensor equipment, we can easily collect electroencephalogram (EEG) signals, which capture the information from central nervous system and are closely related with our brain activities. Through the training on EEG signals, we can make reasonable analysis on people's affection, which is very promising in various areas. Unfortunately, the special properties of EEG dataset have brought difficulties for conventional machine learning methods. The main reasons lie in two aspects: the small set of labeled samples and the noisy channel problem. To overcome these difficulties and successfully identify the affective states, we come up with a novel semi-supervised deep structured framework. Compared with previous deep learning models, our method is more adapted to the EEG classification problem. We first adopt a two-level procedure, which involves both supervised label information and unsupervised structure information to jointly make decision on channel selection. And then, we add a generative Restricted Boltzmann Machine (RBM) model for the classification task, and use the training objectives of generative learning and unsupervised learning to jointly regularize the discriminative training. Finally, we extend it to the active learning scenario, which solves the costly labeling problem. The experiments conducted on real EEG dataset have shown both the convincing result on critical channel selection and the superiority of our method over multiple baselines for the affective state recognition.","","978-1-4799-7502-0","10.1109/BIBE.2014.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033556","EEG;Deep Belief Network;Channel Selection","Feature extraction;Training;Brain models;Mathematical model;Electroencephalography;Equations","","54","9","29","IEEE","9 Feb 2015","","","IEEE","IEEE Conferences"
